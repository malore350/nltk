{"category": "non-abstract", "text": "/\u000f Flexible R epr esentation/. A theory/-guided system should utilize the kno wledge con/-tained in the initial domain theory without ha ving to adhere closely to the initialtheory/'s represen tation language/./\u000f Flexible Structur e/. A theory/-guided system should not b e unnecessarily restricted b ythe structure of the initial domain theory /.c/\r /1/9/9/5 AI Access F oundation and Morgan Kaufmann Publishers/. All righ ts reserv ed/.Donoho /& RendellBefore giving more precise de/\fnitions of our terms/, w e motiv ate our w ork in tuitiv ely /./1/./1 In tuitiv e Motiv ationThe /\frst desirable qualit y /, /\rexibilit y of represen tation/, arises b ecause the theory represen/-tation most appropriate for describing the coarse/, initial domain theory ma y b e inadequatefor the /\fnal/, revised theory /. While the initial domain theory ma y b e compact and concise inone represen tation/, an accurate theory ma y b e quite bulky and cum b ersome in that repre/-sen tation/. F urthermore/, the represen tation that is b est for expressing the initial theory ma ynot b e the b est for c arrying out re/\fnemen ts/. A helpful re/\fnemen t step ma y b e clumsy tomak e in the initial represen tation y et b e carried out quite simply in another represen tation/.As a simple example/, a coarse domain theory ma y b e expressed as the logical conjunctionof N conditions that should b e met/. The most accurate theory /, though/, is one in whic h an y Mof these N conditions holds/. Expressing this more accurate theory in the DNF represen tationused to describ e the initial theory w ould b e cum b ersome and un wieldy /(Murph y /& P azzani/,/1/9/9/1/)/. F urthermore/, arriving at the /\fnal theory using the re/\fnemen t op erators most suitablefor DNF /(drop/-condition/, add/-condition/, mo dify/-condition/) w ould b e a cum b ersome task/.But when an M/-of/-N represen tation is adopted/, the re/\fnemen t simply in v olv es empirically/\fnding the appropriate M/, and the /\fnal theory can b e expressed concisely /(Ba/\u000bes /& Mo oney /,/1/9/9/3/)/.Similarly /, the second desirable qualit y /, /\rexibilit y of structure/, arises b ecause the theorystructure that w as suitable for a coarse domain theory ma y b e insu/\u000ecien t for a /\fne/-tunedtheory /. In order to ac hiev e the desired accuracy /, a restructuring of the initial theory ma y b enecessary /. Man y theory revision systems act b y making a series of lo cal c hanges/, but thiscan lead to b eha vior at t w o extremes/. The /\frst extreme is to rigidly retain the bac kb onestructure of the initial domain theory /, only allo wing small/, lo cal c hanges/. Figure /1 illustratesthis situation/. Minor revisions ha v e b een made /{ conditions ha v e b een added/, dropp ed/,and mo di/\fed /{ but the re/\fned theory is trapp ed b y the bac kb one structure of the initialtheory /. When only lo cal c hanges are needed/, these tec hniques ha v e pro v en useful /(Ourston/& Mo oney /, /1/9/9/0/)/, but often more is required/. When more is required/, these systems oftenmo v e to the other extreme/; they drop en tire rules and groups of rules and then build en tirenew rules and groups of rules from scratc h to replace them/. Th us they restructure/, butthey forfeit v aluable kno wledge in the pro cess/. An ideal theory revision system w ould gleankno wledge from theory substructures that cannot b e /\fxed with small/, lo cal c hanges anduse this in a restructured theory /.As an in tuitiv e illustration/, consider a piece of soft w are that /\\almost w orks/./\" Sometimesit can b e made useful through only a few lo cal op erations/: /\fxing a couple of bugs/, addinga needed subroutine/, and so on/. In other cases/, though/, a piece of soft w are that /\\almostw orks/\" is in fact far from ful l w orking order/. It ma y need to b e redesigned and restructured/.A mistak e at one extreme is to try to /\fx a program lik e this b y making a series of patc hes inthe original co de/. A mistak e at the other extreme is to discard the original program withoutlearning an ything from it and start from scratc h/. The b est approac h w ould b e to examinethe original program to see what can b e learned from its design and to use this kno wledgein the redesign/. Lik ewise/, attempting to impro v e a coarse domain theory through a seriesof lo cal c hanges ma y yield little impro v emen t b ecause the theory is trapp ed b y its initial/4/1/2Rerepresenting and Restr ucturing Domain Theories\nd e h i k l m o p q rRefined Theory\na b c\nf gj n a b c\ne\nf gij\nk ln\np rs\ntu v\nw oInitial TheoryFigure /1/: T ypical theory revision allo ws only limited structural /\rexibilit y /. Although con/-ditions ha v e b een added/, dropp ed/, and mo di/\fed/, the revised theory is m uc hconstrained b y the structure of the initial theory /.structure/. This do es not render the original domain theory useless/; careful analysis of theinitial domain theory can giv e v aluable guidance for the design of the b est /\fnal theory /.This is illustrated in Figure /2 where man y substructures ha v e b een tak en from the initialtheory and adapted for use in the re/\fned theory /. Information from the initial theory hasb een used/, but the structure of the revised theory is not restricted b y the structure of theinitial theory /.\na b c\nd e\nf gh ij\nk l mn\no p q rInitial Theory Refined Theory\nk l\nf g md e h i s o p\nf g ru v\nqFigure /2/: More /\rexible structural mo di/\fcation/. The revised theory has tak en man y sub/-structures from the initial theory and adapted and recom bined them for its use/,but the structure of the revised theory is not restricted b y the structure of theinitial theory /./4/1/3Donoho /& Rendell/1/./2 T erminologyIn this pap er/, all tr aining data consist of examples whic h are classi/\fed v ectors of fea/-ture//v alue pairs/. W e assume that an initial the ory is a set of conditions com bined using theop erators AND/, OR/, and NOT and indicating one or more classes/. While it is unreasonableto b eliev e that all theories will alw a ys b e of this form/, it co v ers m uc h existing theory revisionresearc h/.Our w ork is in tended as an informal exploration of /\rexible represen tation and /\rexiblestructure/. Flexible r epr esentation means allo wing the theory to b e revised using a represen/-tation language other than that of the initial theory /. An example of /\rexible represen tationis the in tro duction of a new op erator for com bining features /| an op erator not used inthe initial theory /. In Section /1/./1 the example w as giv en of in tro ducing the M/-of/-N op eratorto represen t a theory originally expressed in DNF/. Flexible structur e means not limitingrevision of the theory to a series of small/, incremen tal mo di/\fcations/. An example of thisis breaking the theory do wn in to its comp onen ts and using them as building blo c ks in theconstruction of a new theory /.Constructive induction is a pro cess whereb y the training examples are redescrib ed usinga new set of features/. These new features are com binations of the original features/. Biasor kno wledge ma y b e used in the construction of the new features/. A subtle p oin t is thatwhen w e sp eak of /\rexible represen tation/, w e are referring only to the represen tation of thedomain theory /, not the training data/. Although the phrase /\\c hange of represen tation/\" isoften applied to constructiv e induction/, this refers to a c hange of the data/. In our pap er/,the term /\rexible represen tation is reserv ed for a c hange of theory represen tation/. Th usa system can b e p erforming constructiv e induction /(c hanging the feature language of thedata/) without exhibiting /\rexible represen tation /(c hanging the represen tation of the theory/)/./1/./3 Ov erviewTheory revision and constructiv e induction em b o dy complemen tary asp ects of the mac hinelearning researc h comm unit y/'s ultimate goals/. Theory revision uses data to impro v e atheory/; constructiv e induction can use a theory to impro v e data to facilitate learning/. Inthis pap er w e presen t a theory/-guided constructiv e induction approac h whic h addresses thet w o desirable qualities discussed in Section /1/./1/. The initial theory is analyzed/, and newfeatures are constructed based on the comp onen ts of the theory /. The constructed featuresneed not b e expressed in the same represen tational language as the initial theory and canb e re/\fned to b etter matc h the training examples/. Finally /, a standard inductiv e learningalgorithm/, C/4/./5 /(Quinlan/, /1/9/9/3/)/, is applied to the redescrib ed examples/.W e b egin b y analyzing ho w landmark theory revision and learning systems ha v e exhib/-ited /\rexibilit y in handling a domain theory and what part this has pla y ed in their p erfor/-mance/. F rom this analysis/, w e extract guidelines for system design and apply them to thedesign of our o wn limited system/. In an e/\u000bort to in tegrate learning from theory and data/,w e b orro w hea vily from the theory revision/, m ultistrategy learning/, and constructiv e induc/-tion comm unities/, but our guidelines for system design fall closest to classical constructiv einduction metho ds/. The cen tral fo cus of this pap er is not the presen tation of /\\another newsystem/\" but rather a study of /\rexible represen tation and structure/, their manifestation inprevious w ork/, and their guidance for future design/./4/1/4Rerepresenting and Restr ucturing Domain TheoriesSection /2 giv es the con text of our w ork b y analyzing previous researc h and its in/\ru/-ence on our w ork/. Section /3 explores the Promoter Recognition domain and demonstratesho w related theory revision systems b eha v e in this domain/. In Section /4/, guidelines fortheory/-guided constructiv e induction are presen ted/. These guidelines are a syn thesis of thep ositiv e asp ects of related researc h/, and they address the t w o desirable qualities/, /\rexibili t yof represen tation and /\rexibilit y of structure/. Section /4 also presen ts a sp eci/\fc theory/-guidedconstructiv e induction algorithm whic h is an instan tiation of the guidelines set forth earlierin that section/. Results of exp erimen ts in three domains are giv en in Section /5 follo w edb y a discussion of the strengths of theory/-guided constructiv e induction in Section /6/. Sec/-tion /7 presen ts an exp erimen tal analysis of the limits of applicabili t y of our simple algorithmfollo w ed b y a discussion of limitations and future directions of our w ork in Section /8/./2/. Con text and Related W orkAlthough our w ork b ears some resem blance in form and ob jectiv e to man y pap ers in con/-structiv e induction /(Mic halski/, /1/9/8/3/; F u /& Buc hanan/, /1/9/8/5/; Utgo/\u000b/, /1/9/8/6/; Sc hlimmer/, /1/9/8/7/;Drastal /& Raatz/, /1/9/8/9/; Matheus /& Rendell/, /1/9/8/9/; P agallo /& Haussler/, /1/9/9/0/; Raga v an /&Rendell/, /1/9/9/3/; Hirsh /& No ordewier/, /1/9/9/4/)/, theory revision /(Ourston /& Mo oney /, /1/9/9/0/; F eld/-man/, Serge/, /& Kopp el/, /1/9/9/1/; Thompson et al/./, /1/9/9/1/; Cohen/, /1/9/9/2/; P azzani /& Kibler/, /1/9/9/2/;Ba/\u000bes /& Mo oney /, /1/9/9/3/)/, and m ultistrategy approac hes /(Flann /& Dietteric h/, /1/9/8/9/; T o w ell/,Sha vlik/, /& No ordew eir/, /1/9/9/0/; Dzerisk o /& La vrac/, /1/9/9/1/; Blo edorn/, Mic halski/, /& Wnek/, /1/9/9/3/;Clark /& Mat win/, /1/9/9/3/; T o w ell /& Sha vlik/, /1/9/9/4/)/, w e fo cus only up on a handful of these sys/-tems/, those that ha v e signi/\fcan t/, underlying similarities to our w ork/. In this section w eanalyze Mir o /, Either /, F ocl /, LabyrinthK\n/, Kbann /, Neither/-MofN /, and Grendel todiscuss their related underlying con tributions in relationship to our p ersp ectiv e/./2/./1 Mir oMir o /(Drastal /& Raatz/, /1/9/8/9/) is a seminal w ork in kno wledge/-guided constructiv e induc/-tion/. It tak es kno wledge ab out ho w lo w/-lev el features in teract and uses this kno wledge toconstruct high/-lev el features for its training examples/. A standard learning algorithm isthen run on these examples describ ed using the new features/. The domain theory is usedto shift the bias of the induction problem /(Utgo/\u000b/, /1/9/8/6/)/. Empirical results sho w ed thatdescribing the examples in these high/-lev el/, abstract terms impro v ed learning accuracy /.The Mir o approac h pro vides a means of utilizing kno wledge in a domain theory withoutb eing restricted b y the structure of that theory /. Substructures of the domain theory canb e used to construct high/-lev el features that a standard induction algorithm will arrangein to a concept/. Some constructed features will b e used as they are/, others will b e ignored/,others will b e com bined with lo w/-lev el features/, and still others ma y b e used di/\u000beren tly inm ultiple con texts/. The end result is that kno wledge from the domain theory is utilized/,but the structure of the /\fnal theory is not restricted b y the structure of the initial theory /.Mir o pro vides /\rexible structure/.Another b ene/\ft is that Mir o /-lik e tec hniques can b e applied ev en when only a partialdomain theory exists/, i/.e/./, a domain theory that only sp eci/\fes high/-lev el features but do esnot link them together or a domain theory that sp eci/\fes some high/-lev el features but notothers/. One of Mir o /'s shortcomings is that it pro vided no means of making minor c hanges/4/1/5Donoho /& Rendellin the domain theory but rather constructed the features exactly as the domain theorysp eci/\fed/. Also the represen tation of Mir o /'s constructed features w as primitiv e /| eitheran example met the conditions of a high/-lev el feature or did not/. An example of Mir o /'sb eha vior is giv en in Section /3/./2/./2/./2 Either /, F ocl /, and LabyrinthKThe Either /(Ourston /& Mo oney /, /1/9/9/0/)/, LabyrinthK\n/(Thompson et al/./, /1/9/9/1/)/, andF ocl /(P azzani /& Kibler/, /1/9/9/2/) systems represen t a broad sp ectrum of theory revisionw ork/. They mak e steps to w ard e/\u000bectiv e in tegration of bac kground kno wledge and inductiv elearning/. Although these systems ha v e man y sup er/\fcial di/\u000berences with regard to sup er/-vised//unsup ervised learning/, concept description language/, etc/./, they share the underlyingprinciple of incremen tally revising an initial domain theory through a series of lo cal c hanges/.W e will discuss Either as a represen tativ e of this class of systems/. Either /'s theoryrevision op erators include/: remo ving un w an ted conditions from a rule/, adding needed con/-ditions to a rule/, remo ving rules/, and adding totally new rules/. Either /\frst classi/\fes itstraining examples according to the curren t theory /. If an y are misclassi/\fed/, it seeks to repairthe theory b y applying a theory revision op erator that will result in the correct classi/\fcationof some previously misclassi/\fed examples without losing an y of the correct examples/. Th usa series of lo cal c hanges are made that allo w for an impro v emen t of accuracy on the trainingset without losing an y of the examples previously classi/\fed correctly /.Either /-t yp e metho ds pro vide simple y et p o w erful to ols for repairing man y imp ortan tand common faults in domain theories/, but they fail to meet the qualities of /\rexible rep/-resen tation and /\rexible structure/. Because the theory revision op erators mak e small/, lo calmo di/\fcations in the existing domain theory /, the /\fnal theory is constrained to b e similar instructure to the initial theory /. When an accurate theory is signi/\fcan tly di/\u000beren t in struc/-ture from the initial theory /, these systems are forced to one of the t w o extremes discussedin Section /1/. The /\frst extreme is to b ecome trapp ed at a lo cal maxim um similar to theinitial theory unable to reac h the global maxim um b ecause only lo cal c hanges can b e made/.The other extreme is to drop en tire rules and groups of rules and replace them with newrules built from scratc h th us forfeiting the kno wledge con tained in the domain theory /.Also/, Either carries out all theory revision steps in the represen tation of the initialtheory /. Consequen tly /, the represen tation of the /\fnal theory is the same as that of the initialtheory /. Another represen tation ma y b e more appropriate for the revised theory than theone in whic h the initial theory comes/, but facilities are not pro vided to accommo date this/.An adv anced theory revision system w ould com bine the lo cally acting strengths of Either /-t yp e systems with /\rexibilit y of structure and /\rexibilit y of represen tation/. An example ofEither /'s b eha vior is giv en in Section /3/./3/./2/./3 Kbann and Neither/-MofNThe Kbann system /(T o w ell et al/./, /1/9/9/0/; T o w ell /& Sha vlik/, /1/9/9/4/) mak es unique con tribu/-tions to theory revision w ork/. Kbann tak es an initial domain theory describ ed sym b olicallyin logic and creates a neural net w ork whose structure and initial w eigh ts enco de this theory /.Bac kpropagation /(Rumelhart/, Hin ton/, /& McClelland/, /1/9/8/6/) is then applied as a re/\fne/-men t to ol for /\fne/-tuning the net w ork w eigh ts/. Kbann has b een empirically sho wn to giv e/4/1/6Rerepresenting and Restr ucturing Domain Theoriessigni/\fcan t impro v emen t o v er man y theory revision systems for the widely/-used PromoterRecognition domain/. Although our w ork is di/\u000beren t in implemen tation from Kbann /, ourabstract ideologies are similar/.One of Kbann /'s imp ortan t con tributions is that it tak es a domain theory in one repre/-sen tation /(prop ositional logic/) and translates it in to a less restricting represen tation /(neuralnet w ork/)/. While logic is an appropriate represen tation for the initial domain theory for thepromoter problem/, the neural net w ork represen tation is more con v enien t b oth for re/\fningthis theory and for expressing the b est revised theory /. This c hange of represen tation isKbann /'s real source of p o w er/. Muc h atten tion has b een giv en to the fact that Kbann com/-bines sym b olic kno wledge with a subsym b olic learner/, but this com bination can b e view edmore generally as a means of implemen ting the imp ortan t c hange of represen tation/. It ma yb e the c hange of represen tation that giv es Kbann its p o w er/, not necessarily its sp eci/\fcsym b olic//subsym b olic implemen tation/. Th us the Kbann system em b o dies the higher/-lev elprinciple of allo wing re/\fnemen t to o ccur in an appropriate represen tation/.If an alternativ e represen tation is Kbann /'s source of p o w er/, the question m ust b e raisedas to whether the actual Kbann implemen tation is alw a ys the b est means of ac hieving thisgoal/. The neural net w ork represen tation ma y b e more expressiv e than is required/. Accord/-ingly /, bac kpropagation often has more re/\fnemen t p o w er than is needed/. Th us Kbann ma ycarry excess baggage in translating in to the neural net represen tation/, p erforming exp ensiv ebac kpropagation/, and extracting sym b olic rules from the re/\fned net w ork/. Although the fullexten t of Kbann /'s p o w er ma y b e needed for some problems/, man y imp ortan t problems ma yb e solv able b y applying Kbann /'s principles at the sym b olic lev el using less exp ensiv e to ols/.Neither/-MofN /(Ba/\u000bes /& Mo oney /, /1/9/9/3/)/, a descendan t of Either /, is a second exampleof a system that allo ws a theory to b e revised in a represen tation other than that of theinitial theory /. The domain theory input in to Neither/-MofN is expressed in prop ositionallogic as an AND//OR tree/. Neither/-MofN in terprets the theory less rigidly /| an ANDrule is true an y time an y M of its N conditions are true/. Initially M is set equal to N /(allconditions m ust b e true for the rule to b e true/)/, and one theory re/\fnemen t op erator is tolo w er M for a particular rule/. The end result is that examples that are a close enoughpartial matc h to the initial theory are accepted/. Neither/-MofN /, since it is built up onthe Either framew ork/, also includes Either /-lik e theory revision op erators/: add/-condition/,drop/-condition/, etc/.Th us Neither/-MofN allo ws revision to tak e place in a represen tation appropriatefor revision and appropriate for concisely expressing the b est re/\fned theory /. Neither/-MofN has ac hiev ed results comparable to Kbann on the Promoter Recognition domain/,whic h suggests that it is the c hange of represen tation whic h these t w o systems share thatgiv e them their p o w er rather than an y particular implemen tation/. Neither/-MofN alsodemonstrates that a small amoun t of represen tational /\rexibilit y is sometimes enough/. TheM/-of/-N represen tation it emplo ys is not as big a c hange from the original represen tationas the neural net represen tation whic h Kbann emplo ys y et it ac hiev es similar results andarriv es at them m uc h more quic kly than Kbann /(Ba/\u000bes /& Mo oney /, /1/9/9/3/)/.A shortcoming of Neither/-MofN is that since it acts b y making lo cal c hanges in aninitial theory /, it can still b ecome trapp ed b y the structure of the initial theory /. An adv ancedtheory revision system w ould incorp orate Neither/-MofN /'s and Kbann /'s /\rexibilit y of/4/1/7Donoho /& Rendellrepresen tation and allo w kno wledge/-guided theory restructuring/. Examples of Kbann /'sand Neither/-MofN /'s b eha vior are giv en in Sections /3/./4 and /3/./5/./2/./4 GrendelCohen /(/1/9/9/2/) analyzes a class of theory revision systems and dra ws some insigh tful conclu/-sions/. One is that /\\generalit y /[in theory in terpretation/] comes at the exp ense of p o w er/./\"He dra ws this principle from the fact that a system suc h as Either or F ocl treats ev erydomain theory the same and therefore m ust treat ev ery domain theory in the most gen/-eral w a y /. He argues that rather than just applying the most general re/\fnemen t strategyto ev ery problem/, a small set of re/\fnemen t strategies should b e a v ailable that are narro wenough to gain lev erage y et not so narro w that they only apply to a single problem/. Cohenpresen ts Grendel /, a to olb o x of translators eac h of whic h transforms a domain theory in toan explicit bias/. Eac h translator in terprets the domain theory in a di/\u000beren t w a y /, and themost appropriate in terpretation is applied to a giv en problem/.W e apply Cohen/'s principle to the represen tation of domain theories/. If all domaintheories are translated in to the same represen tation/, then the most general/, adaptable rep/-resen tation has to b e used in order to accommo date the most general case/. This comesat the exp ense of higher computational costs and p ossibly lo w er accuracy due to o v er/\ftstemming from un bridled adaptabilit y /. The neural net represen tation in to whic h Kbanntranslates domain theories allo ws /1/) a measure of partial matc h to the domain theory /2/) dif/-feren t parts of the domain theory to b e w eigh ted di/\u000beren tly /3/) conditions to b e added toand dropp ed from the domain theory /. All these options of adaptabilit y are probably notne c essary for most problems and ma y ev en b e detrimen tal/. These options in Kbann alsorequire the computationally exp ensiv e bac kpropagation metho d/.The represen tation used in Neither/-MofN is not as adaptable as Kbann /'s /| it do esnot allo w individual parts of the domain theory to b e w eigh ted di/\u000beren tly /. Neither/-MofN runs more quic kly than Kbann on small problems and probably matc hes or ev ensurpasses Kbann /'s accuracy for man y domains /| domains for whic h /\fne/-grained w eigh tingis unfruitful or ev en detrimen tal/. A to olb o x of theory rerepresen tation translators analogousto Grendel w ould allo w a domain theory to b e translated in to a represen tation ha ving themost appropriate forms of adaptabilit y /./2/./5 Outlo ok and SummaryIn summary /, w e brie/\ry reexamine /\rexible r epr esentation and /\rexible structur e /, the t w odesirable qualities set forth in Section /1/. W e consider ho w the v arious systems exemplifysome subset of these desirable qualities/./\u000f Kbann and Neither/-MofN b oth in terpreted a theory more /\rexibly than its originalrepresen tation allo w ed and revised the theory in this more adaptable represen tation/.A /\fnal/, re/\fned theory often has man y exceptions to the rule/; it ma y tolerate partialmatc hes and missing pieces of evidence/; it ma y w eigh t some evidence more hea vilythan other evidence/. Kbann /'s and Neither/-MofN /'s new represen tation ma y notb e the most concise/, appropriate represen tation for the initial theory /, but the newrepresen tation allo ws concise expression of an otherwise cum b ersome /\fnal theory /.These are cases of the principle of /\rexible r epr esentation /./4/1/8Rerepresenting and Restr ucturing Domain Theories/\u000f Standard induction programs ha v e b een quite successful at building concise theorieswith high predictiv e accuracy when the target concept c an b e concisely expressed usingthe original set of features/. When it can/'t/, constructiv e induction is a means of creatingnew features suc h that the target concept can b e concisely expressed/. Mir o usesconstructiv e induction to tak e adv an tage of the strengths of b oth a domain theory andstandard induction/. Kno wledge from the theory guides the construction of appropriatenew features/, and standard induction structures these in to a concise description ofthe concept/. Th us Mir o /-lik e construction coupled with standard induction pro videsa ready and p o w erful means of /\rexibly restructuring the kno wledge con tained in aninitial domain theory /. This is a case of the principle of /\rexible structur e /.In the follo wing section w e in tro duce the DNA Promoter Recognition domain in orderto illustrate tangibly ho w some of the systems discussed ab o v e in tegrate kno wledge andinduction/./3/. Demonstrations of Related W orkThis section in tro duces the Promoter Recognition domain /(Harley /, Reynolds/, /& No ordewier/,/1/9/9/0/) and brie/\ry illustrates ho w a Mir o /-lik e system/, Either /, Kbann /, and Neither/-MofN b eha v e in this domain/. W e implemen ted a Mir o /-lik e system for the promoter do/-main/; v ersions of Either and Neither/-MofN w ere a v ailable from Ra y Mo oney/'s group/;Kbann /'s b eha vior is describ ed b y analyzing /(T o w ell /& Sha vlik/, /1/9/9/4/)/. W e c hose the pro/-moter domain b ecause it is a non/-trivial/, real/-w orld problem whic h a n um b er of theoryrevision researc hers ha v e used to test their w ork /(Ourston /& Mo oney /, /1/9/9/0/; Thompsonet al/./, /1/9/9/1/; W ogulis/, /1/9/9/1/; Cohen/, /1/9/9/2/; P azzani /& Kibler/, /1/9/9/2/; Ba/\u000bes /& Mo oney /, /1/9/9/3/;T o w ell /& Sha vlik/, /1/9/9/4/)/. The promoter domain is one of three domains in whic h w e ev aluateour w ork/, theory/-guided constructiv e induction/, in Section /5/./3/./1 The Promoter Recognition DomainA promoter sequence is a region of DNA that marks the b eginning of a gene/. Eac h ex/-ample in the promoter recognition domain is a region of DNA classi/\fed either as a promoteror a non/-promoter/. As illustrated in Figure /3/, examples consist of /5/7 features represen t/-ing a sequence of /5/7 DNA n ucleotides/. Eac h feature can tak e on the v alues A/,G/,C/, or Trepresen ting adenine/, guanine/, cytosine/, and th ymine at the corresp onding DNA p osition/.The features are lab eled according to their p osition from p/-/5/0 to p/+/7 /(there is no zerop osition/)/. The notation /\\p/- N /\" denotes the n ucleotide that is N p ositions upstream fromthe b eginning of the gene/. The goal is to predict whether a sequence is a promoter from itsn ucleotides/. A total of /1/0/6 examples are a v ailable/: /5/3 promoters and /5/3 non/-promoters/.The promoter recognition problem comes with the initial domain theory sho wn in Fig/-ure /4 /(quoted almost v erbatim from T o w ell and Sha vlik/'s en try in the UCI Mac hine LearningRep ository/)/. The theory states that promoter sequences m ust ha v e t w o regions that mak econ tact with a protein and m ust also ha v e an acceptable conformation pattern/. There arefour p ossibiliti es for the con tact region at minus /3/5 /(/3/5 n ucleotides upstream from the b e/-ginning of the gene/)/. A matc h of an y of these four p ossibilitie s will satisfy the minus /3/5con tact condition/, th us they are joined b y disjunction/. Similarly /, there are four p ossibilities/4/1/9Donoho /& Rendell\nGAC TCTp-50 p+7DNA SequenceFigure /3/: An instance in the promoter domain consists of a sequence of /5/7 n ucleotideslab eled from p/-/5/0 to p/+/7/. Eac h n ucleotide can tak e on the v alues A/,G/,C/, or Trepresen ting adenine/, guanine/, cytosine/, and th ymine/.for the con tact region at minus /1/0 and four acceptable c onformation patterns/. Figure /5giv es a more pictorial presen tation of p ortions of the theory /. Of the /1/0/6 examples in thedataset/, none matc hed the domain theory exactly /, yielding an accuracy of /5/0/%/./3/./2 Mir o in the Promoter DomainA Mir o /-lik e system in the promoter domain w ould use the rules in Figure /4 to con/-struct new high/-lev el features for eac h DNA segmen t/. Figure /6 sho ws an example of this/. ADNA segmen t is sho wn from p osition p/-/3/8 through p osition p/-/3/0/. The minus /3/5 rules fromthe theory are also sho wn/, and four new features /(feat min us/3/5 A through feat min us/3/5 D/)ha v e b een constructed for that DNA segmen t/, one for eac h minus /3/5 rule/. The new fea/-tures feat min us/3/5 A and feat min us/3/5 D b oth ha v e the v alue /1 b ecause the DNA fragmen tmatc hes the /\frst and fourth minus /3/5 rules/. Lik ewise/, feat min us/3/5 B and feat min us/3/5 Cb oth ha v e the v alue /0 b ecause the DNA fragmen t do es not matc h the second and thirdrules/. F urthermore/, since the four minus /3/5 rules are joined b y disjunction/, a new feature/,feat min us/3/5 all/, is created for the group that w ould ha v e the v alue /1 b ecause at least oneof the minus /3/5 rules matc hes/.New features w ould similarly b e created for the minus /1/0 rules and the c onformationrules/, and a standard induction algorithm could then b e applied/. W e implemen ted a Mir o /-lik e system/; Figure /7 giv es an example theory created b y it/. /(Drastal/'s original Mir o usedthe candidate elimination algorithm /(Mitc hell/, /1/9/7/7/) as its underlying induction algorithm/.W e used C/4/./5 /(Quinlan/, /1/9/9/3/)/./) As opp osed to theory revision systems that incremen tallymo dify the domain theory /, Mir o has brok en the theory do wn in to its comp onen ts and hasfashioned these comp onen ts in to a new theory using a standard induction program/. Th usMir o has exhibited the /\rexible structur e principle for this domain /{ it w as not restrictedin an y w a y b y the structure of the initial theory /. Rather/, Mir o exploited the strengths ofstandard induction to concisely c haracterize the training examples using the new features/./4/2/0Rerepresenting and Restr ucturing Domain TheoriesPromoters have a region where a protein /(RNA polymerase/) must make contact andthe helical DNA sequence must have a valid conformation so that the two piecesof the contact region spatially align/. Prolog notation is used/.promoter /:/- contact/, conformation/.There are two regions /\"upstream/\" from the beginning of the gene at which theRNA polymerase makes contact/.contact /:/- minus/_/3/5/, minus/_/1/0/.The following rules describe the compositions of possible contact regions/.minus/_/3/5 /:/- p/-/3/7/=c/, p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/.minus/_/3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/2/=c/, p/-/3/1/=a/.minus/_/3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/, p/-/3/1/=a/.minus/_/3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/.minus/_/1/0 /:/- p/-/1/4/=t/, p/-/1/3/=a/, p/-/1/2/=t/, p/-/1/1/=a/, p/-/1/0/=a/, p/-/9/=t/.minus/_/1/0 /:/- p/-/1/3/=t/, p/-/1/2/=a/, p/-/1/0/=a/, p/-/8/=t/.minus/_/1/0 /:/- p/-/1/3/=t/, p/-/1/2/=a/, p/-/1/1/=t/, p/-/1/0/=a/, p/-/9/=a/, p/-/8/=t/.minus/_/1/0 /:/- p/-/1/2/=t/, p/-/1/1/=a/, p/-/7/=t/.The following rules describe sequences that produce acceptable conformations/.conformation /:/- p/-/4/7/=c/, p/-/4/6/=a/, p/-/4/5/=a/, p/-/4/3/=t/, p/-/4/2/=t/, p/-/4/0/=a/, p/-/3/9/=c/, p/-/2/2/=g/,p/-/1/8/=t/, p/-/1/6/=c/, p/-/8/=g/, p/-/7/=c/, p/-/6/=g/, p/-/5/=c/, p/-/4/=c/, p/-/2/=c/,p/-/1/=c/.conformation /:/- p/-/4/5/=a/, p/-/4/4/=a/, p/-/4/1/=a/.conformation /:/- p/-/4/9/=a/, p/-/4/4/=t/, p/-/2/7/=t/, p/-/2/2/=a/, p/-/1/8/=t/, p/-/1/6/=t/, p/-/1/5/=g/, p/-/1/=a/.conformation /:/- p/-/4/5/=a/, p/-/4/1/=a/, p/-/2/8/=t/, p/-/2/7/=t/, p/-/2/3/=t/, p/-/2/1/=a/, p/-/2/0/=a/, p/-/1/7/=t/,p/-/1/5/=t/, p/-/4/=t/.Figure /4/: The initial domain theory for recognizing promoters /(from T o w ell and Sha vlik/)/.A w eakness Mir o displa ys in this example is that it allo ws no /\rexibilit y of represen tationof the theory /. The represen tation of the features constructed b y Mir o is basically the sameall/-or/-none represen tation of the initial theory/; either a DNA segmen t matc hed a rule/, or itdid not/./3/./3 Either in the Promoter DomainAn Either /-lik e system re/\fnes the initial promoter theory b y dropping and addingconditions and rules/. W e sim ulated Either b y turning o/\u000b the M/-of/-N option in Neitherand ran it in the promoter domain/. Figure /8 sho ws the re/\fned theory pro duced using arandomly selected training set of size /8/0/. Because the initial promoter domain theory do esnot lend itself to revision through small/, lo cal c hanges/, Either has only limited success/./4/2/1Donoho /& Rendell\np-50 p+7DNA Sequence\nAAA\nTTTTT\nCCCC C\nGGGG\n*\n****\nTTT\nAA\n*-37 -36 -35 -34 -33 -32 -31\nORORORContact at minus_35\nT*\nATA\n****\nAAA T\nTTT\n**\n**\nTTT\nT AAAA\n**\n**-14 -13 -12 -11 -10 -9 -8 -7\nORORORContact at minus_10Figure /5/: The con tact p ortion of the theory /. There are four p ossibilities for b oth theminus /3/5 and minus /1/0 p ortions of the theory /. A /\\/*/\" matc hes an y n ucleotide/.The c onformation p ortion of the theory is to o spread out to displa y pictorially /.In this run/, the program exhibited the second b eha vioral extreme discussed in Section /1/;it en tirely remo v ed groups of rules and then tried to build new rules to replace what w aslost/. The minus /1/0 and c onformation rules ha v e essen tially b een remo v ed/, and new rulesha v e b een added to the minus /3/5 group/. These new minus /3/5 rules con tain the conditionp/-/1/2/=t previously found in the minus /1/0 group and the condition p/-/4/4/=a previously foundin the c onformation group/.Either /'s b eha vior in this example is a direct result of its lac k of /\rexibilit y of represen ta/-tion and /\rexibili t y of structure/. It is di/\u000ecult to transform the minus /1/0 and c onformationrules in to something useful in their initial represen tation using Either /'s lo cally/-acting op/-erators/. Either handles this b y dropping these sets of rules/, losing their kno wledge/, andattempting to redisco v er the lost kno wledge empirically /. The end result of this loss ofkno wledge is lo w er than optimal accuracy sho wn later in Section /5/./3/./4 Kbann in the Promoter DomainFigure /9/, mo deled after a /\fgure b y T o w ell and Sha vlik /(/1/9/9/4/)/, sho ws the setup of a Kbannnet w ork for the promoter theory /. Eac h slot along the b ottom represen ts one n ucleotidein the DNA sequence/. Eac h no de at the /\frst lev el up from the b ottom em b o dies a singledomain rule/, and higher lev els enco de groups of rules with the /\fnal concept at the top/. Thelinks sho wn in the /\fgure are the ones that are initially high/-w eigh ted/. The net is next /\flledout to b e fully connected with lo w/-w eigh t links/. Bac kpropagation is then applied to re/\fnethe net w ork/'s w eigh ts/./4/2/2Rerepresenting and Restr ucturing Domain TheoriesA DNA segmen t fragmen t/:/: /: /: p/-/3/8/=g/, p/-/3/7/=c/, p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/, p/-/3/1/=t/, p/-/3/0/=t /: /: /:The min us /3/5 group of rules and corresp onding constructed features/:min us /3/5 /:/- p/-/3/7/=c/, p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/. feat min us/3/5 A /= /1min us /3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/2/=c/, p/-/3/1/=a/. feat min us/3/5 B /= /0min us /3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/, p/-/3/1/=a/. feat min us/3/5 C /= /0min us /3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/. feat min us/3/5 D /= /1feat min us/3/5 all /= /(feat min us/3/5 A /_ feat min us/3/5 B /_ feat min us/3/5 C /_ feat min us/3/5 D/) /= /1Figure /6/: An example of feature construction in a Mir o /-lik e system/. The constructedfeatures for the /\frst and fourth rules in the minus /3/5 group are true /(v alue /= /1/)b ecause the DNA segmen t matc hes these rules/. The constructed feature for theen tire group/, feat min us/3/5 all/, is true b ecause the four minus /3/5 rules are joinedb y disjunction/.\npromoter non-promoter1\n0promoterpromoter\n11 00feat_minus10_all\nfeat_conf_B\nfeat_minus35_DFigure /7/: An example theory created b y a Mir o /-lik e system/. A DNA segmen t is recognizedas a promoter if it matc hes an y of the minus /1/0 rules/, the second c onformationrule/, or the fourth minus /3/5 rule/.The neural net represen tation is more appropriate for this domain than the prop ositionallogic represen tation of the initial theory /. It allo ws for a measuremen t of partial matc h b yw eigh ting the links in suc h a w a y that a subset of a rule/'s conditions are enough to surpass ano de/'s threshold/. It also allo ws for v ariable w eigh tings of di/\u000beren t parts of the theory/; there/-fore/, more predictiv e n ucleotides can b e w eigh ted more hea vily /, and only sligh tly predictiv en ucleotides can b e w eigh ted less hea vily /. Kbann has only limited /\rexibilit y of structure/.Because the re/\fned net w ork is the result of a series of incremen tal mo di/\fcations in theinitial net w ork/, a fundamen tal restructuring of the theory it em b o dies is unlik ely /. Kbann/4/2/3Donoho /& Rendellpromoter /:/- contact/, conformation/.contact /:/- minus/_/3/5/, minus/_/1/0/.minus/_/3/5 /:/- p/-/3/5/=t/, p/-/3/4/=g/.minus/_/3/5 /:/- p/-/3/6/=t/, p/-/3/3/=a/, p/-/3/2/=c/.minus/_/3/5 /:/- p/-/3/6/=t/, p/-/3/2/=c/, p/-/5/0/=c/.minus/_/3/5 /:/- p/-/3/4/=g/, p/-/1/2/=t/.minus/_/3/5 /:/- p/-/3/4/=g/, p/-/4/4/=a/.minus/_/3/5 /:/- p/-/3/5/=t/, p/-/4/7/=g/.minus/_/1/0 /:/- true/.conformation /:/- true/.Figure /8/: A revised theory pro duce b y Either /.\nminus_35 minus_10conformationpromoter\np+7 p-50 DNA  SequencecontactFigure /9/: The setup of a Kbann net w ork for the promoter theory /.is limited to /\fnding the b est net w ork with the same fundamen tal structure imp osed on itb y the initial theory /.One of Kbann /'s adv an tages is that it uses a standard learning algorithm as its foun/-dation/. Bac kpropagation has b een widely used and consequen tly impro v ed b y previousresearc hers/. Theory re/\fnemen t to ols that are built from the ground up or use a standardto ol only tangen tially su/\u000ber from ha ving to in v en t their o wn metho ds of handling standardproblems suc h as o v er/\ft/, noisy data/, etc/. A w ealth of neural net exp erience and resourcesis a v ailable to the Kbann user/; as neural net tec hnology adv ances/, Kbann tec hnology willpassiv ely adv ance with it/./4/2/4Rerepresenting and Restr ucturing Domain Theories/3/./5 Neither/-MofN in the Promoter DomainNeither/-MofN re/\fnes the initial promoter theory not only b y dropping and adding con/-ditions and rules but also b y allo wing conjunctiv e rules to b e true if only a subset of theirconditions are true/. W e ran Neither/-MofN with a randomly selected training set of size/8/0/, and Figure /1/0 sho ws a re/\fned promoter theory pro duced/. The theory expressed herewith /9 M/-of/-N rules w ould require /3/0 rules using prop ositional logic/, the initial theory/'srepresen tation/. More imp ortan tly /, it is unclear ho w an y system using the initial represen ta/-tion w ould reac h the /3/0/-rule theory from the initial theory /. Th us the M/-of/-N represen tationadopted not only allo ws for the concise expression of the /\fnal theory but also facilitates there/\fnemen t pro cess/.promoter /:/- /2 of /( contact/, conformation /)/.contact /:/- /2 of /( minus/_/3/5/, minus/_/1/0 /)/.minus/_/3/5 /:/- /2 of /( p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/2/=c/, p/-/3/1/=a /)/.minus/_/3/5 /:/- /5 of /( p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c /)/.minus/_/1/0 /:/- /2 of /( p/-/1/2/=t/, p/-/1/1/=a/, p/-/7/=t /)/.minus/_/1/0 /:/- /2 of /( p/-/1/3/=t/, p/-/1/2/=a/, p/-/1/0/=a/, p/-/8/=t /)/.minus/_/1/0 /:/- /6 of /( p/-/1/4/=t/, p/-/1/3/=a/, p/-/1/2/=t/, p/-/1/1/=a/, p/-/1/0/=a/, p/-/9/=t /)/.minus/_/1/0 /:/- /2 of /( p/-/1/3/=t/, p/-/1/2/=a/, p/-/1/0/=a/, p/-/3/4/=g /)/.conformation /:/- true/.Figure /1/0/: A revised theory pro duced b y Neither/-MofN /.Neither/-MofN displa ys /\rexibilit y of represen tation b y allo wing an M/-of/-N in terpreta/-tion of the original prop ositional logic/, but it do es not allo w for as /\fne/-grained re/\fnemen tas Kbann /. Both allo w for a measure of partial matc h/, but Kbann could w eigh t morepredictiv e features more hea vily /. F or example/, in the minus /3/5 rules/, p erhaps p/-/3/6/=t ismore predictiv e of a DNA segmen t b eing a promoter than p/-/3/4/=g and therefore should b ew eigh ted more hea vily /. Neither/-MofN simply coun ts the n um b er of true conditions in arule/; therefore/, ev ery condition is w eigh ted equally /. Kbann /'s /\fne/-grained w eigh ting ma y b eneeded in some domains and not in others/. It ma y actually b e detrimen tal in some domains/.An adv anced theory revision system should o/\u000ber a range of represen tations/.Lik e Kbann /, Neither/-MofN has only limited /\rexibili t y of structure/. The re/\fnedtheory is reac hed through a series of small/, incremen tal mo di/\fcations in the initial theoryprecluding a fundamen tal restructuring/. Neither/-MofN is therefore limited to /\fnding theb est theory with the same fundamen tal structure as the initial theory /./4/. Theory/-Guided Constructiv e InductionIn the /\frst half of this section w e presen t guidelines for the ory/-guide d c onstructive inductionthat summarize the w ork discussed in Sections /2 and /3/. The remainder of the section/4/2/5Donoho /& Rendellpresen ts an algorithm that instan tiates these guidelines/. W e ev aluate the algorithm inSection /5/./4/./1 GuidelinesThe follo wing guidelines are a syn thesis of the strengths of the previously discussed relatedw ork/./\u000f As in Mir o /, new features should b e constructed using comp onen ts of the domaintheory /. These new features are com binations of existing features/, and a /\fnal theory iscreated b y applying a standard induction algorithm to the training examples describ edusing the new features/. This allo ws kno wledge to b e gleaned from the initial theorywithout forcing the /\fnal theory to conform to the initial theory/'s bac kb one structure/.It tak es full adv an tage of the domain theory b y building high/-lev el features from theoriginal lo w/-lev el features/. It also tak es adv an tage of a strength of standard induction/| building concise theories ha ving high predictiv e accuracy when the target conceptc an b e concisely expressed using the giv en features/./\u000f As in Either /, the constructed features should b e mo di/\fable b y v arious op eratorsthat act lo cally /, suc h as adding or dropping conjuncts from a constructed feature/./\u000f As in Kbann and Neither/-MofN /, the represen tation of the constructed featuresneed not b e the exact represen tation in whic h the initial theory is giv en/. F or example/,the initial theory ma y b e giv en as a set of rules written in prop ositional logic/. Anew feature can b e constructed for eac h rule/, but it need not b e a b o olean featuretelling whether all the conditions are met/; for example it ma y b e a coun t of ho wman y conditions of that rule are met/. This allo ws the /\fnal theory to b e formed andexpressed in a represen tation that is more suitable than the represen tation of theinitial theory /./\u000f Lik e Grendel /, a complete system should o/\u000ber a library of in terpreters allo wing thedomain theory to b e translated in to a range of represen tations with di/\u000bering adapt/-abilit y /. One in terpreter migh t em ulate Mir o strictly translating a domain theoryin to b o olean constructed features/. Another in terpreter migh t construct features thatcoun t the n um b er of satis/\fed conditions of the corresp onding comp onen t of the do/-main theory th us pro viding a measure of partial matc h/. Still another in terpretermigh t construct features that are w eigh ted sums of the satis/\fed conditions/. Thew eigh ts could b e re/\fned empirically b y examining a set of training examples/. Th usthe most appropriate amoun t of expressiv e p o w er can b e applied to a giv en problemwithout incurring unnecessary exp ense/./4/./2 A Sp eci/\fc In terpreterThis section describ es an algorithm whic h is a limited instan tiation of the guidelines justdescrib ed/. The algorithm is in tended as a demonstration of the distillation and syn thesisof the principles em b o died in previous landmark systems/. It con tains a main mo dule/,Tgci describ ed in Figure /1/2/, and a sp eci/\fc in terpreter/, Tgci/1 describ ed in Figure /1/1/.The main mo dule Tgci redescrib es the training and testing examples b y calling Tgci/1/4/2/6Rerepresenting and Restr ucturing Domain Theoriesand then applies C/4/./5 to the redescrib ed examples /(just as Mir o applied the candidateelimination algorithm to examples after redescribing them/)/. Tgci/1 can b e view ed as asingle in terpreter from a p oten tial Grendel /-lik e to olb o x/. It tak es as input a single exampleand a domain theory expressed as an AND//OR tree suc h as the one sho wn in Figure /1/3/.It returns a new v ector of features for that example that measure the partial matc h of theexample to the theory /. Th us it creates new features from comp onen ts of the domain theoryas in Mir o /, but b ecause it measures partial matc h/, it allo ws /\rexibilit y in represen tingthe information con tained in the initial theory as in Kbann and Neither/-MofN /. Oneasp ect of the guidelines in /4/./1 that do es not app ear in this algorithm is Either /'s lo callyacting op erators suc h as adding and dropping conditions from a p ortion of the theory /.The follo wing t w o paragraphs explain in more detail the w orkings of Tgci/1 and Tgciresp ectiv ely /.Given/: An example E and a domain theory with ro ot no de R /. The domaintheory is an AND//OR//NOT tree in whic h the lea v es are conditions whic h canb e tested to b e true or false /.R eturn/: A pair P /= /( F /; F /) where F is the top feature measuring the partialmatc h of E to the whole domain theory /, and F is a v ector of new features mea/-suring the partial matc h of E to v arious parts and subparts of the domain theory /./1/. If R is a directly testable condition/, return P/=/(/1/, /</> /) if R is true for Eand P/=/(/-/1/, /</> /) if R is false for E /./2/. n /= the n um b er of c hildren of R/3/. F or eac h c hild Rj\nof R /, call Tgci/1 /( Rj\n/, E /) and store the resp ectiv e resultsin Pj\n/= /( Fj\n/; Fj\n/)/./4/. If the ma jor op erator of R is OR/, Fnew\n/= M AX /( Fj\n/)/.Return P /= /( Fnew\n/; concatenate /( /<Fnew\n/>/; F/1\n/; F/2\n/; /:/:/:/; Fn\n/)/)/./5/. If the ma jor op erator of R is AND/, Fnew\n/= /(\nPnj /=/1\nFj\n/) /=n /.Return P /= /( Fnew\n/; concatenate /( /<Fnew\n/>/; F/1\n/; F/2\n/; /:/:/:/; Fn\n/)/)/./6/. If the ma jor op erator of R is NOT/, Fnew\n/= /BnZr /1 /\u0003 F/1\n/.Return P /= /( Fnew\n/; F/1\n/)/.Figure /1/1/: The Tgci/1 algorithmThe Tgci/1 algorithm/, giv en in Figure /1/1/, is recursiv e/. Its inputs are an example E anda domain theory with ro ot no de R /. It ultimately returns a redescription of E in the formof a v ector of new features F /. It also returns a v alue F called the top fe atur e whic h is usedin in termediate calculations describ ed b elo w/. The base case o ccurs if the domain theory isa single leaf no de /(i/.e/./, R is a simple condition/)/. In this case /(Line /1/)/, Tgci/1 returns thetop feature /1 if the condition is tr ue and /-/1 if the condition is f al se /. No new features arereturned in the base case b ecause they w ould simply duplicate the existing features/. If the/4/2/7Donoho /& Rendelldomain theory is not a single leaf no de/, Tgci/1 recursiv ely calls itself on eac h of R /'s c hildren/(Line /3/)/. When a c hild of R /, Rj\n/, is pro cessed/, it returns a v ector of new features Fj\n/(whic hmeasures the partial matc h of the example to the j th c hild of R and its v arious subparts/)/.It also returns the top feature Fj\nwhic h is included in Fj\nbut is mark ed as sp ecial b ecause itmeasures the partial matc h of the example to the whole of the j th c hild of R /. If there are nc hildren/, the result of Line /3 is n v ectors of new features/, F/1\nto Fn\n/, and n top features/, F/1to Fn\n/. If the op erator at no de R is OR /(Line /4/)/, then Fnew\n/, the new feature created for thatno de/, is the maxim um of Fj\n/. Th us Fnew\nmeasures ho w closely the b est of R /'s c hildren cometo ha ving its conditions met b y the example/. The v ector of new features returned in thiscase is a concatenation of Fnew\nand all the new features from R /'s c hildren/. If the op eratorat no de R is AND /(Line /5/)/, then Fnew\nis the a v erage of Fj\n/. Th us Fnew\nmeasures ho w closelyall of R /'s c hildren as a group come to ha ving their conditions met b y the example/. Thev ector of new features returned in this case is again a concatenation of Fnew\nand all the newfeatures from R /'s c hildren/. If the op erator at no de R is NOT /(Line /6/)/, R should only ha v eone c hild/, and Fnew\nis F/1\nnegated/. Th us Fnew\nmeasures the exten t to whic h the conditionsof R /'s c hild are not met b y the example/.Given/: A set of training examples Etr ain\n/, a set of testing examples Etest\n/, and adomain theory with ro ot no de R /.R eturn/: Learned concept and accuracy on testing examples/./1/. F or eac h example Ei\n/2 Etr ain\n/, call Tgci/1 /( R /, Ei\n/) whic h returns Pi\n/=/( Fi\n/; Fi\n/)/. Etr ain /BnZr new\n/= fFi\ng /./2/. F or eac h example Ei\n/2 Etest\n/, call Tgci/1 /( R /, Ei\n/)/. whic h returns Pi\n/=/( Fi\n/; Fi\n/)/. Etest /BnZr new\n/= fFi\ng /./3/. Call C/4/./5 with training examples Etr ain /BnZr new\nand testing examplesEtest /BnZr new\n/. Return decision tree and accuracy on Etest /BnZr new\n/.Figure /1/2/: The Tgci algorithmIf Tgci/1 is called t wice with t w o di/\u000beren t examples but with the same domain theory /,the t w o v ectors of new features will b e the same size/. F urthermore/, corresp onding featuresmeasure the matc h of corresp onding parts of the domain theory /. The Tgci main mo dulein Figure /1/2 tak es adv an tage of this b y creating redescrib ed example sets from the inputexample sets/. Line /1 redescrib es eac h example in the training set pro ducing a new trainingset/. Line /2 do es the same for the testing set/. Line /3 runs the standard induction programC/4/./5 on these redescrib ed example sets/. The returned decision tree can b e easily in terpretedb y examining whic h new features w ere used and what part of the domain theory theycorresp ond to/./4/./3 Tgci/1 ExamplesAs an example of ho w the Tgci/1 in terpreter w orks/, consider the to y theory sho wn inFigure /1/3/. Tgci/1 redescrib es the input example b y constructing a new feature for eac h no de/4/2/8Rerepresenting and Restr ucturing Domain Theoriesin the input theory /. Consider the situation where the input example matc hes conditions A/,B/, and D but not C and E/. When Tgci/1 ev aluates the c hildren of No de /6/, it gets the v aluesF/1\n/= /1/, F/2\n/= /1/, F/3\n/= /BnZr /1/, F/4\n/= /1/, and F/5\n/= /BnZr /1/. Since the op erator at No de /6 is AND/, Fnewis the a v erage of the v alues receiv ed from the c hildren/, /0/./2/0 /(/(/1 /+ /1 /+ /( /BnZr /1/) /+ /1 /+ /( /BnZr /1/)/) /= /5 /=/0 /: /2/0/)/. Lik ewise/, if condition G matc hs but not F and H/, Fnew\nfor No de /5 will ha v e the v alue/0/./3/3 /( /BnZr /1 /\u0003 /(/(/1 /+ /( /BnZr /1/) /+ /( /BnZr /1/)/) /= /3/)/) b ecause t w o of three matc hing conditions at No de /7 giv ethe v alue /BnZr /0 /: /3/3/, and this is negated b y the NOT at No de /5/. Since No de /2 is a disjunction/,its new feature measures the b est partial matc h of its t w o c hildren and has the v alue /0/./3/3/(MAX/(/0/./2/0/,/0/./3/3 /)/)/, and so on/.\nA B C D E F G H I J K L M N ONOTNOT1\n23\n4\n5\n6789Figure /1/3/: An example theory in the form of an AND//OR tree that migh t b e used b y thein terpreter to generate constructed features/.Figure /1/4 sho ws ho w Tgci/1 redescrib es a particular DNA segmen t using the minus /3/5rules of the promoter theory /. A partial DNA segmen t is sho wn along with the four minus /3/5rules and the new feature constructed for eac h rule /(W e ha v e giv en the new features nameshere to simplify our illustration/)/. F or the /\frst rule/, four of the six n ucleotides matc h/; there/-fore/, for that DNA segmen t feat min us/3/5 A has the v alue /0/./3/3 /(/(/1 /+ /1 /+ /1 /+ /1 /+ /( /BnZr /1/)/+ /( /BnZr /1/)/) /= /6/) /.F or the second rule/, four of the /\fv e n ucleotides matc h/; therefore/, feat min us/3/5 B hasthe v alue /0/./6/0/. Because these and the other t w o minus /3/5 rules are joined b y disjunc/-tion in the original domain theory /, feat min us/3/5 all/, the new feature constructed for thisgroup/, tak es the maxim um v alue of its four c hildren/; therefore/, feat min us/3/5 all has thev alue /0/./6/0 b ecause feat min us/3/5 B has the v alue /0/./6/0/, the highest in the group/. In tu/-itiv ely /, feat min us/3/5 all represen ts the b est partial matc h of this grouping /| the exten tto whic h the disjunction is partially satis/\fed/. The results of running Tgci/1 on eac h DNAsequence is a set of redescrib ed training examples/. Eac h redescrib ed example has a v alue forfeat min us/3/5 A through feat min us/3/5 D/, feat min us/3/5 all/, and all other no des in the pro/-moter domain theory /. The training set is essen tially redescrib ed using a new feature v ectorderiv ed from information con tained in the domain theory /. In this form/, an y o/\u000b/-the/-shelfinduction program can b e applied to the new example set/.Anomalous situations can b e created in whic h Tgci/1 giv es a /\\go o d score/\" to a seeminglybad example and a bad score to a go o d example/. Situations can also b e created wherelogically equiv alen t theories giv e di/\u000beren t scores for a single example/. These o ccur b ecause/4/2/9Donoho /& RendellA DNA segmen t fragmen t/:/: /: /: p/-/3/8/=g/, p/-/3/7/=c/, p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=c/, p/-/3/2/=a/, p/-/3/1/=a/, p/-/3/0/=t /: /: /:The min us /3/5 group of rules and corresp onding constructed features/:min us /3/5 /:/- p/-/3/7/=c/, p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/. feat min us/3/5 A /= /0/./3/3min us /3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/2/=c/, p/-/3/1/=a/. feat min us/3/5 B /= /0/./6/0min us /3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/, p/-/3/1/=a/. feat min us/3/5 C /= /0/./3/3min us /3/5 /:/- p/-/3/6/=t/, p/-/3/5/=t/, p/-/3/4/=g/, p/-/3/3/=a/, p/-/3/2/=c/. feat min us/3/5 D /= /0/./2/0feat min us/3/5 all /= max/(feat min us/3/5 A/, feat min us/3/5 B/, feat min us/3/5 C/, feat min us/3/5 D/) /= /0/./6/0Figure /1/4/: An example of ho w Tgci/1 generates constructed features from a p ortion of thepromoter domain theory and a DNA segmen t/. F our of the conditions in the /\frstminus /3/5 rule matc h the DNA segmen t/; therefore/, the constructed feature forthat rule has the v alue /0/./3/3 /(/(/1 /+ /1 /+ /1 /+ /1 /+ /( /BnZr /1/) /+ /( /BnZr /1/)/) /= /6/)/. F eat min us/3/5 all/,the new feature for the en tire minus /3/5 group tak es the maxim um v alue of itsc hildren th us em b o dying the b est partial matc h of the group/.Tgci/1 is biased to fa v or situations where more matc hed conditions of an AND is desirable/,but more matc hed conditions of an OR is not necessarily b etter/. Eliminating these anomaliesw ould remo v e this bias/./5/. Exp erime n ts and AnalysisThis section presen ts the results of applying theory/-guided constructiv e induction to threedomains/: the promoter domain /(Harley et al/./, /1/9/9/0/)/, the primate splice/-junction domain /(No/-ordewier/, Sha vlik/, /& T o w ell/, /1/9/9/2/)/, and the gene iden ti/\fcation domain /(Cra v en /& Sha vlik/,/1/9/9/5/)/. In eac h case the Tgci/1 in terpreter w as applied to the domain/'s theory and examplesin order to redescrib e the examples using new features/. Then C/4/./5 /(Quinlan/, /1/9/9/3/) w asapplied to the redescrib ed examples/./5/./1 The Promoter DomainFigure /1/5 sho ws a learning curv e for theory/-guided constructiv e induction in the promoterdomain accompanied b y curv es for Either /, LabyrinthK\n/, Kbann /, and Neither/-MofN /.F ollo wing the metho dology describ ed b y T o w ell and Sha vlik /[/1/9/9/4/]/, the set of /1/0/6 examplesw as randomly divided in to a training set of size /8/0 and a testing set of size /2/6/. A learningcurv e w as created b y training on subsets of the training set of size /8/, /1/6/, /2/4/, /: /: /: /7/2/, /8/0/,using the /2/6 examples for testing/. The curv es for Either /, LabyrinthK\n/, and Kbann w eretak en from Ourston and Mo oney /(/1/9/9/0/)/, Thompson/, Langley /, and Iba /(/1/9/9/1/)/, and T o w ell/4/3/0Rerepresenting and Restr ucturing Domain Theoriesand Sha vlik /(/1/9/9/4/) resp ectiv ely and w ere obtained b y a similar metho dology\n/1/. The curv efor Tgci is the a v erage of /5/0 indep enden t random data partitions and is giv en along with /9/5/%con/\fdence ranges/. The Neither/-MofN program w as obtained from Ra y Mo oney/'s groupand w as used in generating the Neither/-MofN curv e using the same /5/0 data partitionsas w ere used for Tgci\n/2/.\n02.557.51012.51517.52022.52527.53032.53537.54042.5\n05101520253035404550556065707580% Error\nSize of Training Sample EITHER\nLabyrinth-k\nNEITHER-MofN\nKBANN\nTGCI\n95% confidence of NEITHER-MofN\n95% confidence of TGCIFigure /1/5/: Learning curv es for theory/-guided constructiv e induction and other systems inthe promoter domain/.Tgci sho w ed impro v emen t o v er Either and LabyrinthK\nfor all p ortions of the curv eand also p erformed b etter than Kbann and Neither/-MofN for all except the small/-est training sets/. Con/\fdence in terv als w ere not a v ailable for Either /, LabyrinthK\n/, and/1/. Either used a testing set of size /2/5 and did not use the c onformation p ortion of the domain theory /. Thetesting set in LabyrinthK\nalw a ys consisted of /1/3 promoters and /1/3 non/-promoters/./2/. Ba/\u000bes and Mo oney /(/1/9/9/3/) rep ort a sligh tly b etter learning curv e for Neither/-MofN than w e obtained/,but after comm unication with P aul Ba/\u000bes/, w e think the di/\u000berence is caused b y the random selection ofdata partitions/./4/3/1Donoho /& RendellKbann /, but in a pairwise comparison with Neither/-MofN /, the impro v emen t of Tgci w assigni/\fcan t at the /0/./0/0/0/5 lev el of con/\fdence for training sets of size /4/8 and larger/.\n100% of\nfirst\nminus_35\nrule100% of\nsecond\nminus_35\nrule100% of\nthird\nminus_35\nrule100% of\nfourth\nminus_35\nrule100% of\nfirst\nminus_10\nrule100% of\nsecond\nminus_10\nrule100% of\nthird\nminus_10\nrule100% of\nfourth\nminus_10\nrule100% of\nfirst\nconf.\nrule100% of\nsecond\nconf.\nrule100% of\nthird\nconf.\nrule100% of\nfourth\nconf.\nrule\n>33% of\nfirst\nminus_10\nrule>33% of\nsecond\nminus_10\nrule>33% of\nthird\nminus_10\nrule>33% of\nfourth\nminus_10\nrule>20% of\nsecond\nminus_35\nruleStructure of Initial Promoter Theory\nStructure of Revised Promoter TheoryFigure /1/6/: The revised theory pro duced b y theory/-guided constructiv e induction has b or/-ro w ed substructures from the initial theory /, but as a whole has not b een re/-stricted b y its structure/.Figure /1/6 compares the initial promoter theory with a theory created b y Tgci /. Reasonsfor Tgci /'s impro v emen t can b e inferred from this /\fgure/. Tgci has extracted the com/-p onen ts of the original theory that are most helpful and restructured them in to a moreconcise theory /. Neither Kbann nor Neither/-MofN facilitates this radical extraction andrestructuring/. As seen in the leaf no des/, the new theory also measures the partial matc hof an example to comp onen ts of the original theory /. This asp ect is similar in Kbann andNeither/-MofN /.P art of Tgci /'s impro v emen t o v er Kbann and Neither/-MofN ma y b e due to a know l/-e dge//bias c on/\rict in the latter t w o systems/, a situation where revision biases con/\rict withkno wledge in suc h a w a y as to undo some of the kno wledge/'s b ene/\fts/. This can o ccurwhenev er detailed kno wledge is op ened up to revision using a set of examples/. The revisionis not guided only b y the examples but rather b y the examples as in terpreted b y a set/4/3/2Rerepresenting and Restr ucturing Domain Theoriesof algorithmic biases/. Biases that are useful in the absence of kno wledge ma y undo go o dkno wledge when improp erly applied/. Y et these biases dev elop ed and p erfected for pure in/-duction are often unquestioningly applied to the revision of theories/. The biases go v erningthe dropping of conditions in Neither/-MofN and rew eigh ting conditions in Kbann ma yb e neutralizing the promoter theory/'s p oten tial/. W e sp eculate this b ecause w e conductedsome exp erimen ts that allo w ed bias/-guided dropping and adding of conditions within Tgci /.W e found that these tec hniques actually reduced accuracy in this domain/.\n7.51012.51517.52022.52527.53032.53537.54042.545\n020406080100120140160180200% Error\nSize of Training Sample \nc4.5\nbackpropagation\nKBANN\nTGCI\n95% confidence of TGCI\ndomain theoryFigure /1/7/: Learning curv es for Tgci and other systems in the primate splice/-junction do/-main/./5/./2 The Primate Splice/-junction DomainThe primate splice/-junction domain /(No ordewier et al/./, /1/9/9/2/) in v olv es analyzing a DNAsequence and iden tifying b oundaries b et w een in trons and exons/. Exons are the parts of aDNA sequence k ept after splicing/; in trons are spliced out/. The task then in v olv es placing a/4/3/3Donoho /& Rendellgiv en b oundary in to one of three classes/: an in tron//exon b oundary /, an exon//in tron b ound/-ary /, or neither/. An imp erfect domain theory is a v ailable whic h has a /3/9/./0/% error rate onthe en tire set of a v ailable examples/.Figure /1/7 sho ws learning curv es for C/4/./5 /, bac kpropagation/, Kbann /, and Tgci in theprimate splice/-junction domain/. The results for Kbann and bac kpropagation w ere tak enfrom T o w ell and Sha vlik /(/1/9/9/4/)/. The curv es for plain C/4/./5 and the Tgci algorithm w erecreated b y training on sets of size /1/0/,/2/0/,/3/0/,/././. /9/0/,/1 /0/0/, /1/2/0/, /./. /./2/0/0 and testing on a set of size/8/0/0/. The curv es for C/4/./5 and Tgci are the a v erage of /4/0 indep enden t data partitions/.No comparison w as made with Neither/-MofN b ecause the implemen tation w e obtainedcould handle only t w o/-class concepts/. F or training sets larger than /2/0/0/, Kbann /, Tgci /, andbac kpropagation all p erformed similarly /.The accuracy of Tgci app ears sligh tly w orse than that of Kbann but p erhaps not sig/-ni/\fcan tly /. Kbann /'s adv an tage o v er Tgci is its abilit y to assign /\fne/-grained w eigh tingsto individual parts of a domain theory /. Tgci /'s adv an tage o v er Kbann is its abilit y tomore easily restructure the information con tained in a domain theory /. W e sp eculate thatKbann /'s capabilit y to assign /\fne/-grained w eigh ts out w eigh ted its somewhat rigid struc/-turing of this domain theory /. Theory/-guided constructiv e induction has an adv an tage ofsp eed o v er Kbann b ecause C/4/./5 /, its underlying learner/, runs m uc h more quic kly thanbac kpropagation/, Kbann /'s underlying learning algorithm/./5/./3 The Gene Iden ti/\fcation DomainThe gene iden ti/\fcation domain /(Cra v en /& Sha vlik/, /1/9/9/5/) in v olv es classifying a giv en DNAsegmen t as a co ding sequence /(one that co des a protein/) or a non/-co ding sequence/. Nodomain theory w as a v ailable in the gene iden ti/\fcation domain/; therefore/, w e created anarti/\fcial domain theory using the information that organisms ma y fa v or certain n ucleotidetriplets o v er others in gene co ding/. The domain theory em b o dies the kno wledge that a DNAsegmen t is lik ely to b e a gene co ding segmen t if its triplets are co ding/-fa v oring triplets or ifits triplets are not nonco ding/-fa v oring triplets/. The decision of whic h triplets w ere co ding/-fa v oring/, whic h w ere nonco ding/-fa v oring/, and whic h fa v ored neither/, w as made empiricallyb y analyzing the mak eup of /2/5/0/0 co ding and /2/5/0/0 nonco ding sequences/. The sp eci/\fc arti/-/\fcial domain theory used is describ ed in Online App endix /1/.Figure /1/8 sho ws learning curv es for C/4/./5 and Tgci in the gene iden ti/\fcation domain/.The original domain theory yields /3/1/./5/% error/. The curv es w ere created b y training onexample sets of size /5/0/,/2/0/0/,/4/0/0/,/./. /./2/0 /0/0 and testing on a separate example set of size /1/0/0/0/.The curv es are the a v erage of /4/0 indep enden t data partitions/.Only a partial curv e is giv en for Neither/-MofN b ecause it b ecame prohibitiv ely slo wfor larger training sets/. In the promoter domain where training sets w ere smaller than /1/0/0/,Tgci and Neither/-MofN ran at comparable sp eeds /(appro ximately /1/0 seconds on a Sun/4w orkstation/)/. In this domain Tgci ran in appro ximately /2 min utes for larger training sets/.Neither/-MofN to ok /2/1 times as long as Tgci on training sets of size /4/0/0/, /6/9 times aslong for size /8/0/0/, and /1/4/4 times as long for size /1/2/0/0/. Consequen tly /, Neither/-MofN /'scurv e only extends to /1/2/0/0 and only represen ts /\fv e randomly selected data partitions/. F orthese reasons/, a solid comparison of Neither/-MofN and Tgci cannot b e made from thesecurv es/, but it app ears that Tgci /'s accuracy is sligh tly b etter/. W e sp eculate that Neither/-/4/3/4Rerepresenting and Restr ucturing Domain Theories\n2022.52527.53032.53537.54042.545\n0200400600800100012001400160018002000% Error\nNumber training examples \nTGCI\n95% confidence of TGCI\nC4.5\nNEITHER-MofN\ndomain theoryFigure /1/8/: Learning curv es for Tgci and other systems in the gene iden ti/\fcation domain/.MofN /'s sligh tly lo w er accuracy is partially due to the fact that it revises the theory tocorrectly classify al l the training examples/. The result is a theory whic h lik ely o v er/\fts thetraining examples/. Tgci do es not need to explicitly a v oid o v er/\ft b ecause this is handledb y its underlying learner/./5/./4 Summary of Exp erimen tsOur goal in this pap er has not b een to presen t a new tec hnique but rather to understandthe b eha vior of landmark systems/, distill their strengths/, and syn thesize them in to a simplesystem/, Tgci /. The ev aluation of this algorithm sho ws that its accuracy roughly matc hes orexceeds that of its predecessors/. In the promoter domain/, Tgci sho w ed sizable impro v emen to v er man y published results/. In the splice/-junction domain/, Tgci narro wly falls short ofKbann /'s accuracy /. In the gene iden ti/\fcation domain/, Tgci outp erforms Neither/-MofN /.In all these domains Tgci greatly impro v es on the original theory alone and C/4/./5 alone/./4/3/5Donoho /& RendellTgci is faster than its closest comp etitors/. Tgci runs as m uc h as /1/0/0 times faster thanNeither/-MofN on large datasets/. A strict quan titativ e comparison of the sp eeds of Tgciand Kbann w as not made b ecause /1/) bac kpropagation is kno wn to b e m uc h slo w er thandecision trees /(Mo oney /, Sha vlik/, T o w ell/, /& Go v e/, /1/9/8/9/)/, /2/) Kbann uses m ultiple hiddenla y ers whic h mak es its training time ev en longer /(T o w ell /& Sha vlik/, /1/9/9/4/)/, and /3/) T o w elland Sha vlik /(/1/9/9/4/) p oin t out that eac h run of Kbann m ust b e made m ultiple times withdi/\u000beren t initial random w eigh ts/, whereas a single run of Tgci is su/\u000ecien t/.Ov erall/, our exp erimen ts supp ort t w o claims of this pap er/: First/, the accuracy of Tgcisubstan tiates our delineation of system strengths in terms of /\rexible theory represen tationand /\rexible theory structure/, since this c haracterization is the basis for this algorithm/'sdesign/. Second/, Tgci /'s com bination of sp eed and accuracy suggest that unnecessary com/-putational complexit y can b e a v oided in syn thesizing the strengths of landmark systems/.In the follo wing section w e tak e a closer lo ok at the strengths of theory/-guided constructiv einduction/./6/. Discussion of StrengthsBelo w a n um b er of strengths of theory/-guided constructiv e induction are discussed withinthe con text of the Tgci algorithm used in our exp erimen ts/./6/./1 Flexible Represen tationAs discussed in Section /1/, for man y domains the represen tation most appropriate for aninitial theory ma y not b e most appropriate for a re/\fned theory /. Because theory/-guided con/-structiv e induction allo ws the translation of the initial theory in to a di/\u000beren t represen tation/,it can accommo date suc h domains/. In the exp erimen ts in this pap er a represen tation w asneeded whic h allo w ed for a measuremen t of partial matc h to the domain theory /. Tgci/1accomplished this b y simply coun ting the matc hing features and propagating this infor/-mation up the theory appropriately /. Either and LabyrinthK\ndo not easily a/\u000bord thismeasure of partial matc h and therefore are more appropriate for problems in whic h the b estrepresen tation of the /\fnal theory is the same as that of the initial theory /. Kbann allo wsa /\fner/-grained measuremen t of partial matc h than b oth Neither/-MofN and our w ork/,but a price is paid in computational complexit y /. The theory/-guided constructiv e induc/-tion framew ork allo ws for a v ariet y of p oten tial to ols with v arying degrees of gran ularit y ofpartial matc h/, although just one to ol is used in our exp erimen ts/./6/./2 Flexible StructureAs discussed in Section /2/./5/, a strength of existing induction programs is fashioning a conciseand highly predictiv e description of a concept when the target concept c an b e conciselydescrib ed with the giv en features/. Consequen tly /, the v alue of a domain theory lies not in itso v erall structure/. If the feature language is su/\u000ecien t/, an y induction program can build ago o d o v erall theory structure/. Instead/, the v alue of a domain theory lies in the informationit con tains ab out ho w to redescrib e examples using high/-lev el features/. These high/-lev elfeatures facilitate a concise description of the target concept/. Systems suc h as Either andNeither/-MofN that reac h a /\fnal theory through a series of mo di/\fcations in the initial/4/3/6Rerepresenting and Restr ucturing Domain Theoriestheory hop e to gain something b y k eeping the theory/'s o v erall structure in tact/. If the initialtheory is su/\u000ecien tly close to an accurate theory /, this metho d w orks/, but often clinging tothe structure hinders full exploitation of the domain theory /. Theory/-guided constructiv einduction pro vides a means of fully exploiting b oth the information in the domain theory andthe strengths of existing induction programs/. Figure /1/6 in Section /5/./1 giv es a comparison ofthe structure of the initial promoter theory to the structure of a revised theory pro duced b ytheory/-guided constructiv e induction/. Substructures ha v e b een b orro w ed/, but the revisedtheory as a whole has b een restructured/./6/./3 Use of Standard Induction as an Underlying LearnerBecause theory/-guided constructiv e induction uses a standard induction program as itsunderlying learner/, it do es not need to rein v en t solutions to o v er/\ft a v oidance/, m ulti/-classconcepts/, noisy data/, etc/. Ov er/\ft a v oidance has b een widely studied for standard induction/,and man y standard tec hniques exist/. An y system whic h mo di/\fes a theory to accommo datea set of training examples m ust also address the issue of o v er/\ft to the training examples/. Inman y theory revision systems existing o v er/\ft a v oidance tec hniques cannot b e easily adapted/,and the problem m ust b e addressed from scratc h/. Theory/-guided constructiv e induction cantak e adv an tage of the full range of previous w ork in o v er/\ft a v oidance for standard induction/.When m ultiple theory parts are a v ailable for m ulti/-class concepts/, the in terpreter isrun on the m ultiple theory parts/, and the resulting new feature sets are com bined/. Theprimate splice/-junction domain presen ted in Section /5/./2 has three classes/: in tron//exonb oundaries/, exon//in tron b oundaries/, and neither/. Theories are giv en for b oth in tron//exonand exon//in tron/. Both theories are used to create new features/, and then all new featuresare concatenated together for learning/. In terpreters suc h as Tgci/1 also trivially handlenegation in a domain theory /./6/./4 Use of Theory F ragmen tsTheory/-guided constructiv e induction is not limited to using full domain theories/. If onlypart of a theory is a v ailable/, this can b e used/. T o demonstrate this/, three exp erimen tsw ere run in whic h only fragmen ts of the promoter domain theory w ere used/. In the /\frstexp erimen t/, only the four minus /3/5 rules w ere used/. Fiv e features w ere constructed /| onefeature for eac h rule and then an additional feature for the group/. Similar exp erimen ts w ererun for the minus /1/0 group and the c onformation group/.Figure /1/9 giv es learning curv es for these three exp erimen ts along with curv es for the en/-tire theory and for no theory /( C/4/./5 using the original features/)/. Although the c onformationp ortion of the theory giv es no signi/\fcan t impro v emen t o v er C/4/./5 /, b oth the minus /3/5 andminus /1/0 p ortions of the theory giv e signi/\fcan t impro v emen ts in p erformance/. Th us ev enpartial theories and theory fragmen ts can b e used b y theory/-guided constructiv e inductionto yield sizable p erformance impro v emen ts/.The use of theory fragmen ts should b e explored as a means of ev aluating the con tributionof di/\u000beren t parts of a theory /. In Figure /1/9/, the c onformation p ortion of the theory is sho wnto yield no impro v emen t/. This could signal a kno wledge engineer that the kno wledge thatshould b e con v ey ed through that p ortion of the theory is not useful to the learner in itspresen t form/./4/3/7Donoho /& Rendell\n02.557.51012.51517.52022.52527.53032.53537.54042.545\n05101520253035404550556065707580% Error\nSize of Training Sample C4.5\nconformation portion of theory\nminus_10 portion of theory\nminus_35 portion of theory\nwhole theoryFigure /1/9/: Learning curv es for theory/-guided constructiv e induction with only fragmen ts ofthe promoter domain theory /. The minus /3/5 p ortion of the theory /, the minus /1/0p ortion of the theory /, and the c onformation p ortion of the theory w ere usedseparately in feature construction/. Curv es are also giv en for the full theory andfor C/4/./5 alone for comparison/./6/./5 Use of Multiple TheoriesTheory/-guided constructiv e induction can use m ultiple comp eting and ev en incompatibledomain theories/. If m ultiple theories exist/, theory/-guided constructiv e induction pro videsa natural means of in tegrating them in suc h a w a y as to extract the b est from all theories/.Tgci/1 w ould b e called for eac h input theory pro ducing new features/. Next/, all the newfeatures are simply p o oled together and the induction program selects from among themin fashioning the /\fnal theory /. This is seen on a v ery small scale in the promoter domain/./4/3/8Rerepresenting and Restr ucturing Domain TheoriesIn Figure /4 some minus /3/5 rules subsume other minus /3/5 rules/. According to the en try inthe UCI Database/, this is b ecause /\\the biological evidence is inconclusiv e with resp ect tothe correct sp eci/\fcit y /./\" This is handled b y simply using all four p ossibilities/, and selectionof the most useful kno wledge is left to the induction program/.Tgci could also b e used to ev aluate the con tributions of comp eting theories just as it w asused to ev aluate theory fragmen ts ab o v e/. A kno wledge engineer could use this ev aluationto guide his o wn revision and syn thesis of comp eting theories/.\n02.557.51012.51517.52022.525\n5101520253035404550556065707580% Error\nSize of Training Sample \nTGCI using C4.5\nTGCI using LFC\n95% confidence of LFCFigure /2/0/: Theory/-guided constructiv e induction with Lf c and C/4/./5 as the underlyinglearning system/. Theory/-guided constructiv e induction can use an y inductiv elearner as its underlying learning comp onen t/. Therefore/, more sophisticatedunderlying induction programs can further impro v e accuracy /./6/./6 Easy Adoption of New T ec hniquesSince theory/-guided constructiv e induction can use an y standard induction metho d as itsunderlying learner/, as impro v emen ts are made in standard induction/, theory/-guided con/-structiv e induction passiv ely impro v es/. T o demonstrate this/, tests w ere also run with Lf c/(Raga v an /& Rendell/, /1/9/9/3/) as the underlying induction program/. Lf c is a decision treelearner that p erforms example/-based constructiv e induction b y lo oking ahead at com bi/-nations of features/. Characteristically /, Lf c impro v es accuracy for a mo derate n um b er ofexamples/. Figure /2/0 sho ws the resulting learning curv e along with the C/4/./5 Tgci curv e/.Both curv es are the a v erage of /5/0 separate runs with the same data partitions used for eac hprogram/. In a pairwise comparison the impro v emen t of Lf c o v er C/4/./5 w as signi/\fcan t at the/0/./0/2/5 lev el of con/\fdence for training sets of size /7/2 and /8/0/. More sophisticated underlyinginduction programs can further impro v e accuracy /./4/3/9Donoho /& Rendell/7/. T esting the Limits of TgciThe purp ose of this section is to explore the p erformance of theory/-guided constructiv einduction on theory revision problems ranging from easy to di/\u000ecult/. In easy problemsthe underlying concept em b o died in the training and testing examples matc hes the domaintheory fairly closely/; therefore/, the examples themselv es matc h the domain theory fairlyclosely /. In di/\u000ecult problems the underlying concept em b o died in the examples do es notmatc h the domain theory v ery w ell so the examples do not either/. Although man y otherfactors determine the di/\u000ecult y of an individual problem/, this asp ect is an imp ortan t com/-p onen t and w orth exploring/. Our exp erimen t in this section is in tended to relate ranges ofdi/\u000ecult y to the amoun t of impro v emen t pro duced b y Tgci /.Since a n um b er of factors a/\u000bect problem di/\u000ecult y w e c hose that the theory revisionproblems for the exp erimen t should all b e v ariations of a single problem/. By doing this w eare able to hold all other factors constan t and v ary the closeness of matc h to the domaintheory /. Because w e w an ted to a v oid totally arti/\fcial domains/, w e c hose to start with thepromoter domain and create /\\new/\" domains b y p erv erting the example set/.These /\\new/\" domains w ere created b y p erv erting the examples in the original promoterproblem to either mor e closely matc h the promoter domain theory or less closely matc h thepromoter domain theory /. Only the p ositiv e examples w ere altered/. F or example/, one domainw as created with /3/0/% few er matc hes to the domain theory than the original promoterdomain as follo ws/: Eac h feature v alue in a giv en example w as examined to see if it matc hedpart of the theory /. If so/, with a /3/0/% probabilit y /, it w as randomly reassigned a new v aluefrom the set of p ossible v alues for that feature/. The end result is a set of examples with /3/0/%few er matc hes to the domain theory than the original example set\n/3/. F or our exp erimen tnew domains suc h as this w ere created with /1/0/%/, /3/0/%/, /6/0/%/, and /9/0/% few er matc hes/.F or some features/, m ultiple v alues ma y matc h the theory b ecause di/\u000beren t disjunctsof the theory sp ecify di/\u000beren t v alues for a single feature/. F or example/, referring bac k toFigure /4/, feature p /-/1/2 matc hes t w o of the minus /1/0 rules if it has the v alue a and anothert w o rules if it has the v alue t /. So a single feature migh t acciden tally matc h one part of atheory when in fact the example as a whole more closely matc hes another part of the theory /.F or cases suc h as these/, true matc hes w ere separated from acciden tal matc hes b y examiningwhic h part of the theory most clearly matc hed the example as a whole and exp ecting amatc h from that part of the theory /.New domains that mor e closely matc hed the theory w ere created in a similar manner/. F orexample/, a domain w as created with /3/0/% few er mis matc hes to the domain theory than theoriginal promoter domain as follo ws/: Eac h feature v alue in a giv en example w as examinedto see if it matc hed its corresp onding part of the theory /. If not/, with a /3/0/% probabilit y /,it w as reassigned a v alue that matc hed the theory /. The end result is a set of examples inwhic h /3/0/% of the mismatc hes with the domain theory are eliminated/. F or our exp erimen tnew domains suc h as this w ere created with /3/0/%/, /6/0/%/, and /9/0/% few er mismatc hes/.T en di/\u000beren t example sets w ere created for eac h lev el of closeness to the domain theory/:/1/0/%/, /3/0/%/, /6/0/%/, /9/0/% few er matc hes/, and /3/0/%/, /6/0/%/, /9/0/% few er mismatc hes/. In total/, fort yexample sets w ere created whic h matc hed the original theory less closely than the original/3/. More precisely /, there w ould b e sligh tly more matc hes than /3/0/% few er matc hes b ecause some featuresw ould b e randomly reassigned bac k to their original matc hing v alue/./4/4/0Rerepresenting and Restr ucturing Domain Theories\n0510152025303540455055\n-100-80-60-40-20020406080100% Error\nCloseness to theory \nC4.5\nTGCIFigure /2/1/: Sev en altered promoter domains w ere created/, three that more closely matc hedthe theory than the original domains and four that less closely matc hed/. A/1/0/0 on the x/-axis indicates a domain in whic h the p ositiv e examples matc h thedomain theory /1/0/0/%/. A negativ e /1/0/0 indicates a domain in whic h an y matc hof the p ositiv e examples to the domain theory is purely c hance/. The accuracyof C/4/./5 and Tgci are plotted for di/\u000beren t lev els of pro ximit y to the domaintheory /.example set/, and thirt y example sets w ere created whic h matc hed the original theory mor eclosely than the original example set/. Eac h of these example sets w as tested using a lea v e/-one/-out metho dology using C/4/./5 and the Tgci algorithm/. The results are summarized inFigure /2/1/. The x/-axis is a measure of the ory pr oximity /{ closeness of an example set to thedomain theory /. /\\/0/\" on the x/-axis indicates no c hange in the original promoter examples/./\\/1/0/0/\" on the x/-axis means that eac h p ositiv e example exactly matc hes the domain theory /./\\/-/1/0/0/\" on the x/-axis means that an y matc h of a feature v alue of a p ositiv e example to the/4/4/1Donoho /& Rendelldomain theory is totally b y c hance\n/4/. Eac h datap oin t in Figure /2/1 is the result of a v eragingthe accuracies of the ten example sets for eac h lev el of theory pro ximit y /(except for thep oin t at zero whic h is the accuracy of the exact original promoter examples/)/.One notable p ortion of Figure /2/1 is the section b et w een /0 and /6/0 on the x/-axis/. Domainsin this region ha v e a greater than trivial lev el of mismatc h with the domain theory but notmore than mo derate mismatc h/. This is the region of Tgci /'s b est p erformance/. On thesedomains/, Tgci ac hiev es high accuracy while a standard learner/, C/4/./5 /, using the originalfeature set giv es medio cre p erformance/. A second region to examine is b et w een /-/6/0 and /0on the x/-axis where the lev el of mismatc h ranges from mo derate to extreme/. In this regionTgci /'s p erformance falls o/\u000b but its impro v emen t o v er the original feature set remains highas sho wn in Figure /2/2 whic h plots the impro v emen t of Tgci o v er C/4/./5 /. The /\fnal t w oregions to notice are greater than /6/0 and less than /-/6/0 on the x/-axis/. As the lev el ofmismatc h b et w een theory and examples b ecomes trivially small /(x/-axis greater than /6/0/)/,C/4/./5 is able to pic k out the theory/'s patterns leading to high accuracy that approac hes thatof Tgci /'s/. As the lev el of mismatc h b ecomes extreme /(x/-axis less than /-/6/0/) the theory giv eslittle help in problem/-solving resulting in similarly p o or accuracy for b oth metho ds/. Insummary /, as sho wn in Figure /2/2 for v arian ts of the promoter problem there is a wide rangeof theory pro ximit y /(cen tered around the real promoter problem/) for whic h theory/-guidedconstructiv e induction yields sizable impro v emen t o v er standard learners/.\n02.557.51012.51517.520\n-100-80-60-40-20020406080100% Error\nCloseness to theory \nerror differenceFigure /2/2/: The di/\u000berence in error b et w een C/4/./5 and Tgci for di/\u000beren t lev els of pro ximit yof the example set to the domain theory /./4/. The scale /0 to /-/1/0/0 on the left half of the graph ma y not b e directly comparable with the scale /0 to /1/0/0on the righ t half of the graph since there w ere not a equal n um b er of matc hes and mismatc hes in theoriginal examples/./4/4/2Rerepresenting and Restr ucturing Domain Theories/8/. ConclusionOur goal in this pap er has not b een just to presen t another new system/, but rather tostudy the t w o qualities /\rexible r epr esentation and /\rexible structur e /. These capabilities arein tended as a frame of reference for analyzing theory/-guided systems/. These t w o principlespro vide guidelines for purp oseful design/. Once w e had distilled the essence of systemssuc h as Mir o /, Kbann /, and Neither/-MofN /, theory/-guided constructiv e induction w asa natural syn thesis of their strengths/. Our exp erimen ts ha v e demonstrated that ev en asimple application of the t w o principles can e/\u000bectiv ely in tegrate theory kno wledge withtraining examples/. Y et there is m uc h ro om for impro v emen t/; the t w o principles could b equan ti/\fed and made more precise/, and the implemen tations that pro ceed from them shouldb e explored and re/\fned/.Quan tifying represen tational /\rexibili t y is one step/. Section /4 ga v e three degrees of/\rexibilit y/: one measured the exact matc h to a theory /, one coun ted the n um b er of matc hingconditions/, and one allo w ed for a w eigh ted sum of the matc hing conditions/. The amoun t of/\rexibilit y should b e quan ti/\fed/, and /\fner/-grained degrees of /\rexibilit y should b e explored/.The accuracy in assorted domains should b e ev aluated as a function of represen tational/\rexibilit y /.Finer/-grained structural /\rexibilit y w ould b e adv an tageous/. W e ha v e presen ted systemsthat mak e small/, incremen tal mo di/\fcations in a theory as lac king structural /\rexibili t y /. Y ettheory/-guided constructiv e induction falls at the other extreme/, p erhaps allo wing excessiv estructural /\rexibili t y /. F ortunately /, existing induction to ols are capable of fashioning simpley et highly predictiv e theory structures when the problem features are suitably high/-lev el/.Nev ertheless/, approac hes should b e explored that tak e adv an tage of the structure of theinitial theory without b eing unduly restricted b y it/.The strength discussed in Section /6/./5 should b e giv en further atten tion/. Although thepromoter domain giv es a v ery small example of syn thesizing comp eting theories/, this shouldb e explored in a domain in whic h en tire comp eting/, inconsisten t theories are a v ailable suc h assyn thesizing the kno wledge giv en b y m ultiple exp erts/. The p oin t w as made in Section /6/./4that Tgci can use theory fragmen ts to ev aluate the con tribution of di/\u000beren t parts of atheory /. This should also b e explored further/.In an exploration of bias in standard induction/, Utgo/\u000b /(/1/9/8/6/) refers to biases as rangingfrom we ak to str ong and from inc orr e ct to c orr e ct /. A strong bias restricts the concepts thatcan b e represen ted more than a w eak bias th us pro viding more guidance in learning/. But asa bias b ecomes stronger/, it ma y also b ecome incorrect b y ruling out useful concept descrip/-tions/. A similar situation arises in theory revision /| a theory represen tation language thatis inappropriately rigid ma y imp ose a strong/, incorrect bias on revision/. A language thatallo ws adaptabilit y along to o man y dimensions ma y pro vide to o w eak a bias/. A Grendel /-lik e to olb o x w ould allo w a theory to b e translated in to a range of represen tations withv arying dimensions of adaptabilit y /. Utgo/\u000b adv o cates starting with a strong/, p ossibly incor/-rect bias and shifting to an appropriately w eak and correct bias/. Similarly /, a theory couldb e translated in to successiv ely more adaptable represen tations un til an appropriate bias isfound/. W e ha v e implemen ted only a single to ol/; man y op en problems remain along this lineof researc h/./4/4/3Donoho /& RendellThe con v erse relationship of theory revision and constructiv e induction w arran ts furtherexamination /| theory revision uses data to impro v e a theory/; constructiv e induction canuse theory to impro v e data to facilitate learning/. Since the long/-term goal of mac hinelearning is to use data/, inference/, and theory to impro v e an y and all of them/, w e b eliev ethat a consideration of these related metho ds can b e b ene/\fcial/, particularly b ecause eac hresearc h area has some strengths that the other lac ks/.An analysis of landmark theory revision and theory/-guided learning systems has ledto the t w o principles /\rexible r epr esentation and /\rexible structur e /. Because theory/-guidedconstructiv e induction w as based up on these high/-lev el principles/, it is simple y et ac hiev esgo o d accuracy /. These principles pro vide guidelines for future w ork/, y et as discussed ab o v e/,the principles themselv es are imprecise and call for further exploration/.Ac kno wledgemen tsW e w ould lik e to thank Geo/\u000b T o w ell/, Kevin Thompson/, Ra y Mo oney /, and Je/\u000b Mahoney fortheir assistance in getting the datap oin ts for Kbann /, LabyrinthK\n/, and Either /. W e w ouldalso lik e to thank P aul Ba/\u000bes for making the Neither program a v ailable and for advice onsetting the program/'s parameters/. W e thank the anon ymous review ers for their constructiv ecriticism of an earlier draft of this pap er/. W e gratefully ac kno wledge the supp ort of thisw ork b y a DoD Graduate F ello wship and NSF gran t IRI/-/9/2/-/0/4/4/7/3/."}
{"category": "abstract", "text": "Theory revision in tegrates inductiv e learning and bac kground kno wledge b y com biningtraining examples with a coarse domain theory to pro duce a more accurate theory /. Thereare t w o c hallenges that theory revision and other theory/-guided systems face/. First/, arepresen tation language appropriate for the initial theory ma y b e inappropriate for animpro v ed theory /. While the original represen tation ma y concisely express the initial theory /,a more accurate theory forced to use that same represen tation ma y b e bulky /, cum b ersome/,and di/\u000ecult to reac h/. Second/, a theory structure suitable for a coarse domain theory ma yb e insu/\u000ecien t for a /\fne/-tuned theory /. Systems that pro duce only small/, lo cal c hanges toa theory ha v e limited v alue for accomplishing complex structural alterations that ma y b erequired/.Consequen tly /, adv anced theory/-guided learning systems require /\rexible r epr esentationand /\rexible structur e /. An analysis of v arious theory revision systems and theory/-guidedlearning systems rev eals sp eci/\fc strengths and w eaknesses in terms of these t w o desiredprop erties/. Designed to capture the underlying qualities of eac h system/, a new system usestheory/-guided constructiv e induction/. Exp erimen ts in three domains sho w impro v em en to v er previous theory/-guided systems/. This leads to a study of the b eha vior/, limitations/,and p oten tial of theory/-guided constructiv e induction/./1/. In tro ductionInductiv e learners normally use training examples/, but they can also use bac kground kno wl/-edge/. E/\u000bectiv ely in tegrating this kno wledge in to induction has b een a widely studied re/-searc h problem/. Most w ork to date has b een in the area of the ory r evision in whic h thekno wledge giv en is a coarse/, p erhaps incomplete or incorrect/, theory of the problem domain/,and training examples are used to shap e this initial theory in to a re/\fned/, more accuratetheory /(Ourston /& Mo oney /, /1/9/9/0/; Thompson/, Langley /, /& Iba/, /1/9/9/1/; Cohen/, /1/9/9/2/; P azzani/& Kibler/, /1/9/9/2/; Ba/\u000bes /& Mo oney /, /1/9/9/3/; Mo oney /, /1/9/9/3/)/. W e dev elop a more /\rexible andmore robust approac h to the problem of learning from b oth data and theory kno wledge b yaddressing the t w o follo wing desirable qualities/"}
{"category": "non-abstract", "text": "/(/1/) compiling a b elief net w ork in to an arithmetic expression called aQuery D A G /(Q/-D A G/)/; and /(/2/) answ ering queries using a simple ev aluation algorithm/.Eac h no de of a Q/-D A G represen ts a n umeric op eration/, a n um b er/, or a sym b ol for ev/-idence/. Eac h leaf no de of a Q/-D A G represen ts the answ er to a net w ork query /, that is/,the probabilit y of some ev en t of in terest/. It app ears that Q/-D A Gs can b e generated us/-ing an y of the standard algorithms for exact inference in b elief net w orks /| w e sho w ho wthey can b e generated using clustering and conditioning algorithms/. The time and spacecomplexit y of a Q/-D A G gener ation algorithm is no w orse than the time complexit y of theinference algorithm on whic h it is based/. The complexit y of a Q/-D A G evaluation algorithmis linear in the size of the Q/-D A G/, and suc h inference amoun ts to a standard ev aluation ofthe arithmetic expression it represen ts/. The in tended v alue of Q/-D A Gs is in reducing thesoft w are and hardw are resources required to utilize b elief net w orks in on/-line/, real/-w orldapplications/. The prop osed framew ork also facilitates the dev elopmen t of on/-line inferenceon di/\u000beren t soft w are and hardw are platforms due to the simplicit y of the Q/-D A G ev aluationalgorithm/. In terestingly enough/, Q/-D A Gs w ere found to serv e other purp oses/: simple tec h/-niques for reducing Q/-D A Gs tend to subsume relativ ely complex optimization tec hniquesfor b elief/-net w ork inference/, suc h as net w ork/-pruning and computation/-cac hing/./1/. In tro ductionConsider designing a car to ha v e a self/-diagnostic system that can alert the driv er to a rangeof problems/. Figure /1 sho ws a simplistic b elief net w ork that could pro vide a rank ed setof diagnoses for car troublesho oting/, giv en input from sensors ho ok ed up to the battery /,alternator/, fuel/-tank and oil/-system/.The standard approac h to building suc h a diagnostic system is to put this b elief net w ork/,along with inference co de/, on to the car/'s computer/; see Figure /2/. W e ha v e encoun tered an um b er of di/\u000eculties when using this approac h to em b o dy b elief net w ork tec hnology in in/-dustrial applications/. First/, w e w ere ask ed to pro vide the tec hnology on m ultiple platforms/.F or some applications/, the tec hnology had to b e implemen ted in AD A to pass certain certi/-/\fcation pro cedures/. In others/, it had to b e implemen ted on domain/-sp eci/\fc hardw are thatonly supp orts v ery primitiv e programm ing languages/. Second/, memory w as limited to k eepc/\r /1/9/9/7 AI Access F oundation and Morgan Kaufmann Publishers/. All righ ts reserv ed/.D ar wiche /& Pr o v an\nalternatoroil-pressure\nsensoroil-pressurefuelsensorfuel\nalternator\nsensorbattery sensorbattery\nfaultFigure /1/: A simple b elief net w ork for car diagnosis/.the cost of a unit b elo w a certain threshold to main tain pro duct pro/\ftabilit y /. The dilemmaw as the follo wing/: b elief net w ork algorithms are not trivial to implemen t/, esp ecially when op/-timization is crucial/, and p orting these algorithms to m ultiple platforms and languages w ouldha v e b een prohibitiv ely exp ensiv e/, time/-consuming and demanding of quali/\fed manp o w er/.T o o v ercome these di/\u000eculties/, w e ha v e devised a v ery /\rexible approac h for implemen tingb elief net w ork systems/, whic h is based on the follo wing observ ation/. Almost all the w orkp erformed b y standard algorithms for b elief net w orks is indep enden t of the sp eci/\fc evidencegathered ab out v ariables/. F or example/, if w e run an algorithm with the battery/-sensor setto low and then run it later with the v ariable set to de ad /, w e /\fnd almost no algorithmicdi/\u000berence b et w een the t w o runs/. That is/, the algorithm will not branc h di/\u000beren tly on an yof the k ey decisions it mak es/, and the only di/\u000berence b et w een the t w o runs is the sp eci/\fcargumen ts to the in v ok ed n umeric op erations/. Therefore/, one can apply a standard inferencealgorithm on a net w ork with evidence b eing a p ar ameter instead of b eing a sp eci/\fc v alue/. Theresult returned b y the algorithm will then b e an arithmetic expression with some parametersthat dep end on sp eci/\fc evidence/. This parameterized expression is what w e call a QueryD A G/, an example of whic h is sho wn in Figure /4/.\n/1The approac h w e are prop osing consists of t w o steps/. First/, giv en a b elief net w ork/, a setof v ariables ab out whic h evidence ma y b e collected /(evidence v ariables/)/, and a set of v ari/-ables for whic h w e need to compute probabilit y distributions /(query v ariables/)/, a Q/-D A Gis compiled o/\u000b/-line/, as sho wn in Figure /3/. The compilation is t ypically done on a sophisti/-cated soft w are// har dw a re platform/, using a traditional b elief net w ork inference algorithm inconjunction with the Q/-D A G compilation metho d/. This part of the pro cess is far and a w a ythe most costly computationally /. Second/, an on/-line system comp osed from the generatedQ/-D A G and an ev aluator sp eci/\fc to the giv en platform is used to ev aluate the Q/-D A G/. Giv enevidence/, the parameterized arithmetic expression is ev aluated in a straigh tfor w ar d mannerusing simple arithmetic op erations rather than complicated b elief net w ork inference/. The/1/. The sharing of sub expressions is what mak es this a Directed Acyclic Graph instead of a tree/./1/4/8A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference\nN\nL\nI\nN\nEO\n0\nN\nL\nI\nN\nEL\nI\nN\nEFFO\nFaultValuesTraditional Approach Compiled Approach\nVariablesSensor\nVariables\nCausal Network\nInference\nSoftwareQ-DAG\nEvaluatorCompilerQ-DAGCausal\nNetwork\nQuery\nDAGSensor\nProbabilitiesFaultFigure /2/: This /\fgure compares the traditional approac h to exact b elief/-net w ork inference/(sho wn on the left/) with our new compiled approac h /(sho wn on the righ t/) in thecon text of diagnostic reasoning/. In the traditional approac h/, the b elief net w orkand sensor v alues are used on/-line to compute the probabilit y distributions o v erfault v ariables/; in the compiled approac h/, the b elief net w ork/, fault v ariables andsensor v ariables are compiled o/\u000b/-line to pro duce a Q/-D A G/, whic h is then ev aluatedon/-line using sensor v alues to compute the required distributions/.computational w ork needed to p erform this on/-line ev aluation is so straigh tforw a rd that itlends itself to easy implemen tations on di/\u000beren t soft w ar e and hardw are platforms/.This approac h shares some commonalit y with other metho ds that sym b olically manip/-ulate probabilit y expressions/, lik e SPI /(Li /& D/'Am brosio/, /1/9/9/4/; Shac h ter/, D/'Am brosio/, /&del F a v ero/, /1/9/9/0/)/; it di/\u000bers from SPI on the ob jectiv e of suc h manipulations and/, hence/,on the results obtained/. SPI explicates the notion of an arithmetic expression to state thatb elief/-net w ork inference can b e view ed as an expression/-factoring op eration/. This allo wsresults from optimization theory to b e utilized in b elief/-net w ork inference/. On the otherhand/, w e de/\fne an arithmetic expression to explicate and formalize the b oundaries b et w eenon/-line and o/\u000b/-line inference/, with the goal of iden tifying the minimal piece of soft w ar e thatis required on/-line/. Our results are therefore orien ted to w ards this purp ose and they include/:/(a/) a formal de/\fnition of a Q/-D A G and its ev aluator/; /(b/) a metho d for generating Q/-D A Gsusing standard inference algorithms /| an algorithm need not subscrib e to the inference/-as/-/1/4/9D ar wiche /& Pr o v an\nCausal Network Query Variables\nQuery DAG Evidence\nQ-DAG EvaluatorQ-DAG Compiler\nOn-lineOff-lineEvidence VariablesFigure /3/: The prop osed framew or k for implemen ting b elief/-net w ork inference/.\nC BA\na\nON\nOFFPr(C=ON|a)\n.5.9aPr(B=ON|a)\nON\nOFF.25\n.8Pr(A=ON) = .3(a)\n* *+\n*+\n* *+\n**+\n*Pr(B=OFF, c) Pr(B=ON, c)\n.075 .56 .225 .14\n.5 .1 .9\n(C,ON) (C,OFF)(b)Figure /4/: A b elief net w ork /(a/)/; and its corresp onding Query/-D A G /(b/)/. Here/, C is an evidencev ariable/, and w e are in terested in the probabilit y of v ariable B /.factoring view to b e used for Q/-D A G generation/; and /(c/) computational guaran tees on thesize of Q/-D A Gs in terms of the computational guaran tees of the inference algorithm usedto generate them/. Although the SPI framew or k is p ositioned to form ulate related results/, ithas not b een pursued in this direction/.It is imp ortan t to stress the follo wing prop erties of the prop osed approac h/. First/, declar/-ing an evidence v ariable in the compilation pro cess do es not mean that evidence m ust b ecollected ab out that v ariable on/-line/|this is imp ortan t b ecause some evidence v alues/, e/.g/./,from sensors/, ma y b e lost in practice/|it only means that evidence may b e collected/. There/-fore/, one can declare all v ariables to b e evidence if one wishes/. Second/, a v ariable can b edeclared to b e b oth evidence and query /. This allo ws one to p erform v alue/-of/-informa tion/1/5/0A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inferencecomputations to decide whether it is w orth collecting evidence ab out a sp eci/\fc v ariable/.Third/, the space complexit y of a Q/-D A G in terms of the n um b er of evidence v ariables is now orse than the time complexit y of its underlying inference algorithm/; therefore/, this is nota simple en umerate/-all/-p ossible/-cases approac h/. Finally /, the time and space complexit y forgenerating a Q/-D A G is no w orse than the time complexit y of the standard b elief/-net w orkalgorithm used in its generation/. Therefore/, if a net w ork can b e solv ed using a standardinference algorithm/, and if the time complexit y of this algorithm is no w orse than its spacecomplexit y /,\n/2then w e can construct a Q/-D A G for that net w ork/.The follo wing section explains the concept of a Q/-D A G with a concrete example andpro vides formal de/\fnitions/. Section /3 is dedicated to the generation of Q/-D A Gs and theircomputational complexit y /, sho wing that an y standard b elief/-net w ork inference algorithmcan b e used to compile a Q/-D A G as long as it meets some general conditions/. Section /4discusses the reduction of a Q/-D A G after it has b een generated/, sho wing that suc h reductionsubsumes k ey optimizations that are t ypically implemen ted in b elief net w ork algorithms/.Section /5 con tains a detailed example on the application of this framew or k to diagnosticreasoning/. Finally /, Section /6 closes with some concluding remarks/./2/. Query D A GsThis section starts our treatmen t of Q/-D A Gs with a concrete example/. W e will consider aparticular b elief net w ork/, de/\fne a set of queries of in terest/, and then sho w a Q/-D A G thatcan b e used to answ er suc h queries/. W e will not discuss ho w the Q/-D A G is generated/; onlyho w it can b e used/. This will allo w a concrete in tro duction to Q/-D A Gs and will help usground some of the formal de/\fnitions to follo w/.The b elief net w ork w e will consider is the one in Figure /4/(a/)/. The class of queries w eare in terested in is Pr /( B j C /)/, that is/, the probabilit y that v ariable B tak es some v aluegiv en some kno wn /(or unkno wn/) v alue of C /. Figure /4/(b/) depicts a Q/-D A G for answ eringsuc h queries/, whic h is essen tially a parameterized arithmetic expression where the v alues ofparameters dep end on the evidence obtained/. This Q/-D A G will actually answ er queries ofthe form Pr /( B /; C /)/, but w e can use normalization to compute Pr /( B j C /)/.First/, a n um b er of observ ations ab out the Q/-D A G in Figure /4/(b/)/:/\u000f The Q/-D A G has t w o leaf no des lab eled Pr /( B /= ON /; c /) and Pr /( B /= OFF /; c /)/. These arecalled query no des b ecause their v alues represen t answ ers to the queries Pr /( B /= ON /; c /)and Pr /( B /= OFF /; c /)/./\u000f The Q/-D A G has t w o ro ot no des lab eled /( C /; ON /) and /( C /; OFF /)/. These are calledEvidenc e Sp e ci/\fc No des /(ESNs/) since their v alues dep end on the evidence collectedab out v ariable C on/-line/.According to the seman tics of Q/-D A Gs/, the v alue of no de /( V /; v /) is /1 if v ariable V isobserv ed to b e v or is unkno wn/, and /0 otherwise/. Once the v alues of ESNs are determined/,w e ev aluate the remaining no des of a Q/-D A G using n umeric m ultiplication and addition/.The n um b ers that get assigned to query no des as a result of this ev aluation are the answ ersto queries represen ted b y these no des/./2/. Algorithms based on join trees ha v e this prop ert y /./1/5/1D ar wiche /& Pr o v an\n.9 0 .5 0.2725.2875.0925.3475\n.0675 .28.2025 .07.0075 .28 .0225.07\n.9 .5\n10010 .10 .5.5 .1\n(a) (b)*\n.225*\n.14+\n*+\n* *+\n*\n.5 .9 .1*+\n*\n.56 .075*\n.225*\n.14+\n*+\n* *+\n*\n.5 .9 .1*+\n*\n.56 .075\n(C,ON) (C,OFF) (C,ON) (C,OFF)Pr(B=ON, c) Pr(B=OFF, c) Pr(B=ON, c) Pr(B=OFF, c)Figure /5/: Ev aluating the Q/-D A G in Figure /4 with resp ect to t w o pieces of evidence/: /(a/)C /= ON and /(b/) C /= OFF /.F or example/, supp ose that the evidence w e ha v e is C /= ON /. Then ESN /( C /; ON /) isev aluated to /1 and ESN /( C /; OFF /) is ev aluated to /0/. The Q/-D A G in Figure /4/(b/) is thenev aluated as giv en in Figure /5/(a/)/, th us leading toPr /( B /= ON /; C /= ON /) /= /: /3/4/7/5 /;andPr /( B /= OFF /; C /= ON /) /= /: /2/7/2/5 /;from whic h w e conclude that Pr /( C /= ON /) /= /: /6/2/. W e can then compute the conditionalprobabilities Pr /( B /= ON j C /= ON /) and Pr /( B /= OFF j C /= ON /) using/:Pr /( B /= ON j C /= ON /) /= Pr /( B /= ON /; C /= ON /) /= Pr /( C /= ON /) /;Pr /( B /= OFF j C /= ON /) /= Pr /( B /= OFF /; C /= ON /) /= Pr /( C /= ON /) /:If the evidence w e ha v e is C /= OFF /, ho w ev er/, then /( C /; ON /) ev aluates to /0 and /( C /; OFF /)ev aluates to /1/. The Q/-D A G in Figure /4/(b/) will then b e ev aluated as giv en in Figure /5/(b/)/,th us leading toPr /( B /= ON /; C /= OFF /) /= /: /2/8/7/5 /;andPr /( B /= OFF /; C /= OFF /) /= /: /0/9/2/5 /:W e will use the follo wing notation for denoting v ariables and their v alues/. V ariablesare denoted using upp ercase letters/, suc h as A/; B /; C /, and v ariable v alues are denoted b ylo w ercase letters/, suc h as a/; b/; c /. Sets of v ariables are denoted b y b oldface upp ercase letters/,suc h as A /; B /; C /, and their instan tiations are denoted b y b oldface lo w ercase letters/, suc h asa /; b /; c /. W e use E to denote the set of v ariables ab out whic h w e ha v e evidence/. Therefore/,/1/5/2A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inferencew e use e to denote an instan tiation of these v ariables that represen ts evidence/. Finally /, thefamily of a v ariable is the set con taining the v ariable and its paren ts in a directed acyclicgraph/.F ollo wing is the formal de/\fnition of a Q/-D A G/.De/\fnition /1 A Q/-D A G is a tuple /( V /; /\u0005 /; I /; D /; Z /) wher e/1/. V is a distinguishe d set of symb ols /(c al le d evidenc e variables /)/2/. /\u0005 is a symb ol /(c al le d unknown value /)/3/. I maps e ach variable in V into a set of symb ols /(c al le d variable values /) di/\u000ber ent fr om/\u0005 /./4/. D is a dir e cte d acyclic gr aph wher e/- e ach non/-r o ot no de is lab ele d with either /+ or /\u0003/- e ach r o ot no de is lab ele d with either/- a numb er in /[/0 /; /1/] or/- a p air /( V /; v /) wher e V is an evidenc e variable and v is a value/5/. Z is a distinguishe d set of no des in D /(c al le d query no des /)Evidence v ariables V corresp ond to net w ork v ariables ab out whic h w e exp ect to collectevidence on/-line/. F or example/, in Figure /5/, C is the evidence v ariable/. Eac h one of thesev ariables has a set of p ossible v alues that are captured b y the function I /. F or example/, inFigure /5/, the evidence v ariable C has v alues ON and OFF /. The sp ecial v alue /\u0005 is usedwhen the v alue of a v ariable is not kno wn/. F or example/, w e ma y ha v e a sensor v ariable withv alues /\\lo w/,/\" /\\medium/,/\" and /\\high/,/\" but then lose the sensor v alue during on/-line reasoning/.In this case/, w e set the sensor v alue to /\u0005 /.\n/3Query no des are those represen ting answ ers touser queries/. F or example/, in Figure /5/, B is the query v ariable/, and leads to query no desP r /( B /= ON /; c /) and P r /( B /= OFF /; c /)/.An imp ortan t notion is that of evidence/:De/\fnition /2 F or a given Q/-D A G /( V /; /\u0005 /; I /; D /; Z /) /, evidenc e is de/\fne d as a function E thatmaps e ach variable V in V into the set of values I /( V /) /[ f/\u0005g /.When a v ariable V is mapp ed in to v /2 I /( V /)/, then evidence tells us that V is instan tiated tov alue v /. When V is mapp ed in to /\u0005 /, then evidence do es not tell us an ything ab out the v alueof V /.W e can no w state formally ho w to ev aluate a Q/-D A G giv en some evidence/. But /\frst w eneed some more notation/:/1/. Numeric/-No de/: n /( p /) denotes a no de lab eled with a n um b er p /2 /[/0 /; /1/]/;/2/. ESN/: n /( V /; v /) denotes a no de lab eled with /( V /; v /)/;/3/. This is also useful in cases where a v ariable will b e measured only if its v alue of information justi/\festhat/./1/5/3D ar wiche /& Pr o v an/3/. Op er ation/-No de/: n/1\n/\n /: /: /: /\n ni\ndenotes a no de lab eled with /\u0003 and ha ving paren tsn/1\n/; /: /: /: /; ni\n/;/4/. Op er ation/-No de/: n/1\n/\b /: /: /: /\b ni\ndenotes a no de lab eled with /+ and ha ving paren tsn/1\n/; /: /: /: /; ni\n/.The follo wing de/\fnition tells us ho w to ev aluate a Q/-D A G b y ev aluating eac h of its no des/.It is a recursiv e de/\fnition according to whic h the v alue assigned to a no de is a function ofthe v alues assigned to its paren ts/. The /\frst t w o cases are b oundary conditions/, assigningv alues to ro ot no des/. The last t w o cases are the recursiv e ones/.De/\fnition /3 F or a Q/-D A G /( V /; /\u0005 /; I /; D /; Z /) and evidenc e E /, the no de evaluator is de/\fne d asa function ME\nthat maps e ach no de in D into a numb er /[/0 /; /1/] such that/:/1/. ME\n/[ n /( p /)/] /= p/(The value of a no de lab ele d with a numb er is the numb er itself/./)/2/. ME\n/[ n /( V /; v /) /] /=\n/(/1 /; if E /( V /) /= v or E /( V /) /= /\u0005 /;/0 /; otherwise/(The value of an evidenc e/-sp e ci/\fc no de dep ends on the available evidenc e/: it is /1 if vis c onsistent with the evidenc e and /0 otherwise/./)/3/. ME\n/[ n/1\n/\n /: /: /: /\n ni\n/] /= ME\n/( n/1\n/) /\u0003 /: /: /: /\u0003 ME\n/( ni\n/)/(The value of a no de lab ele d with /\u0003 is the pr o duct of the values of its p ar ent no des/./)/4/. ME\n/[ n/1\n/\b /: /: /: /\b ni\n/] /= ME\n/( n/1\n/) /+ /: /: /: /+ ME\n/( ni\n/)/(The value of a no de lab ele d with /+ is the sum of the values of its p ar ent no des/./)One is t ypically not in terested in the v alues of all no des in a Q/-D A G since most of theseno des represen t in termediate results that are of no in terest to the user/. It is the query no desof a Q/-D A G that represen t answ ers to user queries and it is the v alues of these no des that oneseeks when constructing a Q/-D A G/. The v alues of these queries are captured b y the notionof a Q/-D A G output/.De/\fnition /4 The no de evaluator ME\nis extende d to Q/-D A Gs as fol lows/:ME\n/(/( V /; /\u0005 /; I /; D /; Z /) /) /= f /( n/; ME\n/( n /)/) j n /2 Z g /:The set ME\n/(/( V /; /\u0005 /; I /; D /; Z /)/) is c al le d the Q/-D A G output/.This output is what one seeks from a Q/-D A G/. Eac h elemen t in this output represen ts aprobabilistic query and its answ er/.Let us consider a few ev aluations of the Q/-D A G sho wn in Figure /4/, whic h are sho wn inFigure /5/. Giv en evidence E /( C /) /= ON /, and assuming that Qnode /( B /= ON /) and Qnode /( B /=OFF /) stand for the Q/-D A G no des lab eled Pr /( B /= ON /; c /) and Pr /( B /= OFF /; c /)/, resp ectiv ely /,w e ha v eME\n/[ n /( C /; ON /) /] /= /1 /;ME\n/[ n /( C /; OFF /) /] /= /0 /;ME\n/[ Qnode /( B /= ON /)/] /= /: /0/7/5 /\u0003 /( /: /9 /\u0003 /1 /+ /: /1 /\u0003 /0/) /+ /: /5/6 /\u0003 /(/1 /\u0003 /: /5 /+ /: /5 /\u0003 /0/) /= /: /3/4/7/5 /;ME\n/[ Qnode /( B /= OFF /)/] /= /( /: /9 /\u0003 /1 /+ /: /1 /\u0003 /0/) /\u0003 /: /2/2/5 /+ /(/1 /\u0003 /: /5 /+ /: /5 /\u0003 /0/) /\u0003 /: /1/4 /= /: /2/7/2/5 /;/1/5/4A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inferencemeaning that Pr /( B /= ON /; C /= ON /) /= /: /3/4/7/5 and Pr /( B /= OFF /; C /= ON /) /= /: /2/7/2/5/. If instead theevidence w ere E /( C /) /= OFF /, a set of analogous computations can b e done/.It is also p ossible that evidence tells us nothing ab out the v alue of v ariable C /, that is/,E /( C /) /= /\u0005 /. In this case/, w e w ould ha v eME\n/[ n /( C /; ON /) /] /= /1 /;ME\n/[ n /( C /; OFF /) /] /= /1 /;ME\n/[ Qnode /( B /= ON /)/] /= /: /0/7/5 /\u0003 /( /: /9 /\u0003 /1 /+ /: /1 /\u0003 /1/) /+ /: /5/6 /\u0003 /(/1 /\u0003 /: /5 /+ /: /5 /\u0003 /1/) /= /: /6/3/5 /;ME\n/[ Qnode /( B /= OFF /)/] /= /( /: /9 /\u0003 /1 /+ /: /1 /\u0003 /1/) /\u0003 /: /2/2/5 /+ /(/1 /\u0003 /: /5 /+ /: /5 /\u0003 /1/) /\u0003 /: /1/4 /= /: /3/6/5 /;meaning that Pr /( B /= ON /) /= /: /6/3/5 and Pr /( B /= OFF /) /= /: /3/6/5/./2/./1 Implemen ting a Q/-D A G Ev aluatorA Q/-D A G ev aluator can b e implemen ted using an ev en t/-driv en/, forw ard propagation sc heme/.Whenev er the v alue of a Q/-D A G no de c hanges/, one up dates the v alue of its c hildren/, and soon/, un til no p ossible up date of v alues is p ossible/. Another w a y to implemen t an ev aluatoris using a bac kw ard propagation sc heme where one starts from a query no de and up datesits v alue b y up dating the v alues of its paren t no des/. The sp eci/\fcs of the application willt ypically determine whic h metho d /(or com bination/) will b e more appropriate/.It is imp ortan t that w e stress the lev el of re/\fnemen t enjo y ed b y the Q/-D A G propaga/-tion sc heme and the implications of this on the e/\u000eciency of query up dates/. Propagation inQ/-D A Gs is done at the arithmetic/-op eration lev el/, whic h is con trasted with propagation atthe message/-op eration lev el /(used b y man y standard algorithms/)/. Suc h propagation sc hemesare t ypically optimized b y k eeping v alidit y /\rags of messages so that only in v alid messagesare recomputed when new evidence arriv es/. This will clearly a v oid some unnecessary com/-putations but can nev er a v oid all unnecessary computations b ecause a message is t ypicallyto o coarse for this purp ose/. F or example/, if only one en try in a message is in v alid/, thewhole message is considered in v alid/. Recomputing suc h a message will lead to man y un/-necessary computations/. This problem will b e a v oided in Q/-D A G propagation since v alidit y/\rags are attributed to arithmetic op erations/, whic h are the building blo c ks of message op er/-ations/. Therefore/, only the necessary arithmetic op erations will b e recomputed in a Q/-D A Gpropagation sc heme/, leading to a more detailed lev el of optimization/.W e also stress that the pro cess of ev aluating and up dating a Q/-D A G is done outside ofprobabilit y theory and b elief net w ork inference/. This mak es the dev elopmen t of e/\u000ecien t on/-line inference soft w are accessible to a larger group of p eople who ma y lac k strong bac kgroundsin these areas/.\n/4/2/./2 The Av ailabilit y of EvidenceThe construction of a Q/-D A G requires the iden ti/\fcation of query and evidence v ariables/. Thisma y giv e an incorrect impression that w e m ust kno w up fron t whic h v ariables are observ edand whic h are not/. This could b e problematic in /(/1/) applications where one ma y lose a sensorreading/, th us c hanging the status of a v ariable from b eing observ ed to b eing unobserv ed/;/4/. In fact/, it app ears that a bac kground in compiler theory ma y b e more relev an t to generating an e/\u000ecien tev aluator than a bac kground in b elief net w ork theory /./1/5/5D ar wiche /& Pr o v an\n.3truea Pr(a)\ntrue\nfalse.1\n.8a Pr(B=true|a)BA\n+ + + +\n.3 .2 .9 .1 (B,true) (B,false)* * * *\n.8 .7Pr(A=true,b) Pr(B=true,b) Pr(B=false,b) Pr(A=false,b)Figure /6/: A b elief net w ork and its corresp onding Q/-D A G in whic h v ariable B is declared tob e b oth query and evidence/.and /(/2/) applications where some v ariable ma y b e exp ensiv e to observ e/, leading to an on/-linedecision on whether to observ e it or not /(using some v alue/-of/-informat ion computation/)/.Both of these situations can b e dealt with in a Q/-D A G framew ork /. First/, as w e men tionedearlier/, Q/-D A Gs allo w us to handle missing evidence through the use of the /\u0005 notation whic hdenotes an unkno wn v alue of a v ariable/. Therefore/, Q/-D A Gs can handle missing sensorreadings/. Second/, a v ariable can b e declared to b e b oth query and evidence/. This meansthat w e can incorp orate evidence ab out this v ariable when it is a v ailable/, and also computethe probabilit y distribution of the v ariable in case evidence is not a v ailable/. Figure /6 depicts aQ/-D A G in whic h v ariable A is declared to b e a query v ariable/, while v ariable B is declared tob e b oth an evidence and a query v ariable /(b oth v ariables ha v e true and false as their v alues/)/.In this case/, w e ha v e t w o ESNs for v ariable B and also t w o query no des /(see Figure /6/)/. ThisQ/-D A G can b e used in t w o w a ys/:/1/. T o compute the probabilit y distributions of v ariables A and B when no evidence isa v ailable ab out B /. Under this situation/, the v alues of n /( B /; true /) and n /( B /; false /) areset to /1/, and w e ha v ePr /( A /= true /) /= /: /3 /\u0003 /: /1 /+ /: /3 /\u0003 /: /9 /= /: /3Pr /( A /= false /) /= /: /8 /\u0003 /: /7 /+ /: /7 /\u0003 /: /2 /= /: /7Pr /( B /= true /) /= /: /3 /\u0003 /: /1 /+ /: /8 /\u0003 /: /7 /= /: /5/9/1/5/6A Pra ctical P aradigm f or Implementing Belief/-Netw ork InferencePr /( B /= false /) /= /: /3 /\u0003 /: /9 /+ /: /7 /\u0003 /: /2 /= /: /4/1/2/. T o compute the probabilit y of v ariable A when evidence is a v ailable ab out B /. F orexample/, supp ose that w e observ e B to b e false /. The v alue of n /( B /; true /) will then b eset to /0 and the v alue of n /( B /; false /) will b e set to /1/, and w e ha v ePr /( A /= true /; B /= false /) /= /: /3 /\u0003 /: /9 /= /: /2/7Pr /( A /= false /; B /= false /) /= /: /7 /\u0003 /: /2 /= /: /1/4The abilit y to declare a v ariable as b oth an evidence and a query v ariable seems to b eessen tial in applications where /(/1/) a decision ma y need to b e made on whether to collectevidence ab out some v ariable B /; and /(/2/) making the decision requires kno wing the probabilit ydistribution of v ariable B /. F or example/, supp ose that w e are using the follo wing form ula/(P earl/, /1/9/8/8/, P age /3/1/3/) to compute the utilit y of observing v ariable B /:Utility Of Observing /( B /) /=\nXb\nP r /( B /= b j e /) U /( B /= b /) /;where U /( B /= b /) is the utilit y for the decision mak er of /\fnding that v ariable B has v alue b /.Supp ose that U /( B /= true /) /= /$ /2 /: /5 and U /( B /= false /) /= /BnZr /$/3/. W e can use the Q/-D A G tocompute the probabilit y distribution of B and use it to ev aluate Utility Of Observing /( B /)/:Utility Of Observing /( B /) /= /(/$ /2 /: /5 /\u0003 /: /5/9/) /+ /( /BnZr /$ /3 /\u0003 /: /4/1/) /= /$/0 /: /2/4 /;whic h leads us to observ e v ariable B /. Observing B /, w e /\fnd that its v alue is false /. W e canthen accommo dat e this evidence in to the Q/-D A G and con tin ue with our analysis/./3/. Generating Query D A GsThis section sho ws ho w Q/-D A Gs can b e generated using traditional algorithms for exactb elief/-net w ork inference/. In particular/, w e will sho w ho w Q/-D A Gs can b e generated using theclustering /(join tree/, Jensen/, LS/) algorithm /(Jensen/, Lauritzen/, /& Olesen/, /1/9/9/0/; Shac h ter/,Andersen/, /& Szolo vits/, /1/9/9/4/; Sheno y /& Shafer/, /1/9/8/6/)/, the p olytree algorithm/, and cutsetconditioning /(P earl/, /1/9/8/8/; P eot /& Shac h ter/, /1/9/9/1/)/. W e will also outline prop erties that m ustb e satis/\fed b y other b elief net w ork algorithms in order to adapt them for generating Q/-D A Gsas w e prop ose/./3/./1 The Clustering AlgorithmW e pro vide a sk etc h of the clustering algorithm in this section/. Readers in terested in moredetails are referred to /(Shac h ter et al/./, /1/9/9/4/; Jensen et al/./, /1/9/9/0/; Sheno y /& Shafer/, /1/9/8/6/)/.According to the clustering metho d/, w e start b y/:/1/. constructing a join tree of the giv en b elief net w ork/;\n/5/5/. A join tree is a tree of clusters that satis/\fes the follo wing prop ert y/: the in tersection of an y t w o clustersb elongs to all clusters on the path connecting them/./1/5/7D ar wiche /& Pr o v an/2/. assigning the matrix of eac h v ariable in the b elief net w ork to some cluster that con tainsthe v ariable/'s family /.The join tree is a secondary structure on whic h the inference algorithm op erates/. W e needthe follo wing notation to state this algorithm/:/- S/1\n/; /: /: /: /; Sn\nare the clusters/, where eac h cluster corresp onds to a set of v ariables in theoriginal b elief net w ork/./- /\ti\nis the p otential function o v er cluster Si\n/, whic h is a mapping from instan tiations ofv ariables in Si\nin to real n um b ers/./- Pi\nis the p osterior pr ob ability distribution o v er cluster Si\n/, whic h is a mapping frominstan tiations of v ariables in Si\nin to real n um b ers/./- Mij\nis the message sen t from cluster Si\nto cluster Sj\n/, whic h is a mapping from instan/-tiations of v ariables in Si\n/\\ Sj\nin to real n um b ers/./- e is the giv en evidence/, that is/, an instan tiation of evidence v ariables E /.W e also assume the standard m ultiplication and marginalization op erations on p oten tials/.Our goal no w is to compute the p oten tial Pr /( X /; e /) whic h maps eac h instan tiation x ofv ariable X in the b elief net w ork in to the probabilit y Pr /( x/; e /)/. Giv en this notation/, w e canstate the algorithm as follo ws/:/\u000f P oten tial functions are initialized using/\ti\n/=\nYX\nPrX\n/\u0015X\n/;where/{ X is a v ariable whose matrix is assigned to cluster Si\n/;/{ PrX\nis the matrix for v ariable X /: a mapping from instan tiations of the family ofX in to conditional probabilities/; and/{ /\u0015X\nis the lik eliho o d v ector for v ariable X /: /\u0015X\n/( x /) is /1 if x is consisten t with giv enevidence e and /0 otherwise/./\u000f P osterior distributions are computed usingPi\n/= /\ti\nYk\nMk i\n/;where Sk\nare the clusters adjacen t to cluster Si\n/./\u000f Messages are computed usingMij\n/=\nXSi\nn Sj\n/\ti\nYk /6/= j\nMk i\n/;where Sk\nare the clusters adjacen t to cluster Si\n/./1/5/8A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference/\u000f The p oten tial Pr /( X /; e /) is computed usingPr /( X /; e /) /=\nXSi\nnf X g\nPi\n/;where Si\nis a cluster to whic h X b elongs/.These equations are used as follo ws/. T o compute the probabilit y of a v ariable/, w e m ustcompute the p osterior distribution of a cluster con taining the v ariable/. T o compute thep osterior distribution of a cluster/, w e collect messages from neigh b oring clusters/. A messagefrom cluster Si\nto Sj\nis computed b y collecting messages from all clusters adjacen t to Siexcept for Sj\n/.This statemen t of the join tree algorithm is appropriate for situations where the evidenceis not c hanging frequen tly since it in v olv es computing initial p oten tials eac h time the evidencec hanges/. This is not necessary in general and one can pro vide more optimized v ersions of thealgorithm/. This issue/, ho w ev er/, is irrelev an t in the con text of generating Q/-D A Gs b ecauseup dating probabilities in face of evidence c hanges will tak e place at the Q/-D A G lev el/, whic hincludes its o wn optimization tec hnique that w e discuss later/./3/./2 Generating Q/-D A GsT o generate Q/-D A Gs using the clustering metho d/, w e ha v e to go through t w o steps/. First/,w e ha v e to mo dify the initialization of p oten tial functions so that the join tree is quan ti/\fedusing Q/-D A G no des instead of n umeric probabilities/. Second/, w e ha v e to replace n umericaddition and m ultiplication in the algorithm b y analogous functions that op erate on Q/-D A Gno des/. In particular/:/1/. Numeric m ultiplication /\u0003 is replaced b y an op eration /\n that tak es Q/-D A G no desn/1\n/; /: /: /: /; ni\nas argumen ts/, constructs and returns a new no de n with lab el /\u0003 and paren tsn/1\n/; /: /: /: /; ni\n/./2/. Numeric addition /+ is replaced b y an op eration /\b that tak es Q/-D A G no des n/1\n/; /: /: /: /; nias argumen ts/, constructs and returns a new no de n with lab el /+ and paren ts n/1\n/; /: /: /: /; ni\n/.Therefore/, instead of n umeric op erations/, w e ha v e Q/-D A G/-no de constructors/. And insteadof returning a n um b er as a computation result/, w e no w return a Q/-D A G no de/.Before w e state the Q/-D A G clustering algorithm/, realize that w e no w do not ha v e evidencee /, but instead w e ha v e a set of evidence v ariables E for whic h w e will collect evidence/.Therefore/, the Q/-D A G algorithm will not compute an answ er to a query Pr /( x/; e /)/, but insteadwill compute a Q/-D A G no de that ev aluates to Pr /( x/; e /) under the instan tiation e of v ariablesE /.In the follo wing equations/, p oten tials are mappings from v ariable instan tiations to Q/-D A G no des /(instead of n um b ers/)/. F or example/, the matrix for v ariable X will map eac hinstan tiation of X /'s family in to a Q/-D A G no de n /( p /) instead of mapping it in to the n um b erp /. The Q/-D A G op erations /\n and /\b are extended to op erate on these new p oten tials in thesame w a y that /\u0003 and /+ are extended in the clustering algorithm/.The new set of equations is/:/1/5/9D ar wiche /& Pr o v an/\u000f P oten tial functions are initialized using/\ti\n/=\nOX\nn /( PrX\n/) /\nOE\nn /( /\u0015E\n/) /;where/{ X is a v ariable whose matrix is assigned to cluster Si\n/;/{ n /( PrX\n/) is the Q/-D A G matrix for X /: a mapping from instan tiations of X /'s familyin to Q/-D A G no des represen ting conditional probabilities/;/{ E is an evidence v ariable whose matrix is assigned to cluster Si\n/; and/{ n /( /\u0015E\n/) is the Q/-D A G lik eliho o d v ector of v ariable E /: n /( /\u0015E\n/)/( e /) /= n /( E /; e /) /, whic hmeans that no de n /( /\u0015E\n/)/( e /) ev aluates to /1 if e is consisten t with giv en evidenceand /0 otherwise/./\u000f P osterior distributions are computed usingPi\n/= /\ti\nOk\nMk i\n/;where Sk\nare the clusters adjacen t to cluster Si\n/./\u000f Messages are computed usingMij\n/=\nMSi\nn Sj\n/\ti\nOk /6/= j\nMk i\n/;where Sk\nare the clusters adjacen t to cluster Si\n/./\u000f The Q/-D A G no des for answ ering queries of the form Pr /( x/; e /) are computed usingQnode /( X /) /=\nMSi\nnf X g\nPi\n/;where Si\nis a cluster to whic h X b elongs/.Here Qnode /( X /) is a p oten tial that maps eac h instan tiation x of v ariable X in to the Q/-D A Gno de Qnode /( X /)/( x /) whic h ev aluates to Pr /( x/; e /) for an y giv en instan tiation e of v ariables E /.Hence/, the only mo di/\fcations w e made to the clustering algorithm are /(a/) c hangingthe initialization of p oten tial functions and /(b/) replacing m ultiplication and addition withQ/-D A G constructors of m ultiplication and addition no des/./3/./3 An ExampleW e no w sho w ho w the prop osed Q/-D A G algorithm can b e used to generate a Q/-D A G forthe b elief net w ork in Figure /4/(a/)/.W e ha v e only one evidence v ariable in this example/, C /. And w e are in terested in gener/-ating a Q/-D A G for answ ering queries ab out v ariable B /, that is/, queries of the form Pr /( b/; e /)/.Figure /7/(a/) sho ws the join tree for the b elief net w ork in Figure /4/(a/)/, where the tables con tainthe p oten tial functions needed for the probabilistic clustering algorithm/. Figure /7/(b/) sho ws/1/6/0A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference\nAC AB AS2 1S\n\u03a81\n\u03a82.25 * .3\n.8 * .7.75 * .3\n.2 * .7B=OFF B=ONA\nON\nOFFA\nOFFON .9 \n.5 .1\n.5 C=OFF C=ONAC AB AS2 1S\n\u03a82\u03a81\nB=OFF B=ONA\nON\nOFFAC=ON C=OFF\nOFFON n(C,OFF)\nn(C,OFF)n(.5) n(.1)\nn(C,ON)n(.5) n(C,ON)n(.9) \nn(.075)\nn(.56) n(.14)n(.225)\n(a) (b)Figure /7/: A join tree quan ti/\fed with n um b ers /(a/)/, and with Q/-D A G no des /(b/)/.the join tree again/, but the tables con tain the p oten tial functions needed b y the Q/-D A Gclustering algorithm/. Note that the tables are /\flled with Q/-D A Gs instead of n um b ers/.W e no w apply the Q/-D A G algorithm/. T o compute the Q/-D A G no des that will ev aluateto Pr /( b/; e /)/, w e m ust compute the p osterior distribution P/2\no v er cluster S/2\nsince this is acluster to whic h v ariable B b elongs/. W e can then sum the distribution o v er v ariable A toobtain what w e w an t/. T o compute the distribution P/2\nw e m ust /\frst compute the messageM/1/2\nfrom cluster S/1\nto cluster S/2\n/.The message M/1/2\nis computed b y summing the p oten tial function /\t/1\nof cluster S/1\no v erall p ossible v alues of v ariable C /, i/.e/./, M/1/2\n/=\nMC\n/\t/1\n/; whic h leads to/:M/1/2\n/( A /= ON /) /= /[ n /( /: /9/) /\n n /( C /; ON /) /] /\b /[ n /( /: /1/) /\n n /( C /; OFF /)/] /;M/1/2\n/( A /= OFF /) /= /[ n /( /: /5/) /\n n /( C /; ON /) /] /\b /[ n /( /: /5/) /\n n /( C /; OFF /)/] /:The p osterior distribution o v er cluster S/2\n/, P/2\n/, is computed using P/2\n/= /\t/2\n/\n M/1/2\n/; whic hleads toP/2\n/( A /= ON /; B /= ON /) /= n /( /: /0/7/5/) /\n /[/[ n /( /: /9/) /\n n /( C /; ON /) /] /\b /[ n /( /: /1/) /\n n /( C /; OFF /)/]/]P/2\n/( A /= ON /; B /= OFF /) /= n /( /: /2/2/5/) /\n /[/[ n /( /: /9/) /\n n /( C /; ON /) /] /\b /[ n /( /: /1/) /\n n /( C /; OFF /)/]/]P/2\n/( A /= OFF /; B /= ON /) /= n /( /: /5/6/) /\n /[/[ n /( /: /5/) /\n n /( C /; ON /)/] /\b /[ n /( /: /5/) /\n n /( C /; OFF /) /]/]P/2\n/( A /= OFF /; B /= OFF /) /= n /( /: /1/4/) /\n /[/[ n /( /: /5/) /\n n /( C /; ON /)/] /\b /[ n /( /: /5/) /\n n /( C /; OFF /) /]/] /:The Q/-D A G no de Qnode /( b /) for answ ering queries of the form Pr /( b/; e /) is computed b ysumming the p osterior P/2\no v er v ariable A /, Qnode /=\nMS/2\nnf B g\nP/2\n/; leading toQnode /( B /= ON /) /= /[ n /( /: /0/7/5/) /\n /[/[ n /( /: /9/) /\n n /( C /; ON /) /] /\b /[ n /( /: /1/) /\n n /( C /; OFF /)/]/]/] /\b/[ n /( /: /5/6/) /\n /[/[ n /( /: /5/) /\n n /( C /; ON /)/] /\b /[ n /( /: /5/) /\n n /( C /; OFF /) /]/]/]Qnode /( B /= OFF /) /= /[ n /( /: /2/2/5/) /\n /[/[ n /( /: /9/) /\n n /( C /; ON /) /] /\b /[ n /( /: /1/) /\n n /( C /; OFF /)/]/]/] /\b/[ n /( /: /1/4/) /\n /[/[ n /( /: /5/) /\n n /( C /; ON /)/] /\b /[ n /( /: /5/) /\n n /( C /; OFF /) /]/]/] /;/1/6/1D ar wiche /& Pr o v anwhic h is the Q/-D A G depicted in Figure /4/(b/)/. Therefore/, the result of applying the algorithmis t w o Q/-D A G no des/, one will ev aluate to Pr /( B /= ON /; e /) and the other will ev aluate toPr /( B /= OFF /; e /) under an y instan tiation e of evidence v ariables E /./3/./4 Computational Complexit y of Q/-D A G GenerationThe computational complexit y of the algorithm for generating Q/-D A Gs is determined b ythe computational complexit y of the clustering algorithm/. In particular/, the prop osed al/-gorithm applies a /\b /-op eration precisely when the clustering algorithm applies an addition/-op eration/. Similarly /, it applies a /\n /-op eration precisely when the clustering algorithm appliesa m ultiplication/-op eration/. Therefore/, if w e assume that /\b and /\n tak e constan t time/, thenb oth algorithms ha v e the same time complexit y /.Eac h application of /\b or /\n ends up adding a new no de to the Q/-D A G/. And this is theonly w a y a new no de can b e added to the Q/-D A G/. Moreo v er/, the n um b er of paren ts of eac hadded no de is equal to the n um b er of argumen ts that the corresp onding arithmetic op erationis in v ok ed on in the clustering algorithm/. Therefore/, the space complexit y of a Q/-D A G isthe same as the time complexit y of the clustering algorithm/.In particular/, this means that the space complexit y of Q/-D A Gs in terms of the n um b erof evidence v ariables is the same as the time complexit y of the clustering algorithm in thoseterms/. Moreo v er/, eac h evidence v ariable E will add only m evidence/-sp eci/\fc no des to theQ/-D A G/, where m is the n um b er of v alues that v ariable E can tak e/. This is imp ortan t tostress b ecause without this complexit y guaran tee it ma y b e hard to distinguish b et w een theprop osed approac h and a brute/-force approac h that builds a big table con taining all p ossibleinstan tiations of evidence v ariables together with their corresp onding distributions of queryv ariables/./3/./5 Other Generation AlgorithmsThe p olytree algorithm is a sp ecial case of the clustering algorithm as sho wn in /(Shac h teret al/./, /1/9/9/4/)/. Therefore/, the p olytree algorithm can also b e mo di/\fed as suggested ab o v eto compute Q/-D A Gs/. This also means that cutset conditioning can b e easily mo di/\fed tocompute Q/-D A Gs/: for eac h instan tiation c of the cutset C /, w e compute a Q/-D A G no de forPr /( x/; c /; e /) using the p olytree algorithm and then tak e the /\b /-sum of the resulting no des/.Most algorithms for exact inference in b elief net w orks can b e adapted to generate Q/-D A Gs/. In general/, an algorithm m ust satisfy a k ey condition to b e adaptable for computingQ/-D A Gs as w e suggested ab o v e/. The condition is that the b eha vior of the algorithm shouldnev er dep end on the sp eci/\fc evidence obtained/, but should only dep end on the v ariablesab out whic h evidence is collected/. That is/, whether v ariable E is instan tiated to v alue v/1or v alue v/2\nshould not a/\u000bect the complexit y of the algorithm/. Only whether v ariable E isinstan tiated or not should matter/.Most b elief net w orks algorithms that w e are a w are of satisfy this prop ert y /. The reasonfor this seems to b e the notion of probabilistic indep endence on whic h these algorithmsare based/. Sp eci/\fcally /, what is read from the top ology of a b elief net w ork is a relationI /( X /; Z /; Y /)/, stating that v ariables X and Y are indep enden t giv en v ariables Z /. That is/,Pr /( x /; y j z /) /= Pr /( x j z /) Pr /( y j z /)/1/6/2A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inferencefor all instan tiations x /; y /; z of these v ariables/. It is p ossible/, ho w ev er/, for this not to holdfor all instan tiations of z but only for sp eci/\fc ones/. Most standard algorithms w e are a w areof do not tak e adv an tag e of this instan tiation/{sp eci/\fc notion of indep endence/.\n/6Therefore/,they cannot attac h an y computational signi/\fcance to the sp eci/\fc v alue to whic h a v ariableis instan tiated/. This prop ert y of existing algorithms is what mak es them easily adaptable tothe generation of Q/-D A Gs/./3/./6 Soundness of the Q/-D A G Clustering AlgorithmThe soundness of the prop osed algorithm is stated b elo w/. The pro of is giv en in App endix A/.Theorem /1 Supp ose that Qnode /( X /) is a Q/-D A G p otential gener ate d by the Q/-D A G clus/-tering algorithm for query variable X and evidenc e variables E /. L et e\n/0b e an instantiationof some variables in E /, and let Q/-D A G evidenc e E b e de/\fne d as fol lows/:E /( E /) /=\n/(e/; if evidenc e e\n/0sets variable E to value e /;/\u0005 /; otherwise/.thenME\n/( Qnode /( X /)/( x /)/) /= Pr /( x/; e\n/0/) /:That is/, the theorem guaran tees that the Q/-D A G no des generated b y the algorithm willalw a ys ev aluate to their corresp onding probabilities under an y partial or full instan tiationof evidence v ariables/./4/. Reducing Query D A GsThis section is fo cused on reducing Q/-D A Gs after they ha v e b een generated/. The mainmotiv ation b ehind this reduction is t w ofold/: faster ev aluation of Q/-D A Gs and less space tostore them/. In terestingly enough/, w e ha v e observ ed that a few/, simple reduction tec hniquestend in certain cases to subsume optimization tec hniques that ha v e b een in/\ruen tial in prac/-tical implemen tations of b elief/-net w ork inference/. Therefore/, reducing Q/-D A Gs can b e v eryimp ortan t practically /.This section is structured as follo ws/. First/, w e start b y discussing four simple reductionop erations in the form of rewrite rules/. W e then sho w examples in whic h these reductions sub/-sume t w o k ey optimization tec hniques kno wn as net w ork/-pruning and computation/-cac hing/./4/./1 ReductionsThe goal of Q/-D A G reduction is to reduce the size of a Q/-D A G while main taining thearithmetic expression it represen ts/. In describing the equiv alence of arithmetic expressions/,w e de/\fne the notion of Q/-D A G equiv alence/:De/\fnition /5 Two Q/-D A Gs ar e e quivalent i/\u000b they have the same set of evidenc e/-sp e ci/\fcno des and they have the same output for al l p ossible Q/-D A G evidenc e/./6/. Some algorithms for t w o/{lev el binary net w orks /(BN/2/0 net w orks/)/, and some v ersions of the SPI algorithmdo tak e adv an tage of these indep endences/./1/6/3D ar wiche /& Pr o v an\n. .\n.\n.Q1\nQ2Q3Q1.\nQ3\n*\nQ1\nQ2Q3\nb) numeric\n     reductionQ2Q3**\nQ1 Q1Q2IQ p q\nQ p q.\n     mergingc) associative (a) Identityd) commutative\n    merging+\n+\n     eliminationFigure /8/: The four main metho ds for Q/-D A G reduction/.Figure /8 sho ws four basic reduction op erations that w e ha v e exp erimen ted with/:/1/. Identity elimination/: eliminates a n umeric no de if it is an iden tit y elemen t of its c hildop eration no de/./2/. Numeric r e duction/: replaces an op eration no de with a n umeric no de if all its paren tsare n umeric no des/./3/. Asso ciative mer ging/: eliminates an op eration no de using op eration asso ciativit y /./4/. Commutative mer ging/: eliminates an op eration no de using op eration comm utativit y /.These rules can b e applied successiv ely and in di/\u000beren t order un til no more applications arep ossible/.W e ha v e pro v en that these op erations are sound in /(Darwic he /& Pro v an/, /1/9/9/5/)/. Basedon an analysis of net w ork structure and preliminary empirical results/, w e ha v e observ edthat man y factors go v ern the e/\u000bectiv es of these op erations/. The degree to whic h reductionop erations/, n umeric reduction in particular/, can reduce the size of the Q/-D A G dep ends onthe top ology of the giv en b elief net w ork and the set of evidence and query v ariables/. F orexample/, if all ro ot no des are evidence v ariables of the b elief net w ork/, and if all leaf no desare query v ariables/, then n umeric reduction will lead to little Q/-D A G reduction/.W e no w fo cus on n umeric reduction/, sho wing ho w it sometimes subsumes t w o optimiza/-tion tec hniques that ha v e b een in/\ruen tial in b elief net w ork algorithms/. F or b oth optimiza/-tions/, w e sho w examples where an unoptimized algorithm that emplo ys n umeric reductionyields the same Q/-D A G as an optimized algorithm/. The ma jor implication is that opti/-mizations can b e done uniformly at the Q/-D A G lev el/, freeing the underlying b elief net w orkalgorithms from suc h implemen tational complications/.The follo wing examples assume that w e are applying the p olytree algorithm to singly/-connected net w orks/./1/6/4A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference\nCBA.6\n.9\n.5\n.8\n.3.6\n.9\n.5BAONON\nOFFONOFFON\nON\nOFFa P(a)\na P(B=ON|a)\nbP(C=ON|b)P(a)a\nP(B=ON|a)a\n(a) (b)Figure /9/: A simple b elief net w ork b efore pruning /(a/) and after pruning /(b/)/. The ligh t/-shadedno de/, A /, is a query no de/, and the dark/-shaded no de/, B /, is an evidence no de/.\n*\n* *\n+*\n(a) Original Q-DAG (b) Reduced Q-DAG*P(A=ON,B=b)\n+\n*\n.1 .9 +\n.7 .3.1P(A=ON,B=b)\n+\n.2 .8.9 (B,OFF) (B,ON) (B,OFF) (B,ON).6 .6Figure /1/0/: A Q/-D A G /(a/) and its reduction /(b/)/./4/./2 Net w ork PruningPruning is the pro cess of deleting irrelev an t parts of a b elief net w ork b efore in v oking infer/-ence/. Consider the net w ork in Figure /9/(a/) for an example/, where B is an evidence v ariableand A is a query v ariable/. One can prune no de C from the net w ork/, leading to the net w orkin Figure /9/(b/)/. An y query of the form Pr /( a j b /) has the same v alue with resp ect to eithernet w ork/. It should b e clear that w orking with the smaller net w ork is preferred/. In general/,pruning can lead to dramatic sa vings since it can reduce a m ultiply/-connected net w ork to asingly/-connected one/./1/6/5D ar wiche /& Pr o v anIf w e generate a Q/-D A G for the net w ork in Figure /9/(a/) using the p olytree algorithm/, w eobtain the one in Figure /1/0/(a/)/. This Q/-D A G corresp onds to the follo wing expression/,Pr /( A /= ON /; e /) /= Pr /( A /= ON /)\nXb\n/\u0015B\n/( b /) Pr /( b j A /= ON /)\nXc\nPr /( c j b /) /:If w e generate a Q/-D A G for the net w ork in Figure /9/(b/)/, ho w ev er/, w e obtain the one inFigure /1/0/(b/) whic h corresp onds to the follo wing expression/,Pr /( A /= ON /; e /) /= Pr /( A /= ON /)\nXb\n/\u0015B\n/( b /) Pr /( b j A /= ON /) /:As exp ected/, this Q/-D A G is smaller than the Q/-D A G in Figure /1/0/(a/)/, and con tains a subsetof the no des in Figure /1/0/(a/)/.The k ey observ ation/, ho w ev er/, is that the optimized Q/-D A G in Figure /1/0/(b/) can b eobtained from the unoptimized one in Figure /1/0/(a/) using Q/-D A G reduction/. In particular/,the no des enclosed in dotted lines can b e collapsed using n umeric reduction in to a singleno de with v alue /1/. Iden tit y elimination can then remo v e the resulting no de/, leading to theoptimized Q/-D A G in Figure /1/0/(b/)/.The more general observ ation/, ho w ev er/, is that prunable no des con tribute iden tit y el/-emen ts when computing answ ers to queries/. These con tributions app ear as Q/-D A G no desthat ev aluate to iden tit y elemen ts under all instan tiations of evidence/. Suc h no des can b eeasily detected and collapsed in to these iden tit y elemen ts using n umeric reduction/. Iden tit yelimination can then remo v e them from the Q/-D A G/, leading to the same e/\u000bect as net w orkpruning/.\n/7Whether Q/-D A G reduction can replace all p ossible pruning op erations is an op enquestion that is outside the scop e of this pap er/./4/./3 Computation Cac hingCac hing computations is another in/\ruen tial tec hnique for optimizing inference in b elief net/-w orks/. T o consider an example/, supp ose that w e are applying the p olytree algorithm tocompute Pr /( c/; b /) in the net w ork of Figure /1/1/. Giv en evidence/, sa y B /= ON /, the algorithmwill compute Pr /( c/; B /= ON /) b y passing the messages sho wn in Figure /1/2/. If the evidencec hanges to B /= OFF /, ho w ev er/, an algorithm emplo ying cac hing will not recompute the mes/-sage /\u0019B\n/( a /) /(whic h represen ts the causal supp ort from A to B /(P earl/, /1/9/8/8/)/) since the v alue ofthis message do es not dep end on the evidence on B /.\n/8This kind of optimization is t ypically/7/. Note/, ho w ev er/, that Q/-D A G reduction will not reduce the computational complexit y of generating a Q/-D A G/, although net w ork pruning ma y /. F or example/, a m ultiply/{con nected net w ork ma y b ecome singly/-connected after pruning/, thereb y /, reducing the complexit y of generating a Q/-D A G/. But using Q/-D A Greduction/, w e still ha v e to generate a Q/-D A G b y w orking with a m ultiply/-connected net w ork/./8/. This can b e seen b y considering the follo wing expression/, whic h is ev aluated incremen tally b y the p olytreealgorithm through its message passes/:Pr /( c/; e /) /=\nXb\nPr /( c j b /) /\u0015B\n/( b /)\nXa\nPr /( b j a /) Pr /( a /)/| /{z /}/\u0019B\n/( a /)/| /{z /}/\u0019C\n/( b /)\n/:It is clear that the sub expression corresp onding to the message /\u0019B\n/( a /) from A to B is indep enden t of theevidence on B /./1/6/6A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference\nB\nCA.6\n.9\n.5\n.8\n.3ON\nON\nOFF\nON\nOFFa Pr(a)\na Pr(B=ON|a)\nPr(C=ON|b)bFigure /1/1/: A simple b elief net w ork for demonstrating the relationship b et w een Q/-D A G re/-duction and computation cac hing/. The ligh t/-shaded no de/, C /, is a query no de/,and the dark/-shaded no de/, B /, is an evidence no de/.\nB\n(b)\u03c0C\nCA\n(a)\u03c0BFigure /1/2/: Message passing when C is queried and B is observ ed/.implemen ted b y cac hing the v alues of messages and b y k eeping trac k of whic h messages area/\u000bected b y what evidence/.No w/, consider the Q/-D A G corresp onding to this problem whic h is sho wn in Figure /1/3/(a/)/.The no des enclosed in dotted lines corresp ond to the message from A to B /.\n/9These no des donot ha v e evidence/-sp eci/\fc no des in their ancestor set and/, therefore/, can nev er c hange v aluesdue to evidence c hanges/. In fact/, n umeric reduction will replace eac h one of these no des andits ancestors with a single no de as sho wn in Figure /1/3/(b/)/.In general/, if n umeric reduction is applied to a Q/-D A G/, one is guaran teed the follo wing/:/(a/) if a Q/-D A G no de represen ts a message that do es not dep end on evidence/, that no de willnot b e re/-ev aluated giv en evidence c hanges/; and /(b/) n umeric reduction will guaran tee this/9/. More precisely /, they corresp ond to the expression\nPa\nPr /( b j a /) Pr /( a /)/./1/6/7D ar wiche /& Pr o v an\n*\n*+.8+\n.3+\n**\n.74*\n* *+\n  .1.26* *\n*\n.6\n.6.3**\n.8\ncached valueP(C=ON,B=b)P(C=ON,B=b)\n(a) Original Q-DAG (b) Reduced Q-DAG(B,OFF)(B,ON) (B,ON) (B,OFF)\n.4 .5\n.5.9\n.4Figure /1/3/: A Q/-D A G /(a/) and its reduction /(b/)/.under an y Q/-D A G ev aluation metho d since it will replace the no de and its ancestor set witha single ro ot no de/.\n/1/0/4/./4 Optimization in Belief/-Net w ork InferenceNet w ork pruning and computation cac hing ha v e pro v en to b e v ery in/\ruen tial in practicalimplemen tations of b elief/-net w ork inference/. In fact/, our o wn exp erience has sho wn thatthese optimizations t ypically mak e the di/\u000berence b et w een a usable and a non/-usable b elief/-net w ork system/.One problem with these optimizations/, ho w ev er/, is their algorithm/-sp eci/\fc implemen/-tations although they are based on general principles /(e/.g/./, taking adv an tag e of net w orktop ology/)/. Another problem is that they can mak e elegan t algorithms complicated and hardto understand/. Moreo v er/, these optimizations are often hard to de/\fne succinctly /, and henceare not w ell do cumen ted within the comm unit y /.In con trast/, b elief/{net w ork inference can b e optimized b y generating Q/-D A Gs using un/-optimized inference algorithms/, and then optimizing the generated Q/-D A G through reduc/-tion tec hniques/. W e ha v e sho wn some examples of this earlier with resp ect to pruning andcac hing optimizations/. Ho w ev er/, whether this alternate approac h to optimization is alw a ysfeasible is y et to b e kno wn/. A p ositiv e answ er will clearly pro vide an algorithm/{indep endent/1/0/. Note that Q/-D A Gs lead to a v ery re/\fned cac hing mec hanism if the Q/-D A G ev aluator /(/1/) cac hes the v alueof eac h Q/-D A G no de and /(/2/) up dates these cac hed v alues only when there is need to /(that is/, when thev alue of a paren t no de c hanges/)/. Suc h a re/\fned mec hanism allo ws cac hing the v alues of messages thatdep end on evidence as w ell/./1/6/8A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference\nalternatoroil-pressure\nsensoroil-pressurefuelsensorfuel\nalternator\nsensorbattery sensorbattery\nfaultFigure /1/4/: A simple b elief net w ork for car diagnosis/.appr o ach to optimizing b elief/{network infer enc e /, whic h is practically imp ortan t for at leastt w o reasons/. First/, Q/-D A G reduction tec hniques seem to b e m uc h simpler to understandand implemen t since they deal with graphically represen ted arithmetic expressions/, withoutha ving to in v ok e probabilit y or b elief net w ork theory /. Second/, reduction op erations are ap/-plicable to Q/-D A Gs generated b y an y b elief/{net w ork algorithm/. Therefore/, an optimizationapproac h based on Q/-D A G reduction w ould b e more systematic and accessible to a biggerclass of dev elop ers/./5/. A Diagnosis ExampleThis section con tains a comprehensiv e example illustrating the application of the Q/-D A Gframew ork to diagnostic reasoning/.Consider the car troublesho oting example depicted in Figure /1/4/. F or this simple casew e w an t to determine the probabilit y distribution for the fault no de/, giv en evidence on foursensors/: the battery/-/, alternator/-/, fuel/- and oil/-sensors/. Eac h sensor pro vides informationab out its corresp onding system/. The fault no de de/\fnes /\fv e p ossible faults/: normal/, clogged/-fuel/-injector/, dead/-battery /, short/-circuit/, and brok en/-fuel/-pump/.If w e denote the fault v ariable b y F /, and sensor v ariables b y E /, then w e w an t to builda system that can compute the probabilit y Pr /( f /; e /) /; for eac h fault f and an y evidence e /.These probabilities represen t an unnormalized probabilit y distribution o v er the fault v ariablegiv en sensor readings/. In a Q/-D A G framew ork/, realizing this diagnostic system in v olv es threesteps/: Q/-D A G generation/, reduction/, and ev aluation/. The /\frst t w o steps are accomplishedo/\u000b/-line/, while the /\fnal step is p erformed on/-line/. W e no w discuss eac h one of the steps inmore detail/./5/./1 Q/-D A G GenerationThe /\frst step is to generate the Q/-D A G/. This is accomplished b y applying the Q/-D A Gclustering algorithm with the fault as a query v ariable and the sensors as evidence v ari/-/1/6/9D ar wiche /& Pr o v an\nfuel\nsubtreefuel\nsubtreefuel\nsubtreesubtreefuel            P(F=pump,e)\nbattery\nalternatoralternatoroilP(F=normal,e)\nP(F=pump)\n   .05P(F=normal)\n   .90\n(normal) (normal)(normal) (normal)(pump) (pump) (pump)\nstructure-sharingfuelbatteryoil\n(pump)Figure /1/5/: A partial Q/-D A G for the car example/, displa ying t w o of the /\fv e query no des/,br oken fuel pump and normal /. The shaded regions are p ortions of the Q/-D A Gthat are shared b y m ultiple query no des/; the v alues of these no des are relev an tto the v alue of more than one query no de/.ables/. The resulting Q/-D A G has /\fv e query no des/, Qnode /( F /= normal /; e /)/, Qnode /( F /=clo gge d fuel inje ctor /; e /)/, Qnode /( F /= de ad b attery /; e /)/, Qnode /( F /= short cir cuit /; e /)/, andQnode /( F /= br oken fuel pump /; e /)/. Eac h no de ev aluates to the probabilit y of the corresp ond/-ing fault under an y instan tiation of evidence/. The probabilities constitute a di/\u000beren tialdiagnosis that tells us whic h fault is most probable giv en certain sensor v alues/.Figure /1/5 sho ws a st ylized description of the Q/-D A G restricted to t w o of the /\fv e queryno des/, corresp onding to Pr /( F /= br oken fuel pump /; e /) and Pr /( F /= normal /; e /)/. The Q/-D A Gstructure is symmetric for eac h fault v alue and sensor/.Giv en that the Q/-D A G is symmetric for these p ossible faults/, for clarit y of exp ositionw e lo ok at just the subset needed to ev aluate no de Pr /( F /= br oken fuel pump /; e /)/. Figure /1/6sho ws a st ylized v ersion of the Q/-D A G pro duced for this no de/. F ollo wing are some obser/-v ations ab out this Q/-D A G/. First/, there is an evidence/-sp eci/\fc no de for ev ery instan tiationof sensor v ariables/, corresp onding to all forms of sensor measuremen ts p ossible/. Second/, allother ro ots of the Q/-D A G are probabilities/. Third/, one of the /\fv e paren ts of the query no deP r /( F /= br oken fuel pump /; e /) is for the prior on F /= br oken fuel pump /, and the other fourare for the con tributions of the four sensors/. F or example/, Figure /1/6 highligh ts /(in dots/) thatpart of the Q/-D A G for computing the con tribution of the battery sensor/./5/./2 Q/-D A G ReductionAfter generating a Q/-D A G/, one pro ceeds b y reducing it using graph rewrite rules/. Figure /1/6sho ws an example of suc h reduction with a Q/-D A G that is restricted to one query no defor simplicit y /. T o giv e an idea of the kind of reduction that has b een applied/, consider thepartial Q/-D A G enclosed b y dots in this /\fgure/. Figure /1/7 compares this reduced Q/-D A G withthe unreduced one from whic h it w as generated/. Giv en our goal of generating Q/-D A Gs that/(a/) can b e ev aluated as e/\u000ecien tly as p ossible and /(b/) require minimal space to store/, it is/1/7/0A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference\nF-S   fuel-sensor\nB-S  battery-sensor\nA-S  alternator-sensor\nO-S  oil-sensorKEY\nempty)ESN(F-S, ESN(B-S, ESN(B-S, ESN(O-S, ESN(O-S, ESN(F-S, ESN(A-S,\nfull) dead) charged) not-OK)ESN(A-S,\nOK) low) normal).4 .6.36 .64 .45 .55 .27 .73P(F=pump)\n   .05P(F=pump,e)\n* *+*\n* *+\n* *+\n* *+Figure /1/6/: A partial Q/-D A G for the car example/.\n* *+\n++\n+\n**\nESN(B-S,\nP(B=charged|\nF=pump)F=pump,B=charged)**\nESN(B-S,\ncharged)\nF=pump)P(B-S=charged|**\nESN(B-S,\nF=pump)P(B-S=dead|\nF=pump,B=dead)P(B=dead|P(B-S=dead|\nF=pump,B=dead)P(B=dead|dead) dead)ESN(B-S, ESN(B-S,\ndead) charged).36 .64\n       .2 .8 .4 .6 .6 .4 .4 .6(a) Reduced Q-DAG\n(b) Original Q-DAG\n**\nESN(B-S,\ncharged)\nP(B=charged|\nF=pump)F=pump,B=charged)P(B-S=charged|Figure /1/7/: Reduced and unreduced Q/-D A Gs for the car diagnosis example/.imp ortan t to see/, ev en in a simple example/, ho w Q/-D A G reduction can mak e a big di/\u000berencein their size/./1/7/1D ar wiche /& Pr o v an/5/./3 Q/-D A G Ev aluationNo w that w e ha v e a reduced Q/-D A G/, w e can use it to compute answ ers to diagnostic queries/.This section presen ts examples of this ev aluation with resp ect to the generated Q/-D A G/.Supp ose that w e obtain the readings dead/, normal/, ok and full for the battery /, oil/,alternator and fuel sensors/, resp ectiv ely /. And let us compute the probabilit y distributiono v er the fault v ariable/. This obtained evidence is formalized as follo ws/:/- E /( b attery sensor /) /= de ad /,/- E /( oil sensor /) /= normal /,/- E /( alternator sensor /) /= ok /,/- E /( fuel sensor /) /= ful l /.Evidence/-sp eci/\fc no des can no w b e ev aluated according to De/\fnition /3/. F or example/, w eha v eME\n/[ n /( b attery sensor /; char ge d /)/] /= /0 /;andME\n/[ n /( b attery sensor /; de ad /)/] /= /1 /:The ev aluation of evidence/-sp eci/\fc no des is sho wn pictorially in Figure /1/8/(a/)/. De/\fnition /3can then b e used to ev aluate the remaining no des/: once the v alues of a no de/'s paren tsare kno wn/, the v alue of that no de can b e determined/. Figure /1/8/(b/) depicts the results ofev aluating other no des/. The result of in terest here is the probabilit y /0/./0/0/4/3/4 assigned to thequery no de Pr /( fault /= br oken fuel pump /; e /)/.Supp ose no w that evidence has c hanged so that the v alue of fuel sensor is empt y insteadof full/. T o up date the probabilit y assigned to no de Pr /( fault /= br oken fuel pump /; e /)/, a bruteforce metho d will re/-ev aluate the whole Q/-D A G/. Ho w ev er/, if a forw ard propagation sc hemeis used to implemen t the no de ev aluator/, then only four no des need to b e re/-ev aluated inFigure /1/8/(b/) /(those enclosed in circles/) instead of thirteen /(the total n um b er of no des/)/. W estress this p oin t b ecause this re/\fned up dating sc heme/, whic h is easy to implemen t in thisframew ork /, is m uc h harder to ac hiev e when one attempts to em b ed it in standard b elief/-net w ork algorithms based on message passing/./6/. Concluding RemarksW e ha v e in tro duced a new paradigm for implemen ting b elief/-net w ork inference that is ori/-en ted to w ards real/-w orld/, on/-line applications/. The prop osed framew ork utilizes kno wledgeof query and evidence v ariables in an application to compile a b elief net w ork in to an arith/-metic expression called a Query D A G /(Q/-D A G/)/. Eac h no de of a Q/-D A G represen ts a n umericop eration/, a n um b er/, or a sym b ol that dep ends on a v ailable evidence/. Eac h leaf no de of aQ/-D A G represen ts the answ er to a net w ork query /, that is/, the probabilit y of some ev en t ofin terest/. Inference on Q/-D A Gs is linear in their size and amoun ts to a standard ev aluationof the arithmetic expressions they represen t/.A most imp ortan t p oin t to stress ab out the w ork rep orted here is that it is not prop osinga new algorithm for b elief/-net w ork inference/. What w e are prop osing is a paradigm for/1/7/2A Pra ctical P aradigm f or Implementing Belief/-Netw ork Inference\nnot-OK)ESN(A-S, ESN(O-S,\nnormal)ESN(B-S,\ncharged)ESN(B-S,\ndead).6\n.6 0not-OK)ESN(A-S, ESN(O-S,\nlow)ESN(O-S,\nnormal)ESN(B-S,\ncharged)ESN(B-S,\ndead)ESN(A-S,\nOK)\nESN(F-S,\nempty) full)ESN(F-S,.4 .6 0 1 0Pr(F=pump,e)\n.36 .64 .45 .55 .27 .73 0 1 1 0 1*\nPr(F=pump)\n*+\n* *+\n* *+\n* *+.05\n.360 0 .55 0 .73.36 .55.73\n*.0043362\nESN(O-S,\nlow)ESN(A-S,\nOK)ESN(F-S,\nempty) full)ESN(F-S,ESN valuesPr(F=pump,e)\n.4 .6 .36 .64 .45 .55 .27 .73* *+\n* *+\n* *+\n* *+.05\n0 1 1 0 1 0 1 0*\nPr(F=pump)(a) Evaluating ESNs\nESN values(b) Propagating probabilitiesFigure /1/8/: Ev aluating the Q/-D A G for the car diagnosis example giv en evidence for sensors/.The bar in /(a/) indicates the instan tiation of the ESNs/. The shaded n um b ers in/(b/) indicate probabilit y v alues that are computed b y the no de ev aluator/. Thecircled op erations on the left/-hand/-side of /(b/) are the only ones that need to b eup dated if evidence for the fuel/-system sensor is altered/, as denoted b y the circledESNs/./1/7/3D ar wiche /& Pr o v animplemen ting b elief/-net w ork inference that is orthogona l to standard inference algorithmsand is engineered to meet the demands of real/-w orld/, on/-line applications/. This class ofapplications is t ypically demanding for the follo wing reasons/:/1/. It t ypically requires v ery short resp onse time/, i/.e/./, milliseconds/./2/. It requires soft w ar e to b e written in sp ecialized languages/, suc h as AD A/, C/+/+/, andassem bly b efore it can pass certi/\fcation pro cedures/./3/. It imp oses sev ere restrictions on the a v ailable soft w are and hardw are resources in orderto k eep the cost of a /\\unit/\" /(suc h as an electromec hanical device/) as lo w as p ossible/.T o address these real/-w orld constrain ts/, w e are prop osing that one compile a b elief net w orkin to a Q/-D A G as sho wn in Figure /3 on and use a Q/-D A G ev aluator for on/-line reasoning/. Thisbrings do wn the required memory to that needed for storing a Q/-D A G and its ev aluator/. Italso brings do wn the required soft w ar e to that needed for implemen ting a Q/-D A G ev aluator/,whic h is v ery simple as w e ha v e seen earlier/.Our prop osed approac h still requires a b elief/-net w ork algorithm to generate a Q/-D A G/,but it mak es the e/\u000eciency of suc h an algorithm less of a critical factor/.\n/1/1F or example/,w e sho w that some standard optimizations in b elief/-net w ork inference/, suc h as pruning andcac hing/, b ecome less critical in a Q/-D A G framew or k since these optimizations tend to b esubsumed b y simple Q/-D A G reduction tec hniques/, suc h as n umeric reduction/.The w ork rep orted in this pap er can b e extended in at least t w o w a ys/. First/, further Q/-D A G reduction tec hniques could b e explored/, some orien ted to w ards reducing the ev aluationtime of Q/-D A Gs/, others to w ards minimizing the memory needed to store them/. Second/, w eha v e sho wn that some optimization tec hniques that dramatically impro v e b elief/-net w orkalgorithms ma y b ecome irrelev an t to the size of Q/-D A Gs if Q/-D A G reduction is emplo y ed/.F urther in v estigation is needed to pro v e formal results and guaran tees on the e/\u000bectiv enessof Q/-D A G reduction/.W e close this section b y noting that the framew or k w e prop osed is also applicable toorder/-of/-mag nitude /(OMP/) b elief net w orks/, where m ultiplication and addition get replacedb y addition and minimization/, resp ectiv ely /(Goldszmidt/, /1/9/9/2/; Darwic he /& Goldszmidt/,/1/9/9/4/)/. The OMP Q/-D A G ev aluator/, ho w ev er/, is m uc h more e/\u000ecien t than its probabilisticcoun terpart since one ma y ev aluate a minimization no de without ha ving to ev aluate all itsparen ts in man y cases/. This can mak e considerable di/\u000berence in the p erformance of a Q/-D A Gev aluator/.Ac kno wledgemen tsMost of the w ork in this pap er w as carried out while the /\frst author w as at Ro c kw ell ScienceCen ter/. Sp ecial thanks to Jac k Breese/, Bruce D/'Am brosio and to the anon ymous review ersfor their useful commen ts on earlier drafts of this pap er/./1/1/. W e ha v e sho wn ho w clustering and conditionin g algorithms can b e used for Q/-D A G generation/, but otheralgorithms suc h as SPI /(Li /& D/'Am brosio/, /1/9/9/4/; Shac h ter et al/./, /1/9/9/0/) can b e used as w ell/./1/7/4A Pra ctical P aradigm f or Implementing Belief/-Netw ork InferenceApp endix A/. Pro of of Theorem /1Without loss of generalit y /, w e assume in this pro of that all v ariables are declared as evidencev ariables/. T o pro v e this soundness theorem/, all w e need to sho w is that eac h Q/-D A G p o/-ten tial will ev aluate to its corresp onding probabilistic p oten tial under all p ossible evidence/.F ormally /, for an y cluster S and v ariables X /, the matrices of whic h are assigned to S /, w eneed to sho w thatME\n/(\nOX\nn /( PrX\n/) /\n n /( /\u0015X\n/)/) /=\nYX\nPrX\n/\u0015X\n/(/1/)for a giv en evidence E /. Once w e establish this/, w e are guaran teed that Qnode /( X /)/( x /) willev aluate to the probabilit y Pr /( x/; e /) b ecause the application of /\n and /\b in the Q/-D A G algo/-rithm is isomorphic to the application of /\u0003 and /+ in the probabilistic algorithm/, resp ectiv ely /.T o pro v e Equation /1/, w e will extend the Q/-D A G no de ev aluator ME\nto mappings in thestandard w a y /. That is/, if f is a mapping from instan tiations to Q/-D A G no des/, then ME\n/( f /)is de/\fned as follo ws/:ME\n/( f /)/( x /) /=def\nME\n/( f /( x /)/) /:That is/, w e simply apply the Q/-D A G no de ev aluator to the range of mapping f /.Note that ME\n/( f /\n g /) will then b e equal to ME\n/( f /) ME\n/( g /)/. Therefore/,ME\n/(\nOX\nn /( PrX\n/) /\n n /( /\u0015X\n/)/)/=\nYX\nME\n/( n /( PrX\n/)/) ME\n/( n /( /\u0015X\n/)/)/=\nYX\nPrX\nME\n/( n /( /\u0015X\n/)/) b y de/\fnition of n /( PrX\n/) /:Note also that b y de/\fnition of n /( /\u0015X\n/)/, w e ha v e that n /( /\u0015X\n/)/( x /) equals n /( X /; x /) /. Therefore/,ME\n/( n /( /\u0015X\n/)/)/( x /) /= ME\n/( n /( /\u0015X\n/)/( x /)/)/= ME\n/( n /( X /; x /) /)/=\n/(/1 /; if E /( X /) /= x or E /( X /) /= /\u0005/0 /; otherwise/= /\u0015X\n/( x /) /:Therefore/,ME\n/(\nOX\nn /( PrX\n/) /\n n /( /\u0015X\n/)/) /=\nYX\nPrX\n/\u0015X\n/:"}
{"category": "abstract", "text": "W e describ e a new paradigm for implemen ting inference in b elief net w orks/, whic h con/-sists of t w o steps/"}
{"category": "non-abstract", "text": "Ob ject/-orien ted template structure of the join t v en tures domainextracted include man ufacturer/, distributor/, and user/, in addition to detailed man ufacturinginformation suc h as materials used and micro c hip sp eci/\fcations suc h as w afer size and devicesp eed/. The micro electronics template structure is similar to that of the join t v en tures buthas few er ob jects and slots/.Both of these extraction tasks m ust identify not only individual en tities but also certainrelationships among them/. Often/, ho w ev er/, a particular piece of extracted informationdescrib es only part of a relationship/. This partial information m ust b e merged with otherpieces of information referring to the same entities/. F or merging to pro duce correct results/,therefore/, correct identi/\fcation of en tit y references is crucial/./3/. Problem de/\fnitionThis section /\frst describ es w ord matc hing problems caused b y the w ord segmen tation am/-biguities/. Di/\u000eculties of reference resolution of compan y names are then explained/. Issuesof discourse segmen tation and concept merging are also discussed using an example text/./3/./1 W ord segmen tationJapanese w ord segmen tation in the prepro cessor giv es rise to a subsequent under/-matchingproblem/. When a k ey w ord in the text is not found in the w ord segmen tor/'s lexicon/, thesegmen tor tends to divide it in to separate w ords/. With our curren t lexicon/, for example/, the/9/1Kit ani/, Eriguchi/, /& Haracomp ound noun /\\ /\u00aa /\u00b9 /\r F /\" /( teikei/-kaisyo /)/, consisting of t w o w ords/, /\\ /\u00aa /\u00b9 /\" /( teikei /: join tv en ture/) and /\\ /\r F /\" /( kaisyo /: dissolv e/)/, is segmen ted in to the t w o individual nouns/. Th us ak ey w ord searc h for /\\ /\u00aa /\u00b9 /\r F /\" /( teikei/-kaisyo /) do es not succeed in the segmen ted sen tence/.On the other hand/, the pattern matc hing pro cess allows/, b y default/, partial matc hingb et w een a k ey w ord and a w ord in the text/. /\\ /\u00aa /\u00b9 /\" /( teikei /) and /\\ /[ Z /\u00aa /\u00b9 /\" /( gyoum/-teikei /)/,b oth meaning /\\a join t v enture/\"/, can b oth b e matc hed against the single k ey w ord /\\ /\u00aa /\u00b9 /\"/( teikei /)/. This /\rexibilit y creates an over/-matching problem/. F or example/, the k ey w ord/\\ /\u0017 J/\u0013S /\" /( silic on /) matc hes /\\ f /) /\u00d8 /\u0017J/\u0013S /\" /( nisanka/-silic on /: silicon dio xide/)/, althoughthey are di/\u000berent materials to b e rep orted in the micro electronics domain/. These segmen/-tation di/\u000eculties for comp ound nouns also cause ma jor problems in w ord/-based Japaneseinformation retriev al systems /(F ujii /& Croft/, /1/9/9/3/)/./3/./2 Compan y name referencesIn the corp orate join t v entures domain/, output templates mostly describ e relationshipsamong companies /(as describ ed in Section /2/)/. Information of interest is therefore found insen tences which men tion companies or their activities/. It is essen tial for the extractor toidentify topic companies/|the main concern of the sentences they app ear in/|in order tocorrelate other information identi/\fed in the sen tence/. There are three problems which mak eit di/\u000ecult to identify topic companies/./1/. Missing sub jectT opic companies are usually the sub ject of a sen tence/. Japanese sen tences frequentlyomit sub jects/, ho w ev er/|ev en in formal newspap er articles/. The veniex system whichnec presented at muc/-/5 can identify the compan y implied b y a missing sub ject ifthere is an explicit reference to it in the immediately preceding sen tence /(Doi et al/./,/1/9/9/3/; Muraki et al/./, /1/9/9/3/)/. It is not clear whether veniex can resolv e the missingreference when the explicit reference app ears in a sen tence further separated from thesub jectless sentence/./2/. Compan y name abbreviationsAs is also seen in English/, compan y names are often abbreviated in a Japanese text af/-ter their /\frst app earance/. A v ariet y of w a ys to abbreviate compan y names in Japaneseis giv en in /(Karasa w a/, /1/9/9/3/)/. The follo wing examples sho w some t ypical abbreviationsof Japanese compan y names/:/(a/) a partial w ord/\\ AK/\u001b/'/\u0019 /\u0006 /9S/$ /\" /! /\\ /9S/$ /\"/(Mercedes/-Benz/) /(Benz/)/(b/) an English abbreviation/\\ o /% /\u00da /\u008f /\u00da /\u0084 /* NTT /+ /\" /! /\\ NTT /\"/(Nipp on T elegraph and T elephone/)/(c/) the /\frst Katak ana c haracter /+ /\\ /\u0099 /\"/\\ /\u0002 AJ/\u000bS /\u0006 /\b/\r/\u0019/7L/\u0019 /\u0099 /\" /! /\\ /\u0002 /\u0099 /\"/(American Express Corp/./)/9/2P a ttern Ma tching and Discourse Pr ocessing/(d/) the /\frst c haracter of eac h primitive segmen t/\\ o /% /\u007f /\u0088 /\" /! /\\ o /\u007f /\"\n/1/(Japan Airlines/)/(e/) some randomly selected c haracters/\\ /\u0098 o /% /\u00fc /\u00c9 /\" /! /\\ /\u0098 o /\u00c9 /\"/(Shin/-nihon Steel/)Lo cating compan y name abbreviations is di/\u000ecult/, since man y are not identi/\fed ascompanies b y either a morphological analyzer or the name recognizer in the prepro/-cessor/. Another problem is that the v ariet y of w a ys of abbreviating names mak es itdi/\u000ecult to unify m ultiple references to one compan y /.Almost all muc/-/5 systems include a string matc hing mec hanism to identify com/-pan y name abbreviations/. These abbreviations are sp eci/\fed in an aliases slot in thecompan y en tit y ob ject/. T o the authors/' kno wledge/, none of the systems other thantextra ct can detect compan y name abbreviations of t yp e /(d/) or /(e/) ab o v e withoutusing a pre/-de/\fned abbreviation table/./3/. Compan y name pronounsCompan y name pronouns are often used in formal texts/. F requently used expressionsinclude /\\ /! /\u0099 /\" /( ryosya /: b oth companies/)/, /\\ /$ /\u0099 /\" /( dosya /: the compan y/)/, and /\\ r/\u0099 /\" /( jisya /: the compan y itself /)/. As sho wn in the follo wing examples/, resolving thereferences is particularly imp ortan t for full understanding of a text/. Direct Englishtranslation follo ws the Japanese sen tences/./(a/) /\\ X /\u0099 // Y /\u0099 /( /\u00aa /\u00b9 /\u0017 /\u0002 /$ /\u0099 /. /\u00fc W R r/\u0099 /6IS/) /' /\u00fd/\u00b5 /\u0019K /\u0003 /\"/* Y /\u0099 /+ /* X /\u0099 /+/\\X Corp/. has tied up with Y Corp/. and sells pro ducts of the compan y b y its o wnbrand name/./\" /(Y Corp/./) /(X Corp/./)/(b/) /\\ X /\u0099 ///\u0013/. /\u0097 /\u0095 /'// /\u00d2 /\u00e2 /\u00b1 /\u0003 /$ /\u0099 /. /\u0099 p // N /\u0081 J /\u0003 /\"/* X /\u0099 /+/\\X Corp/. is the biggest compan y in this /\feld/. The president of the compan y isMr/. Suzuki/./\" /(X Corp/./)Reference resolution for /\\ /$ /\u0099 /\" /( dosya /: the compan y/) is implemented in veniex /(Doiet al/./, /1/9/9/3/)/. veniex resolv es the pronominal reference in the same w a y as it identi/\fesmissing compan y references/. The crl//brandeis diderot system presen ted at muc/-/5 simply c ho oses the nearest compan y name as the referen t of /\\dosy a/\"/. This algorithmw as later impro v ed b y W ak ao using corpus/-based heuristic kno wledge /(W ak ao/, /1/9/9/4/)/.These systems do not handle pronominalized compan y names other than /\\dosy a/\"/.The three problems describ ed in this section often cause individual information to b ecorrelated with the wrong compan y or tie/-up/-relationship ob ject/. T o a v oid this error/, thetopic companies m ust b e trac k ed from the con text/, since they can b e used to determinewhich compan y ob jects an information fragmen t should b e assigned to/. Abbreviated andpronominalized compan y names m ust b e uni/\fed as references to the same compan y /./1/. /\\ o /% /\" /( nihon /: Japan/) and /\\ /\u007f /\u0088 /\" /( koukuu /: airlines/) are the primitiv e segments in this example/./9/3Kit ani/, Eriguchi/, /& Hara/3/./3 Discourse segmen tation and concept mergingIn the join t v en tures domain/, a tie/-up/-relationship ob ject con tains p oin ters to other ob jectssuc h as economic activities /(as sho wn in Figure /1/)/. When a compan y is inv olv ed in m ul/-tiple tie/-ups/, merging information into a tie/-up relationship according to topic companiessometimes yields incorrect results/. Consider the follo wing example/:/\"X Corp/. has tied up with Y Corp/. X will start selling productsin Japan next month/. Last year X started a similar joint venturewith Z Inc/./\"Ob viously /, the sale in the second sen tence is related to the tie/-up relationship of X andY/. Ho w ev er/, since the topic compan y /, which is the sub ject of a sen tence/, is X in all threesen tences/, the sale could also b e related to the X and Z tie/-up relationship/. This incorrectmerging can b e a v oided b y separating the text in to t w o blo c ks/: the /\frst t w o sentencesdescrib e the X and Y tie/-up/, and the last sen tence describ es the X and Z tie/-up/. Th us/,discourse segmen tation is necessary to identify p ortions of text con taining related piecesof information/. The crl//brandeis diderot system segmen ts the join t v en tures text intot w o t yp es of text structures /(Co wie et al/./, /1/9/9/3/)/. It is not kno wn ho w w ell their discoursesegmen tation p erformed/, ho w ev er/.Once the text is segmen ted/, concepts or identi/\fed pieces of information can b e mergedwithin the same discourse segmen t/. F or example/, the exp ected income from a joint v en tureis often stated in a sen tence which do es not explicitly men tion the participating companies/;they app ear in the previous sen tence/. In this case/, the joint v en ture concept iden tifying thecompanies and the income concept identifying the exp ected income m ust b e merged so thatthe latter will b e linked to the correct entit y ob jects/./4/. The solutionThis section describ es details of textra ct /'s pattern matc her and discourse pro cessor asw ell as the system arc hitecture/./4/./1 textra ct arc hitecturetextra ct is an information extraction system dev elop ed for the tipster Japanese do/-mains of corp orate join t v entures and micro electronics /(Jacobs/, /1/9/9/3/; Jacobs et al/./, /1/9/9/3/)/.As sho wn in Figure /2/, the textract join t v en tures system comprises four ma jor comp o/-nen ts/: prepro cessor/, pattern matc her/, discourse pro cessor/, and template generator/. Becauseof its shorter dev elopment time/, the textra ct micro electronics system has a simpler con/-/\fguration than the join t v entures system/. It do es not include the template pattern searc hin the pattern matc her/, or the discourse segmen tation and concept merging in the discoursepro cessor/, as also sho wn in Figure /2/.In the prepro cessor/, a Japanese segmen tor called majesty segmen ts Japanese text intoprimitive w ords tagged with their parts of sp eec h /(Kitani/, /1/9/9/1/)/. Next/, the name recognizeridenti/\fes prop er names and monetary /, n umeric/, and temp oral expressions/. majesty tagsprop er names which app ear in its lexicon/; the name recognizer identi/\fes additional prop ernames b y lo cating name designators suc h as /\\ /\u0099 /\" /( sya /, corresp onding to /\\Inc/./\" or /\\Corp/./\"/)/9/4P a ttern Ma tching and Discourse Pr ocessing\nPre-\nprocessor\n- morphological\n    analysis\n- name \n    recognition- concept\n    identificationconcept\nsearchtemplate\npattern\nsearch\n- concept\n    identification\n- information\n    merging within\n    a sentence\n- output\n    generationTemplate\ngeneratorPattern matcher\n- information \n   merging \n   within a textDiscourse processor\n- company \n    name\n    reference \n    resolution- text\n    segmentationcompany\nname\nunificationconcept\nmergingdiscourse\nsegmenta-\ntionjoint ventures\nsystem onlyFigure /2/: textra ct system arc hitecturefor compan y names/. The recognizer extends the name string forw ard and bac kw ard fromthe designator un til it meets searc h stop conditions /(Kitani /& Mitam ura/, /1/9/9/3/)/. The namesegmen ts are group ed into units which are meaningful to the pattern matc hing pro cess/(Kitani /& Mitam ura/, /1/9/9/4/)/. Most strings to b e extracted directly from the text are identi/\fedb y majesty and the name recognizer/.Details of the pattern matc her and discourse pro cessor are giv en in the follo wing sec/-tions/. The template generator assem bles the extracted information and creates the outputdescrib ed in Section /2/./4/./2 P attern matc herThe follo wing subsections describ e the c onc ept se ar ch and the template p attern se ar ch inthe pattern matc her which identify concepts in the sen tence/. Whereas the former simplysearc hes for k ey w ords/, the latter searc hes for phrasal patterns within a sen tence/. Thetemplate pattern searc h also identi/\fes relationships b et w een matc hed ob jects in the de/\fnedpattern/. In the course of textract developmen t/, k ey w ords and template patterns w ereobtained man ually b y a system dev elop er using a kwic /(Key W ord In Con text/) to ol andreferring to a w ord frequency list obtained from the corpus/./4/./2/./1 Concept sear chKey w ords representing the same concept are group ed into a list and used to recognize theconcept in a sen tence/. The list is written in a simple format/: /(c onc ept/-name wor d/1 wor d/2/./././) /. F or example/, k ey w ords for recognizing a dissolved joint v en ture concept can b e writtenin the following w a y/:/9/5Kit ani/, Eriguchi/, /& Hara/(DISSOLVED /\u00aa /\u00b9 /\r F /\u00ef /\u0002 F n /)or/(DISSOLVED dissolve terminate cancel/)/.The concept searc h mo dule recognizes a concept when it lo cates one of the associatedw ords in a sen tence/. This simple pro cedure sometimes yields incorrect concepts/. F or exam/-ple/, the concept /\\dissolved/\" is erroneously identi/\fed from an expression suc h as /\\ c anc el ahotel reserv ation/\"/. Key/-w ord/-based concept searc h is most successful when pro cessing textin a narro w domain in whic h w ords are used with restricted meanings/.The under/-matching problem o ccurs when a comp ound noun in the k ey w ord list of aconcept fails to matc h the text b ecause the instance of the comp ound in the text has b eensegmen ted into separate primitiv e w ords/. T o a v oid the problem/, adjacen t nouns in the textare automatically concatenated during the concept searc h pro cess/, generating comp oundnouns at run/-time/. The over/-matching problem/, on the other hand/, arises when a k ey w ordsuccessfully matc hes p art of a comp ound noun which as a whole is not associated withthe concept/. Ov er/-matc hing can b e prev en ted b y anc horing the b eginning and//or end of ak ey w ord pattern to w ord b oundaries /(with the sym b ol /\\ /> /\" at the b eginning and /\\ /< /\" atthe end/)/. F or example/, /\\ /> /\u0017J/\u0013S /< /\" /( silic on /) m ust b e matc hed against a single completew ord in the text/. Since this problem is rare/, its solution is not automatic/: system dev elop ersattac h anc hors to k ey w ords which are likely to o v er/-matc h/./4/./2/./2 Templa te p a ttern searchtextra ct /'s pattern matc her is implemented as a /\fnite/-state recognizer/. This c hoice ofimplemen tation is based on the assumption that a /\fnite/-state grammar can e/\u000eciently handleman y of the inputs that a con text/-free grammar co v ers /(P ereira/, /1/9/9/0/)/. The pattern matc heris similar to the pattern recognizer used in the muc/-/4 f astus system dev elop ed at sri/(Hobbs et al/./, /1/9/9/2/)/.P atterns for the textra ct template pattern matc her are de/\fned with rules similarto regular expressions/. Eac h pattern de/\fnition sp eci/\fes the concept associated with thepattern/. /(F or the join t v en tures domain/, textra ct uses eigh teen concepts/./)In the matc her/, state transitions are driv en b y segmen ted w ords or group ed units fromthe prepro cessor/. The matc her identi/\fes all p ossible patterns of in terest in the text thatmatc h de/\fned patterns/, recognizing the concepts asso ciated with the patterns/. F or someinputs/, the matc her m ust skip w ords that are not explicitly de/\fned in the pattern/.Figure /3 sho ws de/\fnitions of equiv alent Japanese and English patterns for recognizingthe concept /*joint/-venture/* /. This English pattern is used to capture expressions suc has /\\XYZ Corp/. created a joint v en ture with PQR Inc/./\" The notation /\\/@string/\" represen tsa v ariable matc hing an arbitrary string/. V ariables whose names b egin with /\\ /@cname /\"are called compan y/-name v ariables and are used where a compan y name is exp ected toapp ear/. In the de/\fnitions sho wn/, a string matc hed b y /\\ /@cname p ar tner subj /\" is likelyto con tain at least one compan y name referring to a joint v en ture partner and functioningas the sub ject in a sen tence/.The pattern /\\ // /# /\f /:strict/:P/\" matc hes the grammatical particles /\\ // /\" /( wa /) and /\\ /\f /\"/( ga /)/, which serv e as sub ject case mark ers/. The sym b ol /\\strict/\" sp eci/\fes a full string matc h/(the default in case of template pattern searc h/)/, whereas /\\lo ose/\" allo ws a partial string/9/6P a ttern Ma tching and Discourse Pr ocessing/(a/) /(JointVenture/1 /6/@CNAME/_PARTNER/_SUBJ// /# /\f /:strict/:P/@CNAME/_PARTNER/_WITH/( /:strict/:P/@SKIP/\u00aa /\u00b9 /:loose/:VN/)\n/(b/) /(JointVenture/1 /3/@CNAME/_PARTNER/_SUBJcreate/:/:Va joint venture/:/:NPwith/:/:P/@CNAME/_PARTNER/_WITH/)Figure /3/: A matc hing pattern for /(a/) Japanese and /(b/) Englishmatc h/. P artial string matc hing is useful for matc hing a de/\fned pattern to comp ound w ords/.The v erbal nominal pattern /\\ /\u00aa /\u00b9 /: lo ose/:VN/\" matc hes comp ound w ords suc h as /\\ /\u00c2 /[ /\u00aa /\u00b9 /\"/( kigyo/-teikei /: a corp orate join t v en ture/) as w ell as /\\ /\u00aa /\u00b9 /\" /( teikei /: a join t v en ture/)/.The /\frst /\feld in a pattern is the pattern name/, whic h refers to the concept associatedwith the pattern/. The second /\feld is a n um b er indexing a /\feld in the pattern/. This /\feld/'scon ten ts are used to decide quickly whether or not to searc h within a giv en string/. Thematc her only applies the entire pattern to a string when the string con tains the text inthe indexed /\feld/. F or e/\u000eciency /, therefore/, this /\feld should con tain the least frequen t w ordin the en tire pattern /(in this case/, /\\ /\u00aa /\u00b9 /\" /( teikei /) for Japanese and /\\a joint v en ture/\" forEnglish/)/.The order of noun phrases is relativ ely unconstrained in a Japanese sen tence/. Casemark ers/, usually attac hed to the ends of noun phrases/, pro vide a strong clue for identifyingthe case role of eac h phrase /(sub ject/, ob ject/, etc/./)/. Th us pattern matc hing driv en mainlyb y case mark ers recognizes the case roles w ell without parsing the sen tence/.Appro ximately /1/5/0 patterns are used to extract v arious concepts in the Japanese join tv en tures domain/. Several patterns usually matc h a single sentence/. Moreo v er/, since patternsare often de/\fned with case mark ers suc h as /\\ // /\" /( wa /)/, /\\ /\f /\" /( ga /)/, and /\\ /( /\" /( to /)/, a singlepattern can matc h a sen tence in more than one w a y when sev eral of the same case mark ersapp ear in the sen tence/. The template generator accepts only the b est matc hed pattern/,which is c hosen b y applying the following three heuristic rules in the order sho wn/:/1/. select patterns that include the largest n um b er of matc hed compan y/-name v ariablescon taining at least one compan y name/;/2/. select patterns that consume the few est input segmen ts /(the shortest string matc h/)/;and/3/. select patterns that include the largest n um b er of v ariables and de/\fned w ords/.These heuristic rules w ere obtained from an examination of matc hed patterns rep ortedb y the system/. T o obtain more reliable heuristics/, a large/-scale statistical ev aluation m ustb e p erformed/. Heuristics for a similar problem of pattern selection in English are discussedin /(Rau /& Jacobs/, /1/9/9/1/)/. Their system c ho oses the pattern which consumes the most inputsegmen ts /(the longest string matc h/)/, as opp osed to textra ct /'s c hoice of the shortest stringmatc h in its second heuristic rule/.\n/2/2/. In Rau and Jacobs/' system/, the third heuristic rule seems to b e applied b efore the second rule/. In thiscase/, there should b e little di/\u000berence in p erformance b et w een the heuristic rules of the t w o systems/./9/7Kit ani/, Eriguchi/, /& HaraAnother imp ortan t feature of the pattern matc her is that rules can b e group ed accordingto their concept/. The rule name /\\Join tV en ture/1/\" in Figure /3/, for example/, represen tsthe concept /*joint/-venture/* /. Using this grouping/, the b est matc hed pattern can b eselected from matc hed patterns of a particular concept group instead of from all the matc hedpatterns/. This feature enables the discourse and template generation pro cesses to narro wtheir searc h for the b est information to /\fll a particular slot/./4/./3 Discourse pro cessorThe follo wing subsections describ e the algorithm of compan y name reference resolutionthroughout the discourse/. Discourse segmen tation and concept merging pro cesses are alsodiscussed/./4/./3/./1 Identifying topic comp aniesSince no syn tactic analysis is p erformed in textra ct /, topic companies are simply iden ti/-/\fed wherev er a sub ject case mark er suc h as /\\ /\f /\" /( ga /)/, /\\ // /\" /( wa /)/, or /\\ B /\" /( mo /) follo wscompan y names/. If no topic companies are found in a sen tence/, the previous sen tence/'stopic companies are inherited /(ev en if the curren t sen tence con tains a non/-compan y sub/-ject/)/. This is based on the supp osition that a sen tence which in tro duces new companiesusually men tions them explicitly in its sub ject/./4/./3/./2 Abbrevia tion detection and unifica tionCompan y name abbreviations ha v e the follo wing observ ed c haracteristics/:/\u000f majesty tags most abbreviations as /\\unkno wn/\"/, /\\compan y/\"/, /\\p erson/\"/, or /\\place/\"/;/\u000f a compan y name precedes its abbreviations/;/\u000f an abbreviation is comp osed of t w o or more c haracters from the compan y name/, intheir original order/;/\u000f the c haracters need not b e consecutive within the compan y name/; and/\u000f English w ord abbreviations m ust b e identical with an English w ord app earing in thecompan y name/.Th us the follo wing are regarded as abbreviations/: /\\unkno wn/\"/, /\\compan y/\"/, /\\p erson/\"/,and /\\place/\" segmen ts comp osed of t w o or more c haracters whic h also app ear in compan ynames previously identi/\fed in the text/. When comparing p ossible abbreviations againstkno wn compan y names/, the length of the longest common subsequence or LCS /(W agner /&Fisc her/, /1/9/7/4/) is computed to determine the maxim um n um b er of c haracters app earing inthe same order in b oth strings/.\n/3T o unify m ultiple references to the same compan y /, a unique n um b er is assigned tothe source and abbreviated companies/. Rep eated compan y names which con tain stringsapp earing earlier in the text are treated as abbreviations /(and th us giv en unique n um b ers/)/3/. F or example/, the LCS of /\\abacbba/\" and /\\b cda/\" is /\\b ca/\"/./9/8P a ttern Ma tching and Discourse Pr ocessing/1/. Step /1/: Initialization to assign eac h en tit y in C a unique n um b er/.for i in C do /(/1 /\u0014 i /\u0014 cmax /)C /[ i/; /\\id/\" /] / idone/2/. Step /2/: Searc h abbreviations and giv e unique n um b ersfor i in C do /(/1 /\u0014 i /\u0014 cmax /)if C /[ i/; /\\id/\" /] /6/= i then /# already recognized as an abbreviationcon tin ue i lo opLE N S RC / length of C /[ i/; /\\string/\" /]for j in C do /( i /+ /1 /\u0014 j /\u0014 cmax /)if C /[ j /; /\\id/\" /] /6/= j then /# already recognized as an abbreviationcon tinue j lo opLE N / length of C /[ j /; /\\string/\" /]LC S / length of the LCS of C /[ i/; /\\string/\" /] and C /[ j/; /\\string/\" /]if LC S /\u0015 /2 then doif C /[ i/; /\\e g/\" /] /= /\\YES/\" and LE N S RC /= LC S /= LE N thenC /[ j/; /\\id/\" /] / C /[ i/; /\\id/\" /] /# an English w ord abbreviationelse if C /[ i/; /\\e g/\" /] /= /\\NO/\" and LC S /= LE N then /# an abbreviationC /[ j/; /\\id/\" /] / C /[ i/; /\\id/\" /]donedonedoneFigure /4/: Algorithm to unify m ultiple references to the same compan yb y the algorithm describ ed in Figure /4/. In the pseudo co de sho wn/, all identi/\fed compan ynames are stored in an asso ciativ e arra y named C /. /\\Unkno wn/\"/, /\\compan y/\"/, /\\p erson/\"/, and/\\place/\" segmen ts are also stored in the arra y as p ossible abbreviations/. Compan y names aresorted in ascending order of their starting p osition in the text and n um b ered from /1 to cmax/(Step /1/)/. A compan y name string which is indexed i can b e addressed b y C /[ i/; /\\string/\" /]/. A/\rag C /[ i/; /\\e g/\" /] records whether the compan y name is an English w ord abbreviation or not/.Step /2 compares eac h compan y name in the arra y C with all names higher in the arra y/(and th us later in the text/)/. When the LCS of a pair of earlier and later compan y namesis equal to the length of the later compan y name/, the later compan y name is recognized asan abbreviation of the earlier compan y name/. Then/, the /\\id/\" of the later compan y name isreplaced with that of the earlier compan y name/. The LCS m ust b e t w o or more c haracters/,and if the abbreviation is an English w ord/, the LCS m ust b e equal to the length of theearlier compan y name/.A t the end of execution/, a n um b er is giv en in C /[ i/; /\\id/\" /]/. If C /[ i/; /\\id/\" /] w as c hanged duringexecution/, C /[ i/; /\\string/\" /] w as recognized as a compan y name abbreviation/./9/9Kit ani/, Eriguchi/, /& Hara/4/./3/./3 Anaphora resolution of comp any name pronounsThe approac h for reference resolution describ ed in this section is based on heuristics obtainedb y corpus analysis rather than linguistic theories/. Three compan y name pronouns are thetarget of reference resolution/: /\\ /! /\u0099 /\" /( ryosya /)/, /\\ /$ /\u0099 /\" /( dosya /)/, and /\\ r/\u0099 /\" /( jisya /)/, meaning/\\b oth companies/\"/, /\\the compan y/\"/, and /\\the compan y itself /\"/. They are three of the mostfrequen t compan y name pronouns app earing in our corpus pro vided b y arp a for the tipsterinformation extraction pro ject/. /\\Ry osy a/\"/, /\\dosy a/\"/, and /\\jisy a/\" app eared /4/5/6/, /2/7/7/, and /1/2/9times/, resp ectively /, in /1/1/0/0 newspap er articles con taining an a v erage of /4/8/1 c haracters p erarticle/.The following heuristics/, derived from analysis of pronoun reference in the corpus/, w ereused for reference resolution/:/\u000f /\\ry osy a/\" almost alw a ys referred to the /\\curren t/\" tie/-up compan y /, with one exceptionin a h undred o ccurrences/;/\u000f ab out ninety p ercent of /\\dosy a/\" o ccurrences referred to the topic compan y when therew as only one p ossible referen t in the same sen tence/, but/:/\u000f when more than t w o companies/, including the topic compan y /, preceded /\\dosy a/\" inthe same sen tence/, ab out sev en t y/-/\fv e p ercen t of the pronoun o ccurrences referred tothe nearest compan y /, not necessarily the topic compan y/; and/\u000f ab out eigh t y p ercent of /\\jisy a/\" o ccurrences referred to the topic compan y /.Tw o additional heuristic rules w ere disco v ered but not implemen ted in textract /:/\u000f ab out four p ercen t of /\\jisy a/\" o ccurrences referred to more than one compan y/; and/\u000f ab out eight p ercen t of /\\jisy a/\" o ccurrences referred to entities which are general ex/-pressions ab out a compan y suc h as /\\ /\f /\u0099 /\" /( kaisya /: a compan y/)/.As a result of the discourse pro cessing describ ed ab o v e/, ev ery compan y name/, includingabbreviations and pronominal references/, is giv en a unique n um b er/./4/./3/./4 Discourse segment a tion and concept mergingIn the /1/5/0 articles of the tipster // muc/-/5 join t v en tures test set/, m ultiple tie/-up relationshipsapp eared in thirt y/-one articles which included ninety individual tie/-up relationships/. Thet w o t ypical discourse mo dels represen ting the discourse structures of tie/-up relationshipsare sho wn in Figure /5/./\u000f T yp e/-I/: tie/-ups are describ ed sequen tiallyDescriptions of tie/-ups app ear sequentially in this mo del/. One tie/-up is not men tionedagain after a new tie/-up has b een describ ed/./\u000f T yp e/-I I/: a main tie/-up reapp ears after other tie/-ups are mentionedA ma jor di/\u000berence from the T yp e/-I mo del is that a description of a main tie/-upreapp ears in the text after other tie/-up relationships ha v e b een introduced/. Non/-maintie/-ups are usually men tioned brie/\ry /./1/0/0P a ttern Ma tching and Discourse Pr ocessing\ntie-up-2\ntie-up-3\ntie-up-ntie-up-1\ntie-up-2\ntie-up-1tie-up-n.\n.\n..\n.\n.\nType-I Type-IItie-up-1\nnon-main tie-upsFigure /5/: Discourse structure of tie/-up relationshipsElev en T yp e/-I structures and thirteen T yp e/-I I structures app eared in the thirt y/-one arti/-cles/. Sev en of the articles con tained complicated discourse structures regarding the tie/-uprelationships/.The t w o t yp es of text structure describ ed ab o v e are similar to the ones implemented inthe crl//brandeis diderot join t v en tures system/. The di/\u000berence is only in the T yp e/-I Istructure/: dider ot pro cesses all tie/-up relationships whic h reapp ear in the text/, not justthe reapp earing main tie/-up fo cused on b y textract /.textra ct /'s discourse pro cessor divides the text when a di/\u000berent tie/-up relationship isidenti/\fed b y the template pattern searc h/. A di/\u000berent tie/-up relationship is recognized whenthe n um b ers assigned to the join t v enture companies are not identical to those app earing inthe previous tie/-up relationships/. diderot segmen ts the discourse if an y other related piecesof information suc h as date and en tit y lo cation are di/\u000berent b et w een the tie/-up relationships/.Suc h strict merging is preferable when the pieces of information in comparison are correctlyidenti/\fed/. The merging conditions of discourse segmen ts should b e c hosen according to theaccuracy of identi/\fcation of the information to b e compared/.After the discourse is segmen ted/, iden ti/\fed concepts and extracted w ords and phrasesare merged/. Figure /6 sho ws the merging pro cess for the follo wing text passage whic h actuallyapp eared in the tipster // muc/-/5 test set /(a direct English translation follo ws/)/:/\\ /\u00d9 /\u00c0 /\u00fc /\u009b // /8 o /\u0002 /\u00fd /; /. D /\u009b A /\u001c /\u000b /\u001c/\u0002 /\b /\u001c/\u0006 AK/\u000f /\u0099 /. /\u0098 /\u009b /. o /% /\u009e U /'/. /$ /\u00de /\u0002/\u00fd /\u00b5 R/\u0019 K /\u00aa /\u00b9 /\u00b1 /\u009a R /\u00dc S / /\u0003 /\u0098 /\u009b /. /\u00fd /\u00b5 /\f /' /\r K H /\u0006/+ /* K /5 /\u0002 /6 /\u0080 /' /+ // /\u0002 /! /\u0099/\f /\u001d /\u00ed /\u00f5Z /\u0017/& /\u0094 /\u00c6 /\f /\u0099 R /\u001e /\f /\u0019K/\u0013/(B /\u0094 /4 /\u0017/\u001f /\u0003 /\"/\\On the eighth /(of this mon th/)/, T anab e Pharmaceuticals made a join tv enture con tract with a German pharmaceutical mak er/, Merc k and Co/.Inc/./, to develop and sell its new medicine in Japan/. They also agreed thatb oth companies w ould inv est equally to establish a join t v en ture compan yin /\fv e or six y ears when they start selling new medicine/./\"/1/0/1Kit ani/, Eriguchi/, /& Hara\nFirst sentence:\n\"On the eighth (of this month),\nTanabe Pharmaceuticals made \na joint venture contract with a \nGerman pharmaceutical  maker, \nMerck and Co. Inc., to develop \nand sell its new medicine in \nJapan.\"\nDiscourse\nprocessorTemplate\npattern\nsearch\"Tanabe Pharmaceuticals\"\n\"Merck and Co. Inc.\"*ECONOMIC-\n  ACTIVITY*Second sentence:\n\"They also agreed that both \ncompanies would invest equally\nto establish a joint venture \ncompany in five or six years when \nthey start selling new medicine.\"\n\"both \n companies\"*ESTABLISH*\n\"Tanabe Pharmaceuticals\"\n\"Merck and Co. Inc.\"*ESTABLISH* *ECONOMIC-\n  ACTIVITY*\"a joint venture \n company\"\"a joint venture \n company\"\n\"both \n companies\"Figure /6/: Example of concept mergingThe t w o compan y names in the /\frst sen tence/, /\\ /\u00d9 /\u00c0 /\u00fc /\u009b /\" /( tanab e seiyaku /: T anab ePharmaceuticals/) and /\\ /\b /\u001c/\u0006 AK/\u000f /\u0099 /\" /( ei meruku sya /: Merc k and Co/. Inc/./)/, are identi/\fedb y either majesty or the name recognizer during prepro cessing/. Next/, the template patternsearc h lo cates in the /\frst sen tence the /\\economic activit y/\" pattern sho wn in Figure /7 /(a/)/.The /*economic/-a ctivity/* concept relating the t w o companies has no w b een recognized/.The template pattern searc h also recognizes the /*est ablish/* concept in the second sen tenceb y the template pattern sho wn in Figure /7 /(b/)/.After sen tence/-level pro cessing/, discourse pro cessing recognizes that /\\ /! /\u0099 /\" /( ryosya /:b oth companies/) in the second sen tence refers to T anab e Pharmaceuticals and Merc k inthe /\frst sen tence b ecause they are the curren t tie/-up companies/. Since the second sen tencedo es not introduce a new tie/-up relationship/, b oth sentences are in the same discoursesegmen t/. Concepts separately identi/\fed in the t w o sen tences can no w b e merged b ecausethe sub jects of the t w o sen tences are the same/. The /*est ablish/* concept is therefore joinedto the /*economic/-a ctivity/* concept/./(a/) /(EconomicActivityE /6/@CNAME/_PARTNER/_SUBJ// /# /\f /:strict/:P/@CNAME/_PARTNER/_SUBJ/. /:strict/:P/@SKIP/$ /\u00de /:loose/:VN/)\n/(b/) /(Establish/3 /6/@CNAME/_PARTNER/_SUBJ// /# /\f /:strict/:P/@CNAME/_CREATED/_OBJR /:strict/:P/@SKIP/\u001e /\f /:loose/:VN/)Figure /7/: Economic activit y pattern /(a/) and establish pattern /(b/)/1/0/2P a ttern Ma tching and Discourse Pr ocessing/5/. P erformance ev aluationThis section sho ws some ev aluation results for textract /'s discourse mo dule/. muc/-/5 ev al/-uation metrics and o v erall textra ct p erformance are also discussed/./5/./1 Unique iden ti/\fcation of compan y name abbreviationsA h undred join t v en tures newspap er articles used for the tipster /1/8/-mon th ev aluationw ere c hosen as a blind test set for this ev aluation/. The ev aluation measures w ere r e c al l /,the p ercen tage of correct answ ers extracted compared to p ossible answ ers/, and pr e cision /,the p ercen tage of correct answ ers extracted compared to actual answ ers/. majesty andthe name recognizer identi/\fed compan y names in the ev aluation set with recall of sev en t y/-/\fv e p ercent and precision of ninety/-/\fv e p ercent when partial matc hes b et w een exp ectedand recognized strings w ere allo w ed/, and with recall of sixt y/-nine p ercen t and precision ofeight y/-sev en p ercen t in an exact matc hing condition/.Compan y names that app eared in a form di/\u000berent from their /\frst app earance in anarticle w ere considered to b e compan y name abbreviations/. Among /3/1/8 abbreviations/, therecall and precision of abbreviation detection w ere sixt y/-sev en and eigh t y/-nine p ercen t/, re/-sp ectively /. Most imp ortan tly /, detected abbreviations w ere uni/\fed correctly with the sourcecompanies as long as the source companies w ere identi/\fed correctly b y majesty and thename recognizer/.The ev aluation results clearly sho w that compan y name abbreviations w ere accuratelydetected and uni/\fed with the source companies as long as compan y names w ere correctlyidenti/\fed b y the preceding pro cesses/. It is p ossible/, ho w ev er/, that the simple string matc hingalgorithm curren tly used could erroneously unify similar compan y names/, which are oftenseen among family companies/./5/./2 Anaphora resolution of compan y name pronounsThe accuracy of reference resolution for /\\ry osy a/\"/, /\\dosy a/\"/, and /\\jisy a/\" is sho wn in T able/1/. The n um b ers in paren theses w ere obtained b y restricting atten tion to pronouns whichreferred to companies identi/\fed correctly b y the preceding pro cesses/. Since companiesreferred to b y /\\ry osy a/\" /(b oth companies/) w ere usually /\\curren t/\" tie/-up companies in thejoin t v en tures domain/, reference resolution accuracy dep ended on the accuracy with whichtie/-up relationships w ere identi/\fed/.compan y name pronouns n um b er of resolutionreferences accuracy/\\ /! /\u0099 /\" /( ryosya /: b oth companies/) /1/0/1 /(/9/3/) /6/4/% /(/7/0/%/)/\\ /$ /\u0099 /\" /( dosya /: the compan y/) /1/0/0 /(/9/0/) /7/8/% /(/8/7/%/)/\\ r/\u0099 /\" /( jisya /: the compan y itself /) /6/0 /(/5/3/) /7/8/% /(/8/9/%/)T able /1/: Accuracy of reference resolutions/1/0/3Kit ani/, Eriguchi/, /& HaraA ma jor cause of incorrect references of /\\dosy a/\" w as the failure to lo cate topic compa/-nies/. The simple mec hanism of searc hing for topic companies using case mark ers did notw ork w ell/. A t ypical problem can b e seen in the follo wing example/: /\\ /\u00aa /\u00b9 /' // X /\u0099 /\u0003 /\" /(Ajoin t v en ture partner is X Corp/./)/. Here X Corp is a topic compan y /, but the sub ject /\\ X /\u0099 /\"/(X Corp/./) is not follow ed b y a sub ject case mark er/. Other errors can b e attributed to thefact that /\\dosy a/\" did not alw a ys refer to a topic compan y as discussed in the heuristic rulesof /\\dosy a/\" reference resolution/.Regarding /\\jisy a/\" resolutions/, /\fv e instances which should ha v e referred to m ultiplecompanies w ere b ound to a single compan y /. Since m ultiple companies are usually listedusing conjunctions such as /\\ /( /\" /( to /: and/) and /\\ /\u0002 /\" /(comma/)/, they can b e iden ti/\fed easilyif a simple phrase analysis is p erformed/.It b ecame clear from this ev aluation that resolving /\\dosy a/\" references to a non/-topiccompan y required in tensive text understanding/. F ort y/-sev en p ercen t of the o ccurrences of/\\dosy a/\" and /\\jisy a/\" w ere b ound to topic companies inherited from a previous sen tence/. Thisresult strongly supp orted the imp ortance of k eeping trac k of topic companies throughoutthe discourse/./5/./3 Discourse segmen tationThirt y/-one of the /1/5/0 tipster // muc/-/5 ev aluation test articles included ninety m ultiple tie/-up relationships/. textract /'s discourse pro cessor segmen ted the thirt y/-one articles intosev en t y/-one individual tie/-up relationship blo c ks/. Only thirt y/-eigh t of the blo c ks w ere cor/-rectly segmen ted/. Main tie/-up relationships which reapp eared in T yp e/-I I discourse struc/-tures w ere not detected w ell/, whic h caused the structures to b e incorrectly recognized asT yp e/-I/. This error w as caused b y the fact that the join t v en ture relationships w ere usu/-ally men tioned implicitly when they reapp eared in the text/. F or example/, a noun phrase/,/\\ /\u00ae /\u000e /. /\u00aa /\u00b9 // /\" /(the join t v enture this time/)/, which w as not detected b y the template pat/-terns used/, brough t the fo cus bac k to the main tie/-up/. As a result/, textra ct iden ti/\fed eigh tp ercen t few er tie/-up relationships than the p ossible n um b er exp ected in the tipster // muc/-/5ev aluation/. This merging error m ust ha v e a/\u000bected system p erformance since the informa/-tion in the reapp earing main tie/-up segmen t w ould not ha v e b een correctly linked to theearlier main tie/-up segmen t/.This preliminary study suggested that recognizing segmen tation p oin ts in the text shouldb e regarded as crucial for p erformance/. The template pattern matc hing alone w as not go o denough to recognize the segmen tation p oints/. The discourse pro cessor simply segmen tedthe text when it found a new tie/-up relationship/. The discourse mo dels/, curren tly un used atrun/-time in textra ct /, could b e used to help infer the discourse structure when the systemis not sure whether to merge or separate discourse segmen ts/. Reference resolution of de/\fniteand inde/\fnite noun phrases m ust also b e solv ed for accurate discourse segmen tation in futureresearc h/.The accuracy of discourse segmen tation might b e impro v ed b y c hecking the di/\u000berence oridentit y of date and entit y lo cation/, as w ell as en tit y name/, when deciding whether or not tomerge a tie/-up relationship/. textract did not tak e date and lo cation ob jects in to accoun tin making segmen tation decisions/, b ecause textra ct /'s identi/\fcation of these ob jects w asnot considered reliable enough/. F or example/, the date ob ject w as identi/\fed with recall of/1/0/4P a ttern Ma tching and Discourse Pr ocessingonly t w ent y/-sev en p ercen t and precision of /\fft y/-nine p ercent/. On the other hand/, en titiesw ere identi/\fed with o v er eight y p ercent accuracy in b oth recall and precision/. T o a v oidincorrect discourse segmen tation/, therefore/, textract /'s merging conditions included onlyen tit y names as reliable information/./5/./4 Ov erall textra ct p erformance/2/5/0 newspap er articles/, /1/5/0 ab out Japanese corp orate join t v en tures and /1/0/0 ab out Japanesemicro electronics/, w ere pro vided b y arp a for use in the tipster // muc/-/5 system ev aluation/.Six join t v en tures systems and /\fv e micro electronics systems/, including textra ct dev el/-op ed at cmu as an optional system of ge/-cmu shogun /, w ere presen ted in the Japanesesystem ev aluation at muc/-/5 /. A scoring program automatically compared the system outputwith answ er templates created b y h uman analysts/. When a h uman decision w as necessary /,analysts instructed the scoring program whether the t w o strings in comparison w ere com/-pletely matc hed/, partially matc hed/, or unmatc hed/. Finally /, the scoring program calculatedan o v erall score com bined from all the newspap er article scores/. Although v arious ev al/-uation metrics w ere measured in the ev aluation /(Chinc hor /& Sundheim/, /1/9/9/3/)/, only thefollo wing error/-based and recall/-precision/-based metrics are discussed in this pap er/. Thebasic scoring categories used are/: correct /(COR/)/, partially correct /(P AR/)/, incorrect /(INC/)/,missing /(MIS/)/, and spurious /(SPU/)/, coun ted as the n um b er of pieces of information in thesystem output compared to the p ossible information/./(/1/) Error/-based metrics/\u000f Error p er resp onse /\fll /(ERR/)/:w rongtotal\n/=\nI N C /+ P AR/= /2 /+ M I S /+ S P UC O R /+ P AR /+ I N C /+ M I S /+ S P U/\u000f Undergeneration /(UND/)/:M I Spossible\n/=\nM I SC O R /+ P AR /+ I N C /+ M I S/\u000f Ov ergeneration /(O V G/)/:S P Uactual\n/=\nS P UC O R /+ P AR /+ I N C /+ S P U/\u000f Substitution /(SUB/)/:I N C /+ P AR /= /2C O R /+ P AR /+ I N C/1/0/5Kit ani/, Eriguchi/, /& Haradomain ERR UND O V G SUB REC PRE P/&Rtextract /(JJV/) /5/0 /3/2 /2/3 /1/2 /6/0 /6/8 /6/3/./8System A /(JJV/) /5/4 /3/6 /2/7 /1/2 /5/7 /6/4 /6/0/./1System B /(JJV/) /6/3 /5/1 /2/3 /1/2 /4/2 /6/7 /5/2/./1textract /(JME/) /5/9 /4/3 /2/8 /1/2 /5/1 /6/3 /5/6/./4System A /(JME/) /5/8 /3/0 /3/8 /1/4 /6/0 /5/3 /5/6/./3System B /(JME/) /6/5 /5/4 /2/4 /1/2 /4/0 /6/6 /5/0/./4T able /2/: Scores of textra ct and t w o other top/-ranking o/\u000ecial systems in tipster // muc/-/5/(/2/) Recall/-precisi on/-based metrics/\u000f Recall /(REC/)/:C OR /+ P AR/= /2possible/\u000f Precision /(PRE/)/:C OR /+ P AR/= /2actual/\u000f P/&R F/-measure /(P/&R/)/:/2 /\u0003 R E C /\u0003 P RERE C /+ P REThe error p er resp onse /\fll /(ERR/) w as the o/\u000ecial measure of muc/-/5 system p erformance/.Secondary ev aluation metrics w ere undergeneration /(UND/)/, o v ergeneration /(O V G/)/, andsubstitution /(SUB/)/. The recall/, precision/, and F/-measure metrics w ere used as uno/\u000ecialmetrics for muc/-/5 /.T able /2 sho ws scores of textra ct and t w o other top/-ranking o/\u000ecial systems tak enfrom the tipster // muc/-/5 system ev aluation results/.\n/4textra ct pro cessed only Japanesedomains of corp orate join t v entures /(JJV/) and micro electronics /(JME/)/, whereas the t w oother systems pro cessed b oth English and Japanese text/. textra ct p erformed as w ell asthe top/-ranking systems in the t w o Japanese domains/.The h uman p erformance of four w ell/-trained analysts w as rep orted to b e ab out eight yp ercen t in b oth recall and precision in the English micro electronics domain /(Will/, /1/9/9/3/)/.This is ab out thirt y p ercen t b etter than the b est tipster // muc/-/5 systems/' p erformancein P/&R F/-measure in the same language domain/. In the Japanese join t v en tures domain/,textra ct scored recall of sev en t y/-/\fv e p ercent and precision of eigh t y/-one p ercen t witha core template comprising only essential ob jects/. This result suggests that the curren ttec hnology could b e used to supp ort h uman extraction w ork if the task is w ell/-constrained/./4/. The textract scores submitted to muc/-/5 w ere uno/\u000ecial/. It w as scored o/\u000ecially after the conference/.T able /2 sho ws textract /'s o/\u000ecial scores/./1/0/6P a ttern Ma tching and Discourse Pr ocessingRunning on a SUN SP AR Cstation IPX/, textra ct pro cessed a join t v entures article inab out sixt y seconds and a micro electronics article in ab out t w en t y/-four seconds on a v erage/.The h uman analysts to ok ab out /\ffteen min utes to complete an English micro electronicstemplate and ab out sixt y minutes for a Japanese joint v en tures template /(Will/, /1/9/9/3/)/.Th us a h uman/-mac hine in tegrated system w ould b e the b est solution for fast/, high qualit y /,information extraction/.Some tipster // muc/-/5 systems pro cessed b oth Japanese and English domains/. Thesesystems generally p erformed b etter in the Japanese domains than in the corresp onding En/-glish domains/. One likely reason is that the structure of Japanese articles is fairly standard/,particularly in the Japanese join t v en tures domain/, and can b e readily analyzed in to thet w o discourse structure t yp es describ ed in this pap er/. Another p ossible reason is a c harac/-teristic of writing st yle/: expressions which need to b e identi/\fed tend to app ear in the /\frstfew sen tences in a form suitable for pattern matc hing/.The textra ct Japanese micro electronics system copied the prepro cessor/, the conceptsearc h of the pattern matc her/, and the compan y name uni/\fcation of the discourse pro cessorused in the textra ct Japanese joint v en tures system/. The micro electronics system w asdev elop ed in only three w eeks b y one p erson who replaced join t v en tures concepts and k eyw ords with representativ e micro electronics concepts and k ey w ords/. The lo w er p erformanceof the textra ct micro electronics system compared to the join t v entures system is largelydue to the short dev elopment time/. It is also probably due to the less homogeneous discoursestructure and writing st yle of the micro electronics articles/./6/. Conclusions and future researc hThis pap er has describ ed the imp ortance of discourse pro cessing in three asp ects of informa/-tion extraction/: identifying k ey information throughout the text/, i/.e/. topic companies andcompan y name references in the tipster // muc/-/5 domains/; segmen ting the text to select rel/-ev an t p ortions of interest/; and merging concepts identi/\fed b y the sen tence level pro cessing/.The basic p erformance of the system dep ends on the prepro cessor/, ho w ev er/, since man ypieces of identi/\fed information are put directly into slots or are otherwise used to /\fll slotsduring later pro cessing/. textract /'s pattern matc her solves the matc hing problem causedb y the segmen tation am biguities often found in Japanese comp ound w ords/. The patternmatc hing system based on a /\fnite/-state automaton is simple and runs fast/. These factorsare essen tial for rapid system dev elopment and p erformance impro v ement/.T o impro v e system p erformance with the pattern matc hing arc hitecture/, an increasein the n um b er of patterns is una v oidable/. Since matc hing a large n um b er of patterns isa lengthy pro cess/, an e/\u000ecien t pattern matc her is required to shorten the running time/.T omita/'s new generalized LR parser/, kno wn to b e one of the fastest parsers for practicalpurp oses/, skips unnecessary w ords during parsing /(Bates /& La vie/, /1/9/9/1/)/. The parser isunder ev aluation to in v estigate if it is appropriate for information extraction from Japanesetext /(Eriguc hi /& Kitani/, /1/9/9/3/)/. P attern matc hing alone/, ho w ev er/, will not b e able to impro v ethe system p erformance to h uman levels in a complicated information extraction task suc has tipster // muc/-/5 /, ev en if the task is w ell/-de/\fned and suitable for pattern matc hing/. Moree/\u000borts should b e made in discourse pro cessing suc h as discourse segmen tation and referenceresolution for de/\fnite and inde/\fnite noun phrases/./1/0/7Kit ani/, Eriguchi/, /& HaraThe researc h discussed in this pap er is based on an application/-orien ted/, domain/-sp eci/\fc/,and language/-sp eci/\fc approac h relying on patterns and heuristic rules collected from aparticular corpus/. It is ob vious that the patterns and heuristic rules describ ed in this pap erdo not co v er a wide range of applications/, domains/, or languages/. The empirical approac hdescrib ed here is w orth inv estigating ev en for an en tirely new task/, ho w ev er/, since it canac hiev e a high level of system p erformance in a relatively short dev elopment time/. Whilelinguistic theory/-based systems tend to b ecome complex and di/\u000ecult to main tain/, esp eciallyif they incorp orate full text parsing/, the simplicit y of an empirically/-based/, pattern/-orien tedsystem suc h as textract k eeps the development time short and the ev aluation cycle quic k/.Corpus analysis is a k ey element in this corpus/-based paradigm/. It is estimated thatcorpus analysis to ok ab out half of the dev elopment time for textra ct /. Statistically/-basedcorpus analysis to ols are necessary to obtain b etter p erformance in a shorter dev elopmenttime/. Suc h to ols could help dev elop ers not only extract imp ortan t patterns and heuristicrules from the corpus/, but also monitor the system p erformance during the ev aluation/-impro v emen t cycle/.Ac kno wledgemen tsThe authors wish to express their appreciation to Jaime Carb onell/, who pro vided the op/-p ortunit y to pursue this researc h at the Cen ter for Mac hine T ranslation/, Carnegie MellonUniversit y /. Thanks are also due to T eruk o Mitam ura and Mic hael Mauldin for their man yhelpful suggestions/."}
{"category": "abstract", "text": "Information extraction is the task of automatically pic king up information of in terestfrom an unconstrained text/. Information of in terest is usually extracted in t w o steps/.First/, sentence lev el pro cessing lo cates relev an t pieces of information scattered throughoutthe text/; second/, discourse pro cessing merges coreferen tial information to generate theoutput/. In the /\frst step/, pieces of information are lo cally iden ti/\fed without recognizingan y relationships among them/. A k ey w ord search or simple pattern search can ac hiev e thispurp ose/. The second step requires deep er kno wledge in order to understand relationshipsamong separately iden ti/\fed pieces of information/. Previous information extraction systemsfo cused on the /\frst step/, partly b ecause they w ere not required to link up eac h pieceof information with other pieces/. T o link the extracted pieces of information and mapthem on to a structured output format/, complex discourse pro cessing is essen tial/. Thispap er rep orts on a Japanese information extraction system that merges information usinga pattern matc her and discourse pro cessor/. Ev aluation results show a high lev el of systemp erformance whic h approac hes h uman p erformance/./1/. In tro ductionIn recen t information extraction systems/, most individual pieces of information to b e ex/-tracted directly from a text are usually identi/\fed b y k ey w ord searc h or simple patternsearc h in the prepro cessing stage /(Lehnert et al/./, /1/9/9/3/; W eischedel et al/./, /1/9/9/3/; Co wie et al/./,/1/9/9/3/; Jacobs et al/./, /1/9/9/3/)/. Among the systems presen ted at the Fifth Message Understand/-ing Conference /( muc/-/5 /)/, ho w ev er/, the main arc hitectures ranged from pattern matc hing tofull or fragmen t parsing /(On yshk evyc h/, /1/9/9/3/)/. F ull or fragmen t parsing systems/, in whichsev eral kno wledge sources such as syn tax/, seman tics/, and domain kno wledge are com binedat run/-time/, are generally so complicated that c hanging a part of the system tends to a/\u000bectthe other comp onents/. In past information extraction researc h/, this in terference has slo w eddev elopment /(Jacobs/, /1/9/9/3/; Hobbs et al/./, /1/9/9/2/)/. A pattern matc her/, which identi/\fes onlypatterns of in terest/, is more appropriate for information extraction from texts in narro wdomains/, since this task do es not require full understanding of the text/.c/\r /1/9/9/4 AI Access F oundation and Morgan Kaufmann Publishers/. All righ ts reserv ed/.Kit ani/, Eriguchi/, /& Haratextra ct /, the information extraction system describ ed here/, uses a pattern matc hersimilar to sri /'s f astus pattern matc her /(Hobbs et al/./, /1/9/9/2/)/. The matc her is implemen tedas a /\fnite/-state automaton/. Unlik e other pattern matc hers/, textra ct /'s matc her dealswith w ord matc hing problems caused b y the w ord segmen tation am biguities often found inJapanese comp ound w ords/.The goal of the pattern matc her is to identify the c onc epts represen ted b y w ords andphrases in the text/. The pattern matc her /\frst p erforms a simple k ey/-w ord/-based c onc eptse ar ch /, lo cating individual w ords asso ciated with concepts/. The second step is a templatep attern se ar ch which lo cates phrasal patterns in v olving critical pieces of information iden/-ti/\fed b y the prepro cessor/. The template pattern searc h iden ti/\fes relationships b et w eenmatc hed ob jects in the de/\fned pattern as w ell as recognizing the concept b ehind the rela/-tionship/. One t ypical concept is the relationship of /\\economic activity/\" whic h companiescan participate in with eac h other/.It is usually di/\u000ecult to determine the relationships among pieces of information whichha v e b een identi/\fed in separate sen tences/. These relationships are often stated implicitly /,and ev en if the text explicitly men tions them the descriptions are often lo cated far enoughapart to mak e detection di/\u000ecult/. Although the imp ortance of discourse pro cessing for infor/-mation extraction has b een emphasized in the Message Understanding Conferences /(Lehnert/& Sundheim/, /1/9/9/1/; Hirsc hman/, /1/9/9/2/)/, no system presented has satisfactorily addressed theissue/.The discourse pro cessor in textra ct is able to correlate individual pieces of informationthroughout the text/. textra ct merges concepts which the pattern matc her has identi/\fedseparately /(and usually in di/\u000beren t sen tences/) when the concepts inv olv e the same compa/-nies/. textra ct can unify m ultiple references to the same compan y ev en when the compan yname is missing/, abbreviated/, or pronominalized/. F urthermore/, the pro cessor segmen ts thediscourse to isolate p ortions of text relev an t to a particular conceptual relationship/. Thediscourse segmen tation lessens the c hance of merging unrelated information /(Kitani/, /1/9/9/4/)/.This pap er analyzes some ev aluation results for textra ct /'s discourse module and de/-scrib es the tipster // muc/-/5 ev aluation results in order to assess o v erall system p erformance/./2/. tipster information extraction taskThe goal of the tipster pro ject sp onsored b y arp a is to capture information of interestfrom English and Japanese newspap er articles ab out corp orate joint v en tures and micro/-electronics/. A system m ust /\fll a generic template with information extracted from the textb y a fully automated pro cess/. The template is comp osed of sev eral ob jects/, eac h con tain/-ing sev eral slots/. Slots ma y con tain p ointers to related ob jects /(Tipster/, /1/9/9/2/)/. Extractedinformation is to b e stored in an ob ject/-orien ted database/.In the joint v en tures domain/, the task is to extract information concerning join t v en turerelationships which organizations form and dissolve/. The template structure represen tsthese relationships with tie/-up/-relationship ob jects/, whic h con tain p oin ters to organizationen tit y ob jects representing the organizations inv olv ed/. En tit y ob jects con tain p oin ters toother ob jects suc h as p erson and facility ob jects/, as sho wn in Figure /1/.In the micro electronics domain/, extraction fo cuses on la y ering/, lithograph y /, etc hing/, andpac k aging pro cesses in semiconductor man ufacturing for micro c hip fabrication/. The en tities/9/0P a ttern Ma tching and Discourse Pr ocessing\nTEMPLATE\ndoc. no.\ndoc. date\ndoc. source\ncontent (*) TIE-UP-REL.\ntie-up status\nentity (+)\ncreated \n       entity (*)\nactivity (*)ENTITY\nname\naliases\nlocation\nnationality\ntype\nentity rel. (+)\nperson(*)\nfacility (*)\nINDUSTRY\nindustry type\nproduct /\n          serviceENTITY-REL.\nentity1 (+)\nentity2 (*)\nrel. of ent2 \n           to ent1\nPERSON\nname\nperson's entity \nposition\nFACILITY\nname\nlocation\ntype\n(*) points to zero or more objects,  (+) points to one or more objectsACTIVITY\nindustry (*)\ndenotes instantiations of multiple objectsFigure /1/"}
{"category": "non-abstract", "text": "/:/: /, with the follo wingprop erties/: /\frst/, /\n/1\nis the class of all strati/\fed kno wledge bases/; second/, if a kno wledgebase /\u0005 is in /\nk\n/, then /\u0005 has at most k stable mo dels/, and all of them ma y b e found in timeO /( l nk /)/, where l is the length of the kno wledge base and n the n um b er of atoms in /\u0005/; third/,for an arbitrary kno wledge base /\u0005/, w e can /\fnd the minim um k suc h that /\u0005 b elongs to /\nkin time p olynomial in the size of /\u0005/; and/, last/, where K is the class of all kno wledge bases/,it is the case that\nS/1i /=/1\n/\ni\n/= K /, that is/, ev ery kno wledge base b elongs to some class in thehierarc h y /./1/. In tro ductionThe task of computing the stable mo dels of a kno wledge base lies at the heart of three ofthe fundamen tal systems in Arti/\fcial In telligence /(AI/)/: truth main tenance systems /(TMSs/)/,default logic/, and auto epistemic logic/. Y et/, this task is in tractable /(Elk an/, /1/9/9/0/; Kautz /&Selman/, /1/9/9/1/; Marek /& T ruszczy /\u0013 nski/, /1/9/9/1/)/. In this pap er/, w e in tro duce a hierarc h y ofclasses of kno wledge bases whic h ac hiev es this task in p olynomial time/. Mem b ership in acertain class in the hierarc h y is testable in p olynomial time/. Hence/, giv en a kno wledge base/,the cost of computing its stable mo dels can b e b ounded prior to the actual computation /(ifthe algorithms on whic h this hierarc h y is based are used/)/.First/, let us elab orate the relev ance of computing stable mo dels to AI tasks/. W e de/\fnea kno wledge base to b e a set of rules of the formC / /BnZr A/1\n/; /:/:/:/; Am\n/; not B/1\n/; /:/:/:/; not Bn\n/(/1/)where C /, all A s/, and all B s are atoms in some prop ositional language/. Substan tial e/\u000borts togiv e a meaning/, or seman tics/, to a kno wledge base ha v e b een made in the logic programmingcomm unit y /(Przym usinsk a /& Przym usinski/, /1/9/9/0/)/. One of the most successful seman tics forkno wledge bases is stable mo del semantics /(Bidoit /& F roidev aux/, /1/9/8/7/; Gelfond /& Lifsc hitz/,/1/9/8/8/; Fine/, /1/9/8/9/)/, whic h asso ciates an y kno wledge base with a /(p ossibly empt y/) set ofmo dels called stable mo dels /. In tuitiv ely /, eac h stable mo del represen ts a set of coheren tc/\r /1/9/9/6 AI Access F oundation and Morgan Kaufmann Publishers/. All righ ts reserv ed/.Ben/-Eliy ahuconclusions one migh t deduce from the kno wledge base/. It turns out that stable mo delspla y a cen tral role in some ma jor deductiv e systems in AI/.\n/1/1/./1 Stable Mo dels and TMSsTMSs /(Do yle/, /1/9/7/9/) are inference systems for nonmonotonic reasoning with default as/-sumptions/. The TMS manages a set of no des and a set of justi/\fcations/, where eac h no derepresen ts a piece of information and the justi/\fcations are rules that state the dep endenciesb et w een the no des/. The TMS computes a gr ounde d set of no des and assigns this set to b ethe information b eliev ed to b e true at a giv en p oin t in time/. In tuitiv ely /, a set of b eliev edno des is grounded if it satis/\fes all the rules/, but no no de is b eliev ed true solely on the basisof a circular c hain of justi/\fcations/. Elk an /(/1/9/9/0/) p oin ted out that the no des of a TMScan b e view ed as prop ositional atoms/, and the set of its justi/\fcations as a kno wledge base/.He sho w ed that the task of computing grounded in terpretations for a set of TMS justi/\fca/-tions corresp onds exactly to the task of computing the stable mo dels of the kno wledge baserepresen ted b y the set of TMS justi/\fcations/./1/./2 Stable Mo dels and Auto epistemic LogicAuto epistemic logic w as in v en ted b y Mo ore /(/1/9/8/5/) in order to formalize the pro cess of anagen t reasoning ab out its o wn b eliefs/. The language of auto epistemic logic is a prop ositionallanguage augmen ted b y a mo dal op erator L /. Giv en a theory /(a set of form ulas/) T inauto epistemic logic/, a theory E is called a stable exp ansion of T i/\u000bE /= /( T\nSf L F j F /2 E g\nSf/: L F j F /= /2 E g /)\n/\u0003where T\n/\u0003denotes the logical closure of T /. W e will no w restrict ourselv es to a subset ofauto epistemic logic in whic h eac h form ula is of the formA/1\n/^ /:/:/: /^ Am\n/^ /: L B/1\n/^ /:/:/: /^ /: L Bn\n/BnZr /! C /(/2/)where C /, eac h of the A s/, and eac h of the B s are prop ositional atoms/. W e call this subsetthe class of auto epistemic pr o gr ams /. Ev ery auto epistemic program T can b e translated in toa kno wledge base /\u0005T\nb y represen ting the form ula /(/2/) as the kno wledge base rule /(/1/) /. Elk an/(/1/9/9/0/) has sho wn that M is a stable mo del of /\u0005T\ni/\u000b there is an expansion E of T suc hthat M is the set of all p ositiv e atoms in E /. Th us/, algorithms for computing stable mo delsma y b e used in computing expansions of auto epistemic programs/. The relationship b et w eenstable mo del seman tics and auto epistemic logic has also b een explored b y Gelfond /(/1/9/8/7/)and Gelfond and Lifsc hitz /(/1/9/8/8/, /1/9/9/1/)/./1/./3 Stable Mo dels and Default LogicDefault logic is a formalism dev elop ed b y Reiter /(/1/9/8/0/) for reasoning with default assump/-tions/. A default theory can b e view ed as a set of defaults/, and a default is de/\fned as anexpression of the form/\u000b /: /\f/1\n/; /:/:/:/; /\fn/\r\n/(/3/)/1/. In logic programming terminology /, the kno wledge bases discussed in this pap er are called normal lo gicpr o gr ams /./2/8A Hierar chy of Tra ct able Subsetswhere /\u000b/; /\r /, and /\f/1\n/; /:/:/:/; /\fn\nare form ulas in some /\frst/-order language/. According to Reiter/, Eis an extension for a default theory /\u0001 i/\u000b E coincides with one of the minimal deductiv elyclosed sets of sen tences E\n/0satisfying the condition\n/2that for an y grounded instance of adefault /(/3/) from /\u0001/, if /\u000b /2 E\n/0and /: /\f/1\n/; /:/:/:/; /: /\fn\n/= /2 E /, then /\r /2 E\n/0/.No w consider the subset of default theories that w e call default pr o gr ams /. A defaultpr o gr am is a set of defaults of the formA/1\n/^ /:/:/: /^ Am\n/: /: B/1\n/; /:/:/:/; /: BnC\n/(/4/)in whic h C /, eac h of the A s/, and eac h of the B s are atoms in a prop ositional language/.Eac h default program /\u0001 can b e asso ciated with a kno wledge base /\u0005/\u0001\nb y replacing eac hdefault of the form /(/4/) with the rule /(/1/) /.Gelfond and Lifsc hitz /(/1/9/9/1/) ha v e sho wn that the logical closure of a set of atoms E isan extension of /\u0001 i/\u000b E is a stable mo del of /\u0005/\u0001\n/. Algorithms for computing stable mo delscan th us b e used in computing extensions of Reiter/'s default theories/./\u0003 /\u0003 /\u0003The pap er is organized as follo ws/. In the next section/, w e de/\fne our terminology /.Section /3 presen ts t w o algorithms for computing all stable mo dels of a kno wledge base/.The complexit y of the /\frst of these algorithms dep ends on the n um b er of atoms app earingnegativ ely in the kno wledge base/, while the complexit y of the other algorithm dep endson the n um b er of rules ha ving negativ e atoms in their b o dies/. In Section /4/, w e presen tthe main algorithm of the pap er/, called algorithm AAS /. Algorithm AAS w orks from theb ottom up on the sup erstructure of the dep endency graph of the kno wledge base and usesthe t w o algorithms presen ted in Section /3 as subroutines/. Section /5 explains ho w the AASalgorithm can b e generalized to handle kno wledge bases o v er a /\frst/-order language/. Finally /,in Sections /6 and /7/, w e discuss related w ork and mak e concluding remarks/./2/. Preliminary De/\fnitionsRecall that here a kno wledge base is de/\fned as a set of rules of the formC / /BnZr A/1\n/; /:/:/:/; Am\n/; not B/1\n/; /:/:/:/; not Bn\n/(/5/)where C /, eac h of the A s/, and eac h of the B s are prop ositional atoms/. The expression to theleft of / /BnZr is called the he ad of the rule/, while the expression to the righ t of / /BnZr is calledthe b o dy of the rule/. Eac h of the A s is said to app e ar p ositive in the rule/, and/, accordingly /,eac h of the B s is said to app e ar ne gative in the rule/. Rule /(/5/) is said to b e ab out C /. A rulewith an empt y b o dy is called a unit rule /. Sometimes w e will treat a truth assignmen t /(inother w ords/, in terpretation/) in prop ositional logic as a set of atoms /| the set of all atomsassigned true b y the in terpretation/. Giv en an in terpretation I and a set of atoms A /, IAdenotes the pro jection of I o v er A /. Giv en t w o in terpretations/, I and J /, o v er sets of atoms/2/. Note the app earance of E in the condition/./2/9Ben/-Eliy ahuA and B /, resp ectiv ely /, the in terpretation I /+ J is de/\fned as follo ws/:I /+ J /( P /) /=\n/8/>/>/>/</>/>/>/:\nI /( P /) if P /2 A n BJ /( P /) if P /2 B n AI /( P /) if P /2 A\nTB and I /( P /) /= J /( P /)unde/\fned otherwiseIf I /( P /) /= J /( P /) for ev ery P /2 A\nTB /, w e sa y that I and J are c onsistent /.A p artial in terpretation is a truth assignmen t o v er a subset of the atoms/. Hence/, a partialin terpretation can b e represen ted as a consisten t set of literals/: p ositiv e literals represen tthe atoms that are true/, negativ e literals the atoms that are false/, and the rest are unkno wn/.A kno wledge base will b e called Horn if all its rules are Horn/. A mo del for a theory /(setof clauses/) in prop ositional logic is a truth assignmen t that satis/\fes all the clauses/. If onelo oks at a kno wledge base as a theory in prop ositional logic/, a Horn kno wledge base has aunique minimal mo del /(recall that a mo del m is minimal among a set of mo dels M i/\u000b thereis no mo del m\n/0/2 M suc h that m\n/0/\u001a m /)/.Giv en a kno wledge base /\u0005 and a set of atoms m /, Gelfond and Lifsc hitz de/\fned what isno w called the Gelfond/-Lifschitz /(GL/) tr ansform of /\u0005 w/.r/.t/. m /, whic h is a kno wledge base/\u0005m\nobtained from /\u0005 b y deleting eac h rule that has a negativ e literal not P in its b o dy withP /2 m and deleting all negativ e literals in the b o dies of the remaining rules/. Note that /\u0005mis a Horn kno wledge base/. A mo del m is a stable mo del of a kno wledge base /\u0005 i/\u000b it is theunique minimal mo del of /\u0005m\n/(Gelfond /& Lifsc hitz/, /1/9/8/8/)/.Example /2/./1 Consider the follo wing kno wledge base /\u0005/0\n/, whic h will b e used as one of thecanonical examples throughout this pap er/:w ar m bl ooded / /BnZr mammal /(/6/)l iv e on l and / /BnZr mammal /; not ab /1 /(/7/)f emal e / /BnZr mammal /; not mal e /(/8/)mal e / /BnZr mammal /; not f emal e /(/9/)mammal / /BnZr dol phin /(/1/0/)ab /1 / /BnZr dol phin /(/1/1/)mammal / /BnZr l ion /(/1/2/)l ion / /BnZr /(/1/3/)m /= f l ion/; mammal /; w ar m bl ooded/; l iv e on l and/; f emal e g is a stable mo del of /\u0005/0\n/. Indeed/,/\u0005/0m\n/(the GL transform of /\u0005/0\nw/.r/.t/. m /) isw ar m bl ooded / /BnZr mammall iv e on l and / /BnZr mammalf emal e / /BnZr mammalmammal / /BnZr dol phinab /1 / /BnZr dol phinmammal / /BnZr l ionl ion / /BnZr/3/0A Hierar chy of Tra ct able Subsetsand m is a minimal mo del of /\u0005/0m\n/.A set of atoms S satis/\fes the b o dy of a rule /\u000e i/\u000b eac h atom that app ears p ositiv e in theb o dy of /\u000e is in S and eac h atom that app ears negativ e in the b o dy of /\u000e is not in S /. A setof atoms S satis/\fes a rule i/\u000b either it do es not satisfy its b o dy /, or it satis/\fes its b o dy andthe atom that app ears in its head b elongs to S /.A pr o of of an atom is a sequence of rules from whic h the atom can b e deriv ed/. F ormally /,w e can recursiv ely de/\fne when an atom P has a pro of w/.r/.t/. a set of atoms S and akno wledge base /\u0005/:/\u000f If the unit rule P / /BnZr is in /\u0005/, then P has a pro of w/.r/.t/. /\u0005 and S /./\u000f If the rule P / /BnZr A/1\n/; /:/:/:/; Am\n/; not B/1\n/; /:/:/:/; not Bn\nis in /\u0005/, and for ev ery i /= /1 /; /:/:/:/; n Bi\nisnot in S /, and for ev ery i /= /1 /; /:/:/:/; m Ai\nalready has a pro of w/.r/.t/. /\u0005 and S /, then P hasa pro of w/.r/.t/. /\u0005 and S /.Theorem /2/./2 /(Elk an/, /1/9/9/0/; Ben/-Eliy ah u /& Dec h ter/, /1/9/9/4/) A set of atoms S is a stablemo del of a know le dge b ase /\u0005 i/\u000b/1/. S satis/\fes e ach rule in /\u0005 /, and/2/. for e ach atom P in S /, ther e is a pr o of of P w/.r/.t /\u0005 and S /.It is a simple matter to sho w that the follo wing lemma is true/.Lemm a /2/./3 L et /\u0005 b e a know le dge b ase/, and let S b e a set of atoms/. De/\fne/:/1/. S/0\n/= /; /, and/2/. Si /+/1\n/= Si\nSf P j P / /BnZr A/1\n/; /:/:/:/; Am\n/; not B/1\n/; /:/:/:/; not Bn\nis in /\u0005 /;al l of the A /'s b elong to Si\nand none of the B /'s b elong to S g /.Then S is a stable mo del of /\u0005 i/\u000b S /=\nS/1/0\nSi\n/.Observ e that although ev ery stable mo del is a minimal mo del of the kno wledge baseview ed as a prop ositional theory /, not ev ery minimal mo del is a stable mo del/.Example /2/./4 Consider the kno wledge baseb / /BnZr not aBoth f a g and f b g are minimal mo dels of the kno wledge base ab o v e/, but only f b g is a stablemo del of this kno wledge base/.Note that a kno wledge base ma y ha v e one or more stable mo dels/, or no stable mo del at all/.If a kno wledge base has at least one stable mo del/, w e sa y that it is c onsistent /.The dep endency gr aph of a kno wledge base /\u0005 is a directed graph where eac h atom is ano de and where there is a p ositiv e edge directed from P to Q i/\u000b there is a rule ab out Qin /\u0005 in whic h P app ears p ositiv e in the b o dy /. Accordingly /, there is a negativ e edge fromP to Q i/\u000b there is a rule ab out Q in whic h P app ears negativ e in the b o dy /. Recall that asour c e of a directed graph is a no de with no incoming edges/, while a sink is a no de with nooutgoing edges/. Giv en a directed graph G and a no de s in G /, the sub gr aph r o ote d by s isthe subgraph of G ha ving only no des t suc h that there is a path directed from t to s in G /.The childr en of s in G are all no des t suc h that there is an arc directed from t to s in G /./3/1Ben/-Eliy ahuExample /2/./5 The dep endency graph of /\u0005/0\nis sho wn in Figure /1/. Negativ e edges aremark ed /\\not/./\" The c hildren of mammal are lion and dolphin /. The subgraph ro oted b yon land is the subgraph that include the no des lion /, mammal /, dolphin /, ab/1 /, and on land /.\nfemalemale\nwarm_blood\non_land\nab1\ndolphinnot\nnotnot\nlionmammalFigure /1/: The dep endency graph of /\u0005/0A kno wledge base is str ati/\fe d i/\u000b w e can assign eac h atom C a p ositiv e in teger iC\nsuc hthat for ev ery rule in the form of /(/5/) ab o v e/, for eac h of the A s/, iA\n/\u0014 iC\n/, and for eac h ofthe B s/, iB\n/< iC\n/. It can b e readily demonstrated that a kno wledge base is strati/\fed i/\u000b inits dep endency graph there are no directed cycles going through negativ e edges/. It is w ellkno wn in the logic programming comm unit y that a strati/\fed kno wledge base has a uniquestable mo del that can b e found in linear time /(Gelfond /& Lifsc hitz/, /1/9/8/8/; Apt/, Blair/, /&W alk er/, /1/9/8/8/)/.Example /2/./6 /\u0005/0\nis not a strati/\fed kno wledge base/. The follo wing kno wledge base/, /\u0005/1\n/, isstrati/\fed /(w e can assign ab/2 and p enguin the n um b er /1/, and eac h of the other atoms then um b er /2/)/:l iv e on l and / /BnZr bir df l y / /BnZr bir d/; not ab /2bir d / /BnZr peng uinab /2 / /BnZr peng uin/3/2A Hierar chy of Tra ct able SubsetsThe strongly connected comp onen ts of a directed graph G mak e up a partition of itsset of no des suc h that/, for eac h subset S in the partition and for eac h x/; y /2 S /, there aredirected paths from x to y and from y to x in G /. The strongly connected comp onen ts areiden ti/\fable in linear time /(T arjan/, /1/9/7/2/)/.\nfemalemale\nwarm_blood\non_land\nab1\ndolphinnot lionmammalnotnotFigure /2/: The sup er dep endency graph of /\u0005/0The sup er dep endency gr aph of a kno wledge base /\u0005/, denoted G/\u0005\n/, is the sup erstructure ofthe dep endency graph of /\u0005/. That is/, G/\u0005\nis a directed graph built b y making eac h stronglyconnected comp onen t in the dep endency graph of /\u0005 in to a no de in G/\u0005\n/. An arc exists froma no de s to a no de v i/\u000b there is an arc from one of the atoms in s to one of the atoms in vin the dep endency graph of /\u0005/. Note that G/\u0005\nis an acyclic graph/.Example /2/./7 The sup er dep endency graph of /\u0005/0\nis sho wn in Figure /2/. The no des in thesquare are group ed in to a single no de/./3/. Tw o Algorithms for Computing Stable Mo delsThe main con tribution of this pap er is the presen tation of an algorithm whose e/\u000eciencydep ends on the /\\distance/\" of the kno wledge base from a strati/\fed kno wledge base/. Thisdistance will b e measured precisely in Section /4/. W e will /\frst describ e t w o other algorithmsfor computing stable mo dels/. These t w o algorithms do not tak e in to accoun t the lev el of/\\strati/\fabilit y/\" of the kno wledge base/, that is/, they will still w ork in exp onen tial time forstrati/\fed kno wledge bases/. Our main algorithm will use these t w o algorithms as pro cedures/./3/3Ben/-Eliy ahuGiv en a truth assignmen t for a kno wledge base/, w e can v erify in p olynomial time whetherit is a stable mo del b y using Lemma /2/./3/. Therefore/, a straigh tforw ard algorithm for comput/-ing all stable mo dels can simply c hec k all p ossible truth assignmen ts and determine whethereac h of them is a stable mo del/. The time complexit y of this straigh tforw ard pro cedure willb e exp onen tial in the n um b er of atoms used in the kno wledge base/. Belo w/, w e presen t t w oalgorithms that can often function more e/\u000ecien tly than the straigh tforw ard pro cedure/./3/./1 An Algorithm That Dep ends on the Num b er of Negativ e A toms in theKno wledge BaseAlgorithm All/-St able/1 /(Figure /3/) enables us to /\fnd all the stable mo dels in time exp o/-nen tial in the n um b er of the atoms that app ear negativ e in the kno wledge base/.The algorithm follo ws from w ork on ab ductiv e extensions of logic programming in whic hstable mo dels are c haracterized in terms of sets of h yp otheses that can b e dra wn as addi/-tional information /(Eshghi /& Ko w alski/, /1/9/8/9/; Dung/, /1/9/9/1/; Kak as /& Mancarella/, /1/9/9/1/)/.This is done b y making negativ e atoms ab ductible and b y imp osing appropriate denialsand disjunctions as in tegrit y constrain ts/. The w ork of Eshghi and Ko w alski /(/1/9/8/9/)/, Dung/(/1/9/9/1/)/, and Kak as and Mancarella /(/1/9/9/1/) implies the follo wing/.Theorem /3/./1 L et /\u0005 b e a know le dge b ase/, and let H b e the set of atoms that app e ar ne gate din /\u0005 /. M is a stable mo del of /\u0005 i/\u000b ther e is an interpr etation I over H such that/1/. for every atom P /2 H /, if P /2 I /, then P /2 M\n/0/,/2/. M\n/0and I ar e c onsistent/, and/3/. M /= I /+ M\n/0/,wher e M\n/0is the unique stable mo del of /\u0005I\n/.Pro of/: The pro of follo ws directly from the de/\fnition of stable mo dels/. Supp ose M is astable mo del of a kno wledge base /\u0005/, and let H b e the set of atoms that app ear negativ e in/\u0005/. Then/, b y de/\fnition/, M is a stable mo del of /\u0005M\n/. But note that /\u0005M\n/= /\u0005MH\n/. Hence/, theconditions of Theorem /3/./1 hold for M /, taking M\n/0/= M and I /= MH\n/. No w/, supp ose /\u0005 isa kno wledge base and M /= M\n/0/+ I /, where M\n/0and I are as in Theorem /3/./1/. Observ e that/\u0005M\n/= /\u0005I\nand/, hence/, since M\n/0is a stable mo del of /\u0005I\n/, M\n/0is a stable mo del of /\u0005M\n/. W ewill sho w that M is a stable mo del of /\u0005M\n/. First/, note that b y condition /1/, M /\u0012 M\n/0/. Th us/,M satis/\fes all the rules in /\u0005M\nand/, if an atom P has a pro of w/. r/. t/. M\n/0and /\u0005M\n/, it hasalso a pro of w/. r/. t/. M and /\u0005M\n/. So/, b y Theorem /2/./2/, M is a stable mo del of /\u0005M\nand/, b yde/\fnition/, M is a stable mo del of /\u0005/.Theorem /3/./1 implies algorithm All/-St able/1 /(Figure /3/)/, whic h computes all stablemo dels of a kno wledge base /\u0005/. Hence/, w e ha v e the follo wing complexit y analysis/.Prop osition /3/./2 A know le dge b ase in which at most k atoms app e ar ne gate d has at most/2\nkstable mo dels and al l of them c an b e found in time O /( nl /2\nk/) /, wher e l is the size of theknow le dge b ase and n the numb er of atoms use d in the know le dge b ase/.Pro of/: F ollo ws from the fact that computing /\u0005I\nand computing the unique stable mo delof a p ositiv e kno wledge base is O /( nl /)/./3/4A Hierar chy of Tra ct able SubsetsAll/-St able/1 /(/\u0005/)Input/: A kno wledge base /\u0005/.Output/: The set of all stable mo dels of /\u0005/./1/. M /:/= /; /;/2/. F or eac h p ossible in terpretation I for the set of all atoms that app ear negativ e in /\u0005/,do/:/(a/) Compute M\n/0/, the unique stable mo del of /\u0005I\n/;/(b/) If M\n/0and I are consisten t/, let M /:/= M\nSf M\n/0/+ I g /;/3/. Return M /;Figure /3/: Algorithm All/-St able/1/3/./2 An Algorithm That Dep ends on the Num b er of Non/-Horn RulesAlgorithm All/-St able/2 /(Figure /4/) dep ends on the n um b er of rules in whic h there arenegated atoms/. It gets as input a kno wledge base /\u0005/, and/, it outputs the set of all stablemo dels of /\u0005/. This algorithm is based up on the observ ation that a stable mo del can b ebuilt b y attempting all p ossible means of satisfying the negated atoms in b o dies of non/-Horn rules/. Tw o pro cedures are called b y All/-St able/2 /: UnitInst/, sho wn in Figure /5/; andNegUnitInst/, sho wn in Figure /6/. Pro cedure UnitInst gets as input a kno wledge base /\u0005 anda partial in terpretation m /. UnitInst lo oks recursiv ely for unit rules in /\u0005/. F or eac h unit ruleP / /BnZr /, if P is assigned false in m /, it follo ws that m cannot b e a part of a mo del for /\u0005/, andthe pro cedure returns false /. If P is not false in m /, the pro cedure instan tiates P to true inthe in terpretation m and deletes the p ositiv e app earances of P from the b o dy of eac h rule/.It also deletes from /\u0005 all the rules ab out P and all the rules in whic h P app ears negativ e/.Pro cedure NegUnitInst receiv es as input a kno wledge base /\u0005/, a partial in terpretationm /, and a set of atoms N eg /. It /\frst instan tiates eac h atom in N eg to false and then up datesthe kno wledge base to re/\rect this instan tiation/. All the instan tiations are recorded in m /.In case of a con/\rict/, namely /, where the pro cedure tries to instan tiate to true an atom thatis already set to false /, the pro cedure returns false /; otherwise/, it returns true /.Prop osition /3/./3 A lgorithm All/-St able/2 is c orr e ct/, that is/, m is a stable mo del of aknow le dge b ase /\u0005 i/\u000b it is gener ate d by All/-St able/2 /(/\u0005/) /.Pro of/: Supp ose m is a stable mo del of a kno wledge base /\u0005/. Then/, b y Theorem /2/./2/, ev eryatom set to true in m has a pro of w/. r/. t/. m and /\u0005/. Let S b e the set of all non/-Hornrules whose b o dies are satis/\fed b y m /. Clearly /, at some p oin t this S is c hec k ed at step /3 ofalgorithm All/-St able/2 /. When this happ ens/, all atoms that ha v e a pro of w/. r/. t/. m and/\u0005 will b e set to true b y the pro cedure NegUnitInst /(as can b e pro v ed b y induction on thelength of the pro of /)/. Hence/, m will b e generated/.Supp ose m is generated b y All/-St able/2 /(/\u0005/)/. Ob viously /, ev ery rule in /\u0005 is satis/\fed b ym /(step /3/.c/.ii/)/, and ev ery atom set to true b y NegUnitInst has a pro of w/. r/. t/. m and /\u0005/3/5Ben/-Eliy ahuAll/-St able/2 /(/\u0005/)Input/: A kno wledge base /\u0005/.Output/: The set of all stable mo dels of /\u0005/./1/. M /:/= /; /;/2/. Let /\u0001 b e the set of all non/-Horn rules in /\u0005/./3/. F or eac h subset S of /\u0001/, do/:/(a/) N eg /= f P j not P is in the b o dy of some rule in S g /;/(b/) /\u0005\n/0/:/= /\u0005/; m /:/= /; /;/(c/) If NegUnitInst/(/\u0005\n/0/; N eg /; m /)/, theni/. F or eac h P suc h that m /[ P /] /= nul l /, let m /[ P /] /:/= false /;ii/. If m satis/\fes all the rules in /\u0005/, then M /:/= M\nSf m g /;/4/. EndF or/;/5/. Return M /;Figure /4/: Algorithm All/-St able/2UnitInst/(/\u0005 /; m /)Input/: A kno wledge base /\u0005 and a partial in terpretation m /.Output/: Up dates m using the unit rules of /\u0005/. Returns false if there is a con/\rict b et w eena unit rule and the v alue assigned to some atom in m /; otherwise/, returns true /./1/. While /\u0005 has unit rules/, do/:/(a/) Let P / /BnZr b e a unit rule in /\u0005/;/(b/) If m /[ P /] /= false /, return false /;/(c/) m /[ P /] /:/= true /;/(d/) Erase P from the b o dy of eac h rule in /\u0005/;/(e/) Erase from /\u0005 all rules ab out P /;/(f /) Erase from /\u0005 all rules in whic h P app ears negativ e/;/2/. EndWhile/;/3/. Return true /;Figure /5/: Pro cedure UnitInst/3/6A Hierar chy of Tra ct able SubsetsNegUnitInst/(/\u0005 /; N eg /; m /)Input/: A kno wledge base /\u0005/, a set of atoms N eg /, and a partial in terpretation m /.Output/: Up dates m assuming the atoms in N eg are false/. Returns false if inconsistency isdetected/; otherwise/, returns true /./1/. F or eac h atom P in N eg/(a/) m /[ P /] /:/= false /;/(b/) Delete from the b o dy of eac h rule in /\u0005 eac h o ccurrence of not P /;/(c/) Delete from /\u0005 eac h rule in whic h P app ears p ositiv e in the b o dy/;/2/. EndF or/;/3/. Return UnitInst/(/\u0005 /; m /)/;Figure /6/: Pro cedure NegUnitInstlion dolphin ab/1 mammal w arm b on land male femaleS /1 T F F T T T F TS /2 T F F T T T T FT able /1/: Mo dels generated b y Algorithm All/-St able/2/(as is readily observ able from the w a y NegUnitInst w orks/)/. Hence/, b y Theorem /2/./2/, m is astable mo del of /\u0005/.Prop osition /3/./4 A know le dge b ase having c non/-Horn rules has at most /2\ncstable mo delsand al l of them c an b e found in time O /( nl /2\nc/) /, wher e l is the size of the know le dge b ase andn the numb er of atoms use d in the know le dge b ase/.Pro of/: Straigh tforw ard/, b y induction on c /.Example /3/./5 Supp ose w e call All/-St able/2 with /\u0005/0\nas the input kno wledge base/. A tstep /2/, /\u0001 is the set of rules /(/7/) /, /(/8/) /, and /(/9/) /. When subsets of /\u0001 whic h include b oth rules/(/8/) and /(/9/) are considered at step /3/, NegUnitInst will return false b ecause UnitInst willdetect inconsistency /. When the subset con taining b oth rules /(/7/) and /(/8/) is considered/, thestable mo del S /1 of T able /1 will b e generated/. When the subset con taining b oth rules /(/7/)and /(/9/) is considered/, the stable mo del S /2 of T able /1 will b e generated/. When all the othersubsets that do not con tain b oth rules /(/8/) and /(/9/) are tested at step /3/, the m generated willnot satisfy all the rules in /\u0005 and/, hence/, will not app ear in the output/.Algorithms All/-St able/1 and All/-St able/2 do not tak e in to accoun t the structureof the kno wledge base/. F or example/, they are not p olynomial for the class of strati/\fedkno wledge bases/. W e presen t next an algorithm that exploits the structure of the kno wledgebase/./3/7Ben/-Eliy ahu/4/. A Hierarc h y of T ractable Subsets Based on the Lev el of Strati/\fabilit yof the Kno wledge BaseAlgorithm A cyclic/-All/-St able /(AAS/) in Figure /7 exploits the structure of the kno wledgebase as it is re/\rected in the sup er dep endency graph of the kno wledge base/. It computes allstable mo dels while tra v ersing the sup er dep endency graph from the b ottom up/, using thealgorithms for computing stable mo dels presen ted in the previous section as subroutines/.Let /\u0005 b e a kno wledge base/. With eac h no de s in G/\u0005\n/(the sup er dep endency graph of/\u0005/)/, w e asso ciate /\u0005s\n/, As\n/, and Ms\n/. /\u0005s\nis the subset of /\u0005 con taining all the rules ab out theatoms in s /, As\nis the set of all atoms in the subgraph of G/\u0005\nro oted b y s /, and Ms\nis the set ofstable mo dels asso ciated with the subset of the kno wledge base /\u0005 whic h con tains only rulesab out atoms in As\n/. Initially /, Ms\nis empt y for ev ery s /. The algorithm tra v erses G/\u0005\nfromthe b ottom up/. When at a no de s /, it /\frst com bines all the submo dels of the c hildren of sin to a single set of mo dels Mc /( s /)\n/. If s is a source/, then Mc /( s /)\nis set to f/;g\n/3/. Next/, for eac hmo del m in Mc /( s /)\n/, AAS con v erts /\u0005s\nto a kno wledge base /\u0005sm\nusing the GL transform andother transformations that dep end on the atoms in m /; then/, it /\fnds all the stable mo delsof /\u0005sm\nand com bines them with m /. The set Ms\nis obtained b y rep eating this op eration foreac h m in Mc /( s /)\n/. AAS uses the pro cedure CartesPro d /(Figure /8/)/, whic h receiv es as inputsev eral sets of mo dels and returns the consisten t p ortion of their Cartesian pro duct/. If oneof the sets of mo dels whic h CartesPro d gets as input is the empt y set/, CartesPro d willoutput an empt y set of mo dels/. The pro cedure Con v ert gets as input a kno wledge base /\u0005/,a mo del m /, and a set of atoms s /, and p erforms the follo wing/: for eac h atom P in m /, eac hp ositiv e o ccurrence of P is deleted from the b o dy of eac h rule in /\u0005/; for eac h rule in /\u0005/, ifnot P is in the b o dy of the rule and P /2 m /, then the rule is deleted from /\u0005/; if not P isin the b o dy of a rule and P /= /2 m /, then/, if P /= /2 s /, not P is deleted from that b o dy /. Thepro cedure All/-St able called b y AAS ma y b e one of the pro cedures previously presen ted/( All/-St able/1 or All/-St able/2 /) or it ma y b e an y other pro cedure that generates all stablemo dels/.Example /4/./1 Supp ose AAS is called to compute the stable mo dels of /\u0005/0\n/. Supp ose furtherthat the algorithm tra v erses the sup er dep endency graph in Figure /2 in the order f lion /,dolphin /, mammal /, ab/1 /, on land /, warm blo o de d /, female/-male g /(recall that all the no des in/-side the square mak e up one no de that w e are calling female/-male or/, for short/, FM/)/.After visiting all the no des except the last/, w e ha v e Mlion\n/= ff l ion gg /, Mdolphin\n/= f/;g /,Mmammal\n/= ff l ion/; mammal gg /, Mon land\n/= ff l ion/; mammal /; onl and g g /, Mw ar m blooded\n/=ff l ion/; mammal /; war m bl ooded gg /. When visiting the no de FM/, w e ha v e after step /1/.c thatMc /( F M /)\n/= Mmammal\n/. So step /1/.d lo ops only once/, for m /= f l ion/; mammal g /. Recall that/\u0005F M\nis the kno wledge basef emal e / /BnZr mammal /; not mal emal e / /BnZr mammal /; not f emal e/3/. Note the di/\u000berence b et w een f/;g /, whic h is a set of one mo del /- the mo del that assigns false to all theatoms/, and /; /, whic h is a set that con tains no mo dels/./3/8A Hierar chy of Tra ct able SubsetsA cyclic/-All/-St able /(/\u0005/)Input/: A kno wledge base /\u0005/.Output/: The set of all stable mo dels of /\u0005/./1/. T ra v erse G/\u0005\nfrom the b ottom up/. F or eac h no de s /, do/:/(a/) Ms\n/:/= /; /;/(b/) Let s/1\n/; /:/:/:/; sj\nb e the c hildren of s /./(c/) If j /= /0/, then Mc /( s /)\n/:/= f/;g /;else Mc /( s /)\n/:/= CartesPro d/( f Ms/1\n/; /:/:/:/; Msj\ng /)/;/(d/) F or eac h m /2 Mc /( s /)\n/, do/:i/. /\u0005sm\n/:/= Con v ert/(/\u0005s\n/; m/; s /)/;ii/. M /:/= All/-St able /(/\u0005sm\n/)/;iii/. If M /6/= /; /, then Ms\n/:/= Ms\nSCartesPro d/( ff m g /; M g /) /;/2/. Output CartesPro d/( f Ms/1\n/; /:/:/:/; Msk\ng /)/, where s/1\n/; /:/:/:/; sk\nare the sinks of G/\u0005\n/.Figure /7/: Algorithm A cyclic/-All/-St able /(AAS/)CartesPro d/( M /)Input/: A set of sets of mo dels M /.Output/: A set of mo dels whic h is the consisten t p ortion of the Cartesian pro duct of thesets in M /./1/. If M has a single elemen t f E g /, then return E /;/2/. M /:/= /; /;/3/. Let M\n/0/2 M /;/4/. D /:/= CartesPro d/( M n f M\n/0g /)/;/5/. F or eac h d in D /, do/:/(a/) F or eac h m in M\n/0/, do/:If m and d are consisten t/, then M /:/= M\nSf m /+ d g /;/(b/) EndF or/;/6/. EndF or/;/7/. Return M /;Figure /8/: Pro cedure CartesPro d/3/9Ben/-Eliy ahuAfter executing step /1/.d/.i/, w e ha v e /\u0005F Mm\nset tof emal e / /BnZr not mal emal e / /BnZr not f emal eThe ab o v e kno wledge base has t w o stable mo dels/: f f emal e g and f mal e g /. The Cartesianpro duct of the ab o v e set with f l ion/; mammal g yields MF M\n/= ff l ion/; mammal /; f emale g /;f l ion/; mammal /; male gg /. A t step /2/, the Cartesian pro duct of Mw ar m blooded\n/, Mon land\n/, andMF M\nis tak en/. Th us/, the algorithm outputs ff l ion/; mammal /; on l and/; w ar m bl ooded/; f emal e g /,f l ion/; mammal /; on l and/; w ar m bl ooded/; mal e gg /, and these are indeed the t w o stable mo delsof /\u0005/0\n/. Note that algorithm AAS is more e/\u000ecien t than either All/-St able/1 or All/-St able/2on the kno wledge base /\u0005/0\n/.Theorem /4/./2 A lgorithm AAS is c orr e ct/, that is/, m is a stable mo del of a know le dge b ase/\u0005 i/\u000b m is gener ate d by AAS when applie d to /\u0005 /.Pro of/: Let s/0\n/; s/1\n/; /:/:/:/; sn\nb e the ordering of the no des of the sup er dep endency graph b ywhic h the algorithm is executed/. W e can sho w b y induction on i that AAS/, when at no desi\n/, generates all and only the stable mo dels of the p ortion of the kno wledge base comp osedof rules that only use atoms from Asi\n/.case i /= /0 /: In this case/, at step /1/.d/.ii of AAS/, /\u0005sm\n/= /\u0005s\n/; th us/, the claim follo ws from thecorrectness of the algorithm All/-St able called in step /1/.d/.ii/.case i /> /0 /: Sho wing that ev ery mo del generated is stable is straigh tforw ard/, b y the induc/-tion h yp othesis and Theorem /2/./2/. The other direction is/: supp ose m is a stable mo delof /\u0005s\n/; sho w that m is generated/. Clearly /, for eac h c hild s of si\n/, the pro jection of mon to As\nis a stable mo del of the part of the kno wledge base that uses only atoms fromAs\n/. By induction/, mc\n/, whic h is the pro jection of m on to the union of As\nfor ev eryc hild s of si\n/, m ust b elong to Mc /( si\n/)\ncomputed at step /1/.c/. Therefore/, to sho w that mis generated/, w e need only sho w that m\n/0/= m /BnZr mc\nis a stable mo del of /\u0005simc\n/. Thisis easily done using Theorem /2/./2/.W e will no w analyze the complexit y of AAS/. First/, giv en a kno wledge base /\u0005 and aset of atoms s /, w e de/\fne\n/^/\u0005s\nto b e the kno wledge base obtained from /\u0005 b y deleting eac hnegativ e o ccurrence of an atom that do es not b elong to s from the b o dy of ev ery rule/.F or example/, if /\u0005 /= f a / /BnZr not b/; c / /BnZr not d/; a g and s /= f b g /, then\n/^/\u0005s\n/= f a / /BnZr not b/; c / /BnZr a g /.While visiting a no de s during the execution of AAS/, w e ha v e to compute at step /1/.d/.ii allstable mo dels of some kno wledge base /\u0005sm\n/. Using either All/-St able/1 or All/-St able/2 /,the estimated time required to /\fnd all stable mo dels of /\u0005sm\nis shorter than or equal to thetime required to /\fnd all stable mo dels of\n/^/\u0005s\n/. This o ccurs b ecause the n um b er of negativ eatoms and the n um b er of rules with negativ e atoms in their b o dies in\n/^/\u0005s\nis higher thanor equal to the n um b er of negativ e atoms and the n um b er of rules with negativ e atoms intheir b o dies in /\u0005sm\n/, regardless of what m is/. Th us/, if\n/^/\u0005s\nis a Horn kno wledge base/, w e can/\fnd the stable mo del of\n/^/\u0005s\n/, and hence of /\u0005sm\n/, in p olynomial time/, no matter what m is/./4/0A Hierar chy of Tra ct able SubsetsIf\n/^/\u0005s\nis not p ositiv e/, then w e can /\fnd all stable mo dels of\n/^/\u0005s\n/, and hence of /\u0005sm\n/, in timemin/( l n /\u0003 /2\nk/; l n /\u0003 /2\nc/)/, where l is the length of\n/^/\u0005s\n/, n the n um b er of atoms used in\n/^/\u0005s\n/, c then um b er of rules in\n/^/\u0005s\nthat con tain negativ e atoms/, and k the n um b er of atoms that app earnegativ ely in\n/^/\u0005s\n/.Then/, with eac h kno wledge base /\u0005/, w e asso ciate a n um b er t/\u0005\nas follo ws/. Asso ciate an um b er vs\nwith ev ery no de in G/\u0005\n/. If\n/^/\u0005s\nis a Horn kno wledge base/, then vs\nis /1/; else/, vs\nismin /(/2\nk/; /2\nc/)/, where c is the n um b er of rules in\n/^/\u0005s\nthat con tain negativ e atoms from s /, andk is the n um b er of atoms from s that app ear negativ ely in\n/^/\u0005s\n/. No w asso ciate a n um b er tswith ev ery no de s /. If s is a leaf no de/, then ts\n/= vs\n/. If s has c hildren s/1\n/; /:/:/:/; sj\nin G/\u0005\n/, thents\n/= vs\n/\u0003 ts/1\n/\u0003 /:/:/: /\u0003 tsj\n/. De/\fne t/\u0005\nto b e ts/1\n/\u0003 /:/:/: /\u0003 tsk\n/, where s/1\n/; /:/:/:/; sk\nare all the sink no des inG/\u0005\n/.De/\fnition /4/./3 A know le dge b ase /\u0005 b elongs to /\nj\nif t/\u0005\n/= j /.Theorem /4/./4 If a know le dge b ase b elongs to /\nj\nfor some j /, then it has at most j stablemo dels that c an b e c ompute d in time O /( l nj /) /.Pro of/: By induction on j /. The dep endency graph and the sup er dep endency graph areb oth built in time linear in the size of the kno wledge base/. So w e ma y only consider thetime it tak es to compute all stable mo dels with the sup er dep endency graph giv en/.case j /= /1 /: /\u0005 /2 /\n/1\nmeans that for ev ery no de s in G/\u0005\n/,\n/^/\u0005s\nis a Horn kno wledge base/. Inother w ords/, /\u0005 is strati/\fed/, and therefore it has exactly one stable mo del/. There areat most n no des in the graph/. A t eac h no de/, the lo op in step /1/.d is executed at mostonce/, b ecause at most one mo del is generated at ev ery no de/. Pro cedure Con v ert runsin time O /( ls\n/)/, where ls\nis the length of /\u0005s\n/(w e assume that m is stored in an arra ywhere the access to eac h atom is in constan t time/)/. Since/, for ev ery no de s /,\n/^/\u0005s\nis aHorn kno wledge base/, /\u0005sm\nis computed in time O /( ls\nn /)/. Th us/, the o v erall complexit yis O /( l n /)/.case j /> /1 /: By induction on n /, the n um b er of no des in the sup er dep endency graph of /\u0005/.case n /= /1 /: Let s b e the single no de in G/\u0005\n/. Th us/, j /= vs\n/. Using the algorithms fromSection /3/, all stable mo dels of /\u0005 /= /\u0005s\ncan b e found in time O /( l nvs\n/)/, and /\u0005 hasat most vs\nmo dels/.case n /> /1 /: Assume without loss of generalit y that G/\u0005\nhas a single sink s /(to get asingle sink/, w e can add to the program the rule P / /BnZr s/1\n/; /:/:/; sk\n/, where s/1\n/; /:/:/:/; sk\nareall the sinks and P is a new atom/)/. Let c/1\n/; /:/:/:/; ck\nb e the c hildren of s /. F or eac hc hild ci\n/, /\u0005/( ci\n/)/, the part of the kno wledge base whic h corresp onds to the subgraphro oted b y ci\n/, m ust b elong to /\nti\nfor some ti\n/\u0014 j /. By induction on n /, for eac hc hild no de ci\n/, all stable mo dels of /\u0005/( ci\n/) can b e computed in time O /( l nti\n/)/, and/\u0005/( ci\n/) has at most ti\nstable mo dels/. No w let us observ e what happ ens when AASis visiting no de s /. First/, the Cartesian pro duct of all the mo dels computed at thec hild no des is tak en/. This is executed in time O /( n /\u0003 t/1\n/\u0003 /:/:/: /\u0003 tk\n/)/, and yields at mostt/1\n/\u0003 /:/:/: /\u0003 tk\nmo dels in Mc /( s /)\n/. F or ev ery m /2 Mc /( s /)\n/, w e call Con v ert /( O /( l n /)/) andcompute all the stable mo dels of /\u0005sm\n/( O /( l nvs\n/)/)/. W e then com bine them with musing CartesPro d /( O /( nvs\n/)/)/. Th us/, the o v erall complexit y of computing Ms\n/, thatis/, of computing all the stable mo dels of /\u0005/, is O /( l nt/1\n/\u0003 /:/:/: /\u0003 tk\n/\u0003 vs\n/) /= O /( l nj /)/./4/1Ben/-Eliy ahuNote that all strati/\fed kno wledge bases b elong to /\n/1\n/, and the more that an y kno wledgebase lo oks strati/\fed/, the more e/\u000ecien t algorithm AAS will b e/.Giv en a kno wledge base /\u0005/, it is easy to /\fnd the minim um j suc h that /\u0005 b elongs to /\nj\n/.This follo ws b ecause building G/\u0005\nand /\fnding c and k for ev ery no de in G/\u0005\nare p olynomial/-time tasks/. Hence/,Theorem /4/./5 Given a know le dge b ase /\u0005 /, we c an /\fnd the minimum j such that /\u0005 b elongsto /\nj\nin p olynomial time/.Example /4/./6 F or all the no des s in G/\u0005/0\nexcept FM/, vs\n/=/1/. vF M\n/= /2/. Th us/, /\u0005/0\n/2 /\n/2\n/. /\u0005/1is a strati/\fed kno wledge base and therefore b elongs to /\n/1\n/.\nfemalemale\nwarm_blood\non_land\nab1\ndolphinnot lionmammalnotnot\nbirdfly\npenguinab2notFigure /9/: The sup er dep endency graph of /\u0005/0\nS/\u0005/1The next example sho ws that step /5 of pro cedure CartesPro d is necessary /.Example /4/./7 Consider kno wledge base /\u0005/4\n/:a / /BnZr not bb / /BnZr not ac / /BnZr ad / /BnZr be / /BnZr c/; df / /BnZr c/4/2A Hierar chy of Tra ct able Subsets\nnot\nnota bd ce fFigure /1/0/: Sup er dep endency graph of /\u0005/4\nnotnot\na\nbc\nnotnot\na\nbc\n(1) (2)not notFigure /1/1/: Dep endency graph /(/1/) and sup er dep endency graph /(/2/) of /\u0005/2/4/3Ben/-Eliy ahuThe sup er dep endency graph of /\u0005/4\nis sho wn in Figure /1/0/. During the run of algorithm AAS/,Mab\n/(the set of mo dels computed at the no de f a/; b g /) is set to ff a/; /: b g /; f/: a/; b gg /. When AASvisits no des c and d /, w e get Mc\n/= ff a/; /: b/; c g /; f/: a/; b gg /, Md\n/= ff/: a/; b/; d g /; f a/; /: b gg /. WhenAAS visits no de e /, CartesPro d is called on the input f Mc\n/; Md\ng /, yielding the output Me\n/=ff a/; /: b/; c g /; f/: a/; b/; d gg /. Note that CartesPro d do es not output an y mo del in whic h b oth cand d are true /, b ecause the mo dels f a/; /: b/; c g and f/: a/; b/; d g are inconsisten t and CartesPro dc hec ks for consistency in step /5/. When visiting no de f /, w e get Mf\n/= ff a/; /: b/; c/; f g /; f/: a/; b gg /.AAS then returns CartesPro d/( f Me\n/; Mf\ng /)/, whic h is ff a/; /: b/; c/; f g /; f/: a/; b/; d gg /.The next example demonstrates that some mo dels generated at some no des of the su/-p er dep endency graph during the run of AAS ma y later b e deleted/, since they cannot b ecompleted to a stable mo del of the whole kno wledge base/.Example /4/./8 Consider kno wledge base /\u0005/2\n/:a / /BnZr not bb / /BnZr not ac / /BnZr a/; not cThe dep endency graph and the sup er dep endency graph of /\u0005/2\nare sho wn in Figure /1/1/.During the run of algorithm AAS/, Mab\n/(the set of mo dels computed at the no de f a/; b g /) isset to ff a g /; f b gg /. Ho w ev er/, only f b g is a stable mo del of /\u0005/2\n/.Despite the de/\fciency illustrated in Example /4/./8/, algorithm AAS do es ha v e desirablefeatures/. First/, AAS enables us to compute stable mo dels in a mo dular fashion/. W e can useG/\u0005\nas a structure in whic h to store the stable mo dels/. Once the kno wledge base is c hanged/,w e need to resume computation only at the no des a/\u000bected b y the c hange/. F or example/,supp ose that after computing the stable mo dels of the kno wledge base /\u0005/0\n/, w e add to /\u0005/0the kno wledge base /\u0005/1\nof Example /2/./6/, whic h giv es us a new kno wledge base/, /\u0005/3\n/= /\u0005/0\nS/\u0005/1\n/.The sup er dep endency graph of the new kno wledge base /\u0005/3\nis sho wn in Figure /9/. No w w eneed only to compute the stable mo dels at the no des p enguin /, bir d /, ab/2 /, /\ry /, and on landand then to com bine the mo dels generated at the sinks/. W e do not ha v e to re/-compute thestable mo dels at all the other no des as w ell/.Second/, in using the AAS algorithm/, w e do not alw a ys ha v e to compute all stable mo delsup to the ro ot no de/. If w e are queried ab out an atom that is somewhere in the middle ofthe graph/, it is often enough to compute only the mo dels of the subgraph ro oted b y theno de that represen ts this atom/. F or example/, supp ose w e are giv en the kno wledge base/\u0005/2\nand ask ed if mammal is true in ev ery stable mo del of /\u0005/2\n/. W e can run AAS for theno des dolphin /, lion /, and mammal /| and then stop/. If mammal is true in all the stablemo dels computed at the no de mammal /(i/.e/. /, in all the mo dels in Mmammal\n/)/, w e answ er/\\y es/\"/, otherwise/, w e m ust con tin ue the computation/.Third/, the AAS algorithm is useful in computing the lab eling of a TMS sub ject tonogo o ds/. A set of no des of a TMS can b e declared no go o d /, whic h means that all acceptablelab eling should assign false to at least one no de in the nogo o d set/.\n/4In stable mo delsterminology /, this means that when handling nogo o ds/, w e lo ok for stable mo dels in whic h/4/. In logic programming terminology nogo o ds are simply in tegrit y constrain ts/./4/4A Hierar chy of Tra ct able Subsetsat least one atom from a nogo o d is false /. A straigh tforw ard approac h w ould b e to /\frstcompute all the stable mo dels and then c ho ose only the ones that comply with the nogo o dconstrain ts/. But since the AAS algorithm is mo dular and w orks from the b ottom up/,in man y cases it can prev en t the generation of un w an ted stable mo dels at an early stage/.During the computation/, w e can exclude the submo dels that do not comply with the nogo o dconstrain ts and erase these submo dels from Ms\nonce w e are at a no de s in the sup erdep endency graph suc h that As\nincludes all the mem b ers of a certain nogo o d/./5/. Computing Stable Mo dels of First/-Order Kno wledge BasesIn this section/, w e sho w ho w w e can generalize algorithm AAS so that it can /\fnd all stablemo dels of a kno wledge base o v er a /\frst/-order language with no function sym b ols/. The newalgorithm will b e called First/-A cyclic/-All/-St able /(F AAS/)/.W e will no w refer to a kno wledge base as a set of rules of the formC / /BnZr A/1\n/; A/2\n/; /:/:/:/; Am\n/; not B/1\n/; /:/:/:/; not Bn\n/(/1/4/)where all A s/, B s/, and C are atoms in a /\frst/-or der language with no function sym b ols/. Thede/\fnitions of head/, b o dy /, and p ositiv e and negativ e app earances of an atom are the sameas in the prop ositional case/. In the expression p /( X/1\n/; /:/:/:/; Xn\n/)/, p is called a pr e dic ate name /.As in the prop ositional case/, ev ery kno wledge base /\u0005 is asso ciated with a directed graphcalled the dep endency gr aph of /\u0005/, in whic h /(a/) eac h predicate name in /\u0005 is a no de/, /(b/)there is a p ositive ar c directed from a no de p to a no de q i/\u000b there is a rule in /\u0005 in whic hp is a predicate name in one of the Ai\ns and q is a predicate name in the head/, and /(c/)there is a ne gative ar c directed from a no de p to a no de q i/\u000b there is a rule in /\u0005 in whic hp is a predicate name in one of the Bi\ns and q is a predicate name in the head/. The sup erdep endency graph/, G/\u0005\n/, is de/\fned in an analogous manner/. W e de/\fne a str ati/\fe d kno wledgebase to b e a kno wledge base in whic h there are no cycles through the negativ e edges in thedep endency graph of the kno wledge base/.A kno wledge base will b e called safe i/\u000b eac h of its rules is safe/. A rule is safe i/\u000b all thev ariables app earing in the head of the rule or in predicates app earing negativ e in the rulealso app ear in p ositiv e predicates in the b o dy of the rule/. In this section/, w e assume thatkno wledge bases are safe/. The Herbr and b ase of a kno wledge base is the set of all atomsconstructed using predicate names and constan ts from the kno wledge base/. The set ofgr ound instanc es of a rule is the set of rules obtained b y consisten tly substituting v ariablesfrom the rule with constan ts that app ear in the kno wledge base in all p ossible w a ys/. Thegr ound instanc e of a know le dge b ase is the union of all ground instances of its rules/. Notethat the ground instance of a /\frst/-order kno wledge base can b e view ed as a prop ositionalkno wledge base/.A mo del for a kno wledge base is a subset M of the kno wledge base/'s Herbrand base/.This subset has the prop ert y that for ev ery rule in the grounded kno wledge base/, if all theatoms that app ear p ositiv e in the b o dy of the rule b elong to M and all the atoms thatapp ear negativ e in the b o dy of the rule do not b elong to M /, then the atom in the head ofthe rule b elongs to M /. A stable mo del for a /\frst/-order kno wledge base /\u0005 is a Herbrandmo del of /\u0005/, whic h is also a stable mo del of the grounded v ersion of /\u0005/./4/5Ben/-Eliy ahuFirst/-A cyclic/-All/-St able /(/\u0005/)Input/: A /\frst/-order kno wledge base /\u0005/.Output/: All the stable mo dels of /\u0005/./1/. T ra v erse G/\u0005\nfrom the b ottom up/. F or eac h no de s /, do/:/(a/) Ms\n/:/= /; /;/(b/) Let s/1\n/; /:/:/:/; sj\nb e the c hildren of s /;/(c/) Mc /( s /)\n/:/= CartesPro d/( f Ms/1\n/; /:/:/:/; Msj\ng /)/;/(d/) F or eac h m /2 Mc /( s /)\ndoMs\n/:/= Ms\nSall/-stable/(/\u0005s\nSf P / /BnZr j P /2 m g /)/2/. Output CartesPro d/( f Ms/1\n/; /:/:/:/; Msk\ng /)/, where s/1\n/; /:/:/:/; sk\nare the sinks of G/\u0005\n/.Figure /1/2/: Algorithm First/-A cyclic/-All/-St able /(F AAS/)W e no w presen t F AAS/, an algorithm that computes all stable mo dels of a /\frst/-orderkno wledge base/. Let /\u0005 b e a /\frst/-order kno wledge base/. As in the prop ositional case/, witheac h no de s in G/\u0005\n/(the sup er dep endency graph of /\u0005/)/, w e asso ciate /\u0005s\n/, As\n/, and Ms\n/. /\u0005s\nisthe subset of /\u0005 con taining all the rules ab out predicates whose names are in s /. As\nis theset of all predicate names P that app ear in the subgraph of G/\u0005\nro oted b y s /. Ms\nare thestable mo dels asso ciated with the sub/{kno wledge base of /\u0005 that con tains only rules ab outpredicates whose names are in As\n/. Initially /, Ms\nis empt y for ev ery s /. Algorithm F AAStra v erses G/\u0005\nfrom the b ottom up/. When at a no de s /, the algorithm /\frst com bines all thesubmo dels of the c hildren of s in to a single set of mo dels/, Mc /( s /)\n/. Then/, for eac h mo delm in Mc /( s /)\n/, it calls a pro cedure that /\fnds all the stable mo dels of /\u0005s\nunion the set of allunit clauses P / /BnZr where P /2 m /. The pro cedure All/-St able called b y F AAS can b e an ypro cedure that computes all the stable mo dels of a /\frst/-order kno wledge base/. Becausepro cedure All/-St able computes stable mo dels for only parts of the kno wledge base/, itma y tak e adv an tage of some fractions of the kno wledge base b eing strati/\fed or ha ving an yother prop ert y that simpli/\fes computation of the stable mo dels of a fraction/.Theorem /5/./1 A lgorithm F AAS is c orr e ct/, that is/, m is a stable mo del of a know le dge b ase/\u0005 i/\u000b m is one of the mo dels in the output when applying F AAS to /\u0005 /.Pro of/: As the pro of of Theorem /4/./2/.Note that the more that a kno wledge base app ears strati/\fed/, the more e/\u000ecien t algorithmF AAS b ecomes/.Example /5/./2 Consider kno wledge base /\u0005/5\n/:w ar m bl ooded /( X /) / /BnZr mammal /( X /)l iv e on l and /( X /) / /BnZr mammal /( X /) /; not ab /1/( X /)f emal e /( X /) / /BnZr mammal /( X /) /; not mal e /( X /)/4/6A Hierar chy of Tra ct able Subsetsmal e /( X /) / /BnZr mammal /( X /) /; not f emal e /( X /)mammal /( X /) / /BnZr dol phin /( X /)ab /1/( X /) / /BnZr dol phin /( X /)mammal /( X /) / /BnZr l ion /( X /)dol phin /( f l ipper /) / /BnZrl iv e on l and /( X /) / /BnZr bir d /( X /)f l y /( X /) / /BnZr bir d /( X /) /; n ot ab /2/( X /)bir d /( X /) / /BnZr peng uin /( X /)ab /2/( X /) / /BnZr peng uin /( X /)bir d /( big b ird /) / /BnZrThe sup er dep endency graph of /\u0005/5\n/, G/\u0005/5\n/, is the same as the sup er dep endency graph ofthe kno wledge base /\u0005/2\n/(see Figure /9/)/. Observ e that when at no de mammal /, for example/,in step /1/.d the algorithm lo oks for all stable mo dels of the kno wledge base /\u0005\n/0/= /\u0005mammal\nSf/ /BnZr dol phin /( f l ipper /) g /, where /\u0005mammal\n/= f mammal /( X /) / /BnZr dol phin /( X /) /; mammal /( X /) / /BnZr l ion /( X /) g /./\u0005\n/0is a strati/\fed kno wledge base that has a unique stable mo del that can b e found e/\u000ecien tly /.Hence/, algorithm F AAS sa v es us from ha ving to ground all the rules of the kno wledge baseb efore starting to calculate the mo dels/, and it can tak e adv an tage of parts of the kno wledgebase b eing strati/\fed/./6/. Related W orkIn recen t y ears/, quite a few algorithms ha v e b een dev elop ed for reasoning with stable mo dels/.Nonetheless/, as far as w e kno w/, the w ork presen ted here is original in the sense that itpro vides a partition of the set of all the kno wledge bases in to a hierarc h y of tractableclasses/. The partition is based on the structure of the dep endency graph/. In tuitiv ely /, thetask of computing all the stable mo dels of a kno wledge base using algorithm AAS b ecomesincreasingly complex as the /\\distance/\" of the kno wledge base from b eing strati/\fed b ecomeslarger/. Next/, w e summarize the w ork that seems to us most relev an t/.Algorithm AAS is based on an idea that app ears in the w ork of Lifsc hitz and T urner/(/1/9/9/4/)/, where they sho w that in man y cases a logic program can b e divided in to t w o parts/,suc h that one part/, the /\\b ottom/\" part/, do es not refer to the predicates de/\fned in the /\\top/\"part/. They then explain ho w the task of computing the stable mo dels of a program can b esimpli/\fed when the program is split in to parts/. Algorithm AAS/, using the sup erstructureof the dep endency graph/, exploits a sp eci/\fc metho d for splitting the program/.Bell et al/. /(/1/9/9/4/) and Subrahmanian et al/. /(/1/9/9/5/) implemen t linear and in teger pro/-gramming tec hniques in order to compute stable mo dels /(among other nonmonotonic log/-ics/)/. Ho w ev er/, it is di/\u000ecult to assess the merits of their approac hes in terms of complexit y /.Ben/-Eliy ah u and Dec h ter /(/1/9/9/1/) illustrate ho w a kno wledge base /\u0005 can b e translated in toa prop ositional theory T/\u0005\nsuc h that eac h mo del of the latter corresp onds to a stable mo delof the former/. It follo ws from this that the problem of /\fnding all the stable mo dels ofa kno wledge base corresp onds to the problem of /\fnding all the mo dels of a prop ositionaltheory /. Satoh and Iw a y ama /(/1/9/9/1/) pro vide a nondeterministic pro cedure for computing/4/7Ben/-Eliy ahuthe stable mo dels of logic programs with in tegrit y constrain ts/. Junk er and Konolige /(/1/9/9/0/)presen t an algorithm for computing TMS/' lab els/. An toniou and Langetep e /(/1/9/9/4/) in tro ducea metho d for represen ting some classes of default theories as normal logic programs in suc ha w a y that SLDNF/-resolution can b e used to compute extensions/. Pimen tel and Cuadrado/(/1/9/8/9/) dev elop a lab el/-propagation algorithm that uses data structures called c ompr essiblesemantic tr e es in order to implemen t a TMS/; their algorithm is based on stable mo del se/-man tics/. The algorithms dev elop ed b y Marek and T ruszczy /\u0013 nski /(/1/9/9/3/) for auto epistemiclogic can also b e adopted for computing stable mo dels/. The pro cedures b y Marek andT ruszczy /\u0013 nski /(/1/9/9/3/)/, An toniou and Langetep e /(/1/9/9/4/)/, Pimen tel and Cuadrado /(/1/9/8/9/)/, Ben/-Eliy ah u and Dec h ter /(/1/9/9/1/)/, Satoh and Iw a y ama /(/1/9/9/1/)/, Bell et al/. /(/1/9/9/4/)/, Subrahmanianet al/. /(/1/9/9/5/)/, and Junk er and Konolige /(/1/9/9/0/) do not tak e adv an tage of the structure ofthe kno wledge base as re/\rected in its dep endency graph/, and therefore are not e/\u000ecien t forstrati/\fed kno wledge bases/.Sacc/\u0012 a and Zaniolo /(/1/9/9/0/) presen t a b acktr acking /\fxp oint algorithm for constructing onestable mo del of a /\frst/-order kno wledge base/. This algorithm is similar to algorithm All/-St able/2 presen ted here in Section /3 but its complexit y is w orse than the complexit y ofAll/-St able/2 /. They sho w ho w the bac ktrac king /\fxp oin t algorithm can b e mo di/\fed tohandle strati/\fed kno wledge bases in an e/\u000ecien t manner/, but the algorithm needs furtheradjustmen ts b efore it can deal e/\u000ecien tly with kno wledge bases that are v ery close to b eingstrati/\fed/. Leone et al/. /(/1/9/9/3/) presen t an impro v ed bac ktrac king /\fxp oin t algorithm forcomputing one stable mo del of a Datalog\n/:program and discuss ho w the impro v ed algorithmcan b e implemen ted/. One of the pro cedures called b y the impro v ed algorithm is based onthe bac ktrac king /\fxp oin t algorithm of Sacc/\u0012 a and Zaniolo /(/1/9/9/0/)/. Lik e the bac ktrac king/\fxp oin t algorithm/, the impro v ed algorithm as is do es not tak e adv an tage of the structureof the program/, i/.e/./, it is not e/\u000ecien t for programs that are close to b eing strati/\fed/.Sev eral tractable sub classes for computing extensions of default theories /(and/, hence/,computing stable mo dels/) are kno wn /(Kautz /& Selman/, /1/9/9/1/; P apadimitriou /& Sideri/,/1/9/9/4/; P alop oli /& Zaniolo/, /1/9/9/6/; Dimop oulos /& Magirou/, /1/9/9/4/; Ben/-Eliy ah u /& Dec h ter/,/1/9/9/6/)/. Some of these tractable sub classes are c haracterized using a graph that re/\rectsdep endencies in the program b et w een atoms and rules/. The algorithms presen ted in thesepap ers are complete only for a sub class of all kno wledge bases/, ho w ev er/. Algorithms forcomputing extensions of str ati/\fe d default theories or extensions of default theories thatha v e no o dd cycles /(in some precise sense/) are giv en b y P apadimitriou and Sideri /(/1/9/9/4/)and Cholewi /\u0013 nski /(/1/9/9/5a/, /1/9/9/5b/)/.Algorithms for handling a TMS with nogo o ds ha v e b een dev elop ed in the AI comm u/-nit y b y Do yle /(/1/9/7/9/) and Charniak et al/. /(/1/9/8/0/)/. But/, as Elk an /(/1/9/9/0/) p oin ts out/, thesealgorithms are not alw a ys faithful to the seman tics of the TMS and their complexities ha v enot b een analyzed/. Dec h ter and Dec h ter /(/1/9/9/4/) pro vide algorithms for manipulating a TMSwhen it is represen ted as a constrain t net w ork/. The e/\u000eciency of their algorithms dep endson the structure of the constrain t net w ork represen ting the TMS/, and the structure theyemplo y di/\u000bers from the dep endency graph of the kno wledge base/./4/8A Hierar chy of Tra ct able Subsets/7/. ConclusionThe task of computing stable mo dels is at the heart of sev eral systems cen tral to AI/,including TMSs/, auto epistemic logic/, and default logic/. This task has b een sho wn to b eNP/-hard/. In this pap er/, w e presen t a partition of the set of all kno wledge bases to classes/\n/1\n/; /\n/2\n/; /:/:/: /, suc h that if a kno wledge base /\u0005 is in /\nk\n/, then /\u0005 has at most k stable mo dels/,and they ma y all b e found in time O /( l nk /)/, where l is the length of the kno wledge base andn the n um b er of atoms in /\u0005/. Moreo v er/, for an arbitrary kno wledge base /\u0005/, w e can /\fnd theminim um k suc h that /\u0005 b elongs to /\nk\nin time linear in the size of /\u0005/. In tuitiv ely /, the morethe kno wledge base is strati/\fed/, the more e/\u000ecien t our algorithm b ecomes/. W e b eliev e thatb ey ond strati/\fed kno wledge bases/, the more expressiv e the kno wledge base is /(i/.e/. the morerules with nonstrati/\fed negation in the kno wledge base/)/, the less lik ely it will b e needed/.Hence/, our analysis should b e quite useful/. In addition/, w e sho w that algorithm AAS hassev eral adv an tages in a dynamically c hanging kno wledge base/, and w e pro vide applicationsfor answ ering queries and implemen ting a TMS/'s nogo o d strategies/. W e also illustrate ageneralization of algorithm AAS for the class of /\frst/-order kno wledge bases/.Algorithm AAS can easily b e adjusted to /\fnd only one stable mo del of a kno wledgebase/. While tra v ersing the sup er dep endency graph/, w e generate only one mo del at eac hno de/. If w e arriv e at a no de where w e cannot generate a mo del based on what w e ha v ecomputed so far/, w e bac ktrac k to the most recen t no de where sev eral mo dels w ere a v ailableto c ho ose from and tak e the next mo del that w as not y et c hosen/. The w orst/-case timecomplexit y of this algorithm is equal to the w orst/-case time complexit y of the algorithm for/\fnding all stable mo dels b ecause w e ma y ha v e to exhaust all p ossible w a ys of generating astable mo del b efore /\fnding out that a certain kno wledge base do es not ha v e a stable mo delat all/. Nev ertheless/, w e b eliev e that in the a v erage case/, /\fnding just one mo del will b eeasier than /\fnding them all/. A similar mo di/\fcation of the AAS algorithm is required if w eare in terested in /\fnding one mo del in whic h one particular atom gets the v alue true /.This w ork is another attempt to bridge the gap b et w een the declarativ e systems /(e/.g/./,default logic/, auto epistemic logic/) and the pro cedural systems /(e/.g/./, A TMs/, Prolog/) of thenonmonotonic reasoning comm unit y /. It is argued that while the declarativ e metho ds aresound/, they are impractical since they are computationally exp ensiv e/, and while the pro ce/-dural metho ds are more e/\u000ecien t/, it is di/\u000ecult to completely understand their p erformanceor to ev aluate their correctness/. The w ork presen ted here illustrates that the declarativ eand the pro cedural approac hes can b e com bined to yield an e/\u000ecien t y et formally supp ortednonmonotonic system/.Ac kno wledgmen tsThanks to Luigi P alop oli for useful commen ts on earlier draft of this pap er and to Mic helleBonnice and Gadi Dec h ter for editing on parts of the man uscript/. Man y thanks to theanon ymous referees for v ery useful commen ts/.Some of this w ork w as done while the author w as visiting the Cognitiv e Systems Lab/-oratory /, Computer Science Departmen t/, Univ ersit y of California/, Los Angeles/, California/,USA/. This w ork w as partially supp orted b y NSF gran t IRI/-/9/4/2/0/3/0/6 and b y Air F orce O/\u000eceof Scien ti/\fc Researc h gran t /#F/4/9/6/2/0/-/9/4/-/1/-/0/1/7/3 /./4/9Ben/-Eliy ahu"}
{"category": "abstract", "text": "Finding the stable mo dels of a kno wledge base is a signi/\fcan t computational problemin arti/\fcial in telligence/. This task is at the computational heart of truth main tenancesystems/, auto epistemic logic/, and default logic/. Unfortunately /, it is NP/-hard/. In thispap er w e presen t a hierarc h y of classes of kno wledge bases/, /\n/1\n/; /\n/2\n/; /"}
{"category": "non-abstract", "text": "they either eliminate a plan altogether b ecause it con tains an irremediable /\ra w/, or they adda unique step or unique causal link /(from the initial state/) to establish an op en conditionthat cannot b e established in an y other w a y /. The strategy is closely related to ones prop osedb y P eot /& Smith /(/1/9/9/3/) and Joslin /& P ollac k /(/1/9/9/4/) but generally app ears to p erform b etterthan either/.W e describ e these t w o classes of tec hniques in Section /2 b elo w/, and in Section /3 w erep ort our exp erimen tal results based on sligh tly mo di/\fed v ersions of ucpop /.\n/2F or themore di/\u000ecult problems tak en from the a v ailable ucpop test suite and elsewhere/, w e obtainimpro v emen ts b y factors ranging from /5 to more than /1/0/0/0/, with the hardest problemsgiving the greatest impro v emen ts/.W e then turn to our prop osal for using computed op erator parameter domains duringplanning/. In particular/, in Section /4 w e motiv ate and describ e a metho d of precomput/-ing parameter domains based on propagating sets of constan ts forw ard from the initialconditions/.\n/3The pro cess is iterativ e/, but the algorithm runs within a time b ound that isp olynomial in the size of the problem sp eci/\fcation/. W e pro vide details of the algorithm/,along with theorems ab out its correctness and tractabilit y /, in Sections /4/./2/{/4/./3 and OnlineApp endix /1/.In Section /5 w e sho w ho w to use parameter domain information in a ucpop /-st yle plan/-ner/. During planning/, parameter domains can b e used to prune op erator instances whoseparameter domains are inconsisten t with binding constrain ts/, and to eliminate spuriousthreats that cannot/, in fact/, b e realized without violating domain constrain ts/. W e illustratethe e/\u000bectiv eness of this tec hnique with examples dra wn from the ucpop test suite as w ell asfrom the trains transp ortation planning w orld dev elop ed at Ro c hester /(Allen /& Sc h ub ert/,/1/9/9/1/; Allen et al/./, /1/9/9/5/)/. In some of these tests/, w e apply the parameter domain informationin the con text of the default ucpop searc h strategy /. W e demonstrate signi/\fcan t gains onmost problems/, particularly the more c hallenging ones /(e/.g/./, sp eedups of more than an orderof magnitude for sev eral problems in the strips w orld/, and a more than /9/0/0/-fold sp eedupfor a trains problem/)/.In another set of tests in the trains w orld/, w e use our o wn impro v ed searc h strategiesas baseline/, i/.e/./, w e ask whether additional sp eedups are obtainable b y use of parameter/1/. The searc h strategy is describ ed as /\\A/* or ID A/*/\" searc h in /(P en b erth y /& W eld/, /1/9/9/2/)/; in the co de forucpop /2/./0 it is describ ed more generally as b est/-/\frst/, since arbitrary ranking functions/, not necessarilycorresp onding to A/* heuristics/, ma y b e plugged in/. But with c hoices lik e S/+OC or S/+OC/+UC asplan/-ranking heuristic /(as discussed in Section /2/./2/)/, it is natural to view the strategy as an A/* strategy /./2/. While the tec hniques w e describ e are applicable to other planners/, our fo cus is on ucpop b ecause it isw ell/-kno wn and the Lisp co de is readily a v ailable/. The system can b e obtained via anon ymous ftp fromcs/.w ashington/.edu/./3/. W e hop e that the notion of a p ar ameter domain/, as a set of admissible bindings /(constan ts/)/, will causeno confusion with the notion of a planning domain/, as a sp eci/\fed set of op erators/, along with constrain tson admissible initial conditions and goal conditions/./9/6A ccelera ting P ar tial/-Order Plannersdomains/, ab o v e those obtainable with the S/+OC and ZLIF O searc h strategies/. Our ex/-p erimen tal results again sho w sp eedups b y ab out a factor of /1/0 through use of parameterdomains/, on top of those obtained b y the impro v ed searc h strategies /(the com bined sp eedupis o v er /2/0/0/0/)/.As evidence that the e/\u000bectiv eness of using parameter domains in com bination with oursearc h strategy is not dep enden t on some p eculiarit y of the latter/, w e also include someresults for ucpop /'s default strategy /, Joslin and P ollac k/'s /\\least cost /\ra w repair/\" /(LCFR/)strategy /(Joslin /& P ollac k/, /1/9/9/4/) and for P eot and Smith/'s /\\least commitmen t/\" /(LC/) op encondition selection strategy /(P eot /& Smith/, /1/9/9/3/) in Section /5/.In Section /6/, w e state our conclusions/, commen t on some related w ork and men tionp ossible extensions of our tec hniques/./2/. Plan Selection and Goal SelectionW e will b e basing our discussion and exp erimen ts on ucpop /, an algorithm exemplifying thestate of the art in w ell/-founded partial/-order planning/. Th us w e b egin with a sk etc h of thisalgorithm/, referring the reader to /(Barrett et al/./, /1/9/9/4/; P en b erth y /& W eld/, /1/9/9/2/) for details/.In the next t w o subsections w e then motiv ate and describ e our impro v ed plan/-selection andgoal/-selection strategies/./2/./1 UCPOPucpop uses strips /-lik e op erators/, with p ositiv e or negativ e preconditions and p ositiv e ornegativ e e/\u000bects/. The initial state consists of p ositiv e predications with constan t argumen ts/(if an y/)/, and all other ground predications are false b y default/. Unlik e strips /, ucpop alsoallo ws c onditional e/\u000bects/, expressed b y /2/-part when /-clauses sp ecifying a /(p ossibly complex/)extra condition needed b y that e/\u000bect and the /(p ossibly complex/) e/\u000bect itself/. F or instance/,an action PUTON/(/?x /?y /?z/) /(/\\put /?x on /?y from /?z /\"/) migh t ha v e conditional e/\u000bects statingthat when /?y is not the table/, it will not b e clear at the end of the action/, and when z isnot the table/, it will b e clear at the end of the action/. The /\\U/\" in ucpop indicatesthat univ ersally quan ti/\fed conditions and e/\u000bects are p ermitted as w ell/. F or instance/, it isp ermissible to ha v e a precondition for a PICKUP/(/?x/) action that sa ys that for all /?y /, /(not/(on /?y /?x/)/) holds/. Univ ersal statemen ts are handled b y explicit substitution of domainconstan ts and need not concern us at this p oin t/.In essence/, ucpop explores a space of partially sp eci/\fed plans/, eac h paired with anagenda of goals still to b e satis/\fed and threats still to b e a v erted/. The initial plan con tainsa dumm y /*start/* action whose e/\u000bects are the giv en initial conditions/, and a dumm y/*end/* action whose preconditions are the giv en goals/. Th us goals are uniformly view ed asaction preconditions/, and are uniformly ac hiev ed through the e/\u000bects of actions/, includingthe /*start/* action/.The plans themselv es consist of a collection of steps /(i/.e/./, actions obtained b y instan ti/-ating the a v ailable op erators/)/, along with a set of c ausal links /, a set of binding c onstr aints /,and a set of or dering c onstr aints /. When an op en goal /(precondition/) is selected from theagenda/, it is established /(if p ossible/) either b y adding a step with an e/\u000bect that uni/\feswith the goal/, or b y using an existing step with an e/\u000bect that uni/\fes with the goal/. /(Inthe latter case/, it m ust b e consisten t with curren t ordering constrain ts to place the existing/9/7Gerevini /& Schuber tstep b efore the goal/, i/.e/./, b efore the step whose preconditions generated the goal/./) When anew or existing step is used to establish a goal in this w a y /, there are sev eral side e/\u000bects/:/\u000f A causal link /( Sp\n/; Q/; Sc\n/) is also added/, where Sp\nindicates the step /\\pro ducing/\" thegoal condition Q and Sc\nindicates the step /\\consuming/\" Q /. This causal link serv es toprotect the in tended e/\u000bect of the added /(or reused/) step from in terference b y othersteps/./\u000f Binding constrain ts are added/, corresp onding to the uni/\fer for the action e/\u000bect inquestion and the goal /(precondition/) it ac hiev es/./\u000f An ordering constrain t is added/, placing the step in question b efore the step whoseprecondition it ac hiev es/./\u000f If the action in question is new/, its preconditions are added to the agenda as newgoals /(except that EQ//NEQ conditions are in tegrated in to the binding constrain ts /{ seeb elo w/)/./\u000f New threats /(unsafe conditions/) are determined/. F or a new step and its causal link/,other steps threaten the causal link if they ha v e e/\u000bects uni/\fable with the conditionprotected b y the causal link /(and these e/\u000bects can o ccur temp orally during the causallink/)/; and the e/\u000bects of the new step ma y similarly threaten other causal links/. Ineither case/, new threats are placed on the agenda/. It is useful to distinguish de/\fnitethreats from p otential threats/: the former are those in whic h the uni/\fcation thatcon/\frmed the threat in v olv ed no new binding of v ariables/.Binding constrain ts assert the iden tit y /( EQ /) or noniden tit y /( NEQ /) of t w o v ariables or a v ariableand a constan t/. EQ /-constrain ts arise from unifying op en goals with action e/\u000bects/, and NEQ /-constrain ts arise /(i/) from NEQ /-preconditions of newly instan tiated actions/, /(ii/) from matc hingnegativ e goals con taining v ariables to the initial state/, and /(iii/) from a v erting threats b y/\\separation/\"/, i/.e/./, forcing non/-equalit y of t w o v ariables or a v ariable and a constan t thatw ere uni/\fed in threat detection/. NEQ /-constrain ts ma y b e disjunctiv e/, but are handled simplyb y generating separate plans for eac h disjunct/.The o v erall con trol lo op of ucpop consists of selecting a plan from the curren t list ofplans /(initially the single plan based on /*start/* and /*end/* /)/, selecting a goal or threat fromits agenda/, and replacing the plan b y the corresp onding re/\fned plans/. If the agenda item isa goal/, the re/\fned plans are those corresp onding to all w a ys of establishing the goal usinga new or existing step/. If the agenda item is a de/\fnite threat to a causal link /( Sp\n/; Q/; Sc\n/)/,then there are at most three re/\fned plans/. Tw o of these constrain the threatening stepto b e b efore step Sp\n/(demotion/) or after step Sc\n/(promotion/)/, th us a v erting the threat/.A third p ossibilit y arises if the e/\u000bect threatening /( Sp\n/; Q/; Sc\n/) is a c onditional e/\u000bect of thethreatening action/. Suc h a conditional threat can b e a v erted b y creating a goal den yingsome precondition needed b y the conditional e/\u000bect/.ucpop has a /\\dela y separation/\" switc h/, /*d/-sep/* /, and when this is turned on/, onlyde/\fnite threats are dealt with/. Note that p oten tial threats ma y b ecome de/\fnite as a resultof added binding constrain ts/. /(They ma y also /\\expire/\" as a result of added binding andordering constrain ts/, i/.e/./, the threatening e/\u000bect ma y no longer unify with the threatenedcondition or it ma y b e forced to o ccur b efore or after the threatened causal link/. Expired/9/8A ccelera ting P ar tial/-Order Plannersthreats are remo v ed from the agenda when selected/./) When /*d/-sep/* is o/\u000b/, p oten tial threatsas w ell as de/\fnite ones are a v erted/, with separation as an additional metho d of doing sob esides the three metho ds ab o v e/.Inconsistencies in binding constrain ts and ordering constrain ts are detected when they/\frst o ccur /(as a result of adding a new constrain t/) and the corresp onding plans are elim/-inated/. Planning fails if no plans remain/. The success condition is the creation of a planwith consisten t binding and ordering constrain ts and an empt y agenda/.The allo w ance for conditional e/\u000bects and univ ersal conditions and e/\u000bects causes onlyminor p erturbations in the op eration of ucpop /. F or instance/, conditional e/\u000bects can leadto m ultiple matc hes against op erators for a giv en goal/, eac h matc h generating di/\u000beren tpreconditions/. /(Of course/, there can b e m ultiple matc hes ev en without conditional e/\u000bects/,if some predicates o ccur more than once in the e/\u000bects/./)The k ey issues for us righ t no w are the strategic ones/: ho w plans are selected from thecurren t set of plans /(discussed in Section /2/./2/)/, and ho w goals are selected for a giv en plan/(discussed in Section /2/./3/)/./2/./2 The T rouble with Coun ting Unsafe ConditionsThe c hoice of the next plan to re/\fne in the ucpop system is based on an A/* b est/-/\frstsearc h/. Recall that A/* uses a heuristic estimate f /( p /) of o v erall solution cost consisting ofa part g /( p /) /= cost of the curren t partial solution /(plan/) p and a part h /( p /) /= estimate ofthe additional cost of the b est complete solution that extends p /. In the curren t con text itis helpful to think of f /( p /) as a measure of plan c omplexity /, i/.e/./, /\\go o d/\" plans are simple/(lo w/-complexit y/) plans/.There are t w o p oin ts of whic h the reader should b e reminded/. First/, in order for A/*to guaran tee disco v ery of an optimal plan /(i/.e/./, the /\\admissibilit y/\" condition/)/, h /( p /) shouldnot over estimate the remaining solution cost /(Nilsson/, /1/9/8/0/)/. Second/, if the aim is notnecessarily to /\fnd an optimal solution but to /\fnd a satisfactory solution quickly /, then f /( p /)can b e augmen ted to include a term that estimates the remaining cost of /\fnding a solution/.One common w a y of doing that is to use a term prop ortional to h/(p/) for this as w ell/, i/.e/./,w e emphasize the h /-comp onen t of f relativ e to the g /-comp onen t/. This is reasonable to theexten t that the plans that are most nearly complete /(indicated b y a lo w h /-v alue/) are lik elyto tak e the least e/\u000bort to complete/. Th us w e will prefer to pursue a plan p\n/0that seems closerto b eing complete to a plan p further from completion/, ev en though the over al l complexit yestimate for p\n/0ma y b e greater than for p /(Nilsson/, /1/9/8/0/) /(pages /8/7/{/8/8/)/. Alternativ ely /, w ecould add a heuristic estimate of the remaining cost of /\fnding a solution to f /( p /) that ismore or less indep enden t of the estimate h /( p /)/.With these considerations in mind/, w e no w ev aluate the advisabilit y of including thev arious terms in ucpop /'s function for guiding its A/* searc h/, namelyS/, OC/, CL/, and UC/,where S is the n um b er of steps in the partial plan/, OC is the n um b er of op en conditions/(unsatis/\fed goals and preconditions/)/, CL is the n um b er of causal links/, and UC is then um b er of unsafe conditions /(the n um b er of pairs of steps and causal links where the step/9/9Gerevini /& Schuber tthreatens the causal link/)/. The default com bination used b y ucpop is S/+OC/+UC/.\n/4Thisb ecomes S/+OC/+UC/+F if sp ecial op en conditions called /\\facts/\" are presen t/. These areconditions that are not state/-dep enden t /(e/.g/./, a n umerical relation lik e /(add/-one /?x /?y/) /, ora geometrical one lik e /(loc/-in/-room /?x /?y /?room/) /) and are established b y Lisp functions/(Barrett et al/./, /1/9/9/4/)/. Since few of our test problems in v olv ed facts/, w e will not discuss theF term further except to sa y that w e follo w ed the ucpop default strategy of including thisterm where it is relev an t /(see the TileW orld problems in Section /3/./2 and also some remarksin Section /5/./2 in connection with the parameter/-domain exp erimen ts/)/./2/./2/./1 S/: the number of steps currentl y in the planThis can naturally b e view ed as comprising g /( p /)/, the plan complexit y so far/. In tuitiv ely /, aplan is complex to the exten t that it con tains man y steps/. While in some domains w e migh tw an t to mak e distinctions among the costs of di/\u000beren t kinds of steps/, a simple step coun tseems lik e a reasonable generic complexit y measure/./2/./2/./2 OC/: the number of open conditionsThis can b e view ed as pla ying the role of h /( p /)/, since eac h remaining op en condition m ust b eestablished b y some step/. The catc h is that it ma y b e p ossible to use existing steps in theplan /(including /*start/* /, i/.e/./, the initial conditions/) to establish remaining op en conditions/.Th us OC can o v erestimate the n um b er of steps still to b e added/, forfeiting admissibilit y /.Despite this criticism/, sev eral considerations fa v or reten tion of the OC term/. First/, ab etter estimator of residual plan complexit y seems hard to come b y /. P erhaps one couldmo dify OC b y discoun ting op en conditions that are matc hed b y existing actions/, but thispresumes that all suc h op en conditions can actually b e ac hiev ed b y action re/-use/, whic h isimprobable if there are remaining threats/, or remaining goals requiring new steps/.\n/5Second/,the p ossibilit y that OC will o v erestimate the residual plan complexit y will rarely b e actu/-alized/, since t ypically further steps still need to b e added to ac hiev e some of the goals/, andthose steps will t ypically in tro duce further op en conditions again requiring new steps/. Fi/-nally /, to the exten t that OC do es at times o v erestimate the residual plan complexit y /, it canb e view ed as emphasizing the the h /( p /) term of f /( p /)/, th us promoting faster problem/-solvingas explained ab o v e/./2/./2/./3 CL/: the number of ca usal linksOne migh t motiv ate the inclusion of this term b y arguing that n umerous causal links areindicativ e of a complex plan/. As suc h/, CL app ears to b e an alternativ e to step/-coun ting/./4/. This is in no w a y the /\\recommended/\" strategy /. The ucpop implemen tation mak es a v ailable v ariousoptions for con trolling searc h/, to b e used at the discretion of exp erimen ters/. Our presen t w ork hasprompted the incorp oration of our particular strategies as an option in ucpop /4/./0/./5/. Note that threats and remaining goals imp ose constrain ts that ma y not b e consisten t with seeminglyp ossible instances of action re/-use/. This is clear enough for threats/, whic h often imply temp oral orderingconstrain ts inconsisten t with re/-use of an action/. It is also fairly clear for remaining goals/. F or instance/,in T o w ers of Hanoi the small disk D/1 is initiall y on the medium disk D/2 /, whic h in turn is on the big diskD/3 /, and D/3 is on p eg P/1 /. The goal is to mo v e the to w er to the third p eg P/3 /, so it seems to ucpop initiall yas if /(on D/1 D/2/) and /(on D/2 D/3/) could b e ac hiev ed b y /\\re/-use/\" of /*start/* /. Ho w ev er/, the third goal /(onD/3 P/3/) implies that v arious actions m ust b e added to the plan whic h are inconsisten t with those t w oseemingly p ossible instances of action re/-use/./1/0/0A ccelera ting P ar tial/-Order PlannersHo w ev er/, note that CL is in general larger than S/, since ev ery step of a plan establishesat least one op en condition and th us in tro duces at least one causal link/. The larger CL isrelativ e to S/, the more subgoals are ac hiev ed b y action re/-use/. Hence/, if w e use CL insteadof /(or in addition to/) S in the g /( p /) term/, w e w ould in e/\u000bect b e sa ying that ac hieving m ultiplesubgoals with a single step is undesirable/; w e w ould tend to searc h for w a ys of ac hievingm ultiple goals with m ultiple steps/, ev en when they can b e ac hiev ed with a single step/. Thisis clearly not a go o d idea/, and justi/\fes the exclusion of CL from f /( p /)/./2/./2/./4 UC/: the number of unsafe conditionsW e note /\frst of all that this is clearly not a g /-measure/. While the n um b er of threats willtend to increase if w e establish more and more subgoals without curtailing threats/, threatsas suc h are not elemen ts of the plan b eing constructed and so do not con tribute to itscomplexit y /. In fact/, when the plan is done all threats will b e gone/.Can UC then b e view ed as an h /-measure/? One argumen t of sorts for the a/\u000ermativ e isthe follo wing/. Not all partial plans are expandable in to complete plans/, and a high v alue ofUC mak es it more lik ely that the partial plan con tains irresolv able con/\ricts/. If w e regardimp ossible plans as ha ving in/\fnite cost/, then inclusion of a term increasing with UC as partof the h /-measure is reasonable/. This carries a serious risk/, though/, since in the case wherethe partial plan do es ha v e a consisten t completion /(despite a high UC/-coun t/)/, inclusion ofsuc h a term can greatly o v erestimate the residual plan complexit y /.Another p ossible a/\u000ermativ e argumen t is that c onditional threats are sometimes resolv edb y /\\confron tation/\"/, whic h in tro duces a new goal den ying a condition required for the threat/-ening conditional e/\u000bect/. This new goal ma y in turn require new steps for its ac hiev emen t/,adding to the plan complexit y /. Ho w ev er/, this link to complexit y is v ery ten uous/. In the /\frstplace/, man y of the ucpop test domains in v olv e no conditional e/\u000bects/, and threat remo v alb y promotion/, demotion or separation adds no steps/. Ev en when conditional e/\u000bects arepresen t/, man y unconditional as w ell as conditional threats are a v erted b y these metho ds/.F urthermore/, UC could sw amp all other terms since threats ma y app ear and expire ingroups of size O /( n /)/, where n is the n um b er of steps in the plan/. F or instance/, considera partial plan that in v olv es mo v es b y a rob ot R to lo cations L/1/, /./././, Ln /, so that thereare n causal links lab eled /(at R L/1/)/, /./././, /(at R Ln/) /. If a new mo v e to lo cation L isno w added/, initially with an inde/\fnite p oin t of departure /?x /, this pro duces e/\u000bects /(atR L/) and /(not /(at R /?x/)/) /. The latter can threaten all of the ab o v e n causal links/, atleast if the new mo v e is at /\frst temp orally unordered relativ e to the n existing mo v es/. Ifthis new action subsequen tly happ ens to b e demoted so as to precede the /\frst mo v e /(orpromoted so as to follo w the last/)/, or if /?x b ecomes b ound to a constan t distinct fromL/1/, /./././, Ln /, all n threats expire/. Keeping in mind that di/\u000beren t steps in a plan ma yha v e similar e/\u000bects/, w e can see that half of the steps could threaten the causal links of theothers/. In suc h a case w e could ha v e O /( n\n/2/) unsafe conditions/, destined to expire as a resultof O /( n /) promotions//demotions/. In fact ev en a single new binding constrain t ma y causeO /( n\n/2/) threats to expire/. F or instance/, if there are n/= /2 e/\u000bects /(not /(P /?x/)/) threateningn/= /2 causal links lab eled /(P /?y/) /, then if binding constrain t /(NEQ /?x /?y/) is added/, all n\n/2/= /4threats expire/. Recall that when expired threats are selected from the agenda b y ucpop /,they are recognized as suc h and discarded without further action/./1/0/1Gerevini /& Schuber tOur conclusion is that it w ould b e a mistak e to include UC in full in a general h /-measure/,though some increasing function of UC that remains small enough not to mask OC ma y b ew orth including in h /.Finally /, can UC b e regarded as a measure of the remaining cost of /\fnding a solution/?Here/, similar argumen ts to those ab o v e apply /. On the a/\u000ermativ e side/, w e can argue thata high v alue of UC indicates that w e ma y b e facing a com binatorially explosiv e/, time/-consuming searc h for a set of promotions and demotions that pro duce a con/\rict/-free stepordering/. In other w ords/, a high v alue of UC ma y indicate a high residual problem/-solvingcost/. /(And at the end of suc h a searc h/, w e ma y still lac k a solution/, if no viable stepordering exists/./) On the other hand/, w e ha v e already noted that unsafe conditions includeman y p ossible con/\ricts whic h ma y expire as a result of subsequen t partial ordering c hoicesand v ariable binding c hoices not sp eci/\fcally aimed at remo ving these con/\ricts/. So coun tingunsafe conditions can arbitrarily o v erestimate the n um b er of gen uine re/\fnemen t steps/, andhence the problem/-solving e/\u000bort/, still needed to complete the plan/.So UC is scarcely more trust w orth y as a measure of residual planning cost than as ameasure of residual plan cost/.Th us w e conclude that the most promising general heuristic measure for plan selection isS/+OC/, p ossibly augmen ted with an atten uated form of the UC term that will not dominatethe S/+OC comp onen t/. /(F or instance/, one migh t add a small fraction of the term/, suc h asUC///1/0/, or more subtly /{ to a v oid sw amping b y a quadratic comp onen t /{ a term prop ortionalto UC\n/: /5/./)/2/./3 The Goal Selection StrategyAn imp ortan t opp ortunit y for impro ving planning p erformance indep enden tly of the domainlies in iden tifying forced re/\fnemen ts/, i/.e/./, re/\fnemen ts that can b e made deterministic al ly /.Sp eci/\fcally /, in considering p ossible re/\fnemen ts of a giv en partial plan/, it mak es sense togiv e top priorit y to op en conditions that cannot b e ac hiev ed/; and then preferring op enconditions that can b e ac hiev ed in only one w a y /{ either through addition of an action noty et in the plan/, or through a unique matc h against the initial conditions/.The argumen t for giving top priorit y to unac hiev able goals is just that a plan con tainingsuc h goals can b e eliminated at once/. Th us w e prev en t allo cation of e/\u000bort to the re/\fnemen tof do omed plans/, and to the generation and re/\fnemen t of their do omed successor plans/.The argumen t for preferring op en conditions that can b e ac hiev ed in only one w a yis equally apparen t/. Since ev ery op en condition m ust ev en tually b e established b y someaction/, it follo ws that if this action is unique/, it m ust b e part of ev ery p ossible completionof the partial plan under consideration/. So/, adding the action is a /\\zero/-commitmen t/\"re/\fnemen t/, in v olving no c hoices or guessw ork/. A t the same time/, adding any re/\fnemen t ingeneral narro ws do wn the searc h space b y adding binding constrain ts/, ordering constrain tsand threats/, whic h constrain b oth existing steps and subsequen tly added steps/. F or uniquere/\fnemen ts this narro wing/-do wn is monotonic/, nev er needing rev o cation/. F or example/,supp ose some re/\fnemen t happ ens to add constrain ts that eliminate a certain action instanceA as a p ossible w a y of ac hieving a certain op en condition C /. If the re/\fnemen t is unique/,then w e are assured that no completion of the plan con tains A as a w a y of establishing C /.If it is not unique/, w e ha v e no suc h assurance/, since some alternativ e re/\fnemen t ma y b e/1/0/2A ccelera ting P ar tial/-Order Plannerscompatible with the use of A to ac hiev e C /. In short/, the zero/-commitmen t strategy cutsdo wn the searc h space without loss of access to viable solutions/.P eot and Smith /(/1/9/9/3/) studied the strategy of preferring forced threats to unforcedthreats/, and also used a /\\least commitmen t/\" /(LC/) strategy for handling op en conditions/.Least commitmen t alw a ys selects an op en condition whic h generates the few est re/\fnedplans/. Th us it entails the priorities for unac hiev able and uniquely ac hiev able goals ab o v e/(while also en tailing a certain prioritization of non uniquely ac hiev able goals/)/. Joslin andP ollac k /(/1/9/9/4/) studied the uniform application of suc h a strategy to b oth threats and op enconditions in ucpop /, terming this strategy /\\least cost /\ra w repair/\" /(LCFR/)/. Com bining thiswith ucpop /'s default plan selection strategy /, they obtained signi/\fcan t searc h reductions/(though less signi/\fcan t running time reductions/, mainly for implemen tation reasons/, butalso b ecause of the in trinsic o v erhead of computing the /\\repair costs/\"/) for a ma jorit y of theproblems in the ucpop test suite/.Joslin /& P ollac k /(/1/9/9/4/) and subsequen tly Sriniv asan /& Ho w e /(/1/9/9/5/) prop osed somev arian ts of LCFR designed to reduce the o v erhead incurred b y LCFR for /\ra w selection/.These strategies emplo y v arious assumptions ab out the /\ra w repair costs/, allo wing the morearduous forms of cost estimation /(requiring lo ok/-ahead generation of plans/) to b e con/\fnedto a subset of the /\ra ws in the plan/, while for the rest an appro ximation is used that do esnot signi/\fcan tly increase the o v erhead/. Both teams obtained quite signi/\fcan t reductionsin o v erhead costs in man y cases/, e/.g/./, b y factors ranging from ab out /3 to ab out /2/0 for themore di/\u000ecult problems/. Ho w ev er/, o v erall p erformance w as sometimes adv ersely a/\u000bected/.Joslin and P ollac k found that their v arian t /(QLCFR/) solv ed few er problems than LCFR/,b ecause of an increase in the n um b er of plans generated in some cases/. Eac h of Sriniv asan /&Ho w e/'s four strategies did sligh tly b etter than LCFR in some of their /1/0 problem domainsbut signi/\fcan tly w orse in others/. In terms of plans examined during the searc h/, their b esto v erall strategy /, whic h uses similar action instances for similar /\ra ws/, did sligh tly b etter on/4 of the domains/, sligh tly w orse on /4/, and signi/\fcan tly w orse on /2 /(and in those cases then um b er of plans examined w as also more than a factor of /2/0 ab o v e that of default ucpop /)/.In the unmo di/\fed form of ucpop /, goals are selected from the agenda according to aLIF O /(last/-in /\frst/-out/, i/.e/./, stac k/) disciplin e/. Based on exp erience with searc h pro cessesin AI in general/, suc h a strategy has m uc h to recommend it/, as a simple default/. In the/\frst place/, its o v erhead cost is lo w compared to strategies that use heuristic ev aluation orlo ok ahead to prioritize goals/. As w ell/, it will tend to main tain fo cus on the ac hiev emen t ofa particular higher/-lev el goal b y regression /{ v ery m uc h as in Prolog goal c haining /{ ratherthan attempting to ac hiev e m ultiple goals in breadth/-/\frst fashion/.Main taining fo cus on a single goal should b e adv an tageous at least when some of thegoals to b e ac hiev ed are indep enden t/. F or instance/, supp ose that t w o goals G/1 and G/2 canb oth b e ac hiev ed in v arious w a ys/, but c ho osing a particular metho d of ac hieving G/1 do esnot rule out an y of the metho ds of ac hieving G/2/. Then if w e main tain fo cus on G/1 un tilit is solv ed/, b efore attempting G/2/, the total cost of solving b oth goals will just b e the sumof the costs of solving them individually /. But if w e switc h bac k and forth/, and solutionsof b oth goals in v olv e searc hes that encoun ter man y dead ends/, the com bined cost can b em uc h larger/. This is b ecause w e will tend to searc h an y unsolv able subtree of the G/1 searc htree rep eatedly /, in com bination with v arious alternativ es in the G/2 searc h tree /(and vicev ersa/)/. This argumen t should still ha v e some v alidit y ev en if G/1 and G/2 are not en tirely/1/0/3Gerevini /& Schuber tindep enden t/; i/.e/./, as long as G/1 giv es rise to subproblems that tend to fail in the samew a y regardless of c hoices made in the attempt to solv e G/2 /(or vice v ersa/)/, then shiftingatten tion b et w een G/1 and G/2 will tend to generate a set of partial plans that unnecessarily/\\cross/-m ultiplies/\" alternativ es/.W e ha v e therefore c hosen to sta y with ucpop /'s LIF O strategy whenev er there are nozero commitmen t c hoices/. This has led to v ery substan tial impro v emen ts o v er LCFR in ourexp erimen ts/.Th us our strategy /, whic h w e term ZLIF O /(/\\zero/-commitmen t last/-in /\frst/-out/\"/)/, c ho osesthe next /\ra w according to the follo wing preferences/:/1/. a de/\fnite threat /( /*d/-sep/* is turned on/)/, using LIF O to pic k among these/;/2/. an op en condition that cannot b e established in an y w a y/;/3/. an op en condition that can b e resolv ed in only one w a y /, preferring op en conditionsthat can b e established b y in tro ducing a new action to those that can b e establishedb y using /*start/* /;\n/6/4/. an op en condition/, using LIF O to pic k among these/.Hence the o v erhead incurred b y ZLIF O for /\ra w selection is limited to the op en con/-ditions/, and is lo w er for these than the o v erhead incurred b y LCFR/. F urthermore/, it canalso b e signi/\fcan tly lo w er in practice than the o v erhead incurred b y LC/, b ecause testingwhether an OC is not a zero/-commitmen t c hoice /(i/.e/./, whether it can b e established in morethan one w a y/) is less exp ensiv e than computing the total n um b er of w a ys to ac hiev e it/.In Online App endix /1 w e giv e the pseudo co de of ZLIF O for the selection of the op encondition /(preferences /2/{/4/)/. V ery recen tly this implemen tation has also b een pac k aged in toucpop /4/./0/, a new v ersion of ucpop whic h is a v ailable b y anon ymous ftp to cs/.w ashington/.edu/./3/. Exp erime n ts Using UCPOPIn order to test our ideas w e mo di/\fed v ersion /2/./0 of ucpop /(Barrett et al/./, /1/9/9/4/)/, replac/-ing its default plan/-selection strategy /(S/+OC/+UC/) and goal/-selection strategy /(LIF O/) toincorp orate strategies discussed in the previous sections/.W e tested the mo di/\fed planner on sev eral problems in the ucpop suite/, emphasizingthose that had pro v ed most c hallenging for previous strategies/, on some arti/\fcial problemsdue to Kam bhampati et al/. /(/1/9/9/5/)/, in the trains transp ortation domain dev elop ed inRo c hester /(Allen /& Sc h ub ert/, /1/9/9/1/; Allen et al/./, /1/9/9/5/)/, and in Joslin /& P ollac k/'s TileW orlddomain /(Joslin /& P ollac k/, /1/9/9/4/)/. W e brie/\ry describ e the test problems and the platformsand parameter settings w e used/, and then presen t the exp erimen tal results for our impro v edsearc h strategies/./6/. /2/. and /3/. are zero/-commitmen t c hoices/. In our exp erimen ts/, whic h are describ ed in the next section/, thesub/-preference in /3/. ga v e impro v emen ts in the con text of Russell/'s tire c hanging domain /(in particularwith Fix/3/)/, without signi/\fcan t deterioration of p erformance in the other domains/./1/0/4A ccelera ting P ar tial/-Order Planners/3/./1 T est Problems and Exp erimen tal SettingsThe ucpop problems include T o w ers of Hanoi /(T of H/)/, Fixa/, Fix/3/, Fixit/, T o w er/-In v ert/4/,T est/-F erry /, and Sussman/-Anomaly /. In the case of T of H/, w e added a /3/-op erator v ersion tothe ucpop single/-op erator v ersion/, since T of H is a particularly hard problem for ucpopand its di/\u000ecult y has long b een kno wn to b e sensitiv e to the formalization /(e/.g/./, /(Green/,/1/9/6/9/)/)/. Fixa is a problem from Dan W eld/'s /\\fridge domain/\"/, in whic h the compressorin the fridge is to b e exc hanged/, requiring unscrewing sev eral screws/, stopping the fridge/,remo ving the bac kplane/, and making the exc hange/. Fix/3 is from Stuart Russell/'s /\\/\rat tiredomain/\"/, where a new wheel is to b e moun ted and lo w ered to the ground /(the old wheel hasb een jac k ed up already and the n uts lo osened/)/; this requires unscrewing the n uts holdingthe old wheel/, remo ving the wheel/, putting on the new wheel/, screwing on the n uts/, jac kingdo wn the h ub/, and tigh tening the n uts/. Fixit is more complicated/, as the wheel is not y etjac k ed up initially and the n uts not y et lo osened/, the spare tire needs to b e in/\rated/, andthe jac k/, wrenc h and pump all need to b e tak en out of the trunk and sto w ed again at theend/. T o w er/-In v ert/4 is a problem in the blo c ks w orld/, requiring the topmost blo c k in a stac kof four blo c ks to b e made b ottom/-most/. T est/-F erry is a simple problem requiring t w o carsto b e mo v ed from A to B using a one/-car ferry /, b y b oarding/, sailing/, and un b oarding foreac h car/.The arti/\fcial problems corresp ond to t w o parameter settings for AR T/-/#est\n/-/#clob\n/, oneof the t w o arti/\fcial domains that serv ed as a testb ed for Kam bhampati et al/. /'s extensiv estudy of the b eha vior of v arious planning strategies as a function of problem parameters/(Kam bhampati et al/./, /1/9/9/5/)/. AR T/-/#est\n/-/#clob\npro vides t w o la y ers of /1/0 op erators eac h/,where those in la y er /1 ac hiev e the preconditions of those in la y er /2/, and eac h op erator inla y er /2 ac hiev es one of the /1/0 goals/. Ho w ev er/, some op erators in eac h la y er can establishor clobb er the preconditions of their neigh b ors/, and this can force op erators to b e used ina certain order/.The v ersion of the trains domain that w e enco ded in v olv es four cities /(Av on/, Bath/,Corning/, Dansville/) connected b y four trac ks in a diamond pattern/, with a /\ffth cit y /(Elmira/)connected to Corning b y a /\ffth trac k/. The a v ailable resources/, whic h are lo cated at v ariouscities/, consist of a banana w arehouse/, an orange w arehouse/, an orange juice factory /, threetrain engines /(not coupled to an y cars/)/, /4 b o xcars /(suitable for transp orting oranges orbananas/)/, and a tank er car /(suitable for transp orting orange juice/)/. Goals are t ypically todeliv er oranges/, bananas/, or orange juice to some cit y /, requiring engine/-car coupling/, carloading and unloading/, engine driving/, and p ossibly OJ/-man ufacture/.The TileW orld domain consists of a grid on whic h holes and tiles are scattered/. A giv entile ma y or ma y not /\ft in to a particular hole/. The goals are to /\fll one or more holes b yusing three p ossible actions/: pic king up a tile/, going to an x/-y lo cation on the grid/, anddropping a tile in to a hole/. The agen t can carry at most four tiles at a time/.F ormalizations of these domains in terms of ucpop /'s language are pro vided in OnlineApp endix /2/. The exp erimen ts for all problems except Fixit/, the trains problems and theTileW orld problems w ere conducted on a sun /1/0 using Lucid Common Lisp /4/./0/./0/, whilethe rest /(T ables X/{XI in the next subsection/) w ere conducted on a sun /2/0 using AllegroCommon Lisp /4/./2/. Judging from some rep eated exp erimen ts/, w e do not think that the/1/0/5Gerevini /& Schuber tGoal/-selection Plan/-selecti on CPU sec PlansLIF O S/+OC/+UC /2/0/4/./5/1 /1/6/0/,/9/1/1///1/0/7/,/6 /4/9LIF O S/+OC /0/./9/7 /7/5/1///5/1/1ZLIF O S/+OC/+UC /6/./9/0 /1/8/1/6///1/2/9/1ZLIF O S/+OC /0/./5/4 /2/5/3///1/8/4T able I/: P erformance of plan//goal selection strategies on T/-of/-H/1di/\u000berences in the platforms signi/\fcan tly impact p erformance impro v emen ts/.\n/7Among thesearc h con trol functions pro vided b y ucpop /, w e used the default bestf/-search when theproblem w as solv able within the searc h limit of /4/0/,/0/0/0 plans generated/, while w e used thefunction id/-bf/-search /(an implemen tation of the linear/-space b est/-/\frst searc h algorithmgiv en b y Korf/, /1/9/9/2/)/, when this limit w as exceeded/.\n/8In all of the exp erimen ts the dela y/-separation switc h/, /*d/-sep/* /, w as on/, except for those using the LCFR strategy /./3/./2 Exp erimen tal Results for ZLIF O and S/+OCT ables I/{XI sho w the CPU time /(seconds/) and the n um b er of plans created//explored b yucpop on t w elv e problems in the domains describ ed ab o v e/: T o w ers of Hanoi with threedisks and either one op erator /(T/-of/-H/1/) or three op erators /(T/-of/-H/3/)/, the fridge domain/(Fixa/)/, the tire c hanging domain /(Fix/3 and Fixit/)/, the blo c ks w orld /(T o w er/-In v ert/4 andSussman/-anomaly/)/, the ferry domain /(T est/-F erry/)/, the arti/\fcial domain AR T/-/#est\n/-/#clob/(sp eci/\fcally /, AR T/-/3/-/6 and AR T/-/6/-/3/)/, the trains domain /(T rains/1/, T rains/2 and T rains/3/)and the TileW orld domain /(t w/-/1/, /./././, t w/-/6/)/. Both the n um b er of plans created//explored andthe CPU time are imp ortan t p erformance measures/. The n um b er of plans/, whic h indicatessearc h space size/, is a more stable measure in the sense that it dep ends only on the searc halgorithm/, not the implemen tation/.\n/9But the time is still of in terest since an impro v emen tin searc h ma y ha v e b een purc hased at the price of a more time/-consuming ev aluation ofalternativ es/. It turns out that w e do pa y some price in o v erhead when w e substitute ourstrategies for the defaults /(factors ranging from ab out /1/./2 to /1/./9/, and rarely higher/, p er plancreated/)/. This ma y b e due to sligh tly greater inheren t complexit y of ZLIF O versus LIF O/,but w e think the di/\u000berences could b e reduced b y substituting mo di/\fed data structures forthose of ucpop /{ w e w ere committed to not altering these/.T ables I and I I sho w that for the T of H the plan selection strategy S/+OC giv es dramaticimpro v emen ts o v er the default S/+OC/+UC strategy /. /(In these tests the default LIF O goalselection strategy w as used/./) In fact/, ucpop solv ed T/-of/-H/1 in /0/./9/7 seconds using S/+OCv ersus /2/0/4/./5 seconds using S/+OC/+UC/. T/-of/-H/3 pro v ed harder to solv e than T/-of/-H/1/, re/-/7/. The di/\u000berences w ere the result of what w as a v ailable at di/\u000beren t times and lo cales o v er the course ofnearly t w o y ears of exp erimen tation/./8/. This c hoice w as motiv ated b y the observ ation that when the problem is relativ ely easy to solv ebestf/-sear ch app ears to b e more e/\u000ecien t than id/-bf/-sea rc h /, while for hard problems it can b e v eryine/\u000ecien t b ecause of the considerable amoun t of space used at run time and the CPU time sp en t ongarbage collection/, whic h in some cases made Lisp crash/, rep orting an in ternal error/./9/. It is also w orth noting that the n um b er of plans created implicitl y tak es in to accoun t plan size/, sinceaddition of a step to a plan is coun ted as creation of a new plan in ucpop /./1/0/6A ccelera ting P ar tial/-Order PlannersGoal/-selection Plan/-selecti on CPU sec PlansLIF O S/+OC/+UC /> /6/0/0 /> /5/0/0/,/0/0/0LIF O S/+OC /8/./5/4 /5/5/0/6///3/4/1/5ZLIF O S/+OC/+UC /> /6/0/0 /> /5/0/0/,/0/0/0ZLIF O S/+OC /1/./2/4 /6/4/1///4/2/0T able I I/: P erformance of plan//goal selection strategies on T/-of/-H/3Goal/-selection Plan/-selecti on CPU sec PlansLIF O S/+OC/+UC /2/./4/5 /2/1/3/1///1/9/0/3LIF O S/+OC /2/./4/8 /2/1/3/1///1/9/0/3ZLIF O S/+OC/+UC /0/./3/3 /9/6///7/4ZLIF O S/+OC /0/./3/3 /9/6///7/4T able I I I/: P erformance of plan//goal selection strategies on Fixaquiring /8/./5 seconds using S/+OC and an unkno wn time in excess of /6/0/0 CPU seconds usingS/+OC/+UC/.Our ZLIF O goal/-selection strategy can signi/\fcan tly accelerate planning compared withthe simple LIF O strategy /. In particular/, when ZLIF O w as com bined with the S/+OC plan/-selection strategy in solving T of H/, it further reduced the n um b er of plans generated b y afactor of /3 in T/-of/-H/1 and b y a factor of /8 in T/-of/-H/3/. The o v erall p erformance impro v emen tfor T/-of/-H/1 w as th us a factor of /6/3/6 in terms of plans created and factor of /3/7/9 in terms ofCPU time /(from /2/0/4/./5 to /0/./5/4 seconds/)/.T ables I I I/{VI I I pro vide data for problems that are easier than T of H/, but still c halleng/-ing to ucpop op erating with its default strategy /, namely Fixa /(T able I I I/)/, Fix/3 /(T able IV/)/,T o w er/-In v ert/4 /(T able V/)/, T est/-F erry /(T able VI/) and the arti/\fcial domain AR T/-/#est\n/-/#clobwith /#est\n/= /3 and /#clob\n/= /6 /(T able VI I/) and with /#est\n/= /6 and /#clob\n/= /3 /(T able VI I/)/.The results sho w that the com bination of S/+OC and ZLIF O substan tially impro v es thep erformance of ucpop in comparison with its p erformance using S/+OC/+UC and LIF O/.The n um b er of plans generated dropp ed b y a factor of /2/2 for Fixa/, b y a factor of /5/./9 forGoal/-selection Plan/-selecti on CPU sec PlansLIF O S/+OC/+UC /6/./5/0 /3/3/9/6///2/0/7/1LIF O S/+OC /0/./4/3 /3/5/1///2/1/5ZLIF O S/+OC/+UC /1/./1/2 /3/5/7///2/2/1ZLIF O S/+OC /1/./5/3 /5/7/4///3/7/3T able IV/: P erformance of plan//goal selection strategies on Fix/3/1/0/7Gerevini /& Schuber tGoal/-selectio n Plan/-selecti on CPU sec PlansLIF O S/+OC/+UC /1/./3/5 /8/0/8///5/4/0LIF O S/+OC /0/./1/9 /1/4/8///1/0/5ZLIF O S/+OC/+UC /2/./8/1 /5/7/1///3/7/8ZLIF O S/+OC /0/./3/6 /1/4/2///9/6T able V/: P erformance of plan//goal selection strategies on T o w er/-In v ert/4Goal/-selectio n Plan/-selecti on CPU sec PlansLIF O S/+OC/+UC /0/./6/3 /7/1/8///4/5/7LIF O S/+OC /0/./3/2 /4/4/1///3/0/1ZLIF O S/+OC/+UC /0/./2/4 /1/3/6///9/1ZLIF O S/+OC /0/./2/2 /1/4/0///9/3T able VI/: P erformance of plan//goal selection strategies on T est/-F erryGoal/-selecti on Plan/-selection CPU sec PlansLIF O S/+OC/+UC /./6/7 /5/6/8///3/9/2LIF O S/+OC /1/./3/6 /1/2/9/9///8/4/0ZLIF O S/+OC/+UC /0/./1/6 /7/2///4/9ZLIF O S/+OC /0/./1/8 /7/9///5/4T able VI I/: P erformance of plan//goal selection strategies on AR T/-/#est\n/-/#clob\nwith /#est\n/= /3and /#clob\n/= /6 /(a v eraged o v er /1/0/0 problems/)Goal/-selection Plan/-selecti on CPU sec PlansLIF O S/+OC/+UC /1/./3/2 /9/8/5///6/5/3LIF O S/+OC /2/./0/8 /1/7/4/3///1/0/4/3ZLIF O S/+OC/+UC /0/./1/4 /5/7///3/7ZLIF O S/+OC /0/./1/4 /5/7///3/7T able VI I I/: P erformance of plan//goal selection strategies on AR T/-/#est\n/-/#clob\nwith /#est\n/= /6and /#clob\n/= /3 /(a v eraged o v er /1/0/0 problems/)Goal/-selection Plan/-selection CPU sec PlansLIF O S/+OC/+UC /0/./0/6 /4/4///2/6LIF O S/+OC /0/./0/4 /3/6///2/1ZLIF O S/+OC/+UC /0/./1/2 /6/7///4/3ZLIF O S/+OC /0/./0/7 /4/1///2/5T able IX/: P erformance of plan//goal selection strategies on Sussman/-anomaly/1/0/8A ccelera ting P ar tial/-Order Planners/1\n/1/0\n/1/0/0\n/1/0/0/0/1/0 /1/0/0 /1/0/0/0 /1/0/0/0/0 /1/0/0/0/0/0\n/1e/+/0/6 /1e/+/0/7Problem size\nSussman/-anomaly\nAR T/-/3/-/6T est/-ferry\nT o w er/-in v ert/4\nFixaFix/3\nT/-of/-H/1T rains/1AR T/-/6/-/3\nFixitP erformanceImpro v emen t/2\n/2/2\n/2\n/2\n/2/2\n/2/2\n/2/2 Se ar ch sp ac e r e ductionSp e e dupFigure /1/: P erformance impro v emen t due to ZLIF O and S/+OC/, relativ e to the n um b er ofplans generated b y LIF O and S/+OC/+UC /(log/-log scale/)/. The impro v emen ts forthe problems that ucpop w as unable to solv e ev en with a v ery high searc h limit/(T rains/2/, T rains/3/, and T/-of/-H/3/) are not included/.Fix/3/, b y a factor of /5/./7 for T o w er/-In v ert/4/, b y a factor of /5/./1 for T est/-F erry /, b y a factor of /7for AR T/-/3/-/6/, and b y a factor of /1/7 for AR T/-/6/-/3/.Concerning AR T/-/#est\n/-/#clob\n/, note that the p erformance w e obtained with unenhanceducpop /(/5/6/8 plans generated for AR T/-/3/-/6 and /9/8/5 for AR T/-/6/-/3/) w as m uc h the same as/(just marginally b etter than/) rep orted b y Kam bhampati et al/. /(/1/9/9/5/) for the b est plannersconsidered there /(/7/0/0 /{ /1/5/0/0 plans generated for AR T/-/3/-/6/, and /1/0/0/0/-/2/0/0/0 for AR T/-/6/-/3/)/.This is to b e exp ected/, since ucpop is a generalization of the earlier partial/-order planners/.Relativ e to standard ucpop and its predecessors/, our /\\accelerated/\" planner is th us an orderof magnitude faster/. In terestingly /, the en tire impro v emen t here can b e ascrib ed to ZLIF O/(rather than S/+OC plan selection/, whic h is actually a little w orse than S/+OC/+UC/)/. Thisis probably due to the un usual arrangemen t of op erators in AR T/-/#est\n/-/#clob\nin to a /\\clobb er/-ing c hain/\" /( An/BnZr\n/; /1\nclobb ers An/BnZr\n/BnZr /1 /; /1\n/'s preconditions/, /./././, A/1 /; /1\nclobb ers A/0 /; /1\n/'s preconditions/;similarly for Ai/; /2\n/)/, whic h mak es immediate atten tion to new unsafe conditions an un usuallygo o d strategy /.In exp erimen ting with v arious com binatorially trivial problems that unmo di/\fed ucpophandles with ease/, w e found that the S/+OC and ZLIF O strategy is neither b ene/\fcial norharmful in general/; there ma y b e a sligh t impro v emen t or a sligh t degradation in p erfor/-mance/. Results for the Sussman anomaly in T able IX pro vide an illustrativ e example/.W e summarize the results of T ables I/{X in Figure /1/, sho wing the p erformance impro v e/-men ts obtained with the com bined ZLIF O goal selection strategy and S/+OC plan selection/1/0/9Gerevini /& Schuber tT rains/1 T rains/2 T rains/3 FixitZLIF O /& Plans /4/0/9/7///2/0/1/9 /1/7/,/4/8/2///1/0/,/9/0/7 /3/1/,/9/5/7///1/9/,/2/8/2 /5/8/8/5///3/6/8/5S/+OC Time /1/3/./7 /8/0/./6 /1/8/9/./8 /3/2/./5LC /& Plans /4/3/8///2/4/2 /3/4/,/8/0/5///2/4/,/0/0/0 /2/5/3/,/8/6/1///1/6/8/,/8/5/2 /7/1/,/1/5/4///4/6/,/7/9/1S/+OC Time /2/./6 /3/6/8/./9 /1/8/7/9/./9 /5/4/7/./8LCFR /& Plans /1/0/9/3///5/9/7 /> /1/,/0/0/0/,/0/0/0 /> /1/,/0/0/0/,/0/0/0 /1/9/0/,/0/9/5///1/1/7/,/9/1/4S/+OC Time /1/0/./6/5 /> /1/0/,/9/0/5 /> /9/9/1/8 /4/4/1/2/./3/6LIF O /& Plans /1/,/0/7/1/,/4/7/9///4/3/2/,/8/8/1 /> /1/0/,/0/0/0/,/0/0/0 /> /1/,/0/0/0/,/0/0/0 /8/,/0/9/0/,/0/1/4///4/,/4/3/6/,/2/0/4S/+OC/+UC Time /3/0/5/0/./1/5 /> /3/7/,/8/7/9 /> /2/5/3/9 /2/7/,/5/8/4/./9T able X/: P erformance of the plan selection strategy S/+OC in com bination with the goalselection strategies ZLIF O/, LCFR and LC in solving problems whic h are v eryhard for the default strategies of ucpop /(S/+OC/+UC//LIF O/)/. /(The CPU secondsdo not include Lisp garbage collection/. The n um b er of plans generated for LCFRdo es not include those created in order to estimate the repair cost of the /\ra ws/./)Problem ZLIF O/* LCFRname CPU time Plans CPU time Planst w/-/1 /0/./0/9 /2/6///1/5 /0/./1/0 /2/6///1/5t w/-/2 /0/./6/1 /7/2///3/9 /0/./6/6 /7/2///3/9t w/-/3 /2/./5/5 /1/3/8///7/1 /3/./1/7 /1/3/9///7/2t w/-/4 /7/./8/0 /2/2/4///1/1/1 /1/0/./9/7 /2/2/7///1/1/4t w/-/5 /1/9/./4/1 /3/3/0///1/5/9 /3/0/./1/7 /3/3/6///1/6/5t w/-/6 /4/2/./5/7 /4/5/6///2/1/5 /7/1/./1/0 /4/6/6///2/2/5T able XI/: P erformance of UCPOP in the TileW orld domain using ZLIF O/* and LCFR forgoal selection/, and S/+OC/+F/+/0/./1UC for plan selectionstrategy as a function of pr oblem di/\u000eculty /(as indicated b y the n um b er of plans generatedb y the default LIF O plus S/+OC/+UC strategy/)/. The trend to w ard greater sp eedups formore complex problems /(though somewhat dep enden t on problem t yp e/) is quite apparen tfrom the log/-log plot/.F or direct comparison with Joslin and P ollac k/'s LCFR strategy and P eot and Smith/'sLC strategy /, w e implemen ted their strategies and applied them to sev eral problems/. Theydid v ery w ell /(sometimes b etter than ZLIF O/) for problems on the lo w er end of the di/\u000ecult ysp ectrum/, but p o orly for harder problems/. /(F or all the problems w e ran/, LC with the/*d/-sep/* switc h on p erformed b etter than LCFR in terms of plans explored and CPUtime required/./) F or T/-of/-H/1 LCFR in com bination b oth with the default S/+OC/+UC planselection strategy /, and with our S/+OC plan strategy did not /\fnd a solution within a searc hlimit of /2/0/0/,/0/0/0 plans generated /(cf/. /2/5/3 for ZLIF O with S/+OC/, and /7/5/1 for ZLIF O withS/+OC/+UC/)/, requiring an unkno wn CPU time in excess of /4/2/5/4 seconds with S/+OC/+UC/,/1/1/0A ccelera ting P ar tial/-Order Plannersand in excess of /4/8/3/4 seconds with S/+OC /(cf/. /0/./5/4 seconds for ZLIF O with S/+OC/)/.\n/1/0LC p erformed m uc h b etter than LCFR but still considerably w orse than ZLIF O/, solvingT/-of/-H/1 b y generating//exploring /8/3/1/3///6/8/7/4 plans with S/+OC and /8/6/9/9///6/4/4/1 plans withS/+OC/+UC/, and requiring /4/4/./4 CPU secs/. and /4/8/./9/5 CPU secs/. resp ectiv ely /. F or T/-of/-H/3/, LC found a solution b y generating//exploring /2/1/,/4/2/9///1/5/,/1 /9/9 plans with S/+OC/+UCand /1/7/,/5/3/9///1/4/,/4/1/9 plans with S/+OC/, requiring /1/4/5/./1/8 CPU secs/. and /7/7/./8/4 CPU secs/.resp ectiv ely /.T able X sho ws the results for the plan strategy S/+OC/, with the goal strategies ZLIF O/,LCFR and LC/, applied to three problems /(T rains/1/, T rains/2 and Fixit/)/. As sho wn b y the datain the table these are v ery hard for the default strategies of ucpop /(LIF O /& S/+OC/+UC/)/,but b ecome relativ ely easy when S/+OC is used in com bination either with ZLIF O/, LCFRor LC/. While LCFR and LC did sligh tly b etter than ZLIF O for T rains/1 /(the easiest ofthese problems/)/, they p erformed quite p o orly for Fixit/, T rains/2 and T rains/3 /(the hardestproblems/) compared to ZLIF O/.Joslin and P ollac k /(/1/9/9/4/) tested their LCFR strategy on six problems in the TileW orld/(t w/-/1/, /./././, t w/-/6/)/, /\fv e of whic h are v ery hard for default ucpop /, but easy for ucpop usingLCFR/.\n/1/1W e tested our ZLIF O strategy in the TileW orld using the same six problems/.ZLIF O did w ell for t w/-/1/{/4/, but for t w/-/5 and t w/-/6 its p erformance dropp ed w ell b elo w thatof LCFR/. This raised the question whether for these particular problems it is crucial tominimize /\\repair cost/\" in /\ra w selection uniformly /, rather than just in certain sp ecial cases/(ZLIF O do es minimize the repair cost when no threat is on the /\ra w list/, and at least one zero/-commitmen t op en condition is presen t/)/. Ho w ev er/, further exp erimen ts aimed at answ eringthis question suggested that the p o or c hoices made b y ZLIF O for some TileW orld problemsw ere not due to selection of /\\high cost/\" o v er /\\lo w cost/\" /\ra ws/. Instead t w o factors app ear b ecrucial for impro ving ZLIF O/: /(a/) emphasizing zero/-commitmen t op en conditions b y givingthem higher priorit y than threats/; /(b/) when there are no zero/-commitmen t op en conditions/,resolving threats as so on as they en ter the agenda/. /(W e realized the relev ance of /(b/) b yobserving that the p erformance of a mo di/\fed v ersions of LCFR/, where the /*d/-sep/* switc his implicitly forced on/, dramatically degraded for t w/-/6 in a sligh tly di/\u000beren t form ulation ofthe TileW orld/./)W e extended our ZLIF O strategy to include /(a/) and /(b/)/, and w e brie/\ry tested theresulting v arian t of ZLIF O /(ZLIF O/*/)/. T able XI sho ws the results for ZLIF O/* together withthe plan selection strategy S/+OC/+/0/./1UC/+F/, where as discussed in Section /2/./3 w e includedan atten uated form of the UC term /(UC///1/0/)/, and an F term equal to the n um b er of factssince TileW orld uses facts to trac k the n um b er of tiles carried b y the agen t/.\n/1/2ZLIF O/*/1/0/. This w as with /*d/-sep/* turned o/\u000b/, whic h is the implicit setting in LCFR /(Joslin/, /1/9/9/5/)/. In our exp erimen tsw e also tested a v arian t of LCFR/, where the switc h is forced to b e on/. The resulting goal strategy incom bination with our plan strategy S/+OC p erformed signi/\fcan tl y b etter for T/-of/-H/1/, solving the problemgenerating//expl ori ng /7/4/2/3///6/0/6/5 plans/, and using /1/1/0/./4/5 CPU seconds/. Note also that a comparison ofour implemen tation of LCFR and Joslin /& P ollac k/'s implemen tation used for the exp erimen ts discussedin /(Joslin /& P ollac k/, /1/9/9/4/) sho w ed that our implemen tation is considerably faster /(Joslin/, /1/9/9/5/)/./1/1/. In their exp erimen ts t w/-/2/, the easiest among t w/-/2/{/6/, w as not solv ed b y ucpop ev en when allo w ed to runfor o v er eigh t hours/. On the other hand/, ucpop using LCFR solv es t w/-/6/, the hardest problem/, withoutev er reac hing a dead/-end no de in the searc h tree/./1/2/. In the ZLIF O/* exp erimen ts the re/\fned plans generated b y resolving a threat w ere added to the /\ra w listin the follo wing order/: /\frst the plan generated b y promotion/, then the plan generated b y demotion/, and/\fnally the plan generated b y confron tation or separation/./1/1/1Gerevini /& Schuber tp erformed v ery e/\u000ecien tly for all six TileW orld problems/, in fact a little b etter than LCFR/.Note that for these problems ZLIF O/* is more e/\u000ecien t than LCFR in terms of the CPU time/,ev en though the n um b er of plans generated//explored b y the t w o strategies is appro ximatelythe same/. This is b ecause the o v erhead of selecting the next /\ra w to b e handled is higherin LCFR than in ZLIF O/* /(and ZLIF O/)/. In fact/, while LCFR needs to compute the /\\repaircost/\" of e ach /\ra w /(including the threats/) in the curren t plan/, ZLIF O/* /(ZLIF O/) only needs toc hec k for the presence of zero/-commitmen t op en conditions/, without pro cessing the threats/.Additional exp erimen ts indicated that the a v erage p erformance of ZLIF O/* is comparableto that of ZLIF O for most of the other problems w e used in our exp erimen ts/, in terms ofplans created//explored/. Ho w ev er/, the CPU time tends to increase since the o v erhead ofcomputing the goal selection function is higher for ZLIF O/* than for ZLIF O/, b ecause of theextra agenda/-managemen t costs/. Because of this o v erhead/, w e do not regard ZLIF O/* asgenerally preferable to ZLIF O/. Ho w ev er/, the TileW orld exp erimen ts underscored for us thatin some w orlds re/\fnemen ts of ZLIF O are adv an tageous/.Finally /, another p ossible v arian t of ZLIF O/, whic h w as suggested to us b y Da vid Smith/,is based on the follo wing preferences of the next /\ra w to b e handled/: /(i/) a threat that cannotb e resolv ed/; /(ii/) an op en condition that cannot b e established/; /(iii/) a threat that has onlyone p ossible resolution/; /(iv/) an op en condition that can only b e established in one w a y/; /(v/)other threats/; /(vi/) other op en conditions /(using LIF O to pic k among these/)/. W e observ ethat while this strategy could giv e further sa vings in terms of plans created//explored/, italso imp oses an additional o v erhead with resp ect to b oth ZLIF O and ZLIF O/* whic h coulddegrade p erformance in terms of CPU time/./4/. Precomputing P arameter DomainsEv en with the sp eedups obtained through impro v ed searc h/, a ucpop /-lik e algorithm remainssev erely limited in the complexit y of problems it can solv e/. W e b eliev e that signi/\fcan tfurther progress requires fuller use of glob al prop erties of the searc h space/, as determined b ythe structure of the op erators/, initial conditions/, and goals/. One w a y to do that w ould b ethrough a more in/-depth analysis of alternativ es during the searc h/, but this can lead to higho v erhead costs/. Another is to pr e c ompute constrain ts on the searc h space/, and to use theseduring planning to prune the searc h/. The parameter domain metho d w e no w motiv ate anddescrib e is of the latter t yp e/./4/./1 Ho w Can P arameter Domains Help/?In our previous exp erimen tation with ucpop strategies/, w e found that ucpop goal regres/-sion often h yp othesized steps that w ere do omed to b e abandoned ev en tually /, b ecause theystipulated imp ossible parameter bindings/. A clear example of this o ccurred in the Molgendomain/, as enco ded in the ucpop test suite/. The goal of the /\\Rat/-insulin/\" test problem is/(and /(bacterium /?b/) /(molecule /?m/)/(contains IG /?m/) /(contains /?m /?b/) /(pure /?b/)/) /,where /?b and /?m are existen tially quan ti/\fed v ariables/. What this means is that w e wishto create a puri/\fed bacterial culture /?b /, where /?b con tains a molecule /?m /(necessarily an/1/1/2A ccelera ting P ar tial/-Order Plannersexosome/, it turns out/)/, and this molecule in turn con tains the insulin gene/, IG /. W e areusing the abbreviations IG/, EE/, JE/, L for insulin/-gene/, e/-coli/-exosome/, junk/-exosome/,and link er/; and E/, J/, A/1 for e/-coli/, junk/, and an tibiotic/-/1/. Roughly sp eaking/, the solutionin v olv es pro cessing the initially giv en mRNA form of the insulin gene so as to pro duce aform of insulin DNA that can b e spliced in to the e/-coli/-exosome/, using a ligate op erator/.In turn/, the exosome is inserted in to the e/-coli bacterium using a transform op erator/, andthe bacterial culture is then puri/\fed using a screen op erator/, with an tibiotic/-/1/. /(The junkbacterium and exosome merely serv e to complicate the task /{ they are nearly /, but not quite/,substitutable for the e/-coli bacterium and exosome/; the junk exosome/, unlik e e/-coli/-exosome/,is not resistan t to an tibiotic/-/1/, violating a precondition of screen /./)No w/, in the initial regression the goals /(bacterium /?b/) and /(molecule /?m/) can b eestablished only with the /*start/* op erator/, i/.e/./, with the initial conditions/, and th us willnot b e instan tiated to bizarre v alues/. /(The initial conditions supply E and J as the onlyinstances of bacterium /, and IG/, EE/, JE /, and L as the only instances of molecule /./) Onthe other hand/, the remaining goals turn out to matc h the e/\u000bects of v arious instances ofthe ligate/, transform /, and screen op erators of Molgen/, as follo ws/:/(contains IG /?m/)/: /(ligate IG /?m/)/, /(transform IG /?m/)/(contains /?m /?b/)/: /(ligate /?m /?b/) /, /(transform /?m /?b/)/(pure /?b/)/: /(screen /?b /?y /?z/)ucpop will happily regress on these actions/. Y et t w o of them/, /(transform IG /?m/) and/(ligate /?m /?b/) /, are do omed to fail/, p erhaps after a great deal of e/\u000bort has b een exp endedon trying to satisfy their preconditions/. In particular/, examination of the constan ts that can/\\/\ro w in to/\" the transform op erator from the initial conditions and other Molgen op eratorssho ws that its /\frst argumen t is restricted to domain f EE/, JE g /, i/.e/./, it m ust b e one ofthe giv en exosomes/, and the second is restricted to f E/, J g /, i/.e/./, it m ust b e one of thegiv en bacteria/. Consequen tly the instance /(transform IG /?m/) is unrealizable/, as its /\frstargumen t IG is not in f EE/, JE g /. /(Note that distinct constan ts denote distinct en titiesaccording to the unique/-names assumption made b y ucpop /./) The /(ligate /?m /?b/) actionis do omed for sligh tly more subtle reasons/. It is the result of a matc h b et w een /(contains /?m/?b/) and a /\\ when /-clause/\" /(conditional e/\u000bect/) of the ligate op erator/, whose preconditionscan b e reac hed only if the second parameter /?b lies in the set of molecules f IG/, JE/, EE g /;y et /?b is also restricted to the set of bacteria f E/, J g /, as a result of the goal condition/(bacterium /?b/) /. The fact that these sets are disjoin t should allo w us to eliminate the/(transform IG /?m/) action/.Note that elimination of action candidates as ab o v e increases the n um b er of zero com/-mitmen t plan re/\fnemen ts that can b e made/. In the example/, w e are left with exactly oneaction for eac h of the three goals/, and so the ZLIF O and LCFR strategies will prefer toregress on these goals rather than regressing on /(bacterium /?b/) and /(molecule /?m/) /{whic h w ould prematurely mak e arbitrary c hoices of /?b and /?m from the initial state/./4/./2 Description of the AlgorithmIn an y completed plan/, eac h precondition of eac h action m ust b e instan tiated b y an e/\u000bectof some earlier action/. So the v alues of the parameters of the action can only b e v alues that/1/1/3Gerevini /& Schuber tcan b e /\\pro duced/\" b y earlier actions/, starting with the initial action/, /*start/* /. Moreo v er/,supp ose that a parameter x of a certain action o ccurs in eac h of preconditions P/1/, /./././, Pk /.Then a constan t c is a p ossible v alue of x only if earlier actions can instan tiate x to c ine ach of P/1/, /./././, Pk /.Our algorithm find/-parameter/-domains is based on these observ ations/. Beginning inthe initial state/, it propagates p ositiv e atomic predications to all p ossible op erator precon/-ditions/. F or a propagated ground atom/, if the atom matc hes an op erator precondition/,the algorithm adds the constan ts in that ground atom to the individual domains of theparameters they w ere uni/\fed with/. These individual domains are particular to sp eci/\fc pre/-conditions/. F or instance/, the individual domain of /?x for an op erator with preconditions/(on /?x /?y/)/, /(clear /?x/) will in general b e distinct for these t w o preconditions/.As so on as w e ha v e nonempt y individual domains for all parameters in all preconditionsof an op erator/, w e form the interse ction of the individual domains of eac h parameter ofthe op erator/. F or example/, if /(on /?x /?y/) has /(so far/) b een matc hed b y /(on A B/) and/(on B C/) /, and /(clear /?x/) has /(so far/) b een matc hed b y /(clear A/) and /(clear Table/) /,then the individual domain of x will b e f A/,B g in the /\frst precondition and f A/,Table gin the second/. Th us /(assuming there are no other preconditions/) the in tersected domainof /?x will b e f A g at this p oin t/. If later /(clear B/) is also matc hed against /(clear /?x/) /,the in tersected domain of /?x will gro w to f A/,B g /. When b oth /?x and /?y ha v e nonempt yin tersected domains/, the e/\u000bects /(p ostconditions/) of the op erator can in turn b e propagated/,with /?x and /?y /\\b ound/\" to their in tersected domains/.The propagated e/\u000bects are again matc hed against all p ossible op erator preconditions/,and when a v ariable /\\b ound/\" to an in tersected domain is successfully uni/\fed with a v ari/-able in a precondition/, it passes its in tersected domain to the individual domain of thatprecondition/-v ariable /(via a union op eration/)/. This can again lead to gro wth of the in ter/-sected domains of the op erator whose precondition w as matc hed/, the e/\u000bects of that op eratorma y then b e propagated/, and so on/. The individual domains and in tersected domains gro wmonotonically during the propagation pro cess/, and in the end represen t the desired param/-eter domains of the op erators/.W e illustrate this pro cess through an example/. Consider the simple planning problemdepicted in Figure /2 where an /\\op erator graph/\" /(Smith /& P eot/, /1/9/9/3/) is used to describ e thelogical dep endencies among the op erators/, while the iterativ e computation of the parameterdomains is graphically illustrated with a /\\domain/-propagation graph/\" b elo w the op eratorgraph/.The initial conditions /(P A/) and /(P B/) unify with the precondition /(P /?x/) of op/1/. So/,the individual domain of /?x relativ e to the precondition P of op/1 is f A/,B g /. On the otherhand/, the precondition /(Q /?x/) of op/1 cannot b e satis/\fed b y the initial state/, and so theindividual domain of /?x relativ e to Q is initially the empt y set/. Hence the in tersected domainof /?x for op/1 is also the empt y set/.F or op/2 w e ha v e a di/\u000beren t situation/, since here w e ha v e only one precondition and itcan b e established b y the initial state/. Therefore/, the individual domain of /?y relativ e toprecondition R of op/2 is the set of constan ts f B/,C g /, and the in tersected domain of /?y forop/2 is the same set /(b ecause R is the only precondition of op/2 in v olving /?y /)/. Since thein tersected domain of /?y has b een enlarged /(initially it w as empt y/)/, it is propagated to theindividual domains of the other op erators through the e/\u000bect /(Q /?y/) of op/2/. In particular/,/1/1/4A ccelera ting P ar tial/-Order Planners\nindicates bundle of edges\n*start* *end*(P ?x)\n(Q ?x)\n(R ?y)\n(S ?z)(T B)op1\nop2\nop3\n            effects:              effects:             effects:op1:     preconds: op3:     preconds:  (P ?x),(Q ?x)\n(S ?x)(R ?y)\n(Q ?y)op2:     preconds:  (S ?z)\n(T ?z)Init state: \nGoal: (T B)(P A),(P B),(R B),(R C),(S C)\nid(?y)={B,C}ID(P,?x)={A,B}\nID(Q,?x)={}id(?x)={} id(?x)={B}\nID(P,?x)={A,B}\nID(Q,?x)={B,C}\nid(?z)={A,B}\nID(S,?z)={A,B} ID(S,?z)={A}id(?z)={A}ID(R,?y)={B,C}\n(S ?z)(Q ?y)(P B)(P A)\n(R C)(R B)\nS(A)(R ?y)(Q ?x)(P ?x)\n(S ?z)(S ?x)(Q ?x) op1\nop2op1\nop3 op3Figure /2/: Op erator and domain/-propagation graphs for a simple planning problem/.ID/(/?x/,P/) indicates the individual domain of the parameter /?x relativ e to pre/-condition P/; id/(/?x/) indicates the in tersected domain of the parameter /?x/; /\fnalin tersected domains are indicated using b old fon ts/./(Q /?y/) matc hes the precondition /(Q /?x/) of op/1/. So/, the individual domain of /?x relativ eto precondition Q of op/1 is up dated b y adding the constan ts of the in tersected domain of /?yto it/. Th us the in tersected domain of /?x is enlarged to f B g /, and can b e propagated throughthe e/\u000bect /(S /?x/) of op/1/.Similarly /, the propagation of /(S /?x/) will enlarge the individual domain of /?z for op/3/,and also the in tersected domain/, to the set f A/,B g /. Therefore/, the /\fnal in tersected domainsare/: f B g for /?x in op/1/; f B/,C g for /?y in op/2/; f A/,B g for /?z in op/3/.Before presen ting the algorithm a little more formally /, w e note that the parameter do/-mains will sometimes b e /\\to o large/\"/, including v alues that w ould b e found to b e imp ossible/1/1/5Gerevini /& Schuber tif a more detailed state space exploration w ere conducted/. Ho w ev er/, all that is requiredfor soundness in our use of the domains is that they not b e /\\to o small/\" /(i/.e/./, that theycon tain all parameter v alues that can actually o ccur in the problem under consideration/)/.Of course/, to b e of practical use the parameter domains of an op erator should excludesome of the constan ts o ccurring in the problem sp eci/\fcation/, particularly those for whic hit is in tuitiv ely ob vious that they are of the wrong sort to /\fll particular argumen t slots ofthe op erator/. This has turned out to b e the case for all problem domains w e ha v e so farexp erimen ted with/.The preceding sk etc h of our metho d is an o v ersimpli/\fcation since preconditions ande/\u000bects of ucpop op erators ma y b e particular to a when /-clause/. In this case w e computeindividual domains and in tersected domains separately for eac h when /-clause/. F or example/,consider the follo wing sc hematic represen tation of an op erator/:/(define /(operator op/1/)/:parameters /(/?x /?y/)/:precondition /(and P/1 P/2 /)/:effect /(and E/1 E/2/(when P\n/0E\n/0/)/(when P /\" E /\" /) /)/)/,where all conditions starting with P or E denote atomic form ulas that ma y in v olv e /?x and /?y /.W e can think of this op erator as consisting of a primary when/-clause whose preconditionsP/1 and P/2 m ust alw a ys b e satis/\fed and whose e/\u000bects E/1 and E/2 are alw a ys asserted/, andt w o se c ondary when/-clauses whose resp ectiv e preconditions P\n/0and P /\" ma y or ma y notb e satis/\fed/, and when they are/, the corresp onding e/\u000bects E\n/0and E /\" are asserted/. Hereour algorithm w ould main tain individual domains for /?x and /?y for eac h of preconditionsP/1/, P/2/, P\n/0/, and P /\"/, and it w ould main tain in tersected domains for /?x and /?y for theprimary when /-clause and eac h of the t w o secondary clauses/. The in tersected domains forthe secondary clauses w ould b e based on the individual domains of /?x and /?y not onlyrelativ e to P\n/0and P /\"/, but also on those relativ e to P/1 and P/2 /, since /(as noted/) the primarypreconditions m ust hold for the op erator to ha v e an y of its e/\u000bects/, including conditionale/\u000bects/.Some further complications arise when ucpop op erators con tain univ ersally quan ti/-/\fed preconditions or e/\u000bects/, disjunctiv e preconditions/, or facts /(men tioned in Section /2/./2/)/.Rather than dealing with these complications directly /, w e will assume that no suc h op/-erators o ccur in the input to the algorithm/. Later w e describ e a semi/-automated w a y ofhandling op erators con taining the additional constructs/.The algorithm is outlined b elo w /(a more detailed description is giv en in Online Ap/-p endix /1/)/. W is a list of /(names of /) when/- clauses whose e/\u000bects are to b e propagated/.Individual parameter domains are initially nil/, and in tersected parameter domains are ini/-tially either nil or T /(where T is the univ ersal domain/)/. The in tersected domain of aparameter/, relativ e to a giv en when/- clause/, is T just in case the parameter o ccurs neitherin the preconditions of the when/- clause nor in the primary preconditions/. /(In suc h a casethe successful instan tiation of the when/- clause is clearly indep enden t of the c hoice of v aluefor the parameter in question/./) Uni/\fcation in step /2/(a/) is as usual/, except that when ane/\u000bect v ariable v is uni/\fed with a constan t c in a precondition/, the uni/\fcation succeeds/,/1/1/6A ccelera ting P ar tial/-Order Plannerswith uni/\fer v /= c/, just in case c is an elemen t of the in tersected domain of v /(for the rel/-ev an t when/- clause/)/. The giv en inits /(initial conditions/) and go als /(whic h ma y b e omitted/,i/.e/./, nil/) are treated as an op erator /*start/* with no preconditions and an op erator /*end/*with no e/\u000bects/. V ariables in go als are treated lik e op erator parameters/. W e use the terms/\\parameters/\" and /\\v ariables/\" in terc hangeably here/.Algorithm/: /\fnd/-parameter/-domains /( op er ators/,inits/,go als /)/1/. Initialize W to the initial conditions/, so that it con tains just the /(primary/) when/- clauseof /*start/* /./2/. Rep eat steps /(a/{c/) un til W /= nil/:/(a/) Unify the p ositiv e e/\u000bects of all when/- clauses in W with all p ossible op eratorpreconditions/, and mark the preconditions successfully matc hed in this w a y as/\\matc hed/\"/. /(This marking is p ermanen t/./) Augmen t the individual domain ofeac h matc hed precondition v ariable with a certain set C of constan ts/, de/\fned asfollo ws/. If the precondition v ariable w as uni/\fed with a constan t c/, then C /= f c g /;if it w as uni/\fed with an e/\u000bect v ariable/, then C is the interse cte d domain of thate/\u000bect v ariable /(relativ e to the when/- clause to whic h the e/\u000bect b elongs/)/./(b/) Mark those when/- clauses as /\\propagation candidates/\" that ha v e all their precon/-ditions /(including corresp onding primary preconditions/) mark ed as /\\matc hed/\"and that in v olv e at least one v ariable for whic h some relev an t individual domainw as augmen ted in step /(a/)/./(c/) Reset W to nil/. F or all when/- clauses that are propagation candidates/, computenew in tersected domains for their v ariables/. If an in tersected domain of a when/-clause is thereb y enlarged/, and all in tersected domains for the when/- clause areno w nonempt y /, then add the when/- clause to W/./3/. F urther restrict in tersected domains using equativ e preconditions of form /(EQ u v /) /,i/.e/./, form a common in tersected domain if b oth u and v are v ariables/. If u is aconstan t and v is a v ariable/, reduce the in tersected domain of v b y in tersecting itwith f u g /; similarly if u is a v ariable and v is a constan t/. If the equation b elongs to aprimary when/- clause/, use it to reduce the in tersected domains of u and v /(whic hev erare v ariables/) in the secondary clauses as w ell/./4/. Return the in tersected domains as the parameter domains/, pro ducing a sequence oflists with eac h list of form/( op /( x/1\na/1\nb/1\nc/1\n/:/:/: /) /( x/2\na/2\nb/2\nc/2\n/:/:/: /) /:/:/: /)/,where eac h op erator op app ears at least once/. If op has k conditional e/\u000bects/, therewill b e k /+ /1 successiv e lists headed b y op /, where the /\frst pro vides the parameterdomains for the primary e/\u000bects of op and the rest pro vide the parameter domains forthe conditional e/\u000bects /(in the order of app earance in the ucpop de/\fnition of op /)/.Note that w e do not matc h or propagate ne gative conditions/. The problem with negativ econditions is that a v ery large n um b er of them ma y b e implicit in the initial conditions/, giv en/1/1/7Gerevini /& Schuber tthe use of the Closed W orld Assumption in ucpop /. F or instance/, in a w orld of n blo c ks/,with at most O /( n /) on /-relations /(assuming that a blo c k can b e on only one other blo c k/)/, w enecessarily ha v e O /( n\n/2/) implicit /(not /(on /./././)/) relations/. In fact/, the individual v ariabledomains of negativ e preconditions or goals can really b e in/\fnitely large/. F or instance/, giv enan empt y initial state and a /(paint/-red /?x/) op eration with precondition /(not /(red /?x/)/)and e/\u000bect /(red /?x/) /, w e can ac hiev e /(red c /) for in/\fnitely man y constan ts c /. P erhapsnegativ e conditions could b e e/\u000bectiv ely dealt with b y main taining an ti/-domains for them/,but w e ha v e not explored this since in practice ignoring negativ e conditions seems to causeonly minimal /\\domain bloating/\"/. /(W e ha v e pro v ed that no actual domain elemen ts can b elost through neglect of some preconditions/./)Our use of EQ /-conditions could b e re/\fned b y making use of them during the propagationpro cess/, and NEQ /-conditions could also b e used/. Ho w ev er/, doing so w ould probably ha v emarginal impact/.As a /\fnal commen t/, w e note that the output format sp eci/\fed in step /4 of the algorithmis actually generalized in our implemen tation so as to rep ort inaccessible preconditionsand goals/. These inaccessible conditions are simply app ended to the list of parameterdomains for the appropriate when /-clause of the appropriate op erator/. F or instance/, if thepreconditions /(oj /?oj/) and /(at /?oj /?city/) in the ld/-oj /(/\\load orange juice/\"/) op eratorof the trains w orld /(see Online App endix /2/) are unreac hable /(sa y /, b ecause no oranges forpro ducing orange juice ha v e b een pro vided/)/, the parameter domain list for the /(unique/)when /-clause of ld/-oj will ha v e the app earance/(ld/-oj /(/?oj /././. /) /(/?car /././. /) /(/?city /././. /) /(oj /?oj/) /(at /?oj /?city/)/) /.This feature turns out to b e v ery useful for debugging op erator sp eci/\fcations and detectingunreac hable goals/./4/./3 Correctness and T ractabilit yIn k eeping with the remarks in the previous section/, w e will call an algorithm for computingparameter domains c orr e ct if the domains it computes subsume all p ossible parameter v aluesthat can actually o ccur /(in a giv en primary or secondary when /-clause/) if w e consider allp ossible sequences of op erator applications starting at the giv en initial state/.The p oin t is that this prop ert y will main tain the soundness of a planning algorithm thatuses the precomputed parameter domains to prune imp ossible actions /(as w ell as spuriousthreats/) from a partially constructed plan/. W e assert the follo wing/:Theorem /1 The find/-parameter/-domains algorithm is c orr e ct for c omputing p ar ameterdomains of ucpop /-style sets of op er ators /(without quanti/\fc ation/, disjunction/, or facts/)/,initial c onditions/, and /(p ossibly/) go al c onditions/.The pro of is giv en in App endix A/. A preliminary step is to establish termination/, using themonotonic gro wth of domains and the /\fniteness of the set of constan ts in v olv ed/. Correctnessis then established b y sho wing that if there exists a v alid sequence A/0\nA/1\n/:/:/:An\nof actions/(op erator instances/) starting with A/0\n/= /*start/* /, and if An\nis an instance of the op eratorOp /, then the bindings that the parameters of Op receiv ed in instance An\nare ev en tually addedto the relev an t in tersected domains of Op /(where /\\relev an t/\" refers to the when/- clauses of Opwhose preconditions are satis/\fed at the b eginning of An\n/)/. This is pro v ed b y induction on n /./1/1/8A ccelera ting P ar tial/-Order PlannersW e no w indicate ho w w e can deal with univ ersally quan ti/\fed preconditions and e/\u000bects/,disjunctiv e preconditions/, and facts/. W e mak e some simple c hanges to op erator de/\fnitionsb y hand in preparation for parameter domain precomputation/, and then use the domainscomputed b y find /- parameter /- domains /, together with the original op erators/, in runningthe planner/. The steps for preparing an op erator for parameter domain precomputation areas follo ws/:/\u000f Delete disjunctiv e preconditions/, fact/-preconditions/,\n/1/3and univ ersally quan ti/\fed pre/-conditions /(this includes univ ersally quan ti/\fed goals/; it w ould also include univ ersallyquan ti/\fed sen tences em b edded within the an teceden ts of when /-clauses/, e/.g/./, in themanner /(/:when /(/:forall /(/?x/) /\b /) /\t /) /, though these do not o ccur in an y problemdomains w e ha v e seen/)/./\u000f Drop univ ersal quan ti/\fers o ccurring p ositiv ely in op erator e/\u000bects/, i/.e/./, o ccurring atthe top lev el or em b edded b y one or more /:and /'s/. F or example/, an e/\u000bect/(/:and /(at robot /?to/)/(/:not /(at robot /?from/)/)/(/:forall /(/?x/)/(/:when /(/:and /(grasping /?x/) /(object /?x/)/)/(/:and /(at /?x /?to/) /(/:not /(at /?x /?from/)/)/) /)/)/)w ould b ecome/(/:and /(at robot /?to/)/(/:not /(at robot /?from/)/)/(/:when /(/:and /(grasping /?x/) /(object /?x/)/)/(/:and /(at /?x /?to/) /(/:not /(at /?x /?from/)/)/) /)/)/)Note that the univ ersally quan ti/\fed v ariable should b e renamed/, if necessary /, to b edistinct from all other suc h v ariables and from all op erator parameters/.In the example ab o v e the univ ersally quan ti/\fed v ariable is unrestricted/. When thequan ti/\fed v ariable includes a t yp e restriction/, as in /(/:forall /(object /?x/) /\b /) /, thenthis t yp e restriction needs to b ecome an an teceden t of the matrix sen tence /\b/. Inthe example at hand/, /\b should b e rewritten as the equiv alen t of /(/:when /(object /?x/)/\b /) /. Since /\b is often a when /-clause/, this can b e done b y adding /(object /?x/) as aconjunct to the an teceden t of the when /-clause/. In some cases /\b is a conjunction ofwhen /-clauses/, and in suc h a case the quan ti/\fer restriction can b e added in to eac hwhen /-clause an teceden t/./\u000f Drop existen tial quan ti/\fers in preconditions and goals/, adding an y restrictions on thequan ti/\fed v ariables as conjuncts to the matrix sen tence/. F or example/, the goal/(/:exists /(bacterium /?y/)/(/:exists /(molecule /?x/)/(/:and /(contains IG /?x/)/(contains /?x /?y/)/(pure /?y/) /)/)/)/1/3/. E/.g/./, in the strips /-w orld w e w ould drop /(fact /(loc/-in/-r oom /?x /?y /?room/)/) /, whic h c hec ks whetherthe giv en co ordinates lie in the giv en ro om/./1/1/9Gerevini /& Schuber tb ecomes/(/:and /(bacterium /?y/) /(molecule /?x/) /(contains IG /?x/)/(contains /?x /?y/) /(pure /?y/) /)/(Actually /, the /:and is dropp ed as w ell/, when supplying goals to find /- parameter /-domains /./)With these reductions/, find/-parameter/-domains will then compute correct parameterdomains for the op erators and goals/. T o see this/, note /\frst of all that dropping pre/-conditions /(in the initial step ab o v e/) will not forfeit correctness/, since doing so can onlywe aken the constrain ts on admissible parameter v alues/, and th us can only add constan tsto the domains/. The e/\u000bect of dropping a univ ersal quan ti/\fer/, from the p ersp ectiv e offind/-parameter/-domains /, is to in tro duce a new parameter in place of the univ ersal v ari/-able/. /(The op erator normalization subroutine detects v ariables in op erator preconditionsand e/\u000bects that are not listed as parameters/, and treats them as additional parameters/./)While this is of course a drastic c hange in the meaning of the op erator/, it preserv es correct/-ness of the parameter domain calculation/. This is b ecause the domain of the new parameterwill certainly con tain all constan ts /(and hence/, under the Closed W orld Assumption/, all ob/-jects/) o v er whic h the quan ti/\fed v ariable ranges/. F or example/, if /?x is treated as a parameterrather than a univ ersally quan ti/\fed v ariable in the conditional e/\u000bect/(/:forall /(/?x/) /(/:when /(object /?x/) /(in /?x box/)/)/) /,then the domain of /?x for the when /-clause will consist of ev erything that can b e an ob ject/, inan y state where the op erator can b e applied/. Th us the e/\u000bect /(in /?x box/) will also b e prop/-agated for all suc h ob jects/, as is required/. Finally /, the elimination of existen tial quan ti/\fersfrom preconditions and goals can b e seen to preserv e the meaning of those preconditionsand goals/, and hence preserv es the correctness of the parameter domain calculation/.Next w e formally state our tractabilit y claim for the algorithm/, as follo ws /(with sometacit assumptions/, men tioned in the pro of /)/.Theorem /2 A lgorithm find/-parameter/-domains c an b e implemente d to run in O /( mnp\nne\n/( np\n/+ne\n/)/) time and O /( mnp\n/) sp ac e in the worst c ase/, wher e m is the numb er of c onstants in thepr oblem sp e ci/\fc ation/, np\nis the c ombine d numb er of pr e c onditions for al l op er ators /(andgo als/, if include d/)/, and ne\nis the c ombine d numb er of op er ator e/\u000be cts /(including those of/*start/* /)/.Again the pro of is in App endix A/. The time complexit y of find /- parameter /- domains isdetermined as the sum of /(/1/) the cost of all the uni/\fcations p erformed/, /(/2/) the costs of allthe individual domain up dates attempted/, and /(/3/) the cost of all the in tersected domainup dates attempted/. The space complexit y b ound is easily deriv ed b y assuming that thereis a /\fxed upp er b ound on the n um b er of argumen ts that a predicate /(in a precondition ore/\u000bect/) can ha v e/, and from the fact that for eac h when /-clause at most O /( m /) constan ts arestored/.By adding some additional data structures in find/-parameter/-domains w e can obtaina v ersion of the algorithm whose w orst/-case time complexit y is sligh tly impro v ed/. In fact/,in step /2/./(c/) instead of propagating al l of the e/\u000bects of a when /-clause with an enlarged/1/2/0A ccelera ting P ar tial/-Order Plannersin tersected domain /(i/.e/./, adding suc h a when /-clause to the list W/)/, it is su/\u000ecien t to propagatejust those e/\u000bects of the when /-clause that in v olv e an enlarged in tersected/-domain/. This couldb e done b y setting up for eac h when /-clause a table that maps eac h parameter to a list ofe/\u000bects /(of that when /-clause/) in v olving that parameter/.In the impro v ed algorithm w e use W to store the list of e/\u000bects /(instead of the list of when /-clauses/) that will b e propagated in the next cycle of the algorithm/, and steps /1/, and /2 offind/-parameter/-domains are mo di/\fed in the follo wing w a y/:/1\n/0/. Initialize W to the list of the e/\u000bects of /*start/* /./2\n/0/. Rep eat steps /(a/{c/) un til W /= nil/:/(a\n/0/) Unify the p ositiv e e/\u000bects in W with all p ossible op erator preconditions/, and markthe preconditions successfully matc hed in this w a y as /\\matc hed/\" /./././(b\n/0/) same as /2/./(b/)/./(c\n/0/) Reset W to nil/. F or all when/- clauses that are propagation candidates/, com/-pute new in tersected domains for their v ariables/. If an in tersected domain of awhen/- clause is thereb y enlarged/, and all in tersected domains for the when/- clauseare no w nonempt y /, then add to W the subset of the e/\u000bects of the when /-clausein v olving at least one parameter whose in tersected domain is enlarged/.Note that the w orst/-case time complexit y of the revised algorithm is impro v ed/, b ecause no weac h e/\u000bect of eac h when /-clause is propagated at most O /( m /) times/. This decreases the upp erb ound on the n um b er of uni/\fcations p erformed/, reducing the complexit y estimated in step/(/1/) of the pro of of Theorem /2 to O /( mne\nnp\n/)/. Hence w e ha v e pro v ed the follo wing corollary /.Corollary /1 Ther e exists an impr ove d version of find/-parameter/-domains that c an b eimplemente d to run in O /( mn\n/2p\nne\n/) time in the worst c ase/./5/. Using P arameter Domains for Accelerating a PlannerW e ha v e already used the example of Molgen to motiv ate the use of precomputed parameterdomains in planning/, sho wing ho w suc h domains ma y allo w us to prune non/-viable actionsfrom a partial plan/.More fundamen tally /, they can b e used eac h time the planner needs to unify t w o predi/-cations in v olving a parameter/, either during goal regression or during threat detection/. /(Ineither case/, one predication is a /(sub/)goal and the other is an e/\u000bect of an action or aninitial condition/./) If the uni/\fer is inconsisten t with a parameter domain/, it should coun tas a failure ev en if it is consisten t with other binding constrain ts in the curren t /(partial/)plan/. And if there is no inconsistency /, w e can use the uni/\fer to interse ct and thus r e/\fnethe domains of parameters equated b y the uni/\fer/.F or example/, supp ose that G /= /(at /?x /?y/) is a precondition of a step in the curren tplan/, and that E /= /(at /?w /?z/) is an e/\u000bect of another /(p ossibly new/) step/, where /?x /, /?y /,/?w and /?z are parameters /(or/, in the case of /?w and /?z /, existen tially quan ti/\fed v ariables/)whic h ha v e no binding constrain ts asso ciated with them in the curren t plan/. Assume alsothat the domains of the parameters are/:/1/2/1Gerevini /& Schuber t/?x /: /{Agent/1/, Agent/2/, Agent/3/} /?y /: /{City/1/, City/2/}/?w /: /{Agent/1/, Agent/2/} /?z /: /{City/3/, City/4/}The uni/\fcation of G and E giv es the binding constrain ts f /?x /= /?w /, /?y /= /?z g /, whic h arenot viable b ecause the parameter domains of /?y and of /?z ha v e an empt y in tersection/.On the other hand/, if the domain of /?z had b een f City/2/, City/3/, City/4 g /, then the uni/\f/-cation of G and E w ould ha v e b een judged viable/, and the domains of the parameters w ouldha v e b een re/\fned to/:/?x /: /{Agent/1/, Agent/2/} /?y /: /{City/2/}/?w /: /{Agent/1/, Agent/2/} /?z /: /{City/2/}Th us parameter domains can b e incremen tally re/\fned as the planning searc h progresses/;and the narro w er they b ecome/, the more often they lead to pruning/./5/./1 Incorp orating P arameter Domains in to UCPOPThe preceding consistency c hec ks and domain re/\fnemen ts can b e used in a partial/-order/,causal/-link planner lik e ucpop as follo ws/. Giv en a goal /(op en condition/) G selected b yucpop as the next /\ra w to b e repaired/, w e can/(/1/) restrict the set of new op erator instances that ucpop w ould use for establishing G/; aninstance of an op erator with e/\u000bect E /(matc hing G/) is disallo w ed if the precomputedparameter domains relev an t to E are incompatible with the curren t parameter do/-mains or binding constrain ts relev an t to G/; /(note that the curren t parameter domainsasso ciated with G ma y b e re/\fnemen ts of the initial domains/)/;/(/2/) restrict the set of existing steps that ucpop w ould reuse for establishing G/; reusing astep with e/\u000bect E /(matc hing G/) is disallo w ed if the curren t parameter domains relev an tto E are incompatible with the curren t parameter domains or binding constrain tsrelev an t to G/.Moreo v er/, giv en a p otential threat b y an e/\u000bect Q against a protected condition P /, insp ectionof the relev an t parameter domains ma y rev eal that the threat is actually spurious/. Thishapp ens if the uni/\fer of P and Q violates the /(p ossibly re/\fned/) domain constrain ts of aparameter in P or Q/. Th us w e can often/(/3/) reduce the n um b er of threats that are generated b y the planner when a new causallink is in tro duced in to the plan /(this happ ens when an op en condition is establishedeither b y reusing a step or b y in tro ducing a new one/)/;/(/4/) recognize that a threat on the list of the /\ra ws to b e pro cessed is redundan t/, allo wingits elimination/. /(Note that since parameter domains are incremen tally re/\fned duringplanning/, ev en if w e use /(/3/) during the generation of the threats/, it is still p ossible fora threat to b ecomes spurious after it has b een added to the /\ra w list/)/.These four uses of parameter domains cut do wn the searc h space without loss of viablesolutions/, since the options that are eliminated cannot lead to a correct/, complete plan/./1/2/2A ccelera ting P ar tial/-Order PlannersNote that /(/3/) and /(/4/) can b e useful ev en when the planner only deals with de/\fnitethreats /(i/.e/./, /*d/-sep/* switc h is turned on/) for at least three reasons/. First/, determiningthat a threat is not a de/\fnite threat when /*d/-sep/* is on incurs an o v erhead cost/. So/,earlier elimination of a spurious threat could lead to considerable sa vings if the threat isdela y ed man y times during the searc h/. The second reason relates to the plan/-selectionstrategies adopted/. If one uses a function that includes an /(atten uated/) term corresp ondingto the n um b er of threats curren tly on the /\ra w list/, then eliminating spurious threats inadv ance can giv e a more accurate measure of the /\\badness/\" of a plan/. Finally /, parameterdomains could b e used in threat pro cessing so as to prune the searc h ev en when /*dsep/* ison/. In particular/, supp ose that w e mo dify the notion of a de/\fnite threat/, when w e ha v eparameter domains/, so that e/.g/./, /(P /?x/) and /(not /(P /?y/)/) comprise a de/\fnite threat ifthe parameter domains asso ciated with /?x and /?y are b oth c /. So in that case/, ev en withd/-sep/* on/, w e ma y disco v er early that a threat has b ecome de/\fnite /{ in whic h case it migh talso b e a forced threat/, i/.e/./, the c hoice b et w een promotion and demotion ma y b e dictatedb y ordering constrain ts/; and that can prune the searc h space/. Ho w ev er/, in our curren timplemen tation w e do not exploit this third p oin t/.W e ha v e incorp orated these tec hniques in to ucpop /(v ersion /2/./0/)/, along with our earlierimpro v emen ts to the plan and goal selection strategies/. P arameter domains are handledthrough an extension of the /\\ v arset /\" data structure /(W eld/, /1/9/9/4/) to include the domainsof the v ariables /(parameters/)/, and b y extending the uni/\fcation pro cess to implemen t the/\fltering discussed ab o v e/.\n/1/4W e no w describ e our exp erimen ts with this enhanced system/./5/./2 Exp erimen tal Results Using P arameter DomainsOur main goal here is to sho w that while the o v erhead determined b y computing the param/-eter domains is not signi/\fcan t /(b oth at prepro cessing time and at planning time/)/, exploita/-tion of the parameter domains during planning can signi/\fcan tly prune the searc h/. In theexp erimen ts w e used the v ersion of find/-parameter/-domains whic h is describ ed in Section/4/./2 and in Online App endix /1/. Note that for domains more complex than the ones w e ha v econsidered it migh t b e w orth while to use the impro v ed v ersion of the algorithm discussed inSection /4/./3/. /(Ho w ev er/, it remains to b e seen whether problems signi/\fcan tly more complexthan those w e consider here can b e solv ed b y an y ucpop /-st yle planner/./)The CPU times needed b y our implemen tation of find/-parameter/-domains are negli/-gible for the problems w e ha v e lo ok ed at/. They w ere /1/0 msec or less for man y problemsin the ucpop test suite /(when running compiled Allegro CL /4/./2 on a sun /2/0/)/, /2/0 msec fort w o problems /(Fixa from the fridge repair domain and Fixit from the /\rat tire domain/)/, and/3/0msec on the trains w orld problems describ ed b elo w/.In our /\frst set of tests/, w e relied on the searc h strategy used as default in ucpop /. Thefunction used for A/* plan selection w as th us S/+OC/+UC/+F /(allo wing for problems thatin v olv e /\\facts/\"/)/, and the goals w ere selected from the agenda according to a pure LIF Odiscipline /.\n/1/5/1/4/. In the curren t implemen tation new threats are /\fltered only when the protected condition is establishedb y a step already in the plan/./1/5/. In all exp erimen ts the /*d/-sep/* switc h w as on/. The default dela y/-separation strategy for selecting unsafeconditions w as sligh tly mo di/\fed in the v ersion of ucpop using parameter domains/. In particular/, the/1/2/3Gerevini /& Schuber tW e b egan b y exp erimen ting with a v ariet y of problems from ucpop /'s test suite/, com/-paring p erformance with and without the use of parameter domains/. While relativ ely easyproblems suc h as Sussman/-anomaly /, Fixa/, T est/-ferry /, and T o w er/-in v ert/4 sho w ed no im/-pro v emen t through the use of parameter domains/, most problems /{ particularly the harderones /{ w ere solv ed more easily with parameter domains/. F or example/, the Rat/-insulinproblem from the Molgen domain w as solv ed nearly t wice as fast/, and some strips /-w orldproblems /(Mo v e/-b o xes and v arian ts/)\n/1/6and T o w ers of Hanoi /(T/-of/-H/1/) w ere solv ed ab out/1/0 times as fast/. Note that the strips /-w orld problems in v olv e b oth facts and univ ersallyquan ti/\fed conditional e/\u000bects/. Tw o problems from the o/\u000ece w orld/, O/\u000ece/5 and O/\u000ece/6/,whic h w e knew to b e readily solv able with our impro v ed searc h strategy /, remained di/\u000e/-cult /(in the case of O/\u000ece/6/, unsolv able/) with the default ucpop strategy /, despite the useof parameter domains/.\n/1/7F urther exp erimen ts rev ealed that the source of this ine/\u000eciencyw as the default plan/-selection strategy of ucpop /. In fact/, using our S/+OC/+F strategyinstead of S/+OC/+UC/+F/, without parameter domains O/\u000ece/5 and O/\u000ece/6 w ere solv ed gen/-erating//exploring /3/0/5/8///2/1/7/5 and /8/7/7/0///6/9/4/0 plans resp ectiv ely/; while using the parameterdomains the plans n um b ered /1/5/3/1///1/0/5/5 and /2/9/5/4///2/2/0/4 resp ectiv ely /.These initial exp erimen ts suggested to us that the most promising application of com/-puted parameter domains w ould b e for non trivial problems that in v olv ed a v ariet y of typ es ofen tities and relationships/, and signi/\fcan t amoun ts of goal c haining /(i/.e/./, with eac h successiv eaction establishing preconditions for the next/)/. F rom this p ersp ectiv e/, the trains w orldstruc k us as a natural c hoice for further exp erimen tation/, with the additional adv an tagethat its design w as indep enden tl y motiv ated b y researc h at Ro c hester in to mixed/-initiativ eproblem solving through natural/-language in teraction/. /(Refer again to the formalization inOnline App endix /2/./) Recall from T able X that the T rains/1 problem w as extremely hard forunmo di/\fed ucpop /, requiring ab out /5/0 min utes and generating o v er a million plans/.Running the same problem with parameter domains pro duced a solution in /3/./3 seconds/(with /1/2/0/7 plans generated/)/, i/.e/./, /9/2/7 times faster/.In tuitiv ely /, the use of parameter domains to constrain planning is analogous to usingt yp e constrain ts on the parameters /(although parameter domains also tak e accoun t of initialconditions/)/. So it is of in terest to see whether adding t yp e constrain ts can pro vide similare/\u000eciency gains as the use of parameter domains/. Our /\frst set of exp erimen ts thereforeincluded T/-T rains/1/, a /\\t yp ed/\" v ersion of T rains/1/; the op erators ha v e b een sligh tly c hangedb y adding new preconditions stating the t yp es of the parameters in v olv ed/. F or example/,the op erator uncouple has b een augmen ted with the preconditions /(engine /?eng/) and/(car /?car/) /. This problem w as also extremely hard for the unmo di/\fed ucpop /, exceedingthe searc h limit of /1/,/0/0/0/,/0/0/0 plans generated and requiring more than /2/6/0/0 seconds/. Withparameter domains/, the solution w as obtained in one second/.threats that can b e resolv ed b y separation and whic h are recognized to b e redundan t through the use ofparameter domains w ere selected to b e eliminated/./1/6/. Mo v e/-b o xes/-/2 di/\u000bers sligh tly from the Mo v e/-b o xes problem in the ucpop suite/, in that its goal is /(in/-roombox/2 rclk/) /; Mo v e/-b o xes/-a di/\u000bers sligh tly from the Mo v e/-b o xes/-/2/, in that its initial state con tains t w ob o xes/./1/7/. O/\u000ece/5 is directly from ucpop /'s test suite and O/\u000ece/6 is minor v arian t of O/\u000ece/5/. In O/\u000ece/5/, all p ersonsare to b e furnished with c hec ks made out to them/, using a c hec k prin ter at the o/\u000ece and a briefcase forpic king up the c hec ks and bringing them home/. /\\Sam/\" and /\\Sue/\" are the giv en p ersons/, and in O/\u000ece/6w e ha v e added /(person Alan/) and /(person Smith/) in the initial conditions/./1/2/4A ccelera ting P ar tial/-Order PlannersProblems without domains with domains DomainPlans CPU sec Plans CPU sec ratioT rains/1 /1/,/0/7/1/,/4/7/9///4/3 /2/,/8 /8/1 /3/0/5/0/./1/5 /1/2/0/7///8/2/4 /3/./2/9 /0/./4/2/5T/-T rains/1 /> /1/,/0/0/0/,/0/0/0 /> /2/3/3/5 /4/0/4///2/9/6 /0/./9/8 /0/./4/2/5Mo v e/-b o xes /6/0/8/,/2/3/1///1/6/7/,/4 /1/8 /1/0/2/4/./0/4 /5/7/4/6///3/2/5/3 /1/8/./8 /0/./7/0/5Mo v e/-b o xes/-/1 /> /1/,/0/0/0/,/0/0/0 /> /6/1/6/5 /1/2/6/4///6/4/5 /3/./5/9 /0/./7/0/5Mo v e/-b o xes/-/2 /1/3/,/8/1/6///3/9/2/7 /4/5/./0/5 /1/1/7/5///5/8/7 /2/./6/6 /0/./7/0/5Mo v e/-b o xes/-a /1/3/,/8/0/5///3/9/1/8 /4/6/./1/1 /1/1/7/5///5/8/7 /2/./5/4 /0/./7/0/2T/-of/-H/1 /1/6/0/,/9/1/1///1/0/7/,/6 /4/9 /2/0/4/./5/1\n/\u0003/1/7/,/6/0/3///1/2/,/2/5 /0 /3/7/./5 /0/./7/2/2Rat/-insulin /3/6/4///2/6/2 /0/./3/6 /1/9/6///1/2/9 /0/./1/9 /0/./7/1/4Monk ey/-test/1 /9/6///6/2 /0/./1/2 /7/5///4/6 /0/./1/1 /0/./7/3/3Monk ey/-test/2 /4/1/5///2/6/2 /0/./6/1 /2/4/7///1/4/9 /0/./5/0 /0/./5/2/9Fix/3 /3/3/9/5///2/0/7/0 /5/./7/7 /3/1/0/3///1/9/8/3 /6/./0/2 /0/./5/3/2O/\u000ece/5 /8/0/9/,/3/4/5///5/0/0/,/5 /7/8 /1/9/2/7/./4 /5/7/5/,/2/2/4///3/5/8/,/5 /2/3 /1/5/5/6/./8 /0/./6/2/5O/\u000ece/6 /> /1/,/0/0/0/,/0/0/0 /> /2/7/3/0 /> /1/,/0/0/0/,/0/0/0 /> /2/6/4/0 /0/./6/6/7T o w er/-in v ert/4 /8/0/6///5/3/8 /1/./5/5 /8/0/6///5/3/8 /1/./5/9 /0/./7/3/3Sussman/-anomal y /4/4///2/6 /0/./0/5 /4/4///2/6 /0/./0/6 /0/./9/1/7Fixa /2/1/3/1///1/9/0/3 /2/./2 /2/1/3/1///1/9/0/3 /2/./3/4 /1T est/-ferry /7/1/8///4/5/7 /0/./6/5 /7/1/8///4/5/7 /0/./7/1 /1T able XI I/: Plans generated//visited and CPU time /(secs/) for standard ucpop with andwithout parameter domains/. /(\n/\u0003This result w as obtained on a sun /1/0 withLucid Common Lisp/; the others on a sun /2/0 with Allegro Common Lisp/./)These results indicate that adding t yp e constrain ts to op erator sp eci/\fcations is notnearly as e/\u000bectiv e as the use of parameter domains in b o osting planning e/\u000eciency /. W ediscuss this p oin t further in the con text of the second set of tests /(b elo w/)/.T able XI I summarizes the exp erimen tal results for all of the exp erimen ts that used thedefault ucpop searc h strategy /. The table giv es the n um b er of plans generated//visited b ythe planner and the CPU time /(seconds/) required to solv e the problems/.\n/1/8Note that theuse of the parameter domains ga v e v ery dramatic impro v emen ts not only in the trains do/-main/, but also in the strips /-w orld domain/. The righ tmost column supplies /\\domain ratio/\"data/, as a metric that w e hop ed w ould predict the lik ely e/\u000bectiv eness of using parameterdomains/. The idea is that parameter domains should b e e/\u000bectiv e to the exten t that they/\flter out man y parameter bindings that can b e reac hed b y c haining bac k from individualpreconditions of an op erator to the initial state/. These bindings can b e found b y using av arian t of the algorithm for propagating in tersected domains that instead propagates unionsof individual domains/, and comparing these union domains to the in tersected domains/.\n/1/9/1/8/. The systems w ere compiled under Allegro CL /4/./2/, with settings /(space /0/) /(sp eed /3/) /(safet y /1/) /(debug/0/)/, and run on a sun /2/0/. The CPU time includes the Lisp garbage collection /(it is the time giv en in theoutput b y ucpop /)/./1/9/. Actually /, w e do not need to explicitly propagate union domains/, but can propagate /(partial/) bindings forone predication at a time/, starting with the initial conditions/. W e matc h the predication to all p ossiblepreconditions /, adding the constan t argumen ts it con tains to the union domains of the matc hed op erator/1/2/5Gerevini /& Schuber ttrains without domains with domains Domainproblems Plans CPU sec Plans CPU sec ratioT rains/1 /4/0/9/7///2/0/1/9 /1/3/./7 /2/9/7///2/3/8 /1/./4 /0/./4/2/5T rains/2 /1/7/,/4/8/2///1/0/,/9 /0/7 /8/0/./6 /1/3/1/2///1/0/6/5 /7/./1/6 /0/./4/2/5T rains/3 /3/1/,/9/5/7///1/9/,/2 /8/2 /1/8/9/./8 /3/8/8/5///3/1/7/5 /2/5/./1 /0/./4/1/1T able XI I I/: Plans generated//visited and CPU time /(secs/) for ucpop with and withoutparameter domains in the trains domain using the ZLIF O strategy /.trains without domains with domains Domainproblems Plans CPU sec Plans CPU sec ratioT rains/1 /1/0/9/3///5/9/7 /8/./1 /2/6/5///1/9/4 /2/./3 /0/./4/2/5T rains/2 /> /5/0/,/0/0/0 /> /6/0/7 /> /5/0/,/0/0/0 /> /5/3/4 /0/./4/2/5T rains/3 /> /5/0/,/0/0/0 /> /6/5/5 /> /5/0/,/0/0/0 /> /5/6/4 /0/./4/1/1T able XIV/: Plans generated//visited and CPU time /(secs/) for ucpop with and withoutparameter domains in the trains domain using the LCFR strategy /.The /\\domain ratio/\" pro vides this comparison/, dividing the a v erage union domain size b y thea v erage in tersected domain size/, with a v erages tak en o v er all parameters of all when/- clausesof all op erators/.The largest sp eedups /(e/.g/./, for the trains problems/) do tend to correlate with thesmallest domain ratios/, and the smallest sp eedups with the largest domain ratio /(unit y /{see the last few ro ws/)/. Ho w ev er/, it can b e seen from the table that the problem di/\u000ecult y /(asmeasured b y plans or CPU time/) is m uc h more useful than the domain ratio as a predictorof sp eedups to b e exp ected when using parameter domains/. Problems that generate on theorder of a million plans or more with standard ucpop tend to pro duce sp eedups b y /3 ordersof magnitude/, whereas the domain ratio for some of these problems /(e/.g/./, Mo v e/-b o xes/-/1/) isno b etter /(or ev en w orse/) than for problems with m uc h smaller sp eedups /(e/.g/./, Mo v e/-b o xes/-a/, Rat/-insulin/, Monk ey/-test/1/, Monk ey/-test/2/)/. The m uc h lo w er di/\u000ecult y of these problemspredicts their reduced sp eedup/. But to complicate matters/, not all di/\u000ecult problems giv ehigh sp eedups /(see T/-of/-H/1 and esp ecially O/\u000ece/5/)/; w e do not kno w what subtleties ofproblem structure accoun t for these un usual cases/.In our second round of exp erimen ts/, w e tested the e/\u000bectiv eness of the parameter domaintec hnique in com bination with our impro v ed searc h strategy /, i/.e/./, S/+OC//ZLIF O/. In addi/-tion/, w e com bined S/+OC with LCFR /(least cost /\ra w selection/) /(Joslin /& P ollac k/, /1/9/9/4/)/, so/(or when/- clause/)/. W e then /\fnd corresp onding /(partially b ound/) e/\u000bects/, and add an y new e/\u000bects to thelist of predications still to b e propagated/. A partially b ound e/\u000bect suc h as /(P A /?x /?y/) is new if thereis no iden tical or similar predication suc h as /(P A /?u /?v/) among the previously propagated predicationsor among those still to b e propagated/./1/2/6A ccelera ting P ar tial/-Order Plannerst/-trains without domains with domains Domainproblems Plans CPU sec Plans CPU sec ratioT/-T rains/1 /3/1/3/4///2/1/8/3 /1/7/./2 /5/0/5///4/1/6 /3/./4 /0/./4/2/5T/-T rains/2 /5/7/3/9///4/3/2/5 /3/7/./3 /3/4/8/2///2/7/4/9 /2/7/./3 /0/./4/2/5T/-T rains/3 /1/7/,/9/3/1///1/3/,/1/3/4 /1/3/0/./4 /1/1/,/9/6/2///9/4/0/1 /1/0/5/./1 /0/./4/2/5T able XV/: Plans generated//visited and CPU time /(secs/) for ucpop with and without pa/-rameter domains in the /\\t yp ed/\" trains domain using the ZLIF O strategy /.t/-trains without domains with domains Domainproblems Plans CPU sec Plans CPU sec ratioT/-T rains/1 /3/1/3/8///2/4/1/2 /3/1/./5 /1/4/2/9///1/1/5/7 /1/4/./5 /0/./4/2/5T/-T rains/2 /> /5/0/,/0/0/0 /> /1/0/3/5 /> /5/0/,/0/0/0 /> /1/1/3/6 /0/./4/2/5T/-T rains/3 /> /5/0/,/0/0/0 /> /9/7/6 /> /5/0/,/0/0/0 /> /9/6/2 /0/./4/2/5T able XVI/: Plans generated//visited and CPU time /(secs/) for ucpop with and withoutparameter domains in the /\\t yp ed/\" trains domain using the LCFR strategy /.as to test for p ossible sensitivit y of the parameter/-domains tec hnique to the precise strategyused/. F or the presen t set of tests w e used a searc h limit of /5/0/,/0/0/0 plans generated/.Once again w e b egan b y sampling some problems from the ucpop test suite/, and theseinitial trials yielded results quite analogous to those for the default ucpop strategy /. W eobtained no impro v emen ts for sev eral easier problems and signi/\fcan t impro v emen ts forharder ones /(e/.g/./, again close to a factor of /2 for Rat/-insulin/)/. Notew orth y mem b ers of thelatter category w ere O/\u000ece/5 and O/\u000ece/6 /{ recall that O/\u000ece/5 had sho wn little sp eedup withstandard ucpop and O/\u000ece/6 had b een unsolv able/. Ho w ev er/, in view of the computationalexp ense of testing b oth ZLIF O and LCFR/, w e then decided to narro w our fo cus to thetrains w orld/. As men tioned/, the adv an tages of this w orld are its inheren t in terest andrelativ e complexit y /.T ables XI I I/-XVI pro vide exp erimen tal results for the trains domain with the S/+OC//ZLIF O strategy and the S/+OC//LCFR strategy /, in eac h case with and without parameterdomains/.The results in T ables XI I I and XIV sho w that using parameter domains can still giv e v erysigni/\fcan t impro v emen ts in p erformance/, o v er and ab o v e those obtained through the useof b etter searc h strategies/. F or example/, the use of parameter domains pro vided an /1/1/-foldsp eedup for T rains/2/, for the S/+OC//ZLIF O strategy /. In this particular problem the sp eedup/(on all metrics/) w as the result of pruning /1/4/8/2 plans /(more than half of those generated/)during the searc h/./, and recognizing /3/0/5 unsafe conditions as redundan t/. Eviden tly /, thee/\u000bect of this pruning is ampli/\fed b y an order of magnitude in the o v erall p erformance/,b ecause of the futile searc hes that are cut short/. Note that the sp eedups for T rains/1/-/3 are/1/2/7Gerevini /& Schuber troughly comparable /(within a factor of /2/) to those obtained for problems in the previous setwith comparable initial di/\u000ecult y /(e/.g/./, see Mo v e/-b o xes/-/2 and Mo v e/-b o xes/-a in T able XI I/)/.This again p oin ts to a rather consisten t correlation b et w een problem di/\u000ecult y and sp eedupsobtainable using parameter domains/. The constan t domain ratios are also compatible withthe more or less in v arian t sp eedups here/, though this is of little imp ort/, giv en the earlierresults/. F or S/+OC//LCFR the gains app ear to b e less/, though the single result sho winga /3/./5/-fold sp eedup pro vides only anecdotal evidence for suc h a conclusion/. T rains/2 andT rains/3 remained to o di/\u000ecult for solution b y LCFR/. Similar gains w ere observ ed for theS/+OC//LC strategies where the b est observ ed gain in the T rains domain w as a /1/./7/-foldsp eedup for T rains/2/. In an y case/, all results con/\frm the e/\u000bectiv eness of the parameter/-domains tec hnique/.T ables XV and XVI are again for the /\\t yp ed/\" v ersion of trains /. In this case parametert yping ga v e mo dest impro v emen ts in the absence of parameter/-domains/, and /(in con trastwith the results for T rains/1 under the default searc h strategy/) signi/\fcan t deterioration intheir presence/. While w e do not kno w ho w to accoun t for these results in detail/, it seemsclear that con trary e/\u000bects are in v olv ed/. On the one hand/, t yping do es tend to help in thatit tends to limit c hoices of parameter v alues to /\\sensible/\" ones/. F or example/, a precondition/(engine /?eng/) will b e satis/\fable only through use of /*start/* /, and the initial state will th usconstrain /?eng to assume sensible v alues/. On the other hand/, adding t yp e/-preconditionswill tend to broaden the searc h space/, b y adding further op en conditions to the /\ra w list/.The lesson from the /\\t yp ed/\" exp erimen ts app ears to b e that it is b est not to supplyexplicit t yp e constrain ts on op erator parameters/, instead using our automated metho d ofcalculating and up dating domains to constrain parameter bindings/./6/. Conclusions and F urther W orkW e b egan b y exploring some simple/, domain/-indep enden t impro v emen ts to searc h strategiesin partial order planning/, and then describ ed a metho d of using precomputed parameter do/-mains to prune the searc h space/. W e no w summarize our conclusions ab out these tec hniquesand then p oin t to promising directions for further w ork/./6/./1 Impro ving Searc hOur prop osed impro v emen ts to searc h strategies w ere based on the one hand on a carefullyconsidered c hoice of terms in the A/* heuristic for plan selection/, and on the other on apreference for c ho osing op en conditions that cannot b e ac hiev ed at all or can b e ac hiev edin only one w a y /(with a default LIF O prioritization of other op en conditions/)/. Since theplan re/\fnemen ts corresp onding to uniquely ac hiev able goals are logically necessary /, w e ha v etermed the latter strategy a zero/-commitmen t strategy /. One adv an tage of this tec hniqueo v er other similar strategies is that it incurs a lo w er computational o v erhead/.Our exp erimen ts based on mo di/\fcations of ucpop indicate that our strategies can giv elarge impro v emen ts in planning p erformance/, esp ecially for problems that are hard forucpop /(and its /\\relativ es/\"/) to b egin with/. The b est p erformance w as ac hiev ed when ourstrategies for plan selection and goal selection w ere used in com bination/. In practical terms/,w e w ere able to solv e nearly ev ery problem w e tried from the ucpop test suite in a fractionof a second /(except for Fixit/, whic h required /3/8/./2 seconds/)/, where some of these problems/1/2/8A ccelera ting P ar tial/-Order Plannerspreviously required min utes or w ere unsolv able on the same mac hine/. This included asu/\u000ecien t v ariet y of problems to indicate that our tec hniques are of broad p oten tial utilit y /.F urther/, our results suggest that zero/-commitmen t is b est supplemen ted with a LIF Ostrategy for op en conditions ac hiev able in m ultiple w a ys/, rather than a generalization ofzero/-commitmen t fa v oring goals with the few est c hildren/. This somewhat surprising resultmigh t b e though t to b e due to the w a y in whic h the designer of a domain orders thepreconditions of op erators/; i/.e/./, the /\\natural/\" ordering of preconditions ma y correlate withthe b est planning order/, giving a fortuitous adv an tage to a LIF O strategy relativ e to astrategy lik e LC/.\n/2/0Ho w ev er/, some preliminary exp erimen ts w e p erformed with randomized preconditionsfor T/-of/-H/1 and T rains/1 indicate otherwise/. In /5 randomizations of the preconditions ofT/-of/-H/1/, b oth LC and ZLIF O w ere slo w ed do wn somewhat/, b y a v erage factors of /2/./2 /(/2/)and /3/./3 /(/4/./2/) in terms of plans expanded /(CPU time used/) resp ectiv ely /. /(In b oth cases/,S/+OC w as used for plan searc h/./) This still left ZLIF O with a p erformance adv an tage ofa factor of /2/2 in terms of plans created and /3/9 in terms of CPU time/. F or T rains/1 thep erformance of LC greatly deteriorated in /2 out of /5 cases /(b y a factor close to /7/0 in termsof b oth plans and time/)/, while that of ZLIF O actually impro v ed marginally /. This no w leftZLIF O with an a v erage p erformance adv an tage o v er LC /(whereas it had b een sligh tly slo w erin the unrandomized case/) /{ a factor of /3/./3 in terms of plans and /6/./7 in terms of CPU time/(though these v alues are v ery unreliable/, in view of the fact that the standard deviationsare of the same order as the means/)/.Despite these results w e b eliev e that a satisfactory understanding of the dep endence of/\ra w/-selection strategies on the order of op erator preconditions will require a more extensiv eexp erimen tal in v estigation/. W e are curren tly undertaking this w ork/./6/./2 Using P arameter DomainsW e describ ed an implemen ted/, tractable algorithm for precomputing parameter domains ofplanning op erators/, relativ e to giv en initial conditions/. W e sho w ed ho w to use the precom/-puted domains during the planning pro cess to prune non/-viable actions and b ogus threats/,and ho w to up date them dynamically for maxim um e/\u000bect/.The idea of using precomputed parameter domains to constrain planning w as apparen tly/\frst prop osed in a tec hnical rep ort b y Goldszmidt et al/. /(/1/9/9/4/)/. This con tains the essen tialidea of accum ulating domains b y forw ard propagation from the initial conditions/. Thoughthe rep ort only sk etc hes a single/-sw eep propagation pro cess from the initial conditionsto the goals/, the implemen ted Ro c kw ell Planner /(RNLP/) handles cyclic op erator graphs/,rep eatedly propagating bindings un til quiescence/, m uc h as in our algorithm/. Our algorithmdeals with the additional complexities of conditional e/\u000bects and equalities /(and in semi/-automated fashion with quan ti/\fcation/) and app ears to b e more e/\u000ecien t /(Smith/, /1/9/9/6/)/.Other distinctiv e features of our w ork are the metho d of incremen tally re/\fning domains/2/0/. This w as suggested to us b y Da vid Smith as w ell as Mik e Willia mson/. Willi amson tried ZLIF O with /5randomized v ersions of T/-of/-H/1/, and rep orted a large p erformance degradation /(Willia mson /& Hanks/,/1/9/9/6/)/. W e recen tly ran these v ersions using our implemen tation/, obtaining far more fa v orable results/(three of the /\fv e v ersions w ere easier to solv e than the original v ersion of T/-of/-H/1/, while the other t w ov ersions slo w ed do wn ZLIF O b y a factor of /1/./8/4 and /4/./8/6 in terms of plans explored/./)/1/2/9Gerevini /& Schuber tduring planning/, the theoretical analysis of our algorithm/, and the systematic exp erimen taltests/.Another closely related study is that of Y ang and Chan /(/1/9/9/4/)/, who used hand/-suppliedparameter domains in planning m uc h as w e use precomputed domains/. An in terestingasp ect of their w ork is the direct use of sets of constan ts as v ariable bindings/. F or instance/,in establishing a precondition /(P /?x/) using an initial state con taining /(P a/) /, /(P b/) and/(P c/) /, they w ould bind /?x to f a/, b/, c g rather than to a sp eci/\fc constan t/. They re/\fnethese /\\noncommittal/\" bindings during planning m uc h as w e re/\fne v ariable domains/, andp erio dically use constrain t satisfaction metho ds to c hec k their consistency with curren tEQ//NEQ constrain ts/. They conclude that dela ying v ariable bindings w orks b est for problemswith lo w solution densities /(while degrading p erformance for some problems with highsolution densities/)/, and that the optimal frequency of making consistency c hec ks dep ends onwhether dead ends tend to o ccur high or lo w in the searc h tree/. Our w ork is distinguishedfrom theirs b y our metho d of precomputing parameter domains/, our use of sp eci/\fc bindingswhen matc hing initial conditions to OCs/, our use of parameter domains in threat detectionand resolution/, and our handling of the enric hed syn tax of ucpop op erators as comparedsnlp op erators/.Judging from the examples w e ha v e exp erimen ted with/, our tec hniques are w ell/-suitedto non trivial problems that in v olv e div erse t yp es of ob jects/, relations and actions/, and sig/-ni/\fcan t logical in terdep endencies among the steps needed to solv e a problem/. When used inconjunction with the default searc h strategy of ucpop /, our metho d ga v e signi/\fcan t sp eedupsfor non trivial problems/, reac hing a sp eedup factor of /9/2/7 in the trains transp ortation plan/-ning domain/, and more than /1/7/1/7 for the hardest strips/- w orld problem w e tried /. Whencom bined with our S/+OC and ZLIF O searc h strategies/, the parameter domain tec hniquestill ga v e sp eedups b y a factor of around /1/0 for some trains problems/. Though our im/-plemen tation is aimed at a ucpop /-st yle planner/, essen tially the same tec hniques w ould b eapplicable to man y other planners/.W e also found the parameter domain precomputations to b e a v ery useful debuggingaid/. In fact/, the domain precomputation for our initial form ulation of the trains w orldimmediately rev ealed sev eral errors/. F or instance/, the domain of the /?eng parameter ofmv/-engine turned out to con tain oranges/, bananas/, and an OJ factory /, indicating the needfor a t yp e constrain t on /?eng /. /(Without this/, transp ortation problems w ould ha v e b eensolv able without the b ene/\ft of engines and trains/!/) Another immediately apparen t problemw as rev ealed b y the parameter domains for /?city/1 and /?city/2 in mv/-engine /: the domainfor /?city/1 excluded Elmira/, and that for /?city/2 excluded Av on/. The ob vious diagnosisw as that w e had neglected to assert b oth /(connected c/1 c/2/) and /(connected c/2 c/1/) foreac h trac k connecting t w o cities/. F urthermore/, the parameter domains can quic kly iden tifyunreac hable op erators and goals in some cases/. F or instance/, without the make/-oj op erator/,the computed domains sho w that the ld/-oj op erator is unreac hable/, and that a goal lik e/(and /(oj /?oj/) /(at /?oj Bath/)/) /(getting some orange juice to Bath/) is unattainable /(theparameter domain for /?oj will b e empt y/)/.Of course/, running the planner itself can also b e used for debugging a formalization/, butplanning is in general far more time/-consuming than our form of prepro cessing /(esp eciallyif the goal w e p ose happ ens to b e unac hiev able in the formalization/!/)/, and the trace of/1/3/0A ccelera ting P ar tial/-Order Plannersan anomalous planning attempt can b e quite hard to in terpret/, compared to a listing ofparameter domains/, obtained in a fraction of a second/./6/./3 F urther w orkFirst of all/, some additional exp erimen tation w ould b e of in terest/, to further assess andp erhaps re/\fne our searc h strategies/. Some of this exp erimen tation migh t fo cus on threat/-handling strategies/, including the b est general form of an atten uated UC/-term in planselection/, and the b est w a y to com bine threat selection with op en condition selection/. Thepreference for de/\fnite threats o v er op en conditions used b y ZLIF O do es app ear to b e ago o d default according to our exp erience/, but the TileW orld exp erimen ts indicated that are/-ordering of priorities b et w een threats and op en conditions is sometimes desirable/. Con/-cerning the c hoice of a UC/-related term for inclusion in the heuristic for plan selection/, w eshould men tion that w e ha v e brie/\ry tried using S/+OC/+UCd\n/, where UCd\nis the n um b er ofde/\fnite threats/, but did not obtain signi/\fcan t uniform impro v emen ts/.One promising direction for further dev elopmen t of our searc h strategy is to mak e thezero/-commitmen t strategy apply more often b y /\fnding w a ys of iden tifying false options asearly as p ossible/. That is/, if a p ossible action instance /(obtained b y matc hing an op encondition against a v ailable op erators as w ell as against existing actions/) is easily recogniz/-able as inconsisten t with the curren t plan/, then its elimination ma y lea v e us with a singleremaining matc h and hence an opp ortunit y to apply the zero/-commitmen t strategy /.One w a y of implemen ting this strategy w ould b e to c hec k at once/, b efore acceptinga matc hed action as a p ossible w a y to attain an op en condition/, whether the temp oralconstrain ts on that action force it to violate a causal link/, or alternativ ely /, force its causallink to b e violated/. In that case the action could immediately b e eliminated/, p erhapslea ving only one /(or ev en no/) alternativ e/. This could p erhaps b e made ev en more e/\u000bectiv eb y broadening the de/\fnition of threats so that preconditions as w ell as e/\u000bects of actionscan threaten causal links/, and hence bring to ligh t inconsistencies so oner/. Note that if aprecondition of an action is inconsisten t with a causal link/, it will ha v e to b e establishedwith another action whose e/\u000be cts violate the causal link/; so the precondition really p oses athreat from the outset/.Tw o p ossible extensions to our parameter domain tec hniques are /(i/) fully automatedhandling of univ ersally quan ti/\fed preconditions and e/\u000bects/, disjunctions and facts in theprepro cessing algorithm/; and /(ii/) more /\\in telligen t/\" calculation of domains/, b y applying aconstrain t propagation pro cess to the sets of ground predications that ha v e b een matc hed tothe preconditions of an op erator/; this can b e sho wn to yield tigh ter domains/, though at somecomputational exp ense/. Blum and F urst /(/1/9/9/5/) recen tly explored a similar idea/, but ratherthan computing parameter domains/, they directly stored sets of ground atoms that could b egenerated b y one op erator application /(starting in the initial state/)/, t w o successiv e op eratorapplications/, and so on/, and then used these sets of atoms /(and exclusivit y relations amongthe atoms and the actions connecting them/) to guide the regressiv e searc h for a plan/. Thealgorithm they describ e do es not allo w for conditional e/\u000bects/, though this generalizationapp ears en tirely p ossible/. F or the examples used in their tests/, they obtained dramaticsp eedups/./1/3/1Gerevini /& Schuber tFinally /, w e are also w orking on another prepro cessing tec hnique/, namely the inferenceof state constrain ts from op erator sp eci/\fcations/. One useful form of constrain t is impli/-cational /(e/.g/./, /(implies /(on /?x /?y/) /(not /(clear /?y/)/)/) /)/, and another is single/-v aluednessconditions /(e/.g/./, /(on /?x /?y/) ma y b e single/-v alued in b oth /?x and /?y /)/. W e conjecture thatsuc h constrain ts can b e tractably inferred and used for further large sp eedups in domain/-indep enden t/, w ell/-founded planning/.In view of the results w e ha v e presen ted and the p ossibiliti es for further sp eedups w eha v e men tioned/, w e think it plausible that w ell/-founded/, domain/-indep enden t planners ma yy et b ecome comp etitiv e with more pragmatically designed planners/.Ac kno wledgemen tsThis w ork amalgamates and extends t w o conference pap ers on impro ving searc h /(Sc h ub ert/& Gerevini/, /1/9/9/5/) and using computed parameter domains /(Gerevini /& Sc h ub ert/, /1/9/9/6/) toaccelerate partial/-order planners/. The researc h w as supp orted in part b y Rome Lab con/-tract F/3/0/6/0/2/-/9/1/-C/-/0/0/1/0 and NA TO Collab orativ e Researc h Gran t CR G/9/5/1/2/8/5/. Some of thew ork b y A G w as carried out at IRST/, /3/8/0/5/0 P o v o /(TN/)/, Italy /, and at the CS Departmen tof the Univ ersit y of Ro c hester/, Ro c hester NY USA/. The helpful commen ts and p erceptiv equestions of Marc F riedman/, Da vid Joslin/, Rao Kam bhampati/, Colm O/'Riain/, Martha P ol/-lac k/, Da vid Smith/, Dan W eld/, Mik e Williamson/, and of Asso ciate Editor Mic hael W ellmanand the anon ymous review ers are gratefully ac kno wledged/.App endix A /(Pro ofs of the Theorems/)Theorem /1 The find/-parameter/-domains algorithm is c orr e ct for c omputing p ar ameterdomains of ucpop /-style sets of op er ators /(without quanti/\fc ation/, disjunction/, or facts/)/,initial c onditions/, and /(p ossibly/) go al c onditions/.Pr o of/. As a preliminary observ ation/, the in tersected parameter domains computed it/-erativ ely b y the algorithm ev en tually stabilize/, since they gro w monotonically and thereare only /\fnitely man y constan ts that o ccur in the initial conditions and in op erator e/\u000bects/.Th us the algorithm terminates/.In order to pro v e correctness w e need to sho w that if there exists a v alid sequenceA/0\nA/1\n/:/:/:An\nof actions /(op erator instances/) starting with A/0\n/= /*start/* /, and if An\nis aninstance of the op erator Op /, then the bindings that the parameters of Op receiv ed in instanceAn\nare ev en tually added to the relev an t in tersected domains of Op /(where /\\relev an t/\" refersto the when/- clauses of Op whose preconditions are satis/\fed at the b eginning of An\n/)/. W epro v e this b y induction on n /.If n /= /0/, then An\n/= A/0\n/= /*start/* /, so there are no parameters and the claim is triviallytrue/.No w assume that the claim holds for n /= /1 /; /2 /; /:/:/:/; k /. Then consider an y op erator instanceAk /+/1\nthat can v alidly follo w A/0\nA/1\n/:/:/:Ak\n/, i/.e/./, suc h that Ak /+/1\nis an instance of an op eratorOp whose primary preconditions/, p ossibly along with some secondary ones/, are satis/\fed atthe end of A/0\nA/1\n/:/:/:Ak\n/. Let p b e suc h a precondition/, and write its instance in Ak /+/1\nas/(P c/1 c/2 /././) /. Then /(P c/1 c/2 /././) m ust b e an e/\u000bect of some Ai\n/, where /0 /\u0014 i /\u0014 k /. If i /= /0/1/3/2A ccelera ting P ar tial/-Order Plannersthen /(P c/1 c/2 /././) holds in the initial state/, and hence this predication is propagated andsuccessfully matc hed to p in the initial propagation phase of find/-parameter/-domains /. Ifi /> /0/, then Ai\nis an instance of some op erator Op/' and /(P c/1 c/2 /././) is the corresp ondinginstance of some e/\u000bect /(P t/1\nt/2\n/././) of Op/' /, where eac h tj\nis either a parameter of Op/' or isequal to cj /. Diagrammatically /,A/0\n/. /. /. Ai\n/. /. /. Ak\nAk /+/1j jOp/' Ope/\u000bect /(P t/1\nt/2\n/./. /) /BnZr /BnZr /BnZr /BnZr /! precond p/(P c/1 c/2 /./. /) /(P c/1 c/2 /./. /)By the induction assumption/, the bindings of the parameters in Ai\nare ev en tually addedto the relev an t in tersected domains of Op/' /. This also implies that the in tersected domainsof Op/' b ecome nonempt y /, and so the e/\u000bect /(P t/1\nt/2\n/././) is ev en tually propagated/, wherean y v ariables among the tj\nha v e the corresp onding constan t cj in the relev an t in tersecteddomain/. Consequen tly /, m uc h as in the case i /= /0/, e/\u000bect /(P t/1\nt/2\n/././) is successfully matc hedto precondition p of Op at some stage of the propagation/. Giv en these observ ations/, it isclear that for b oth i /= /0 and i /> /0/, p will b e mark ed /\\matc hed/\" in Op ev en tually /, andfurthermore an y parameters of Op that o ccur in p will ha v e the bindings resulting fromthe uni/\fcation with /(P c/1 c/2 /././) added to the appropriate individual domains asso ciatedwith p /.This argumen t applies to all preconditions of Op satis/\fed in its instance Ak /+/1\n/, in partic/-ular to all the primary preconditions/. Since these are all mark ed /\\matc hed/\"/, the algorithmwill compute in tersected domains for all Op /-parameters that o ccur in them/. In view of theindividual domain up dates just con/\frmed/, and since individual domains gro w monotoni/-cally /, these in tersected domains will ev en tually con tain the parameter bindings of Ak /+/1\n/.F or instance/, if a parameter /?x of Op o ccurs in a primary precondition and is b ound toc in Ak /+/1\n/, w e ha v e sho wn that c will ev en tually b e added to the in tersected domain of/?x asso ciated with the primary when/- clause of Op /. If a parameter do es not o ccur in theprimary preconditions of Op /, then its in tersected domain is set to T at the outset/, and thisimplicitly con tains whatev er binding the parameter has in Ak /+/1\n/.A v ery similar argumen t can b e made for an y secondary when/- clause of Op whose pre/-conditions are also satis/\fed for Ak /+/1\n/. Again/, all preconditions of the secondary clause/,as w ell as the primary preconditions/, will b e mark ed /\\matc hed/\"/, and so for an y parametero ccurring in these com bined preconditions/, its in tersected domain /(relativ e to the secondaryclause/) will b e up dated to include its binding in Ak /+/1\n/. F or parameters of Op not o ccurringin an y of these preconditions/, the in tersected domains will again b e set to T initially /, andthis implicitly con tains an y p ossible binding/. Finally /, w e note that since the in tersecteddomains relativ e to b oth primary and secondary when /-clauses gro w monotonically /, the aug/-men tations of in tersected domains w e ha v e just con/\frmed is p ermanen t/. /(In the case ofT/-domains/, these remain T/./)W e lea v e some additional details concerned with the ultimate use of EQ /-preconditions infind/-parameter/-domains to the reader/. /2/1/3/3Gerevini /& Schuber tTheorem /2 A lgorithm find/-parameter/-domains c an b e implemente d to run in O /( mnp\nne\n/( np\n/+ne\n/)/) time and O /( mnp\n/) sp ac e in the worst c ase/, wher e m is the numb er of c onstants in thepr oblem sp e ci/\fc ation/, np\nis the c ombine d numb er of pr e c onditions for al l op er ators /(andgo als/, if include d/)/, and ne\nis the c ombine d numb er of op er ator e/\u000be cts /(including those of/*start/* /)/.Pr o of/. The time complexit y of find/-parameter/-domains can b e determined as the sumof /(/1/) the cost of all the uni/\fcations p erformed/, /(/2/) the costs of all the individual domainup dates attempted/, and /(/3/) the cost of all the in tersected domain up dates attempted/. W eestimate an upp er b ound for eac h of these terms under the follo wing assumptions/:/(a/) the uni/\fcation of an y op erator e/\u000bect with an y op erator precondition requires constan ttime/;/(b/) there is a /\fxed upp er b ound on the n um b er of argumen ts that a predicate /(in aprecondition or e/\u000bect/) can ha v e/. It follo ws that O /( ne\n/) is an upp er b ound on the totaln um b er of in tersected domains/;\n/2/1/(c/) individual domains and in tersected domains are stored in hash tables /(indexed b y theconstan ts in the domain/)/. So/, w e can c hec k whether an elemen t b elongs to a particular/(individual or in tersected/) domain/, and p ossibly add it to that domain essen tially inconstan t time/. F urthermore for eac h individual and in tersected domain/, appropriatedata structures are used to k eep trac k of the /(p ossibly empt y/) set of new elemen tsthat ha v e b een added to the domain in the last up date attempt/./(/1/) F or an y particular in tersected domain of an y particular op erator/, there can b e atmost m up dates of this domain/. Eac h suc h up date causes all of the e/\u000bects of the when/-clause to whic h the in tersected domain b elongs to b e propagated/. An upp er b ound on thisn um b er is ne\n/. Eac h propagated e/\u000bect ma y then b e uni/\fed with O /( np\n/) preconditions/. Th usthe O /( m /) up dates of an in tersected domain ma y cause O /( mne\nnp\n/) uni/\fcations/. Hence from/(b/)/, the o v erall n um b er of uni/\fcations caused b y the propagation of in tersected domainsto individual domains is O /( mn\n/2e\nnp\n/)/. T o these uni/\fcations w e ha v e to add those whic h areinitially p erformed b et w een the e/\u000bects of /*start/* and the preconditions of the op erators/.There are O /( mnp\n/) suc h uni/\fcations/, and so they do not increase the previous upp er b oundon the n um b er of uni/\fcations/. Th us/, from /(a/)/, the cost of all of the uni/\fcations p erformedb y the algorithm is O /( mn\n/2e\nnp\n/)/./(/2/) Eac h uni/\fcation is p oten tially follo w ed b y an attempt to up date the individualdomain/(s/) of the relev an t parameter/(s/)/. Ho w ev er/, with assumption /(c/) the n um b er of suc hattempts is limited to those where the set of new elemen ts in the in tersected domain/(s/)of the unifying e/\u000bect is /(are/) not empt y /. F urthermore/, when w e attempt to up date anindividual domain DI\nb y p erforming the union of a relev an t in tersected domain Di\nand DI\n/,only the subset of the new elemen ts of Di\nneed to b e added to DI\n/(if they are not alreadythere/)/. Th us/, since an y in tersected domain gro ws monotonically /, from /(b/) and /(c/) w e ha v ethat the o v erall cost of all the up date attempts for one particular individual domain caused/2/1/. Note that if a parameter app ears in a precondition of a when /-clause/, but in none of its e/\u000bects/, then thein tersected domain of the parameter will not b e propagated b y the algorithm/. Hence in implemen tingthe algorithm w e can ignore suc h parameters/./1/3/4A ccelera ting P ar tial/-Order Plannersb y one particular e/\u000bect is O /( m /)/. But in the w orst case one e/\u000bect can unify with all theO /( np\n/) preconditions of all the op erators/, yielding an o v erall b ound on all of the attemptsto up date the individual domains of O /( mne\nnp\n/)/./(/3/) There can b e an attempt to up date a particular in tersected domain for eac h relev an tindividual domain up date/, and eac h relev an t individual domain can b e up dated O /( m /) times/(b ecause the domains gro w monotonically/)/. Therefore from /(b/) there are at most O /( mnp\n/)attempts to up date one in tersected domain/. By /(c/) the total cost of these attempts isO /( mn\n/2p\n/)/, b ecause c hec king whether a new elemen t of an individual domain b elongs to allthe other O /( np\n/) relev an t individual domains tak es O /( np\n/) time/. So/, since from /(b/) thereare no more than O /( ne\n/) in tersected domains/, the total cost incurred b y the algorithm forup dating all of the in tersected domains is O /( mne\nn\n/2p\n/)/.It follo ws that the time complexit y of find/-parameter/-domains is/:O /( mn\n/2e\nnp\n/) /+ O /( mne\nnp\n/) /+ O /( mne\nn\n/2p\n/) /= O /( mnp\nne\n/( np\n/+ ne\n/)/)/.The space complexit y b ound is easily deriv ed from /(b/)/, and from the fact that for eac hwhen /-clause at most O /( m /) constan ts are stored/. /2"}
{"category": "abstract", "text": "W e prop ose some domain/-indep enden t tec hniques for bringing w ell/-founded partial/-order planners closer to practicalit y /. The /\frst t w o tec hniques are aimed at impro vingsearc h con trol while k eeping o v erhead costs lo w/. One is based on a simple adjustmen t tothe default A/* heuristic used b y ucpop to select plans for re/\fnemen t/. The other is basedon preferring /\\zero commitm en t/\" /(forced/) plan re/\fnemen ts whenev er p ossible/, and usingLIF O prioritization otherwise/. A more radical tec hnique is the use of op erator parameterdomains to prune searc h/. These domains are initially computed from the de/\fnitions ofthe op erators and the initial and goal conditions/, using a p olynomial/- tim e algorithm thatpropagates sets of constan ts through the op erator graph/, starting in the initial conditions/.During planning/, parameter domains can b e used to prune non viable op erator instances andto remo v e spurious clobb ering threats/. In exp erimen ts based on mo di/\fcations of ucpop /,our impro v ed plan and goal selection strategies ga v e sp eedups b y factors ranging from /5to more than /1/0/0/0 for a v ariet y of problems that are non trivial for the unmo di/\fed v ersion/.Crucially /, the hardest problems ga v e the greatest impro v emen ts/. The pruning tec hniquebased on parameter domains often ga v e sp eedups b y an order of magnitude or more fordi/\u000ecult problems/, b oth with the default ucpop searc h strategy and with our impro v edstrategy /. The Lisp co de for our tec hniques and for the test problems is pro vided in on/-lineapp endices/./1/. In tro ductionW e are concerned here with impro ving the p erformance of /\\w ell/-founded/\" domain/-indep end/-en t planners /{ planners that p ermit pro ofs of soundness/, completeness/, or other desirabletheoretical prop erties/. A state/-of/-the/-art example of suc h a planner is ucpop /(Barrettet al/./, /1/9/9/4/; P en b erth y /& W eld/, /1/9/9/2/)/, whose in tellectual ancestry includes strips /(Fik es /&Nilsson/, /1/9/7/1/)/, tweak /(Chapman/, /1/9/8/7/)/, and snlp /(McAllester /& Rosen blitt/, /1/9/9/1/)/. Suc hplanners unfortunately do not p erform w ell at presen t/, in comparison with more practicallyorien ted planners suc h as sipe /(Wilkins/, /1/9/8/8/)/, prs /(George/\u000b /& Lansky /, /1/9/8/7/)/, or O/-Plan/(Currie /& T ate/, /1/9/9/1/)/.Ho w ev er/, there app ear to b e ample opp ortunities for bringing w ell/-founded plannerscloser to practicalit y /. In the follo wing/, w e b egin b y suggesting some impro v emen ts tosearc h con trol in planning/, based on more carefully form ulated strategies for selecting partialplans for re/\fnemen t/, and for c ho osing op en conditions in a selected partial plan/. Our plan/-c/\r /1/9/9/6 AI Access F oundation and Morgan Kaufmann Publishers/. All righ ts reserv ed/.Gerevini /& Schuber tselection strategy uses S/+OC /{ the n um b er of steps in a plan plus the n um b er of op enconditions still to b e established /{ as a heuristic measure for ucpop /'s A/* searc h of theplan space/. /(Addition of an atten uated term re/\recting the n um b er of threats or /\\unsafeconditions/\" UC/, suc h as /0/./1UC/, is sometimes adv an tageous/./)\n/1Our /\ra w/-selection strategy /,whic h w e term ZLIF O/, prefers /\\zero commitmen t/\" plan re/\fnemen ts to others/, and otherwiseuses a LIF O /(stac k/) discipline/. Zero commitmen t re/\fnemen ts are logically necessary ones/"}
{"category": "non-abstract", "text": "/\u000f Discourse/\\ /: /: /: w e migh t ha v e the concept of say a researc her who has w ork ed for /\ffteen y earson a certain pro ject /: /: /: /\"/\\ F urther /, and this is crucial in AI and probably for exp ert databases as w ell /: /: /: /\"/\u000f Sen ten tial/\\ /: /: /: let me just say that it b ears a strong resem blance to m uc h of the w ork that/'sdone in seman tic nets and ev en frames/./\"/\\ /: /: /: from a place that is ev en stranger and further a w a y /: /: /: /\"F or example/, when used in the discourse sense/, the cue phrase /\\sa y/\" con v eys the structuralinformation that an example is b eginning/. When used in the sen ten tial sense/, /\\sa y/\" do esnot con v ey an y structural information and instead functions as a v erb/.c/\r /1/9/9/6 AI Access F oundation and Morgan Kaufmann Publishers/. All righ ts reserv ed/.LitmanThe abilit y to correctly classify cue phrases as discourse or sen ten tial is critical fornatural language pro cessing systems that need to recognize or con v ey discourse structure/,for tasks suc h as impro ving anaphora resolution /(Grosz /& Sidner/, /1/9/8/6/; Reic hman/, /1/9/8/5/)/.Consider the follo wing example/, again tak en from the corpus that will b e describ ed inSection /2\n/1/:If the system attempts to hold rules/, say as an exp ert database for an exp ert system /,then w e exp ect it not only to hold the rules but to in fact apply them for us inappropriate situations/.In this example/, the cue phrases /\\sa y/\" and /\\then/\" are discourse usages/, and explicitlysignal the b oundaries of an in terv ening subtopic in the discourse structure/. F urthermore/,the referen ts of the noun phrases /\\the system/,/\" /\\an exp ert database/,/\" and /\\an exp ertsystem/\" are all p ossible referen ts for the pronoun /\\it/./\" With the structural informationcon v ey ed b y the cue phrases/, the system can determine that /\\the system/\" is more relev an tfor in terpreting the pronoun /\\it/,/\" as b oth /\\an exp ert database/\" and /\\an exp ert system/\"o ccur within the em b edded /(and no w concluded/) subtopic/. Without the cue phrases/, thereasoning required to determine that the referen t of the /\\the system/\" is the in tended referen tof /\\it/\" w ould b e m uc h more complex/.Correctly classifying cue phrases as discourse or sen ten tial is imp ortan t for other naturallanguage pro cessing tasks as w ell/. The discourse//sen ten tial distinction can b e used toimpro v e the naturalness of syn thetic sp eec h in text/-to/-sp eec h systems /(Hirsc h b erg/, /1/9/9/0/)/.T ext/-to/-sp eec h systems generate syn thesized sp eec h from unrestricted text/. If a cue phrasecan b e classi/\fed as discourse or sen ten tial using features of the input text/, it can then b esyn thesized using di/\u000beren t in tonational mo dels for the discourse and sen ten tial usages/. Inaddition/, b y explicitly iden tifying rhetorical and other relationships/, discourse usages of cuephrases can b e used to impro v e the coherence of m ultisen ten tial texts in natural languagegeneration systems /(Zuc k erman /& P earl/, /1/9/8/6/; Moser /& Mo ore/, /1/9/9/5/)/. Cue phrases canalso b e used to reduce the complexit y of discourse pro cessing in suc h areas as argumen tunderstanding /(Cohen/, /1/9/8/4/) and plan recognition /(Litman /& Allen/, /1/9/8/7/; Grosz /& Sidner/,/1/9/8/6/)/.While the problem of cue phrase classi/\fcation has often b een noted /(Grosz /& Sidner/,/1/9/8/6/)/, un til recen tly /, mo dels for classifying cue phrases w ere neither dev elop ed nor ev aluatedbased on careful empirical analyses/. Ev en though the literature suggests that some featuresmigh t b e useful for cue phrase classi/\fcation/, there are no quan titativ e analyses of an y actualclassi/\fcation algorithms that use suc h features /(nor an y suggestions as to ho w di/\u000beren t t yp esof features migh t b e com bined/)/. Most systems that recognize or generate cue phrases simplyassume that discourse uses are utterance or clause initial /(Reic hman/, /1/9/8/5/; Zuc k erman /&P earl/, /1/9/8/6/)/. While there are empirical studies sho wing that the in tonational prominenceof certain w ord classes v aries with resp ect to discourse function /(Hallida y /& Hassan/, /1/9/7/6/;Alten b erg/, /1/9/8/7/)/, these studies do not in v estigate cue phrases p er se/.T o address these limitations/, Hirsc h b erg and Litman /(/1/9/9/3/) conducted sev eral empiricalstudies sp eci/\fcally addressing cue phrase classi/\fcation in text and sp eec h/. Hirsc h b erg andLitman pre/-classi/\fed a set of naturally o ccurring cue phrases/, describ ed eac h cue phrase interms of proso dic and textual features /(the features w ere p osited in the literature or easy/1/. This example is also describ ed in more detail b y Hirsc h b erg and Litman /(/1/9/9/3/)/./5/4Cue Phrase Classifica tion Using Ma chine Learningto automatically co de/)/, then man ually examined the data to construct classi/\fcation mo delsthat b est predicted the classi/\fcations from the feature v alues/.This pap er examines the utilit y of mac hine learning for automating the constructionof mo dels for classifying cue phrases from suc h empirical data/. A set of exp erimen ts aredescrib ed that use t w o mac hine learning programs/, cgrendel /(Cohen/, /1/9/9/2/, /1/9/9/3/) andC/4/./5 /(Quinlan/, /1/9/9/3/)/, to induce classi/\fcation mo dels from sets of pre/-classi/\fed cue phrasesand their features/. The features/, classes and training examples used in the studies ofHirsc h b erg and Litman /(/1/9/9/3/)/, as w ell as additional features/, classes and training exam/-ples/, are giv en as input to the mac hine learning programs/. The results are ev aluated b othquan titativ ely and qualitativ ely /, b y comparing b oth the error rates and the con ten t of theman ually deriv ed and learned classi/\fcation mo dels/. The exp erimen tal results sho w thatmac hine learning is indeed an e/\u000bectiv e tec hnique/, not only for automating the generationof classi/\fcation mo dels/, but also for impro ving up on previous results/. The accuracy ofthe learned classi/\fcation mo dels is often higher than the accuracy of the man ually deriv edmo dels/, and the learned mo dels often con tain new linguistic implications/. The learningparadigm also mak es it easier to compare the utilit y of di/\u000beren t kno wledge sources/, and toup date the mo del giv en new features/, classes/, or training data/.The next section summarizes previous w ork on cue phrase classi/\fcation/. Section /3then describ es the mac hine learning approac h to cue phrase classi/\fcation that is tak en inthis pap er/. In particular/, the section describ es four sets of exp erimen ts that use mac hinelearning to automatically induce cue phrase classi/\fcation mo dels/. The t yp es of inputs andoutputs of the mac hine learning programs are presen ted/, as are the metho dologies that areused to ev aluate the results/. Section /4 presen ts and discusses the exp erimen tal results/, andhighligh ts the man y b ene/\fts of the mac hine learning approac h/. Section /5 discusses thepractical utilit y of the results of this pap er/. Finally /, Section /6 discusses the use of mac hinelearning in other studies of discourse/, while Section /7 concludes/./2/. Previous W ork on Classifying Cue PhrasesThis section summarizes Hirsc h b erg/'s and Litman/'s empirical studies of the classi/\fcation ofcue phrases in sp eec h and text /(Hirsc h b erg /& Litman/, /1/9/8/7/, /1/9/9/3/; Litman /& Hirsc h b erg/,/1/9/9/0/)/. Hirsc h b erg/'s and Litman/'s data /(cue phrases tak en from corp ora of recorded andtranscrib ed sp eec h/, classi/\fed as disc ourse or sentential /, and co ded using b oth sp eec h/-basedand text/-based features/) will b e used to create the input for the mac hine learning exp eri/-men ts/. Hirsc h b erg/'s and Litman/'s results /(p erformance /\fgures for man ually dev elop ed cuephrase classi/\fcation mo dels/) will b e used as a b enc hmark for ev aluating the p erformanceof the classi/\fcation mo dels pro duced b y mac hine learning/.The /\frst study b y Hirsc h b erg and Litman in v estigated usage of the cue phrase /\\no w/\"b y m ultiple sp eak ers in a radio call/-in sho w /(Hirsc h b erg /& Litman/, /1/9/8/7/)/. A classi/\fcationmo del based on proso dic features w as dev elop ed based on man ual analysis of a /\\training/\"set of /4/8 examples of /\\no w/\"/, then ev aluated on a previously unseen test set of /5/2 examplesof /\\no w/\"/. In a follo w/-up study /(Hirsc h b erg /& Litman/, /1/9/9/3/)/, Hirsc h b erg and Litman testedthis classi/\fcation mo del on a larger set of cue phrases/, namely all single w ord cue phrasesin a tec hnical k eynote address b y a single sp eak er/. This corpus yielded /9/5/3 instances of /3/4/5/5LitmanProso dic Mo del /:if comp osition of in termediat e phrase /= alone then disc ourse /(/1/)elseif comp osition of in termediate phrase /= /: alone then /(/2/)if p osition in in termedia te phrase /= /\frst then /(/3/)if accen t /= deaccen ted then disc ourse /(/4/)elseif accen t /= L/* then disc ourse /(/5/)elseif accen t /= H/* then sentential /(/6/)elseif accen t /= complex then sentential /(/7/)elseif p osition in in termediate phrase /= /: /\frst then sentential /(/8/)T extual Mo del /:if preceding orthograph y /= true then disc ourse /(/9/)elseif preceding orthograph y /= false then sentential /(/1/0/)Figure /1/: Decision tree represen tation of the man ually deriv ed classi/\fcation mo dels ofHirsc h b erg and Litman/.di/\u000beren t single w ord cue phrases deriv ed from the literature/.\n/2Hirsc h b erg and Litman alsoused the cue phrases in the /\frst /1/7 min utes of this corpus to dev elop a complemen tary cuephrase classi/\fcation mo del based on textual features /(Litman /& Hirsc h b erg/, /1/9/9/0/)/, whic hthey then tested on the full corpus /(Hirsc h b erg /& Litman/, /1/9/9/3/)/. The /\frst study will b ereferred to as the /\\no w/\" study /, and the follo w/-up study as the /\\m ultiple cue phrase/\" study /.Note that the term /\\m ultiple/\" means that /3/4 di/\u000beren t single w ord cue phrases /(as opp osedto just the cue phrase /\\no w/\"/) are considered/, not that cue phrases consisting of m ultiplew ords /(e/.g/. /\\b y the w a y/\"/) are considered/.The metho d that Hirsc h b erg and Litman used to dev elop their proso dic and textual clas/-si/\fcation mo dels w as as follo ws/. They /\frst separately classi/\fed eac h example cue phrase inthe data as disc ourse /, sentential or ambiguous while listening to a recording and reading atranscription/.\n/3Eac h example w as also describ ed as a set of proso dic and textual features/.\n/4Previous observ ations in the literature correlating discourse structure with proso dic infor/-mation/, and discourse usages of cue phrases with initial p osition in a clause/, con tributed tothe c hoice of features/. The set of classi/\fed and describ ed examples w as then examined inorder to man ually dev elop the classi/\fcation mo dels sho wn in Figure /1/. These mo dels aresho wn here using decision trees for ease of comparison with the results of C/4/./5 and will b eexplained b elo w/.Pr oso dy w as describ ed using Pierreh um b ert/'s theory of English in tonation /(Pierreh um/-b ert/, /1/9/8/0/)/. In Pierreh um b ert/'s theory /, in tonational con tours are describ ed as sequencesof lo w /(L/) and high /(H/) tones in the fundamental fr e quency /(F/0/) c ontour /(the ph ysical/2/. Figure /2 con tains a list of the /3/4 cue phrases/. Hirsc h b erg and Litman /(/1/9/9/3/) pro vide full details regardingthe distribution of these cue phrases/. The most frequen t cue phrase is /\\and/\"/, whic h o ccurs /3/2/0 times/.The next most frequen t cue phrase is /\\no w/\"/, whic h o ccurs /6/9 times/. /\\But/,/\" /\\lik e/,/\" /\\or/\" and /\\so/\" alsoeac h o ccur more than /\fft y times/. The four least frequen t cue phrases /{ /\\essen tially /,/\" /\\otherwise/,/\" /\\since/\"and /\\therefore/\" /{ eac h o ccur /2 times/./3/. The class ambiguous w as not in tro duced un til the m ultiple cue phrase study /(Hirsc h b erg /& Litman/, /1/9/9/3/;Litman /& Hirsc h b erg/, /1/9/9/0/)/./4/. Although a limited set of textual features w ere noted in the /\\no w/\" data/, the analysis of the /\\no w/\" datadid not yield a textual classi/\fcation mo del/./5/6Cue Phrase Classifica tion Using Ma chine Learningcorrelate of pitc h/)/. In tonational con tours ha v e as their domain the in tonational phrase/.A /\fnite/-state grammar describ es the set of tonal sequences for an in tonational phrase/. Aw ell/-formed intonational phr ase consists of one or more in termediate phrases follo w ed b y ab oundary tone/. A w ell/-formed interme diate phr ase has one or more pitc h accen ts follo w edb y a phrase accen t/. Boundary tones and phr ase ac c ents eac h consist of a single tone/, whilepitch ac c ents consist of either a single tone or a pair of tones/. There are t w o simple pitc haccen ts /(H/* and L/*/) and four c omplex accen ts /(L/*/+H/, L/+H/*/, H/*/+L/, and H/+L/*/)/. The/* indicates whic h tone is aligned with the stressed syllable of the asso ciated lexical item/.Note that not ev ery stressed syllable is accen ted/. Lexical items that b ear pitc h accen ts arecalled ac c ente d /, while those that do not are called de ac c ente d /.Proso dy w as man ually transcrib ed b y Hirsc h b erg b y examining the fundamen tal fre/-quency /(F/0/) con tour/, and b y listening to the recording/. This transcription pro cess w asp erformed separately from the pro cess of discourse//sen ten tial classi/\fcation/. T o pro duce theF/0 con tour/, the recording of the corpus w as digitized and pitc h/-trac k ed using sp eec h analy/-sis soft w are/. This resulted in a displa y of the F/0 where the x/-axis represen ted time and they/-axis represen ted frequency in Hz/. V arious phrase /\fnal c haracteristics /(e/.g/./, phrase accen ts/,b oundary tones/, as w ell as pauses and syllable lengthening/) help ed to iden tify in termediateand in tonational phrases/, while p eaks or v alleys in the displa y of the F/0 con tour help ed toiden tify pitc h accen ts/. Similar man ual transcriptions of proso dic phrasing and accen t ha v eb een sho wn to b e reliable across co ders /(Pitrelli/, Bec kman/, /& Hirsc h b erg/, /1/9/9/4/)/.Once proso dy w as co ded/, Hirsc h b erg and Litman represen ted ev ery cue phrase in termsof the follo wing proso dic features/.\n/5A c c ent corresp onded to the pitc h accen t /(if an y/) thatw as asso ciated with the cue phrase/. F or b oth the in tonational and in termediate phrasescon taining eac h cue phrase/, the feature c omp osition of phr ase represen ted whether or notthe cue phrase w as alone in the phrase /(the phrase con tained only the cue phrase/, or onlythe cue phrase and other cue phrases/)/. Position in phr ase represen ted whether the cuephrase w as /\frst /(the /\frst lexical item in the proso dic phrase unit /{ p ossibly preceded b yother cue phrases/) or not/.The textual fe atur es used in the m ultiple cue phrase study /(Hirsc h b erg /& Litman/, /1/9/9/3/;Litman /& Hirsc h b erg/, /1/9/9/0/) w ere extracted automatically from the transcript/. The p art ofsp e e ch of eac h cue phrase w as obtained b y running a program for tagging w ords with one ofappro ximately /8/0 parts of sp eec h /(Ch urc h/, /1/9/8/8/) on the transcript/.\n/6Sev eral c haracteristicsof the cue phrase/'s immediate con text w ere also noted/, in particular/, whether it w as im/-mediately preceded or succeeded b y ortho gr aphy /(punctuation or a paragraph b oundary/)/,and whether it w as immediately preceded or succeeded b y a lexical item corresp onding toanother cue phr ase /.With this bac kground/, the classi/\fcation mo dels sho wn in Figure /1 can no w b e explained/.The proso dic mo del uniquely classi/\fes an y cue phrase using the features c omp osition ofinterme diate phr ase /, p osition in interme diate phr ase /, and ac c ent /. When a cue phrase isuttered as a single in termediate phrase /{ p ossibly with other cue phrases /(i/.e/./, line /(/1/) inFigure /1/)/, or in a larger in termediate phrase with an initial p osition /(p ossibly preceded b y/5/. Only the features used in Figure /1 are discussed here/./6/. Another syn tactic feature /- dominating constituen t /- w as obtained b y running the parser Fidditc h /(Hindle/,/1/9/8/9/) on the transcript/. Ho w ev er/, since this feature did not app ear in an y mo dels man ually deriv ed fromthe training data /(Litman /& Hirsc h b erg/, /1/9/9/0/)/, the feature w as not pursued/./5/7LitmanMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)Proso dic /2/4/./6 /\u0006 /3/./0 /1/4/./7 /\u0006 /3/./2T extual /1/9/./9 /\u0006 /2/./8 /1/6/./1 /\u0006 /3/./4Default Class /3/8/./8 /\u0006 /3/./2 /4/0/./8 /\u0006 /4/./4T able /1/: /9/5/% con/\fdence in terv als for the error rates /(/%/) of the man ually deriv ed classi/\fca/-tion mo dels of Hirsc h b erg and Litman/, testing data /(m ultiple cue phrase corpus/)/.other cue phrases/) and a L/* accen t or deaccen ted/, it is classi/\fed as disc ourse /. When part ofa larger in termediate phrase and either in initial p osition with a H/* or complex accen t/, orin a non/-initial p osition/, it is sentential /. The textual mo del classi/\fes cue phrases using onlythe single feature pr e c e ding ortho gr aphy /.\n/7When a cue phrase is preceded b y an y t yp e oforthograph y /, it is classi/\fed as disc ourse /; otherwise/, the cue phrase is classi/\fed as sentential /.When the proso dic mo del w as used to classify eac h cue phrase in its training data/, i/.e/./,the /1/0/0 examples of /\\no w/\" from whic h the mo del w as dev elop ed/, the error rate w as /2/./0/%/.\n/8The error rate of the textual mo del on the training examples from the m ultiple cue phrasecorpus w as /1/0/./6/% /(Litman /& Hirsc h b erg/, /1/9/9/0/)/.The proso dic and textual mo dels w ere ev aluated b y quan tifying their p erformance incorrectly classifying example cue phrases in t w o test sets of data/, as sho wn in the ro wslab eled /\\Proso dic/\" and /\\T extual/\" in T able /1/. Eac h test set is a subset of the /9/5/3 examplesfrom the m ultiple cue phrase corpus/. The /\frst test set /(/8/7/8 examples/) consists of only theclassi/\fable cue phr ases /, i/.e/./, the cue phrases that b oth Hirsc h b erg and Litman classi/\fed asdisc ourse or that b oth classi/\fed as sentential /. Note that those cue phrases that Hirsc h b ergand Litman classi/\fed as ambiguous or that they w ere unable to agree up on are not includedin the classi/\fable subset/. /(These cue phrases will b e considered in the learning exp erimen tsdescrib ed in Section /4/./4/, ho w ev er/./) The second test set/, the classi/\fable non/-c onjuncts/(/4/9/5 examples/)/, w as created from the classi/\fable cue phrases b y remo ving all instances of/\\and/\"/, /\\or/\" and /\\but/\"/. This subset w as considered particularly reliable since /9/7/./2/% of non/-conjuncts w ere classi/\fable compared to /9/2/./1/% of all example cue phrases/. The error rate ofthe proso dic mo del w as /2/4/./6/% for the classi/\fable cue phrases and /1/4/./7/% for the classi/\fablenon/-conjuncts /(Hirsc h b erg /& Litman/, /1/9/9/3/)/. The error rate of the textual mo del w as /1/9/./9/%for the classi/\fable cue phrases and /1/6/./1/% for the classi/\fable non/-conjuncts /(Hirsc h b erg /&Litman/, /1/9/9/3/)/. The last ro w of the table sho ws error rates for a simple /\\Default Class/\"baseline mo del that alw a ys predicts the most frequen t class in the corpus /( sentential /)/. Theserates are /3/8/./8/% for the classi/\fable cue phrases and /4/0/./8/% for the classi/\fable non/-conjuncts/./7/. A classi/\fcation mo del based on part/-of/-sp eec h w as also dev elop ed /(Litman /& Hirsc h b erg/, /1/9/9/0/;Hirsc h b erg /& Litman/, /1/9/9/3/)/; ho w ev er/, it did not p erform as w ell as the mo del based on orthograph y/(the error rate of the part/-of/-sp eec h mo del w as /3/6/./1/% in the larger test set/, as opp osed to /1/9/./9/% for theorthographic mo del/)/. F urthermore/, a mo del that com bined orthograph y and part/-of/-sp eec h p erformedcomparably to the simpler orthographic mo del /(Hirsc h b erg /& Litman/, /1/9/9/3/)/. Hirsc h b erg and Litmanalso had preliminary observ ations suggesting that adjacency of cue phrases migh t pro v e useful/./8/. F ollo wing Hirsc h b erg and Litman /(/1/9/9/3/)/, the original /4/8/- and /5/2/-example sets /(Hirsc h b erg /& Litman/,/1/9/8/7/) are com bined/./5/8Cue Phrase Classifica tion Using Ma chine LearningAlthough not computed b y Hirsc h b erg and Litman/, T able /1 also asso ciates margins of er/-rors with eac h error p ercen tage/, whic h are used to compute con/\fdence in terv als /(F reedman/,Pisani/, /& Purv es/, /1/9/7/8/)/. /(The margin of error is /\u0006 /2 standard errors for a /9/5/% con/\fdencein terv al using a normal table/./) The lo w er b ound of a con/\fdence in terv al is computed b ysubtracting the margin of error from the error rate/, while the upp er b ound is computed b yadding the margin of error/. Th us/, the /9/5/% con/\fdence in terv al for the proso dic mo del onthe classi/\fable cue phrase test set is /(/2/1/./6/%/, /2/7/./6/%/)/. Analysis of the con/\fdence in terv alsindicates that the impro v emen t of b oth the proso dic and textual mo dels o v er the defaultmo del is signi/\fcan t/. F or example/, the upp er b ounds of the error rates of the proso dic andtextual mo dels on the classi/\fable cue phrase test set /- /2/7/./6/% and /2/2/./7/% /- are b oth lo w erthan the lo w er b ound of the default class error rate /- /3/5/./6/%/. This metho dology of using sta/-tistical inference to determine whether di/\u000berences in error rates are signi/\fcan t is discussedmore fully in Section /3/./3/./3/. Exp erime n ts using Mac hine LearningThis section describ es exp erimen ts that use the mac hine learning programs C/4/./5 /(Quinlan/,/1/9/9/3/) and cgrendel /(Cohen/, /1/9/9/2/, /1/9/9/3/) to automatically induce cue phrase classi/\fcationmo dels/. cgrendel and C/4/./5 are similar to eac h other and to other learning metho ds suc has neural net w orks and car t /(Brieman/, F riedman/, Olshen/, /& Stone/, /1/9/8/4/) in that all induceclassi/\fcation mo dels from preclassi/\fed examples/. Eac h program tak es the follo wing inputs/:names of the classes to b e learned/, names and p ossible v alues of a /\fxed set of features/, andthe training data /(i/.e/./, a set of examples for whic h the class and feature v alues are sp eci/\fed/)/.The output of eac h program is a classi/\fcation mo del/, expressed in C/4/./5 as a decision treeand in cgrendel as an ordered set of if/-then rules/. Both cgrendel and C/4/./5 learn theclassi/\fcation mo dels using greedy searc h guided b y an /\\information gain/\" metric/.The /\frst group of mac hine learning exp erimen ts replicate the training and testing condi/-tions used b y Hirsc h b erg and Litman /(/1/9/9/3/) /(review ed in the previous section/)/, to supp orta direct comparison of the man ual and mac hine learning approac hes/. The second group ofexp erimen ts ev aluate the utilit y of training from larger amoun ts of data than w as feasiblefor the man ual analysis of Hirsc h b erg and Litman/. The third set of exp erimen ts allo w themac hine learning algorithms to distinguish among the /3/4 cue phrases/, to ev aluate the util/-it y of dev eloping classi/\fcation mo dels sp ecialized for particular cue phrases/. The fourth setof exp erimen ts consider all the examples in the m ultiple cue phrase corpus/, not just theclassi/\fable cue phrases/. This set of exp erimen ts attempt to predict a third classi/\fcationunknown /, as w ell as the classi/\fcations disc ourse and sentential /. Finally /, within eac h of thesefour sets of exp erimen ts/, eac h individual exp erimen t learns a classi/\fcation mo del using adi/\u000beren t feature represen tation of the training data/. Some exp erimen ts consider features inisolation/, to comparativ ely ev aluate the utilit y of eac h individual feature for classi/\fcation/.Other exp erimen ts consider linguisticall y motiv ated sets of features/, to gain insigh t in tofeature in teractions/./3/./1 The Mac hine Learning InputsThis section describ es the inputs to b oth of the mac hine learning programs/, namely /, thenames of the classi/\fcations to b e learned/, the names and p ossible v alues of a /\fxed set of/5/9LitmanT otal Classi/\fable Cue PhrasesClassi/\fcation Discourse Sen ten tial Unkno wnJudge/1//Judge /2 D//D S//S /?///? D//S S//D D///? S///? /?//D /?//SAll Cue Phrases /9/5/3 /3/4/1 /5/3/7 /5/9 /5 /0 /0 /0 /5 /6Non/-Conjuncts /5/0/9 /2/0/2 /2/9/3 /1/1 /1 /0 /0 /0 /0 /2T able /2/: Determining the classi/\fcation of cue phrases/.features/, and training data sp ecifying the class and feature v alues for eac h example in thetraining set/./3/./1/./1 Classifica tionsThe /\frst input to eac h learning program sp eci/\fes the names of a /\fxed set of classi/\fc ations /.Hirsc h b erg and Litman/'s /3/-w a y classi/\fcation of cue phrases b y /2 judges /(Hirsc h b erg /&Litman/, /1/9/9/3/) is transformed in to the classi/\fcations used b y the mac hine learning programsas sho wn in T able /2/. Recall from Section /2 that eac h judge classi/\fed eac h cue phrase asdisc ourse /, sentential /, or ambiguous /; these classi/\fcations are sho wn as D/, S/, and /? in T able /2/.As discussed in Section /2/, the classi/\fable cue phr ases are those cue phrases that the judgesb oth classi/\fed as either discourse or as sen ten tial usages/. Th us/, in the mac hine learningexp erimen ts/, a cue phrase is assigned the classi/\fcation disc ourse if b oth judges classi/\fed itas discourse /(D//D/, as sho wn in column /3 of T able /2/)/. Similarly /, a cue phrase is assigned theclassi/\fcation sentential if b oth judges classi/\fed it as sen ten tial /(S//S/, as sho wn in column/4/)/. /8/7/8 /(/9/2/./1/%/) of the /9/5/3 examples in the full corpus w ere classi/\fable/, while /4/9/5 /(/9/7/./2/%/)of the /5/0/9 non/-conjuncts w ere classi/\fable/.F or some of the mac hine learning exp erimen ts/, a third cue phrase classi/\fcation will alsob e considered/. In particular/, a cue phrase is assigned the classi/\fcation unknown if b othHirsc h b erg and Litman classi/\fed it as ambiguous /(/?///?/, as sho wn in column /5/)/, or if theyw ere unable to agree up on its classi/\fcation /(D//S/, S//D/, D///?/, S///?/, /?//D/, /?//S/, as sho wn incolumns /6/-/1/1/)/. In the full corpus/, /5/9 cue phrases /(/6/./2/%/) w ere judged am biguous b y b othjudges /(/?///?/)/. There w ere only /5 cases /(/./5/%/) of true disagreemen t /(D//S/)/. /1/1 cue phrases/(/1/./2/%/) w ere judged am biguous b y the /\frst judge but classi/\fed b y the second judge /(/?//Dand /?//S/)/. When the conjunctions /\\and/,/\" /\\or/\" and /\\but/\" w ere remo v ed from the corpus/,only /1/1 examples /(/2/./2/%/) w ere judged am biguous b y b oth judges/: /3 instances of /\\actually /,/\"/2 instances eac h of /\\b ecause/\" and /\\essen tially /,/\" and /1 instance of /\\generally /,/\" /\\indeed/,/\"/\\lik e/\" and /\\no w/./\" There w as only /1 case /(/./2/%/) of true disagreemen t /(an instance of /\\lik e/\"/)/./2 cue phrases /(/./4/%/) /- an instance eac h of /\\lik e/\" and /\\otherwise/\" /- w ere judged am biguousb y the /\frst judge/./3/./1/./2 Fea turesA second comp onen t of the input to eac h learning program sp eci/\fes the names and p oten tialv alues of a /\fxed set of fe atur es /. The set of primitiv e features considered in the learningexp erimen ts are sho wn in Figure /2/. F eature v alues can either b e a n umeric v alue or one of a/\fxed set of user/-de/\fned sym b olic v alues/. The feature represen tation sho wn here follo ws therepresen tation of Hirsc h b erg and Litman except as noted/. L ength of intonational phr ase /(P/-/6/0Cue Phrase Classifica tion Using Ma chine Learning/\u000f Proso dic F eatures/{ length of in tonation al phrase /(P/-L/)/: in teger/./{ p osition in in tonational phrase /(P/-P/)/: in teger/./{ length of in termedia te phrase /(I/-L/)/: in teger/./{ p osition in in termediate phrase /(I/-P/)/: in teger/./{ comp ositio n of in termediat e phrase /(I/-C/)/: only /, only cue phrases/, other/./{ accen t /(A/)/: H/*/, L/*/, L/*/+H/, L/+H/*/, H/*/+L/, H/+L/*/, deaccen ted/, am biguous/./{ accen t/* /(A/*/)/: H/*/, L/*/, complex/, deaccen ted/, am biguous/./\u000f T extual F eatures/{ preceding cue phrase /(C/-P/)/: true/, false/, NA/./{ succeeding cue phrase /(C/-S/)/: true/, false/, NA/./{ preceding orthograph y /(O/-P/)/: comma/, dash/, p erio d/, paragraph/, false/, NA/./{ preceding orthograph y/* /(O/-P/*/)/: true/, false/, NA/./{ succeeding orthograph y /(O/-S/)/: comma/, dash/, p erio d/, false/, NA/./{ succeeding orthograph y/* /(O/-S/*/)/: true/, false/, NA/./{ part/-of/-sp e ec h /(POS/)/: article/, co ordinatin g conjunction /, cardinal n umeral/, sub ordinatin g conjunction /,prep osition /, adjectiv e/, singular or mass noun/, singular prop er noun/, in tensi/\fer/, adv erb/, v erb base form/,NA/./\u000f Lexical F eature/{ tok en /(T/)/: actually /, also/, although/, and/, basically /, b ecause/, but/, essen tially /, except/, /\fnally /, /\frst/, further/,generally /, ho w ev er/, indeed/, lik e/, lo ok/, next/, no/, no w/, ok/, or/, otherwise/, righ t/, sa y /, second/, see/, similarly /,since/, so/, then/, therefore/, w ell/, y es/.Figure /2/: Represen tation of features/, for use b y C/4/./5 and cgrendel /.L/) and length of interme diate phr ase /(I/-L/) represen t the n um b er of w ords in the in tonationaland in termediate phrases con taining the cue phrase/, resp ectiv ely /. This feature w as not co dedin the /\\no w/\" data/, but w as co ded /(although not used/) in the later m ultiple cue phrasedata/. Position in intonational phr ase /(P/-P/) and p osition in interme diate phr ase /(I/-P/) usen umeric v alues rather than the earlier sym b olic v alues /(e/.g/./, /\frst in Figure /1/)/. Comp ositionof interme diate phr ase /(I/-C/) replaces the v alue alone /(meaning that the phrase con tainedonly the example cue phrase/, or only the example plus other cue phrases/) from Figure /1with the more primitiv e v alues only and only cue phr ases /(whose disjunction is equiv alen t toalone /)/; I/-C also uses the v alue other rather than /: alone /(as w as used in Figure /1/)/. A c c ent/(A/) uses the v alue ambiguous to represen t all cases where the proso dic analysis yields adisjunction /(e/.g/./, /\\H/*/+L or H/*/\"/)/. A c c ent/* /(A/*/) re/-represen ts some of the sym b olic v aluesof the feature ac c ent /(A/) using a more abstract lev el of description/. In particular/, L/*/+H/,L/+H/*/, H/*/+L/, and H/+L/* are represen ted as separate v alues in A but as a single v alue /{ thesup erclass c omplex /{ in A/*/. While useful abstractions can often result from the learningpro cess/, A/* is explicitly represen ted in adv ance as it is a proso dic feature represen tationthat has the p oten tial to b e automated /(see Section /5/)/.In all the textual features/, the v alue NA /(not applicable/) re/\rects the fact that /3/9 recordedexamples w ere not included in the transcription/, whic h w as done indep enden tly of the/6/1Litmanstudies p erformed b y Hirsc h b erg and Litman /(/1/9/9/3/)/. In the co ding used b y Hirsc h b erg andLitman/, pr e c e ding cue phr ase /(C/-P/) and suc c e e ding cue phr ase /(C/-S/) represen ted the actualcue phrase /(e/.g/./, /\\and/\"/) when there w as a preceding or succeeding cue phrase/; here the v aluetrue enco des all suc h cases/. As with the proso dic feature set A/*/, pr e c e ding ortho gr aphy/*/(O/-P/*/) and suc c e e ding ortho gr aphy/* /(O/-S/*/) re/-represen t some of the sym b olic v alues ofpr e c e ding ortho gr aphy /(O/-P/) and suc c e e ding ortho gr aphy /(O/-S/)/, resp ectiv ely /, using a moreabstract lev el of description /(e/.g/./, c omma /, dash /, and p erio d are represen ted as separate v aluesin O/-S but as the single v alue true in O/-S/*/)/. This is done b ecause the reliabilit y of co dingdetailed transcriptions of orthograph y is not kno wn/. Part/-of/-sp e e ch /(POS/) represen ts thepart of sp eec h assigned to eac h cue phrase b y Ch urc h/'s program for tagging part of sp eec h inunrestricted text /(Ch urc h/, /1/9/8/8/)/; while the program can assign appro ximately /8/0 di/\u000beren tv alues/, only the subset of v alues that w ere actually assigned to the cue phrases in thetranscripts of the corp ora are sho wn in the /\fgure/. Finally /, the lexical feature token /(T/) isnew to this study /, and represen ts the actual cue phrase b eing describ ed/./3/./1/./3 Training D a t aThe /\fnal input to eac h learning program is tr aining data /, i/.e/./, a set of examples for whic hthe class and feature v alues are sp eci/\fed/. Consider the follo wing utterance/, tak en from them ultiple cue phrase corpus /(Hirsc h b erg /& Litman/, /1/9/9/3/)/:Example /1 /[/( Now /) /( now that w e ha v e all b een w elcomed here/)/] it/'s time to get on withthe business of the conference/.This utterance con tains t w o cue phrases/, corresp onding to the t w o instances of /\\no w/\"/. Thebrac k ets and paren theses illustrate the in tonational and in termediate phrases/, resp ectiv ely /,that con tain the example cue phrases/. Note that a single in tonational phrase con tains b othexamples/, but that eac h example is uttered in a di/\u000beren t in termediate phrase/. If w e w ereonly in terested in the feature length of intonational phr ase /(P/-L/)/, the t w o examples w ouldb e represen ted in the training data as follo ws/:P/-L Class/9 discourse/9 sen ten tialThe /\frst column indicates the v alue assigned to the feature P/-L/, while the second columnindicates ho w the example w as classi/\fed/. Th us/, the length of the in tonational phrasecon taining the /\frst instance of /\\no w/\" is /9 w ords/, and the example cue phrase is classi/\fedas a discourse usage/. If w e w ere only in terested in the feature c omp osition of interme diatephr ase /(I/-C/)/, the t w o examples w ould instead b e represen ted in the training data as follo ws/:I/-C Classonly discourseother sen ten tialThat is/, the in termediate phrase con taining the /\frst instance of /\\no w/\" con tains only thecue phrase /\\no w/\"/, while the in termediate phrase con taining the second instance of /\\no w/\"con tains /\\no w/\" as w ell as /7 other lexical items that are not cue phrases/. Note that whilethe v alue of P/-L is the same for b oth examples/, the v alue of I/-C is di/\u000beren t/./6/2Cue Phrase Classifica tion Using Ma chine Learning/3/./2 The Mac hine Learning OutputsThe output of b oth mac hine learning programs are classi/\fc ation mo dels /. In C/4/./5 the mo delis expressed as a de cision tr e e /, whic h consists of either a leaf no de /(a class assignmen t/)/, or adecision no de /(a test on a feature/, with one branc h and subtree for eac h p ossible outcome ofthe test/)/. The follo wing example illustrates the non/-graphical represen tation for a decisionno de testing a feature with n p ossible v alues/:if test/1\nthen /: /: /:/: /: /:elseif testn\nthen /: /: /:T ests are of the form /\\feature op erator v alue/\"\n/9/. /\\F eature/\" is the name of a feature /(e/.g/.ac c ent /)/, while /\\v alue/\" is a v alid v alue for that feature /(e/.g/./, de ac c ente d /)/. F or features withsym b olic v alues /(e/.g/./, ac c ent /)/, there is one branc h for eac h sym b olic v alue/, and the op erator/\\/=/\" is used/. F or features with n umeric v alues /(e/.g/./, length of intonational phr ase /)/, thereare t w o branc hes/, eac h comparing the n umeric v alue with a threshold v alue/; the op erators/\\ /\u0014 /\" and /\\ /> /\" are used/. Giv en a decision tree/, a cue phrase is classi/\fed b y starting at thero ot of the tree and follo wing the appropriate branc hes un til a leaf is reac hed/. Section /4sho ws example decision trees pro duced b y C/4/./5/.In cgrendel the classi/\fcation mo del is expressed as an ordered set of if/-then rules ofthe follo wing form/:if test/1\n/^ /: /: /: /^ testk\nthen classThe /\\if /\" part of a rule is a conjunction of tests on the v alues of /(v arying/) features/, wheretests are again of the form /\\feature op erator v alue/./\" As in C/4/./5/, /\\feature/\" is the name ofa feature/, and /\\v alue/\" is a v alid v alue for that feature/. Unlik e C/4/./5/, the op erators /= or /6/=are used for features with sym b olic v alues/, while /\u0014 or /\u0015 are used for features with n umericv alues/. The /\\then/\" part of a rule sp eci/\fes a class assignmen t /(e/.g/, disc ourse /)/. Giv en a setof if/-then rules/, a cue phrase is classi/\fed using the rule whose /\\if /\" part is satis/\fed/. If thereor t w o or more suc h rules and the rules disagree on the class of an example/, cgrendelapplies one of t w o con/\rict resolution strategies /(c hosen b y the user/)/: c ho ose the /\frst rule/,or c ho ose the rule that is most accurate on the data/. The exp erimen ts rep orted here usethe second strategy /. If there are no suc h rules/, cgrendel assigns a default class/. Section /4sho ws example rules pro duced b y cgrendel /.Both C/4/./5 and cgrendel learn their classi/\fcation mo dels using greedy searc h guidedb y an /\\information gain/\" metric/. C/4/./5 uses a divide and conquer pro cess/: training examplesare recursiv ely divided in to subsets /(using the tests discussed ab o v e/)/, un til all of the subsetsb elong to a single class/. The test c hosen to divide the examples is that whic h maximizesa metric called a gain ratio /(a lo cal measure of progress/, whic h do es not consider an ysubsequen t tests/)/; this metric is based on information theory and is discussed in detail b yQuinlan /(/1/9/9/3/)/. Once a test is selected/, there is no bac ktrac king/. Ideally /, the set of c hosentests should result in a small /\fnal decision tree/. cgrendel generates its set of if/-then rulesusing a metho d called sep ar ate and c onquer /(to highligh t the similarit y with divide andconquer/)/:/9/. An additional t yp e of test ma y b e in v ok ed b y a C/4/./5 option/./6/3LitmanMan y rule learning systems generate h yp otheses using a greedy strategy in whic hrules are added to the rule set one b y one in an e/\u000bort to form a small co v er ofthe p ositiv e examples/; eac h rule/, in turn is created b y adding one conditionafter another to the an teceden t un til the rule is consisten t with the negativ edata/. /(Cohen/, /1/9/9/3/)Although cgrendel is claimed to ha v e t w o adv an tages o v er C/4/./5/, these adv an tages donot come in to pla y for the exp erimen ts rep orted here/. First/, if/-then rules app ear to b e easierfor p eople to understand than decision trees /(Quinlan/, /1/9/9/3/)/. Ho w ev er/, for the cue phraseclassi/\fcation task/, the decision trees pro duced b y C/4/./5 are quite compact and th us easilyundersto o d/. F urthermore/, a rule represen tation can b e deriv ed from C/4/./5 decision trees/,using the program C/4/./5rules/. Second/, cgrendel allo ws users to exploit prior kno wledge ofa learning problem/, b y constraining the syn tax of the rules that can b e learned/. Ho w ev er/, noprior kno wledge is exploited in the cue phrase exp erimen ts/. The main reason for using b othC/4/./5 and cgrendel is to increase the reliabilit y of an y comparisons b et w een the mac hinelearning and man ual results/. In particular/, if comparable results are obtained using b othC/4/./5 and cgrendel /, then an y p erformance di/\u000berences b et w een the learned and man uallyderiv ed classi/\fcation mo dels are less lik ely to b e due to the sp eci/\fcs of a particular learningprogram/, and more lik ely to re/\rect the learned//man ual distinction/./3/./3 Ev aluationThe output of eac h mac hine learning exp erimen t is a classi/\fcation mo del that has b eenlearned from the training data/. These learned mo dels are qualitativ ely ev aluated b y exam/-ining their linguistic con ten t/, and b y comparing them with the man ually deriv ed mo dels ofFigure /1/. The learned mo dels are also quan titativ ely ev aluated b y examining their errorrates on testing data and b y comparing these error rates to eac h other and to the errorrates sho wn in T able /1/. The err or r ate of a classi/\fcation mo del is computed b y using themo del to predict the classi/\fcations for a set of examples where the classi/\fcations are alreadykno wn/, then comparing the predicted and kno wn classi/\fcations/. In the cue phrase domain/,the error rate is computed b y summing the n um b er of discourse examples misclassi/\fed assen ten tial with the n um b er of sen ten tial examples misclassi/\fed as discourse/, then dividingb y the total n um b er of examples/.The error rates of the learned classi/\fcation mo dels are estimated using t w o metho dolo/-gies/. T r ain/-and/-test err or r ate estimation /(W eiss /& Kulik o wski/, /1/9/9/1/) /\\holds out/\" a testset of examples/, whic h are not seen un til after training is completed/. That is/, the mo del isdev elop ed b y examining only the training examples/; the error of the mo del is then estimatedb y using the mo del to classify the test examples/. This w as the ev aluation metho d used b yHirsc h b erg and Litman/. The resampling metho d of cr oss/-validation /(W eiss /& Kulik o wski/,/1/9/9/1/) estimates error rate using m ultiple train/-and/-test exp erimen ts/. F or example/, in /1/0/-fold cross/-v alidation/, instead of dividing examples in to training and test sets once/, /1/0 runs ofthe learning program are p erformed/. The total set of examples is randomly divided in to /1/0disjoin t test sets/; eac h run th us uses the /9/0/% of the examples not in the test set for trainingand the remaining /1/0/% for testing/. Note that for eac h iteration of the cross/-v alidation/, thelearning pro cess b egins from scratc h/; th us a new classi/\fcation mo del is learned from eac htraining sample/. An estimated error rate is obtained b y a v eraging the error rate on the test/-/6/4Cue Phrase Classifica tion Using Ma chine Learninging p ortion of the data from eac h of the /1/0 runs/. While this metho d do es not mak e sense forh umans/, computers can truly ignore previous iterations/. F or sample sizes in the h undreds/(the classi/\fable subset of the m ultiple cue phrase sample and the classi/\fable non/-conjunctsubset pro vide /8/7/8 and /4/9/5 examples/, resp ectiv ely/) /1/0/-fold cross/-v alidation often pro videsa b etter p erformance estimate than the hold/-out metho d /(W eiss /& Kulik o wski/, /1/9/9/1/)/. Thema jor adv an tage is that in cross/-v alidation all examples are ev en tually used for testing/, andalmost all examples are used in an y giv en training run/.The b est p erforming learned mo dels are iden ti/\fed b y comparing their error rates tothe error rates of the other learned mo dels and to the man ually deriv ed error rates/. T odetermine whether the fact that an error rate E/1 is lo w er than another error rate E/2 isalso signi/\fcan t/, statistical inference is used/. In particular/, con/\fdence in terv als for the t w oerror rates are computed/, at a /9/5/% con/\fdence lev el/. When an error rate is estimated usingonly a single error rate on a test set /(i/.e/./, the train/-and/-test metho dology/)/, the con/\fdencein terv al is computed using a normal appro ximation to the binomial distribution /(F reedmanet al/./, /1/9/7/8/)/. When the error rate is estimated using the a v erage from m ultiple errorrates /(i/.e/./, the cross/-v alidation metho dology/)/, the con/\fdence in terv al is computed using at /-T able /(F reedman et al/./, /1/9/7/8/)/. If the upp er b ound of the /9/5/% con/\fdence in terv al for E/1is lo w er than the lo w er b ound of the /9/5/% con/\fdence in terv al for the error rate E/2/, then thedi/\u000berence b et w een E/1 and E/2 is assumed to b e signi/\fcan t/.\n/1/0/3/./4 The Exp erimen tal ConditionsThis section describ es the conditions used in eac h set of mac hine learning exp erimen ts/. Theexp erimen ts di/\u000ber in their use of training and testing corp ora/, metho ds for estimating errorrates/, and in the features and classi/\fcations used/. The actual results of the exp erimen ts arepresen ted in Section /4/./3/./4/./1 F our Sets of ExperimentsThe learning exp erimen ts can b e conceptually divided in to four sets/. Eac h exp erimen t inthe /\frst set estimates error rate using the train/-and/-test metho d/, where the training andtesting samples are those used b y Hirsc h b erg and Litman /(/1/9/9/3/) /(the /\\no w/\" data and thet w o subsets of the m ultiple cue phrase corpus/, resp ectiv ely/)/. This allo ws a direct comparisonof the man ual and mac hine learning approac hes/. Ho w ev er/, only the proso dic exp erimen tsconducted b y Hirsc h b erg and Litman /(/1/9/9/3/) are replicated/. The textual training and testingconditions are not replicated as the original training corpus /(the /\frst /1/7 min utes of them ultiple cue phrase corpus/) /(Litman /& Hirsc h b erg/, /1/9/9/0/) is a subset of/, rather than disjoin tfrom/, the test corpus /(the full /7/5 min utes of the m ultiple cue phrase corpus/) /(Hirsc h b erg /&Litman/, /1/9/9/3/)/.In con trast/, eac h exp erimen t in the second set uses cross/-v alidation to estimate errorrate/. F urthermore/, b oth training and testing samples are tak en from the m ultiple cuephrase corpus/. Eac h exp erimen t uses /9/0/% of the examples from the m ultiple cue phrasedata for training/, and the remaining /1/0/% for testing/. Th us eac h exp erimen t in the secondset trains from m uc h larger amoun ts of data /(/7/9/0 classi/\fable examples/, or /4/4/5 classi/\fable/1/0/. Thanks to William Cohen for suggesting this metho dology /./6/5LitmanP/-L P/-P I/-L I/-P I/-C A A/* C/-P C/-S O/-P O/-P/* O/-S O/-S/* POSproso dy X X X X X X Xhl/9/3features X X X Xphrasing X X X X Xlength X Xp osition X Xin tonationa l X Xin termediat e X X Xtext X X X X X X Xadjacency X Xorthograph y X X X Xpreceding X X Xsucceeding X X Xsp eec h/-text X X X X X X X X X X X X X XT able /3/: Multiple feature sets and their comp onen ts/.non/-conjuncts/) than eac h exp erimen t in the /\frst set /(/1/0/0 /\\no ws/\"/)/. The reliabilit y of thetesting is not compromised due to the use of cross/-v alidation /(W eiss /& Kulik o wski/, /1/9/9/1/)/.Eac h exp erimen t in the third set replicates an exp erimen t in the second set/, with the ex/-ception that the learning program is no w allo w ed to distinguish b et w een cue phrases/. Thisis done b y adding a feature represen ting the cue phrase /(the feature token from Figure /2/)to eac h exp erimen t from the second set/. Since the p oten tial use of suc h a lexical featurew as noted but not used b y Hirsc h b erg and Litman /(/1/9/9/3/)/, these exp erimen ts pro vide qual/-itativ ely new linguistic insigh ts in to the data/. F or example/, the same features ma y no w b eused di/\u000beren tly to predict the classi/\fcations of di/\u000beren t cue phrases or sets of cue phrases/.Finally /, eac h exp erimen t in the fourth set replicates an exp erimen t in the /\frst/, second/,and third set/, with the exception that all /9/5/3 examples in the m ultiple cue phrase corpusare no w considered/. This is b ecause in practice/, an y learned cue phrase classi/\fcation mo delwill lik ely b e used to classify all cue phrases/, ev en those that are di/\u000ecult for h uman judgesto classify /. The exp erimen ts in the fourth set allo w the learning programs to attempt tolearn the class unknown /, in addition to the classes disc ourse and sentential /./3/./4/./2 Fea ture Represent a tions within Experiment SetsWithin eac h of these four sets of exp erimen ts/, eac h individual exp erimen t represen ts thedata using a di/\u000beren t subset of the a v ailable features/. First/, the data is represen ted ineac h of /1/4 single fe atur e sets /, corresp onding to eac h proso dic and textual feature sho wn inFigure /2/. These exp erimen ts comparativ ely ev aluate the utilit y of eac h individual featurefor classi/\fcation/. The represen tations of Example /1 sho wn ab o v e illustrate ho w data isrepresen ted using the single feature set P/-L/, and using the single feature set I/-C/.Second/, the data is represen ted in eac h of the /1/3 multiple fe atur e sets sho wn in T able /3/.Eac h of these sets con tains a linguisticall y motiv ated subset of at least /2 of the /1/4 features/.The /\frst /7 sets use only proso dic features/. Pr oso dy considers all the proso dic features thatw ere co ded for eac h example cue phrase/. Hl/9/3fe atur es considers only the co ded featuresthat w ere also used in the mo del sho wn in Figure /1/. Phr asing considers all features of b oththe in tonational and in termediate phrases con taining the example cue phrase /(i/.e/./, length/6/6Cue Phrase Classifica tion Using Ma chine LearningExample /1 /[/( Now /) /( now that w e ha v e all b een w elcomed here/)/] it/'s time to get on with the business of the conference/.P/-L P/-P I/-L I/-P I/-C A A/* C/-P C/-S O/-P O/-P/* O/-S O/-S/* POS Class/9 /1 /1 /1 only H/*/+L complex f t par/. t f f adv/. disc/./9 /2 /8 /1 other H/* H/* t f f f f f adv/. sen t/.Figure /3/: Represen tation of Example /1 in feature set sp eec h/-text/.of phrase/, p osition of example in phrase/, and comp osition of phrase/)/. L ength and p ositioneac h consider only one of these features/, but with resp ect to b oth the in tonational andin termediate phrase/. Con v ersely /, intonational and interme diate eac h consider only one t yp eof phrase/, but consider all of the features/. The next /5 sets use only textual features/. T extconsiders all the textual features/. A djac ency and ortho gr aphy eac h consider a single textualfeature/, but consider b oth the preceding and succeeding immediate con text/. Pr e c e ding andsuc c e e ding consider con textual features relating to b oth orthograph y and cue phrases/, butlimit the con text/. The last set/, sp e e ch/-text /, uses all of the proso dic and textual features/.Figure /3 illustrates ho w the t w o example cue phrases in Example /1 w ould b e represen tedusing sp e e ch/-text /. Consider the feature v alues for the /\frst example cue phrase/. Since thisexample is the /\frst lexical item in b oth the in tonational and in termediate phrases whic hcon tain it/, its p osition in b oth phrases /(P/-P and I/-P/) is /1/. Since the in termediate phrasecon taining the cue phrase con tains no other lexical items/, its length /(I/-L/) is /1 w ord and itscomp osition /(I/-C/) is only the cue phrase/. The v alues for A and A/* indicate that when thein tonational phrase is describ ed as a sequence of tones/, the complex pitc h accen t H/*/+L isasso ciated with the cue phrase/. With resp ect to the textual features/, the utterance w astranscrib ed suc h that it b egan a new paragraph/. Th us the example cue phrase w as notpreceded b y another cue phrase /(C/-P/)/, but it w as preceded b y a form of orthograph y /(O/-Pand O/-P/*/)/. Since the example cue phrase w as immediately follo w ed b y another instanceof /\\no w/\" in the transcription/, the cue phrase w as succeeded b y another cue phrase /(C/-S/)but w as not succeeded b y orthograph y /(O/-S and O/-S/*/)/. Finally /, the output of the part ofsp eec h tagging program when run on the transcript of the corpus yields the v alue adverbfor the cue phrase/'s part of sp eec h /(POS/)/.The /\frst set of exp erimen ts replicate only the proso dic exp erimen ts conducted b yHirsc h b erg and Litman /(/1/9/9/3/)/; cue phrases are represen ted using the subset of the fea/-ture sets that only consist of proso dic features/. In the second set of exp erimen ts/, examplesare represen ted using all /2/7 di/\u000beren t feature sets /(the /1/4 single feature sets and the /1/3m ultiple feature sets/)/. In the third set of exp erimen ts/, examples are represen ted using /2/7tokenize d fe atur e sets /, constructed b y adding the lexical feature token from Figure /2 /(thecue phrase b eing describ ed/) to eac h of the /1/4 single and /1/3 m ultiple feature sets from thesecond set of exp erimen ts/. These tok enized feature sets will b e referred to using the namesof the single and m ultiple feature sets/, concatenated with /\\/+/\"/. The follo wing illustratesho w the t w o cue phrases in Example /1 w ould b e represen ted using P/-L/+/:P/-L T Class/9 no w discourse/9 no w sen ten tial/6/7LitmanThe represen tation is similar to the P/-L represen tation sho wn earlier/, except for the secondcolumn whic h indicates the v alue assigned to the feature token /(T/)/./4/. ResultsThis section examines the results of running the t w o learning programs /{ C/4/./5 and cgren/-del /{ in the four sets of cue phrase classi/\fcation exp erimen ts describ ed ab o v e/. The learnedclassi/\fcation mo dels will b e compared with the classi/\fcation mo dels sho wn in Figure /1/,while the error rates of the learned classi/\fcation mo dels will b e compared with the errorrates sho wn in T able /1 and with the error rates of the other learned mo dels/. As will b eseen/, the results suggest that mac hine learning is useful for automating the generation oflinguisticall y viable classi/\fcation classi/\fcation mo dels/, for generating classi/\fcation mo delsthat p erform with lo w er error rates than man ually dev elop ed h yp otheses/, and for adding tothe b o dy of linguistic kno wledge regarding cue phrases/./4/./1 Exp erimen t Set /1/: Replicating Hirsc h b erg and LitmanThe /\frst group of exp erimen ts replicate the training/, testing/, and ev aluation conditionsused b y Hirsc h b erg and Litman /(/1/9/9/3/)/, in order to in v estigate ho w w ell mac hine learningp erforms in comparison to the man ual dev elopmen t of cue phrase classi/\fcation mo dels/.Figure /4 sho ws the b est p erforming proso dic classi/\fcation mo dels learned b y the t w omac hine learning programs/; the top of the /\fgure replicates the man ually deriv ed proso dicmo del from Figure /1 for ease of comparison/. When all of the proso dic features are usedto represen t the /1/0/0 training examples of /\\no w/\" /(i/.e/./, eac h example is represen ted usingfeature set pr oso dy from T able /3/)\n/1/1/, the classi/\fcation mo dels that are learned are sho wnafter the man ually deriv ed mo del at the top of Figure /4/. Note that using b oth learningprograms/, the same decision tree is also learned when the smaller feature sets phr asing andp osition are used to represen t the /\\no w/\" data/. The b ottom p ortion of the /\fgure sho ws theclassi/\fcation mo dels that are learned when the same examples are represen ted using onlythe single proso dic feature p osition in intonational phr ase /(P/-P/)/; the same mo del is alsolearned when the examples are represen ted using the m ultiple feature set intonational /.Recall that C/4/./5 represen ts eac h learned classi/\fcation mo del as a decision tree/. Eac hlev el of the tree /(sho wn b y inden tation/) sp eci/\fes a test on a single feature/, with a branc h forev ery p ossible outcome of the test/. A branc h can either lead to the assignmen t of a class/, orto another test/. F or example/, the C/4/./5 classi/\fcation mo del learned from pr oso dy classi/\fescue phrases using the t w o features p osition in intonational phr ase /(P/-P/) and p osition ininterme diate phr ase /(I/-P/)/. Note that not all of the a v ailable features in pr oso dy /(recallT able /3/) are used in the decision tree/. The tree initially branc hes based on the v alue ofthe feature p osition in intonational phr ase /.\n/1/2The /\frst branc h leads to the class assignmen tdisc ourse /. The second branc h leads to a test of the feature p osition in interme diate phr ase /.The /\frst branc h of this test leads to the class assignmen t disc ourse /, while the second branc hleads to sentential /. C/4/./5 pro duces b oth unsimpli/\fed and pruned decision trees/. The goal/1/1/. In Exp erimen t Set /1/, the feature set pr oso dy do es not con tain the features P/-L and I/-L/. Recall thatphrasal length w as only co ded in the later m ultiple cue phrase study /./1/2/. F or ease of comparison to Figure /1/, the original sym b olic represen tation of the feature v alue is usedrather than the in teger represen tation sho wn in Figure /2/./6/8Cue Phrase Classifica tion Using Ma chine LearningMan ually deriv ed proso dic mo del /(rep eated from Figure /1/) /:if comp osition of in termediat e phrase /= alone then disc ourse /(/1/)elseif comp osition of in termediate phrase /= /: alone then /(/2/)if p osition in in termedia te phrase /= /\frst then /(/3/)if accen t /= deaccen ted then disc ourse /(/4/)elseif accen t /= L/* then disc ourse /(/5/)elseif accen t /= H/* then sentential /(/6/)elseif accen t /= complex then sentential /(/7/)elseif p osition in in termediate phrase /= /: /\frst then sentential /(/8/)Decision tree learned from proso dy /, from phrasing/, and from p osition using C/4/./5/:if p osition in in tonational phrase /= /\frst then disc ourseelseif p osition in in tonationa l phrase /= /: /\frst thenif p osition in in termedia te phrase /= /\frst then disc ourseelseif p osition in in termediate phrase /= /: /\frst then sententialRuleset learned from proso dy /, from phrasing/, and from p osition using CGRENDEL /:if /(p osition in in tonationa l phrase /6/= /\frst/) /^ /(p osition in in termediate phrase /6/= /\frst/) then sententialdefault is on disc ourseDecision tree learned from P/-P and from in tonationa l using C/4/./5/:if p osition in in tonational phrase /= /\frst then disc ourseelseif p osition in in tonationa l phrase /= /: /\frst then sententialRuleset learned from P/-P and from in tonational using CGRENDEL /:if p osition in in tonational phrase /6/= /\frst then sententialdefault is on disc ourseFigure /4/: Example C/4/./5 and cgrendel classi/\fcation mo dels learned from di/\u000beren t proso dicfeature represen tations of the /\\no w/\" data/./6/9LitmanMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)P/-P /1/8/./3 /\u0006 /2/./6 /1/6/./6 /\u0006 /3/./4proso dy /2/7/./3 /\u0006 /3/./0 /1/7/./8 /\u0006 /3/./4phrasing /2/7/./3 /\u0006 /3/./0 /1/7/./8 /\u0006 /3/./4p osition /2/7/./3 /\u0006 /3/./0 /1/7/./8 /\u0006 /3/./4in tonationa l /1/8/./3 /\u0006 /2/./6 /1/6/./6 /\u0006 /3/./4man ual proso dic /2/4/./6 /\u0006 /3/./0 /1/4/./7 /\u0006 /3/./2T able /4/: /9/5/%/-con/\fdence in terv als for the error rates /(/%/) of the b est p erforming cgrendelproso dic classi/\fcation mo dels/, testing data/. /(T raining data w as the /\\no w/\" corpus/;testing data w as the m ultiple cue phrase corpus/./)of the pruning pro cess is to tak e a complex decision tree that ma y also b e o v er/\ftted to thetraining data/, and to pro duce a tree that is more comprehensible and whose accuracy isnot comprised /(Quinlan/, /1/9/9/3/)/. Since almost all trees are impro v ed b y pruning /(Quinlan/,/1/9/9/3/)/, only simpli/\fed decision trees are considered in this pap er/.In con trast/, cgrendel represen ts eac h learned classi/\fcation mo del as a set of if/-thenrules/. Eac h rule sp eci/\fes a conjunction of tests on v arious features/, and results in theassignmen t of a class/. F or example/, the cgrendel ruleset learned from pr oso dy classi/\fescue phrases using the t w o features p osition in intonational phr ase /(P/-P/) and p osition ininterme diate phr ase /(I/-P/) /(the same t w o features used in the C/4/./5 decision tree/)/. If thev alues of b oth features are not /\frst /, the if/-then rule applies and the cue phrase is classi/\fedas sentential /. If the v alue of either feature is /\frst /, the default applies and the cue phrase isclassi/\fed as disc ourse /.An examination of the learned classi/\fcation mo dels of Figure /4 sho ws that they arecomparable in con ten t to the p ortion of the man ually deriv ed mo del that classi/\fes cuephrases solely on phrasal p osition /(line /(/8/)/)/. In particular/, all of the classi/\fcation mo delssa y that if the cue phrase is not in an initial phrasal p osition classify it as sentential /.On the other hand/, the man ually deriv ed mo del also assigns the class sentential giv en aninitial phrasal p osition in conjunction with certain com binations of phrasal comp osition andaccen t/; the learned classi/\fcation mo dels instead classify the cue phrase as disc ourse in allother cases/. As will b e sho wn/, the further discrimination of the man ually obtained mo deldo es not signi/\fcan tly impro v e p erformance when compared to the learned classi/\fcationmo dels/, and in fact in one case signi/\fcan tly degrades p erformance/.The error rates of the learned classi/\fcation mo dels on the /\\no w/\" training data fromwhic h they w ere dev elop ed is as follo ws/: /6/% for the mo dels learned from pr oso dy /, phr asingand p osition /, and /9/% for the mo dels learned from P/-P and intonational /. Recall fromSection /2 that the error rate of the man ually dev elop ed proso dic mo del of Figure /1 onthe same training data w as /2/%/.T able /4 presen ts /9/5/% con/\fdence in terv als for the error rates of the b est p erformingcgrendel proso dic classi/\fcation mo dels/. F or ease of comparison/, the ro w lab eled /\\man ualproso dic/\" presen ts the error rates of the man ually dev elop ed proso dic mo del of Figure /1 onthe same t w o test sets/, whic h w ere originally sho wn in T able /1/. The table includes all thecgrendel mo dels whose p erformance matc hes or exceeds the man ual p erformance/./7/0Cue Phrase Classifica tion Using Ma chine LearningComparison of the error rates of the learned and man ually dev elop ed mo dels suggeststhat mac hine learning is an e/\u000bectiv e tec hnique for automating the dev elopmen t of cue phraseclassi/\fcation mo dels/. In particular/, within eac h test set/, the /9/5/% con/\fdence in terv al forthe error rate of the classi/\fcation mo dels learned from the m ultiple feature sets pr oso dy /,phr asing /, and p osition eac h o v erlaps with the con/\fdence in terv al for the error rate of theman ual proso dic mo del/. This is also true for the error rates of P/-P and intonational in theclassi/\fable non/-conjunct test set/. Th us/, mac hine learning supp orts the automatic construc/-tion of a v ariet y of cue phrase classi/\fcation mo dels that ac hiev e similar p erformance as theman ually constructed mo dels/.The results from P/-P and from intonational in the classi/\fable cue phrase test set aresho wn in italics/, as they suggest that mac hine learning ma y also b e useful for impro vingp erformance/. Although the v ery simple classi/\fcation mo del learned from P/-P and intona/-tional p erforms w orse than the man ually deriv ed mo del on the training data/, when testedon the classi/\fable cue phrases/, the learned mo del /(with an upp er b ound error rate of /2/0/./9/%/)outp erforms the man ually dev elop ed mo del /(with a lo w er b ound error rate of /2/1/./6/%/)/. Thissuggests that the man ually deriv ed mo del migh t ha v e b een o v er/\ftted to the training data/,i/.e/./, that the proso dic feature set most useful for classifying /\\no w/\" did not generalize toother cue phrases/. As noted ab o v e/, the use of simpli/\fed learned classi/\fcation mo dels helpsto guard against o v er/\ftting in the learning approac h/. The ease of inducing classi/\fcationmo dels from man y di/\u000beren t sets of features using mac hine learning supp orts the generationand ev aluation of a wide v ariet y of h yp otheses /(e/.g/. P/-P /, whic h w as a high p erforming butnot the optimal p erforming mo del on the training data/)/.Note that the man ual proso dic man ual p erforms signi/\fcan tly b etter in the smaller testset /(whic h do es not con tain the cue phrases /\\and/\"/, /\\or/\"/, and /\\but/\"/)/. In con trast/, thep erformance impro v emen t for P/-P and intonational in the smaller test set is not signi/\fcan t/.This also suggests that the man ually deriv ed mo del do es not generalize as w ell as the learnedmo dels/.Finally /, for the feature sets sho wn in T able /4/, the decision trees pro duced b y C/4/./5 p erformwith the same error rates as the rulesets pro duced b y cgrendel /, for b oth test sets/. Recallfrom Figure /4 that the C/4/./5 decision trees and cgrendel rules are in fact seman ticallyequiv alen t for eac h feature set/. The fact that comparable results are obtained using C/4/./5and cgrendel adds an extra degree of reliabilit y to the exp erimen ts/. In particular/, theduplication of the results suggests that the abilit y to matc h and p erhaps ev en to impro v eup on man ual p erformance b y using mac hine learning is not due to the sp eci/\fcs of eitherlearning program/./4/./2 Exp erimen t Set /2/: Using Di/\u000beren t T raining SetsThe second group of exp erimen ts ev aluate the utilit y of training from larger amoun ts ofdata/. This is done b y using /1/0/-fold cross/-v alidation to estimate error/, where for eac h run/9/0/% of the examples in a sample are used for training /(and o v er the /1/0 runs/, all of theexamples are used for testing/)/. In addition/, the exp erimen ts in this second set tak e b oththe training and testing data from the m ultiple/-cue phrase corpus/, in con trast to the previousset of exp erimen ts where the training data w as tak en from the /\\no w/\" corpus/. As will b eseen/, these c hanges impro v e the results/, suc h that more of the learned classi/\fcation mo dels/7/1LitmanMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)P/-L /3/3/./0 /\u0006 /5/./9 /(/3/3/./2 /\u0006 /1/./9/)P/-P /1/6/./1 /\u0006 /3/./5 /1/8/./8 /\u0006 /4/./2I/-L /2/5/./5 /\u0006 /3/./7 /(/2/5/./6 /\u0006 /2/./8/)I/-P /2/5/./9 /\u0006 /4/./9 /1/9/./4 /\u0006 /3/./1I/-C /(/3/6/./5 /\u0006 /5/./4/) /(/3/5/./2 /\u0006 /3/./4/)A /2/8/./6 /\u0006 /3/./6 /(/3/0/./2 /\u0006 /3/./1/)A/* /2/8/./3 /\u0006 /4/./3 /(/2/8/./4 /\u0006 /1/./7/)proso dy /1/5/./5 /\u0006 /2/./6 /1/7/./2 /\u0006 /3/./1hl/9/3feature s /2/9/./4 /\u0006 /3/./3 /1/8/./2 /\u0006 /4/./2phrasing /1/6/./1 /\u0006 /3/./4 /1/9/./6 /\u0006 /3/./9length /2/6/./1 /\u0006 /3/./8 /(/2/7/./4 /\u0006 /3/./4/)p osition /1/8/./2 /\u0006 /2/./3 /1/9/./4 /\u0006 /2/./8in tonationa l /1/7/./0 /\u0006 /4/./0 /2/0/./6 /\u0006 /3/./6in termedia te /2/1/./9 /\u0006 /2/./3 /1/9/./4 /\u0006 /5/./7man ual proso dic /2/4/./6 /\u0006 /3/./0 /1/4/./7 /\u0006 /3/./2T able /5/: /9/5/%/-con/\fdence in terv als for the error rates /(/%/) of all cgrendel proso dic classi/-/\fcation mo dels/, testing data/. /(T raining and testing w ere done from the m ultiplecue phrase corpus using cross/-v alidation/./)p erform with lo w er or comparable error rates when compared to the man ually dev elop edmo dels/./4/./2/./1 Pr osodic ModelsT able /5 presen ts the error rates of the classi/\fcation mo dels learned b y cgrendel /, inthe /2/8 di/\u000beren t proso dic exp erimen ts/. /(F or Exp erimen t Sets /2 and /3/, the C/4/./5 error ratesare presen ted in App endix A/./) Eac h n umeric cell sho ws the /9/5/% con/\fdence in terv al for theerror rate/, whic h is equal to the error p ercen tage obtained b y cross/-v alidation /\u0006 the marginof error /( /\u0006 /2/./2/6 standard errors/, using a t /-T able/)/. The top p ortion of the table considersthe mo dels learned from the single proso dic feature sets /(Figure /2/)/, the middle p ortionconsiders the mo dels learned from the m ultiple feature sets /(T able /3/)/, while the last ro wconsiders the man ually dev elop ed proso dic mo del/. The error rates sho wn in italics indicatethat the p erformance of the learned classi/\fcation mo del exceeds the p erformance of theman ual mo del /(giv en the same test set/)/. The error rates sho wn in paren theses indicate theopp osite case /- that the p erformance of the man ual mo del exceeds the p erformance of thelearned mo del/. Suc h cases w ere omitted in T able /4/.As in Exp erimen t Set /1/, comparison of the error rates of the learned and man uallydev elop ed mo dels suggests that mac hine learning is an e/\u000bectiv e tec hnique for not onlyautomating the dev elopmen t of cue phrase classi/\fcation mo dels/, but also for impro vingp erformance/. When ev aluated on the classi/\fable cue phrase test set/, /\fv e learned mo delsha v e impro v ed p erformance compared to the man ual mo del/; all of the mo dels except I/-Cp erform at least comparably to the man ual mo del/. Note that in Exp erimen t Set /1/, only t w olearned mo dels outp erformed the man ual mo del/, and only /\fv e learned mo dels p erformedat least comparably /. The abilit y to use large training sets th us app ears to b e an adv an tageof the automated approac h/./7/2Cue Phrase Classifica tion Using Ma chine LearningMan ually deriv ed proso dic mo del /(rep eated from Figure /1/) /:if comp osition of in termediat e phrase /= alone then disc ourse /(/1/)elseif comp osition of in termediate phrase /= /: alone then /(/2/)if p osition in in termedia te phrase /= /\frst then /(/3/)if accen t /= deaccen ted then disc ourse /(/4/)elseif accen t /= L/* then disc ourse /(/5/)elseif accen t /= H/* then sentential /(/6/)elseif accen t /= complex then sentential /(/7/)elseif p osition in in termediate phrase /= /: /\frst then sentential /(/8/)Decision tree learned from P/-P using C/4/./5 /:if p osition in in tonational phrase /\u0014 /1 then disc ourseelseif p osition in in tonationa l phrase /> /1 then sententialRuleset learned from P/-P using CGRENDEL /:if p osition in in tonational phrase /\u0015 /2 then sententialdefault is on disc ourseDecision tree learned from proso dy using C/4/./5 /:if p osition in in tonational phrase /\u0014 /1 thenif p osition in in termedia te phrase /\u0014 /1 then disc ourseelseif p osition in in termediate phrase /> /1 then sententialelseif p osition in in tonationa l phrase /> /1 thenif length of in termediat e phrase /\u0014 /1 then disc ourseelseif length of in termedia te phrase /> /1 then sententialRuleset learned from proso dy using CGRENDEL /:if /(p osition in in tonationa l phrase /\u0015 /2/) /^ /(length of in termediate phrase /\u0015 /2/) then sententialif /(/7 /\u0015 p osition in in tonational phrase /\u0015 /4/) /^ /(length of in tonationa l phrase /\u0015 /1/0/) then sententialif /(length of in termediate phrase /\u0015 /2/) /^ /(length of in tonational phrase /\u0014 /7/) /^ /(accen t /= H/*/) then sententialif /(length of in termediate phrase /\u0015 /2/) /^ /(length of in tonational phrase /\u0014 /9/) /^ /(accen t /= H/*/+L/) then sententialif /(length of in termediate phrase /\u0015 /2/) /^ /(accen t /= deaccen ted/) then sententialif /(length of in termediate phrase /\u0015 /8/) /^ /(length of in tonational phrase /\u0014 /9/) /^ /(accen t /= L/*/) then sententialdefault is on disc ourseFigure /5/: Example C/4/./5 and cgrendel classi/\fcation mo dels learned from di/\u000beren t proso dicfeature represen tations of the classi/\fable cue phrases in the m ultiple cue phrasecorpus/.When tested on the classi/\fable non/-conjuncts /(where the error rate of the man uallyderiv ed mo del decreases/)/, mac hine learning is useful for automating but not for impro vingp erformance/. This migh t re/\rect the fact that the man ually deriv ed theories already ac hiev eoptimal p erformance with resp ect to the examined features in this less noisy sub corpus/,and//or that the automatically deriv ed theory for this sub corpus w as based on a smallertraining set than used in the larger sub corpus/.An examination of some of the b est p erforming learned classi/\fcation mo dels sho ws thatthey are quite comparable in con ten t to relev an t p ortions of the proso dic mo del of Figure /1/,and often con tain further linguistic insigh ts/. Consider the classi/\fcation mo del learned fromthe single feature p osition in intonational phr ase /(P/-P/)/, sho wn near the top of Figure /5/./7/3LitmanBoth of the learned classi/\fcation mo dels sa y that if the cue phrase is not in the initialp osition of the in tonational phrase/, classify as sentential /; otherwise classify as disc ourse /.Note the corresp ondence with line /(/8/) in the man ually deriv ed proso dic mo del/. Also notethat the classi/\fcation mo dels are comparable\n/1/3to the P/-P classi/\fcation mo dels learnedfrom Exp erimen t Set /1 /(sho wn in Figure /4/)/, despite the di/\u000berences in training data/. Thefact that the single proso dic feature p osition in intonational phr ase /(P/-P/) can classify cuephrases at least as w ell as the more complicated man ual and m ultiple feature learned mo delsis again a new result of the learning exp erimen ts/.Figure /5 also illustrates the more complex classi/\fcation mo dels learned using pr oso dy /,the largest proso dic feature set/. The C/4/./5 mo del is similar to lines /(/1/) and /(/8/) of the man ualmo del/. /(The length v alue /1 is equiv alen t to the comp osition v alue alone /./) In the rulesetinduced from pr oso dy b y cgrendel /, the /\frst /2 if/-then rules correlate sentential status with/(among other things/) non/-initial p osition\n/1/4/, and the second /2 rules with H/* and H/*/+Laccen ts/; these rules are similar to lines /(/6/)/-/(/8/) in Figure /1/. Ho w ev er/, the last /2 if/-then rulesin the ruleset also correlate no accen t and L/* with sen ten tial status when the phrase is of acertain length/, while lines /(/4/) and /(/5/) in Figure /1 pro vide a di/\u000beren t in terpretation and donot tak e length in to accoun t/. Recall that length w as co ded b y Hirsc h b erg and Litman onlyin their test data/. Length w as th us nev er used to generate or revise their proso dic mo del/.The utilit y of length is a new result of this exp erimen t set/.Although not sho wn/, the mo dels learned from phr asing /, p osition /, and intonational alsooutp erform the man ual mo del/. As can b e seen from T able /3/, these mo dels corresp ond toall of the feature sets that are sup ersets of P/-P but subsets of pr oso dy /./4/./2/./2 Textual ModelsT able /6 presen ts the error rates of the classi/\fcation mo dels learned b y cgrendel /, in the/2/4 di/\u000beren t textual exp erimen ts/. Unlik e the exp erimen ts in v olving the proso dic feature sets/,none of the learned textual mo dels p erform signi/\fcan tly b etter than the man ually deriv edmo del/. Ho w ev er/, the results suggest that mac hine learning is still an e/\u000bectiv e tec hniquefor automating the dev elopmen t of cue phrase classi/\fcation mo dels/. In particular/, /\fv elearned mo dels /(O/-P /, O/-P/*/, text /, ortho gr aphy /, and pr e c e ding /) p erform comparably to theman ually deriv ed mo del/, in b oth test sets/. Note that these /\fv e mo dels are learned fromthe /\fv e textual feature sets that include either the feature O/-P or O/-P/* /(recall Figure /2and T able /3/)/. These mo dels p erform signi/\fcan tly b etter than all of the remaining learnedtextual mo dels/.Figure /6 sho ws the b est p erforming learned textual mo dels/. Note the similarit y to theman ually deriv ed mo del/. As with the proso dic results/, the b est p erforming single featuremo dels p erform comparably to those learned from m ultiple features/. In fact/, in cgrendel /,the rulesets learned from the m ultiple feature sets ortho gr aphy and pr e c e ding are iden ticalto the rulesets learned from the single features O/-P and O/-P/*/, ev en though more featuresw ere a v ailable for use/. /(The corresp onding error rates in T able /6 are not iden tical due to the/1/3/. The di/\u000beren t feature v alues in the t w o /\fgures re/\rect the fact that phrasal p osition w as represen ted inthe /\\no w/\" corpus using sym b olic v alues /(as in Figure /1/)/, and in the m ultiple cue phrase corpus usingin tegers /(as in Figure /2/)/./1/4/. T ests suc h as /\\feature /\u0015 x/\" and /\\feature /\u0014 y/\" are merged in the /\fgure for simplici t y /, e/.g/./, /\\y /\u0015 feature/\u0015 x/./\"/7/4Cue Phrase Classifica tion Using Ma chine LearningMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)C/-P /(/4/0/./7 /\u0006 /6/./2/) /(/4/0/./2 /\u0006 /4/./5/)C/-S /(/4/1/./3 /\u0006 /5/./9/) /(/3/9/./8 /\u0006 /4/./2/)O/-P /2/0/./6 /\u0006 /5/./7 /1/7/./6 /\u0006 /3/./3O/-P/* /1/8/./4 /\u0006 /3/./7 /1/7/./2 /\u0006 /2/./4O/-S /(/3/4/./1 /\u0006 /6/./3/) /(/3/0/./2 /\u0006 /1/./8/)O/-S/* /(/3/5/./2 /\u0006 /5/./5/) /(/3/2/./6 /\u0006 /3/./0/)POS /(/3/7/./7 /\u0006 /4/./1/) /(/3/8/./2 /\u0006 /4/./6/)text /1/8/./8 /\u0006 /4/./2 /1/9/./0 /\u0006 /3/./6adjacency /(/3/9/./7 /\u0006 /5/./7/) /(/4/0/./2 /\u0006 /3/./4/)orthograph y /1/8/./9 /\u0006 /3/./4 /1/8/./8 /\u0006 /3/./0preceding /1/8/./8 /\u0006 /3/./8 /1/7/./6 /\u0006 /3/./2succeeding /(/3/3/./9 /\u0006 /6/./0/) /(/3/0/./0 /\u0006 /2/./7/)man ual textual /1/9/./9 /\u0006 /2/./8 /1/6/./1 /\u0006 /3/./4T able /6/: /9/5/%/-con/\fdence in terv als for the error rates /(/%/) of all cgrendel textual classi/\f/-cation mo dels/, testing data/. /(T raining and testing w ere done from the m ultiplecue phrase corpus using cross/-v alidation/./)Man ually deriv ed textual mo del /(rep eated from Figure /1/) /:if preceding orthograph y /= true then disc ourseelseif preceding orthograph y /= false then sententialDecision tree learned from O/-P/*/, from text/, from orthograph y /, and from preceding using C/4/./5 /:if preceding orthograph y/* /= NA then disc ourseelseif preceding orthograph y /* /= false then sententialelseif preceding orthograph y /* /= true then disc ourseRuleset learned from O/-P /, from O/-P/*/, from orthograph y /, and from preceding using CGRENDEL /:if preceding orthograph y/* /= false then sententialdefault is on disc ourseRuleset learned from text using CGRENDEL /:if preceding orthograph y/* /= false then sententialif part/-of/-sp eec h /= article then sententialdefault is on disc ourseFigure /6/: Example C/4/./5 and cgrendel classi/\fcation mo dels learned from di/\u000beren t textualfeature represen tations of the classi/\fable cue phrases in the m ultiple cue phrasecorpus/.estimation using cross/-v alidation/./) The cgrendel mo del text also incorp orates the featurep art/-of/-sp e e ch /. In C/4/./5/, the mo dels text /, ortho gr aphy and pr e c e ding are all iden tical to O/-P/*/./4/./2/./3 Pr osodic//Textual ModelsT able /7 presen ts the error rates of the classi/\fcation mo dels learned b y cgrendel when thedata is represen ted using sp e e ch/-text /, the complete set of proso dic and textual features /(recall/7/5LitmanMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)sp eec h/-text /1/5/./9 /\u0006 /3/./2 /1/4/./6 /\u0006 /4/./6man ual proso dic /2/4/./6 /\u0006 /3/./0 /1/4/./7 /\u0006 /3/./2man ual textual /1/9/./9 /\u0006 /2/./8 /1/6/./1 /\u0006 /3/./4T able /7/: /9/5/%/-con/\fdence in terv als for the error rates /(/%/) of the cgrendel proso dic//textualclassi/\fcation mo del/, testing data/. /(T raining and testing w ere done from the m ul/-tiple cue phrase corpus using cross/-v alidation/./)T able /3/)/. Since Hirsc h b erg and Litman did not dev elop a similar classi/\fcation mo del thatcom bined b oth t yp es of features/, for comparison the last t w o ro ws sho w the error rates ofthe separate proso dic and textual mo dels/. Only when the learned mo del is compared to theman ual proso dic mo del/, using the classi/\fable cue phrases for testing/, do es learning result ina signi/\fcan t p erformance impro v emen t/. This is consisten t with the results discussed ab o v e/,where sev eral learned proso dic mo dels p erformed b etter than the man ually deriv ed proso dicmo del in this test set/. The p erformance of sp e e ch/-text is not signi/\fcan tly b etter or w orsethan the p erformance of either the b est proso dic or textual learned mo dels /(T ables /5 and /6/,resp ectiv ely/)/.Figure /7 sho ws the C/4/./5 and cgrendel h yp otheses learned from sp e e ch/-text /. The C/4/./5mo del classi/\fes cue phrases using the proso dic and textual features that p erformed b est inisolation /( p osition in intonational phr ase and pr e c e ding ortho gr aphy/* /, as discussed ab o v e/)/, inconjunction with the additional feature length of interme diate phr ase /(whic h also app earsin the mo del learned from pr oso dy in Figure /5/)/. Lik e line /(/9/) in the man ually deriv edtextual mo del/, the learned mo del asso ciates the presence of preceding orthograph y withthe class disc ourse /. Unlik e line /(/1/0/)/, ho w ev er/, cue phrases not preceded b y orthograph yma y b e classi/\fed as either disc ourse or sentential /, based on proso dic feature v alues /(whic hw ere not a v ailable for use b y the textual mo del/)/. The branc h of the learned decision treecorresp onding to the last three lines is also similar to lines /(/1/)/, /(/2/)/, and /(/8/) of the man uallyderiv ed proso dic mo del/. /(Recall that a length v alue of /1 is equiv alen t to a comp osition v aluealone /./)The cgrendel mo del uses similar features to those used b y C/4/./5 as w ell as the proso dicfeature ac c ent /(also used in pr oso dy in Figure /5/)/, and the textual features p art/-of/-sp e e ch/(also used in text in Figure /6/) and pr e c e ding cue phr ase /. Lik e C/4/./5/, and unlik e line /(/1/0/)of the man ually deriv ed textual mo del/, the cgrendel mo del classi/\fes cue phrases lac kingpreceding orthograph y as sentential only in conjunction with certain other feature v alues/.Unlik e line /(/9/) in the man ual mo del/, the learned mo del also classi/\fes some cue phrases withpreceding orthograph y as sentential /(if the orthograph y is a comma/, and other feature v aluesare presen t/)/. Finally /, the third and /\ffth learned rules elab orate line /(/6/) with additionalproso dic as w ell as textual features/, while the /\frst and last learned rules elab orate line /(/8/)/./4/./3 Exp erimen t Set /3/: Adding the F eature tokenEac h exp erimen t in the third group replicates an exp erimen t from the second group/, withthe exception that the data represen tation no w also includes the lexical feature token from/7/6Cue Phrase Classifica tion Using Ma chine LearningMan ually deriv ed proso dic mo del /(rep eated from Figure /1/) /:if comp osition of in termediat e phrase /= alone then disc ourse /(/1/)elseif comp osition of in termediate phrase /= /: alone then /(/2/)if p osition in in termedia te phrase /= /\frst then /(/3/)if accen t /= deaccen ted then disc ourse /(/4/)elseif accen t /= L/* then disc ourse /(/5/)elseif accen t /= H/* then sentential /(/6/)elseif accen t /= complex then sentential /(/7/)elseif p osition in in termediate phrase /= /: /\frst then sentential /(/8/)Man ually deriv ed textual mo del /(rep eated from Figure /1/) /:if preceding orthograph y /= true then disc ourse /(/9/)elseif preceding orthograph y /= false then sentential /(/1/0/)Decision tree learned from sp eec h/-text using C/4/./5 /:if p osition in in tonational phrase /\u0014 /1 thenif preceding orthograph y/* /= NA then disc ourseelseif preceding orthograph y/* /= true then disc ourseelseif preceding orthograph y/* /= false thenif length of in termediate phrase /> /1/2 then disc ourseelseif length of in termediat e phrase /\u0014 /1/2 thenif length of in termediat e phrase /\u0014 /1 then disc ourseelseif length of in termediate phrase /> /1 then sententialelseif p osition in in tonationa l phrase /> /1 thenif length of in termediat e phrase /\u0014 /1 then disc ourseelseif length of in termedia te phrase /> /1 then sententialRuleset learned from sp eec h/-text using CGRENDEL /:if /(preceding orthograph y /= false/) /^ /(/4 /\u0014 p osition in in tonationa l phrase /\u0014 /6/) /^ then sententialif /(preceding orthograph y /= false/) /^ /(length of in termedia te phrase /\u0015 /2/) then sententialif /(preceding orthograph y /= false/) /^ /(length of in tonation al phrase /\u0015 /7/) /^ /(preceding cue phrase /= NA/)/^ /(accen t /= H/*/) then sententialif /(preceding orthograph y /= comma/) /^ /(length of in termedia te phrase /\u0015 /5/) /^ /(length of in tonation al phrase /\u0014 /1/7/)/^ /(part/-of/-sp e ec h /= adv erb/) then sententialif /(preceding orthograph y /= comma/) /^ /(/3 /\u0014 length of in tonational phrase /\u0014 /8/) /^ /(accen t /= H/*/) then sententialif /(preceding orthograph y /= comma/) /^ /(/3 /\u0014 length of in termediate phrase /\u0014 /8/)/^ /(length of in tonational phrase /\u0015 /1/5/) then sententialif /(p osition in in tonationa l phrase /\u0015 /2/) /^ /(length of in termediate phrase /\u0015 /2/)/^ /(preceding cue phrase /= NA/) then sententialdefault is on disc ourseFigure /7/: C/4/./5 and cgrendel classi/\fcation mo dels learned from the proso dic//textual fea/-ture represen tation of the classi/\fable cue phrases in the m ultiple cue phrase cor/-pus/./7/7LitmanMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)P/-L/+ /2/1/./8 /\u0006 /4/./6 /1/7/./4 /\u0006 /2/./7P/-P/+ /1/6/./7 /\u0006 /2/./8 /1/4/./8 /\u0006 /5/./0I/-L/+ /2/0/./3 /\u0006 /3/./4 /1/6/./0 /\u0006 /3/./3I/-P/+ /2/5/./1 /\u0006 /4/./1 /1/7/./0 /\u0006 /3/./6I/-C/+ /2/7/./0 /\u0006 /3/./6 /1/8/./4 /\u0006 /3/./4A/+ /1/9/./8 /\u0006 /3/./2 /1/2/./8 /\u0006 /3/./1A/*/+ /1/8/./6 /\u0006 /3/./8 /1/5/./4 /\u0006 /2/./8proso dy/+ /1/6/./7 /\u0006 /2/./9 /1/5/./8 /\u0006 /3/./1hl/9/3feature s/+ /2/4/./0 /\u0006 /4/./5 /1/7/./4 /\u0006 /4/./3phrasing/+ /1/4/./5 /\u0006 /3/./3 /1/2/./6 /\u0006 /3/./3length/+ /1/8/./6 /\u0006 /2/./0 /1/6/./2 /\u0006 /3/./5p osition/+ /1/5/./6 /\u0006 /3/./3 /1/3/./0 /\u0006 /3/./9in tonationa l/+ /1/5/./1 /\u0006 /2/./2 /1/6/./6 /\u0006 /4/./6in termedia te/+ /1/8/./5 /\u0006 /3/./7 /1/6/./6 /\u0006 /4/./0man ual proso dic /2/4/./6 /\u0006 /3/./0 /1/4/./7 /\u0006 /3/./2T able /8/: /9/5/%/-con/\fdence in terv als for the error rates /(/%/) of all cgrendel proso dic/, tok/-enize d classi/\fcation mo dels/, testing data/. /(T raining and testing w ere done fromthe m ultiple cue phrase corpus using cross/-v alidation/./)Figure /2/. These exp erimen ts in v estigate ho w p erformance c hanges when classi/\fcation mo d/-els are allo w ed to treat di/\u000beren t cue phrases di/\u000beren tly /. As will b e seen/, learning fromtok enized feature sets often further impro v es the p erformance of the learned classi/\fcationmo dels/. In addition/, the classi/\fcation mo dels no w con tain new linguistic information re/-garding particular tok ens /(e/.g/./, /\\so/\"/)/./4/./3/./1 Pr osodic ModelsT able /8 presen ts the error of the learned classi/\fcation mo dels on b oth test sets from them ultiple cue phrase corpus/, for eac h of the tokenize d proso dic feature sets/. Again/, the errorrates in italics indicate that the p erformance of the learned classi/\fcation mo del meaningfullyexceeds the p erformance of the /\\man ual proso dic/\" mo del /(whic h did not consider the featuretoken /)/.One w a y that the impro v emen t obtained b y adding the feature token can b e seen is b ycomparing the p erformance of the learned and man ually deriv ed mo dels/. In T able /8/, sixcgrendel classi/\fcation mo dels ha v e lo w er /(italicized/) error rates than the man ual mo del/.In T able /5/, only /\fv e of these mo dels are italicized/. Th us/, adding the feature token resultsin an additional learned mo del /- length/+ /- outp erforming the man ually deriv ed mo del/.Con v ersely /, in T able /8/, no learned mo dels p erform signi/\fcan tly w orse than the man uallyderiv ed man ual/. In con trast/, in T able /5/, sev eral non/-tok enized mo dels p erform w orse thanthe man ual mo del /(I/-C in the larger test set/, and P/-L/, I/-L/, I/-C/, A/, A/*/, and length in thenon/-conjunct test set/)/.The impro v emen t obtained b y adding the feature token can also b e seen b y comparingthe p erformance of the tok enized /(T able /8/) and non/-tok enized /(T able /5/) v ersions of eac hmo del to eac h other/. F or con v enience/, cases where tok enization yields impro v emen t arehighligh ted in T able /9/. The table sho ws that the error rate of the tok enized v ersions of thefeature sets is signi/\fcan tly lo w er than the error of the non/-tok enized v ersions/, for P/-L/, I/-C/,/7/8Cue Phrase Classifica tion Using Ma chine LearningMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)Non/-T ok enized T ok enized /(/+/) Non/-T ok enized T ok enized /(/+/)P/-L /3/3/./0 /\u0006 /5/./9 /2/1/./8 /\u0006 /4/./6 /3/3/./2 /\u0006 /1/./9 /1/7/./4 /\u0006 /2/./7I/-L /- /- /2/5/./6 /\u0006 /2/./8 /1/6/./0 /\u0006 /3/./3I/-C /3/6/./5 /\u0006 /5/./4 /2/7/./0 /\u0006 /3/./6 /3/5/./2 /\u0006 /3/./4 /1/8/./4 /\u0006 /3/./4A /2/8/./6 /\u0006 /3/./6 /1/9/./8 /\u0006 /3/./2 /3/0/./2 /\u0006 /3/./1 /1/2/./8 /\u0006 /3/./1A/* /2/8/./3 /\u0006 /4/./3 /1/8/./6 /\u0006 /3/./8 /2/8/./4 /\u0006 /1/./7 /1/5/./4 /\u0006 /2/./8length /2/6/./1 /\u0006 /3/./8 /1/8/./6 /\u0006 /2/./0 /2/7/./4 /\u0006 /3/./4 /1/6/./2 /\u0006 /3/./5T able /9/: Cases where adding the feature token impro v es the p erformance of a proso dicmo del/.A/, A/*/, and length in b oth test sets/, and for I/-L in only the non/-conjunct test set/. Note theo v erlap b et w een the feature sets of T able /9 and those discussed in the previous paragraph/.Figure /8 sho ws sev eral tok enized single feature proso dic classi/\fcation mo dels/. The /\frstcgrendel mo del in the /\fgure sho ws the ruleset learned from P/-L/+/, whic h reduces the/3/3/./2/% /\u0006 /1/./9/% error rate of P/-L /( length of intonational phr ase /) to /1/7/./4/% /\u0006 /2/./7/%/, whentrained and tested using the classi/\fable non/-conjuncts /(T able /9/)/. Note that the /\frst ruleuses only a proso dic feature /(lik e the rules of Exp erimen t Sets /1 and /2/)/, and is in factsimilar to line /(/1/) of the man ual mo del/. /(Recall that the length v alue /1 is equiv alen t tothe comp osition v alue alone /./) Ho w ev er/, unlik e the rules of the previous exp erimen t sets/,the next /5 rules use b oth the proso dic feature and the lexical feature token /. Also unlik ethe rules of the previous exp erimen t sets/, the remaining rules classify cue phrases usingonly the feature token /. Examination of the learned rulesets in Figures /8 and /9 sho ws thatthe same cue phrases often app ear in this last t yp e of rule/. Some of these cue phrases/, forexample/, /\\/\fnally/\"/, /\\ho w ev er/\"/, and /\\ok/\"/, are in fact alw a ys disc ourse usages in the m ultiplecue phrase corpus/. F or the other cue phrases/, classifying cue phrases using only tokencorresp onds to classifying cue phrases using their default class /(the most frequen t t yp e ofusage in the m ultiple cue phrase corpus/)/. Recall the use of a non/-tok enized default classmo del in T able /1/.The second example sho ws the ruleset learned from I/-C/+ /( c omp osition of interme diatephr ase/+ /)/. The /\frst rule corresp onds to line /(/1/) of the man ually deriv ed mo del/.\n/1/5Thenext six rules classify particular cue phrases as disc ourse /, indep enden tly of the v alue of I/-C/.Note that although in this mo del the cue phrase /\\sa y/\" is classi/\fed using only token /, in theprevious mo del a more sophisticated strategy for classifying /\\sa y/\" could b e found/.The third example sho ws the cgrendel ruleset learned from A/+ /( ac c ent/+ /)/. The /\frstrule corresp onds to line /(/5/) of the man ually deriv ed proso dic mo del/. In con trast to line/(/4/)/, ho w ev er/, cgrendel uses deaccen ting to predict disc ourse for only the tok ens /\\sa y/\"and /\\so/./\" If the tok en is /\\/\fnally/\"/, /\\ho w ev er/\"/, /\\no w/\" or /\\ok/\"/, disc ourse is assigned /(for allaccen ts/)/. In all other deaccen ted cases/, sentential is assigned /(using the default/)/. Similarly /,in con trast to line /(/7/)/, the complex accen t L/+H/* predicts disc ourse for the cue phrases/\\further/\" and /\\indeed/\" /(and also for /\\/\fnally/\"/, /\\ho w ev er/\"/, /\\no w/\" and /\\ok/\"/)/, and sententialotherwise/./1/5/. As discussed in relation to Figure /2/, the I/-C v alues only and only cue phr ases in the m ultiple cue phrasecorpus replace the v alue alone in the /\\no w/\" corpus/./7/9LitmanMan ually deriv ed proso dic mo del /(rep eated from Figure /1/) /:if comp osition of in termediat e phrase /= alone then disc ourse /(/1/)elseif comp osition of in termediate phrase /= /: alone then /(/2/)if p osition in in termedia te phrase /= /\frst then /(/3/)if accen t /= deaccen ted then disc ourse /(/4/)elseif accen t /= L/* then disc ourse /(/5/)elseif accen t /= H/* then sentential /(/6/)elseif accen t /= complex then sentential /(/7/)elseif p osition in in termediate phrase /= /: /\frst then sentential /(/8/)Ruleset learned from P/-L/+ using CGRENDEL /:if length of in tonational phrase /\u0014 /1 then disc ourseif /(/7 /\u0014 length of in tonationa l phrase /\u0014 /1/1/) /^ /(tok en /= although/) then disc ourseif /(/9 /\u0014 length of in tonationa l phrase /\u0014 /1/6/) /^ /(tok en /= indeed/) then disc ourseif /(length of in tonational phrase /\u0014 /2/0/) /^ /(tok en /= sa y/) then disc ourseif /(/1/1 /\u0014 length of in tonational phrase /\u0014 /1/3/) /^ /(tok en /= then/) then disc ourseif /(length of in tonational phrase /= /5/) /^ /(tok en /= w ell/) then disc ourseif tok en /= /\fnally then disc ourseif tok en /= further then disc ourseif tok en /= ho w ev er then disc ourseif tok en /= no w then disc ourseif tok en /= ok then disc ourseif tok en /= otherwise then disc ourseif tok en /= so then disc oursedefault is on sententialRuleset learned from I/-C/+ using CGRENDEL /:if comp osition of in termediat e phrase /= only then disc ourseif tok en /= /\fnally then disc ourseif tok en /= ho w ev er then disc ourseif tok en /= no w then disc ourseif tok en /= ok then disc ourseif tok en /= sa y then disc ourseif tok en /= so then disc oursedefault is on sententialRuleset learned from A/+ using CGRENDEL /:if accen t /= L/* then disc ourseif /(accen t /= deaccen ted/) /^ /(tok en /= sa y/) then disc ourseif /(accen t /= deaccen ted/) /^ /(tok en /= so/) then disc ourseif /(accen t /= L/+H/*/) /^ /(tok en /= further/) then disc ourseif /(accen t /= L/+H/*/) /^ /(tok en /= indeed/) then disc ourseif tok en /= /\fnally then disc ourseif tok en /= ho w ev er then disc ourseif tok en /= no w then disc ourseif tok en /= ok then disc oursedefault is on sententialFigure /8/: Example cgrendel classi/\fcation mo dels learned from di/\u000beren t tokenize d /,proso dic feature represen tations of the classi/\fable non/-conjuncts in the m ultiplecue phrase corpus/./8/0Cue Phrase Classifica tion Using Ma chine LearningMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)C/-P/+ /(/2/8/./2 /\u0006 /3/./9/) /1/6/./4 /\u0006 /4/./6C/-S/+ /(/2/8/./9 /\u0006 /3/./6/) /1/7/./2 /\u0006 /4/./0O/-P/+ /1/7/./5 /\u0006 /4/./4 /1/0/./0 /\u0006 /3/./1O/-P/*/+ /1/7/./7 /\u0006 /2/./9 /1/2/./2 /\u0006 /2/./9O/-S/+ /2/6/./9 /\u0006 /4/./7 /1/8/./4 /\u0006 /3/./9O/-S/*/+ /(/2/7/./3 /\u0006 /3/./5/) /1/6/./0 /\u0006 /3/./2POS/+ /(/2/7/./4 /\u0006 /3/./6/) /1/7/./2 /\u0006 /3/./9text/+ /1/8/./4 /\u0006 /3/./0 /1/2/./0 /\u0006 /2/./6adjacency/+ /(/2/8/./6 /\u0006 /4/./1/) /1/5/./2 /\u0006 /3/./1orthograph y/+ /1/7/./6 /\u0006 /3/./0 /1/3/./6 /\u0006 /3/./9preceding/+ /1/7/./0 /\u0006 /4/./1 /1/3/./6 /\u0006 /2/./6succeeding/+ /2/5/./6 /\u0006 /3/./9 /1/8/./0 /\u0006 /4/./5man ual textual /1/9/./9 /\u0006 /2/./8 /1/6/./1 /\u0006 /3/./4T able /1/0/: /9/5/%/-con/\fdence in terv als for the error rates /(/%/) of all cgrendel textual/, tok/-enize d classi/\fcation mo dels/, testing data/. /(T raining and testing w ere done fromthe m ultiple cue phrase corpus using cross/-v alidation/./)T o summarize/, new proso dic results of Exp erimen t Set /3 are that features relating tolength/, comp osition/, and accen t/, while not useful /(in isolation/) for predicting the classi/\f/-cation of all cue phrases/, are in fact quite useful for predicting the class of individual cuephrases or subsets of cue phrases/. /(Recall that the result of Exp erimen t Sets /1 and /2 w asthat without token /, only the proso dic feature p osition in intonational phr ase w as useful inisolation/./)/4/./3/./2 Textual ModelsT able /1/0 presen ts the error of the learned classi/\fcation mo dels on b oth test sets fromthe m ultiple cue phrase corpus/, for eac h of the tokenize d textual feature sets/. As in Exp er/-imen t Set /2 /(T able /6/)/, none of the cgrendel classi/\fcation mo dels ha v e lo w er /(italicized/)error rates than the man ual mo del/. Ho w ev er/, adding the feature token do es impro v e thep erformance of man y of the learned rulesets/, in that the follo wing mo dels /(unlik e theirnon/-tok enized coun terparts/) are no longer outp erformed b y the man ual mo del/: O/-S/+ andsuc c e e ding/+ in the larger test set/, and C/-P/+/, C/-S/+/, O/-S/+/, O/-S/*/+/, POS/+/, adjac ency/+ /,and suc c e e ding/+ in the non/-conjunct test set/.The impro v emen t obtained b y adding the feature token can also b e seen b y comparingthe p erformance of the tok enized /(T able /1/0/) and non/-tok enized /(T able /6/) v ersions of eac hmo del to eac h other/, as sho wn in T able /1/1/. The table sho ws that the error rates of thetok enized v ersions of the feature sets are signi/\fcan tly lo w er than the error of the non/-tok enized v ersions/, for C/-P /, C/-S/, POS/, and adjac ency in b oth test sets/, and for O/-P /, O/-S/,O/-S/*/, text /, and suc c e e ding in the non/-conjunct test set/. Note the o v erlap b et w een the featuresets of T able /1/1 and those discussed in the previous paragraph/.Figure /9 sho ws sev eral tok enized single textual feature classi/\fcation mo dels/. The /\frstcgrendel mo del sho ws the ruleset learned from C/-P/+ /( pr e c e ding cue phr ase/+ /)/, whic hreduces the /4/0/./2/% /\u0006 /4/./5/% error rate of C/-P to /1/6/./4/% /\u0006 /4/./6/% when trained and tested usingthe classi/\fable non/-conjuncts /(T able /1/1/)/. This ruleset correlates preceding cue phrases withdiscourse usages of /\\indeed/\"/, and omitted transcriptions of /\\further/\"/, /\\no w/\"/, and /\\so/\" with/8/1LitmanMan ually deriv ed textual mo del /(rep eated from Figure /1/) /:if preceding orthograph y /= true then disc ourseelseif preceding orthograph y /= false then sententialRuleset learned from C/-P/+ using CGRENDEL /:if /(preceding cue phrase /= true/) /^ /(tok en /= indeed/) then disc ourseif /(preceding cue phrase /= NA/) /^ /(tok en /= further/) then disc ourseif /(preceding cue phrase /= NA/) /^ /(tok en /= no w/) then disc ourseif /(preceding cue phrase /= NA/) /^ /(tok en /= so/) then disc ourseif tok en /= although then disc ourseif tok en /= /\fnally then disc ourseif tok en /= ho w ev er then disc ourseif tok en /= ok then disc ourseif tok en /= sa y then disc ourseif tok en /= similarly then disc oursedefault is on sententialRuleset learned from O/-P/+ using CGRENDEL /:if preceding orthograph y /= false then sententialif /(preceding orthograph y /= comma/) /^ /(tok en /= then/) then sententialdefault is on disc ourseRuleset learned from O/-S/+ using CGRENDEL /:if succeeding orthograph y /= comma then disc ourseif /(succeeding orthograph y /= false/) /^ /(tok en /= so/) then disc ourseif succeeding orthograph y /= NA then disc ourseif tok en /= although then disc ourseif tok en /= /\fnally then disc ourseif tok en /= no w then disc ourseif tok en /= ok then disc ourseif tok en /= sa y then disc oursedefault is on sententialRuleset learned from POS/+ using CGRENDEL /:if /(part/-of/-sp eec h /= adv erb/) /^ /(tok en /= /\fnally/) then disc ourseif /(part/-of/-sp eec h /= singular prop er noun/) /^ /(tok en /= further/) then disc ourseif /(part/-of/-sp eec h /= adv erb/) /^ /(tok en /= ho w ev er/) then disc ourseif /(part/-of/-sp eec h /= adv erb/) /^ /(tok en /= indeed/) then disc ourseif /(part/-of/-sp eec h /= sub ordinatin g conjunctio n/) /^ /(tok en /= so/) then disc ourseif tok en /= although then disc ourseif tok en /= no w then disc ourseif tok en /= sa y then disc ourseif tok en /= ok then disc oursedefault is on sententialFigure /9/: Example cgrendel classi/\fcation mo dels learned from di/\u000beren t tokenize d /, textualfeature represen tations of the classi/\fable non/-conjuncts in the m ultiple cue phrasecorpus/./8/2Cue Phrase Classifica tion Using Ma chine LearningMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)Non/-T ok enized T ok enized /(/+/) Non/-T ok enized T ok enized /(/+/)C/-P /4/0/./7 /\u0006 /6/./2 /2/8/./2 /\u0006 /3/./9 /4/0/./2 /\u0006 /4/./5 /1/6/./4 /\u0006 /4/./6C/-S /4/1/./3 /\u0006 /5/./9 /2/8/./9 /\u0006 /3/./6 /3/9/./8 /\u0006 /4/./2 /1/7/./2 /\u0006 /4/./0O/-P /- /- /1/7/./6 /\u0006 /3/./3 /1/0/./0 /\u0006 /3/./1O/-S /- /- /3/0/./2 /\u0006 /1/./8 /1/8/./4 /\u0006 /3/./9O/-S/* /- /- /3/2/./6 /\u0006 /3/./0 /1/6/./0 /\u0006 /3/./2POS /3/7/./7 /\u0006 /4/./1 /2/7/./4 /\u0006 /3/./6 /3/8/./2 /\u0006 /4/./6 /1/7/./2 /\u0006 /3/./9text /- /- /1/9/./0 /\u0006 /3/./6 /1/2/./0 /\u0006 /2/./6adjacency /3/9/./7 /\u0006 /5/./7 /2/8/./6 /\u0006 /4/./1 /4/0/./2 /\u0006 /3/./4 /1/5/./2 /\u0006 /3/./1succeeding /- /- /3/0/./0 /\u0006 /2/./7 /1/8/./0 /\u0006 /4/./5T able /1/1/: Cases where adding the feature token impro v es the p erformance of a textualmo del/.discourse usages/. The classi/\fcations for the rest of the cue phrases are predicted using onlythe feature tok en/.The second example sho ws the cgrendel ruleset learned from O/-P/+ /( pr e c e ding ortho g/-r aphy/+ /)/. This ruleset correlates no preceding orthograph y with sen ten tial usages of cuephrases /(as in b oth the man ually deriv ed mo del and the learned mo dels from Exp erimen tSet /2/)/. Unlik e those mo dels/, ho w ev er/, the cue phrase /\\then/\" is also classi/\fed as sentential /,ev en when it is preceded b y orthograph y /(namely /, b y a comma/)/.The third example sho ws the cgrendel ruleset learned from O/-S/+ /( suc c e e ding ortho g/-r aphy /)/. This ruleset correlates the presence of succeeding commas with discourse usages ofcue phrases/, except for the cue phrase /\\so/\"/, whic h is classi/\fed as a discourse usage withoutan y succeeding orthograph y /. The mo del also correlates cue phrases that w ere omitted fromthe transcript with discourse usages/. The classi/\fcations for the rest of the cue phrases arepredicted using only the feature token /.The last example sho ws the cgrendel ruleset learned from POS/+ /( p art/-of/-sp e e ch/+ /)/.This ruleset classi/\fes certain cue phrases as discourse usages dep ending on b oth p art/-of/-sp e e ch and token /, as w ell as indep enden tly of p art/-of/-sp e e ch /.Finally /, Figure /1/0 sho ws the classi/\fcation mo del learned from text/+ /, the largest tok/-enized textual feature set/. Note that three of the four features used in the tok enized/, singletextual feature mo dels of Figure /9 are incorp orated in to this tok enized/, m ultiple textualfeature mo del/.T o summarize/, new textual results of Exp erimen t Set /3 are that features based on adja/-cen t cue phrases/, succeeding orthograph y /, and part/-of/-sp eec h/, while not useful /(in isolation/)for predicting the classi/\fcation of all cue phrases/, are in fact quite useful in conjunction withonly the feature token /. /(Recall that the result of Exp erimen t Set /2 w as that without token /,only the textual features pr e c e ding ortho gr aphy and pr e c e ding ortho gr aphy/* w ere useful inisolation/./)/4/./3/./3 Pr osodic//Textual ModelsT able /1/2 presen ts the error rates of the classi/\fcation mo dels learned b y cgrendelwhen the data is represen ted using sp e e ch/-text/+ /, the complete set of proso dic and textual/8/3LitmanRuleset learned from text/+ using CGRENDEL /:if preceding orthograph y /= false then sententialif /(preceding orthograph y /= comma/) /^ /(tok en /= although/) then sententialif /(preceding orthograph y /= comma/) /^ /(tok en /= no/) then sententialif /(preceding orthograph y /= comma/) /^ /(tok en /= then/) then sententialif /(succeeding orthograph y /= false/) /^ /(preceding cue phrase /= NA/) /^ /(tok en /= similarly/) then sententialif tok en /= actually then sententialif tok en /= /\frst then sententialif tok en /= since then sententialif tok en /= y es then sententialdefault is on disc ourseFigure /1/0/: cgrendel classi/\fcation mo del learned from a tok enized/, multiple textual featurerepresen tation of the classi/\fable non/-conjuncts in the m ultiple cue phrase corpus/.Mo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)sp eec h/-text /+ /1/6/./9 /\u0006 /3/./4 /1/6/./6 /\u0006 /4/./1man ual proso dic /2/4/./6 /\u0006 /3/./0 /1/4/./7 /\u0006 /3/./2man ual textual /1/9/./9 /\u0006 /2/./8 /1/6/./1 /\u0006 /3/./4T able /1/2/: /9/5/%/-con/\fdence in terv als for the error rates /(/%/) of the cgrendelproso dic//textual/, tokenize d classi/\fcation mo dels/, testing data/. /(T raining andtesting w ere done from the m ultiple cue phrase corpus using cross/-v alidation/./)features/. As in Exp erimen t Set /2/, the p erformance of sp e e ch/-text/+ is not b etter than thep erformance of either the b est learned /(tok enized/) proso dic or textual mo dels /(T ables /8and /1/0/, resp ectiv ely/)/.Comparison of T ables /7 and /1/2 also sho ws that for the feature set sp e e ch/-text /, tok eniza/-tion do es not impro v e p erformance/. This is in con trast to the proso dic and textual featuresets/, where tok enization impro v es the p erformance of man y learned mo dels /(namely thosesho wn in T ables /9 and /1/1/)/./4/./4 Exp erimen t Set /4/: Adding the Classi/\fcation ambiguousIn practice/, a cue phrase classi/\fcation mo del will ha v e to classify al l the cue phrases in arecording or text/, not just those that are /\\classi/\fable/./\" The exp erimen ts in the fourth setreplicate the exp erimen ts in Exp erimen t Sets /1/, /2/, and /3/, with the exception that all /9/5/3 cuephrases in the m ultiple cue phrase corpus are no w used/. This means that cue phrases areno w classi/\fed as disc ourse /, sentential /, as w ell as unknown /(de/\fned in T able /2/)/. Exp erimen tSet /4 in v estigates whether mac hine learning can explicitly recognize the new class unknown /.Recall that the studies of Hirsc h b erg and Litman did not attempt to predict the classunknown /, as it did not o ccur in their /\\no w/\" training corpus/. Th us in Exp erimen t Set /1/, theclass unknown similarly can not b e learned from the training data/. Ho w ev er/, the unknownexamples can b e added to the testing data of Exp erimen t Set /1/. Ob viously p erformance willdegrade/, as the mo dels m ust incorrectly classify eac h unknown example as either disc ourse/8/4Cue Phrase Classifica tion Using Ma chine Learningor sentential /. F or example/, when tested on the full corpus of /9/5/3 example cue phrases/,the /9/5/% con/\fdence in terv als for the error rates of P/-P and intonational are /2/4/./8/% /\u0006 /2/./8/%/;recall that when tested on the subset of the corpus corresp onding to the /8/7/8 classi/\fable cuephrases/, the error w as /1/8/./3/% /\u0006 /2/./6/% /(T able /4/)/.Unfortunately /, the results of rerunning Exp erimen t Sets /2 and /3 do not sho w promisingresults for classifying cue phrases as unknown /. Despite the presence of /7/5 examples ofunknown /, most of the learned mo dels still classify unknown cue phrases as only disc ourse orsentential /. F or example/, when cgrendel is used for learning/, only /2 of the p ossible /2/7 non/-tok enized mo dels\n/1/6/( phr asing and sp e e ch/-text /) con tain rules that predict the class unknown /.F urthermore/, eac h of these mo dels only con tains one rule for unknown /, and eac h of theserules only applies to /2 of the p ossible /9/5/3 examples/! Similarly /, only four of the p ossible /2/7tok enized mo dels /( length/+ /, phr asing/+ /, pr oso dy/+ /, and sp e e ch/-text/+ /) con tain at least one rulefor the class unknown /. When compared to training and testing using only the classi/\fablecue phrases in the corpus/, the error rate on the full corpus is t ypically /(but not alw a ys/)signi/\fcan tly higher/. The b est p erforming mo del in Exp erimen t Set /4 is sp e e ch/-text/+ /, witha /2/2/./4/% /\u0006 /4/./1/% error rate /(/9/5/% con/\fdence in terv al/)/.In sum/, Exp erimen t Set /4 addressed a problem that w as previously unexplored in theliterature /- the abilit y to dev elop classi/\fcation mo dels that predict not only disc ourse andsentential usages of cue phrases/, but also usages whic h h uman judges /\fnd di/\u000ecult to classify /.Unfortunately /, the results of the exp erimen ts suggest that learning ho w to classify cuephrases as unknown is a di/\u000ecult problem/. P erhaps with more training data /(recall thatthere are only /7/5 examples of unknown /) or with additional features b etter results could b eobtained/./4/./5 DiscussionThe exp erimen tal results suggest that mac hine learning is a useful to ol for b oth automatingthe generation of classi/\fcation mo dels and impro ving up on man ually deriv ed results/. InExp erimen t Sets /1 and /2 the p erformance of man y of the learned classi/\fcation mo dels iscomparable to the p erformance of the man ually deriv ed mo dels/. In addition/, when testedon the classi/\fable cue phrases/, sev eral learned proso dic classi/\fcation mo dels /(as w ell asthe learned proso dic//textual mo del/) outp erform Hirsc h b erg and Litman/'s man ually deriv edproso dic mo del/. Exp erimen t Set /3 sho ws that learning from tok enized feature sets ev enfurther impro v es p erformance/, esp ecially in the non/-conjunct test set/. More tok enized thannon/-tok enized learned mo dels p erform at least as w ell as the man ually deriv ed mo dels/.Man y tok enized learned mo dels also outp erform their non/-tok enized coun terparts/.While the textual classi/\fcation mo dels do not outp erform the b etter proso dic classi/\fca/-tion mo dels/, they ha v e the adv an tage that the textual feature v alues are obtained directlyfrom the transcript/, while determining the v alues of proso dic features requires man ual anal/-ysis/. /(See/, ho w ev er/, Section /5 for a discussion of the feasibilit y of automating the proso dicanalysis/. In addition/, a transcript ma y not alw a ys b e a v ailable/./) On the other hand/, almostall the high p erforming textual mo dels are dep enden t on orthograph y /. While man ual tran/-/1/6/. Recall that Exp erimen t Sets /2 and /3 constructed /1/4 proso dic mo dels/, /1/2 textual mo dels/, and /1proso dic//textual mo del/./8/5Litmanscriptions of proso dic features ha v e b een sho wn to b e reliable across co ders /(Pitrelli et al/./,/1/9/9/4/)/, there are no corresp onding results for the reliabilit y of orthograph y /.Examination of the b est p erforming learned mo dels sho ws that they are often compa/-rable in con ten t to the relev an t p ortions of the man ually deriv ed mo dels/. Examinationof the mo dels also pro vides new con tributions to the cue phrase literature/. F or example/,Exp erimen t Sets /1 and /2 demonstrate the utilit y of classifying cue phrases based on only asingle proso dic feature /- phrasal p osition/.\n/1/7Exp erimen t Set /2 also demonstrates the utilit yof the proso dic feature length and the textual feature pr e c e ding cue phr ase for classifyingcue phrases /- in conjunction with other proso dic and textual features/. Finally /, the results ofExp erimen t Set /3 demonstrate that ev en though man y features are not useful b y themselv esfor classifying all cue phrases/, they ma y nonetheless b e v ery informativ e in their tokenize dform/. This is true for the proso dic features based on phrasal length/, phrasal comp osition/,and accen t/, and for the textual features based on adjacen t cue phrases/, succeeding p osition/,and part/-of/-sp eec h/.\n/1/8/5/. Utilit yThe results of the mac hine learning exp erimen ts are quite promising/, in that when comparedto man ually deriv ed classi/\fcation mo dels already in the literature/, the learned classi/\fcationmo dels often p erform with comparable if not higher accuracy /. Th us/, mac hine learningapp ears to b e an e/\u000bectiv e tec hnique for automating the generation of classi/\fcation mo dels/.Ho w ev er/, giv en that the exp erimen ts rep orted here still rely on man ually created trainingdata/, a discussion of the practical utilit y of the results is in order/.Ev en giv en man ually created training data/, the results established b y Hirsc h b erg andLitman /(/1/9/9/3/) /- obtained using ev en less automation than the exp erimen ts of this pap er/- are already ha ving practical imp ort/. In particular/, the man ually deriv ed cue phraseclassi/\fcation mo dels are used to impro v e the naturalness of the syn thetic sp eec h in a text/-to/-sp eec h system /(Hirsc h b erg/, /1/9/9/0/)/. Using the text/-based mo del/, the text/-to/-sp eec h systemclassi/\fes eac h cue phrase in a text to b e syn thesized as either a discourse or sen ten tialusage/. Using the proso dic mo del/, the system then con v eys this usage b y syn thesizing thecue phrase with the appropriate t yp e of in tonation/. The sp eec h syn thesis could b e furtherimpro v ed /(and the output made more v aried/) b y using an y one of the higher p erforminglearned proso dic mo dels presen ted in this pap er/.The results of this pap er could also b e directly applied in the area of text generation/.F or example/, Moser and Mo ore /(/1/9/9/5/) are concerned with the implemen tation of cue selec/-tion and placemen t strategies in natural language generation systems/. Suc h systems couldb e enhanced b y using the text/-based mo dels of cue phrase classi/\fcation /(particularly the/1/7/. The empirical studies p erformed b y Holte /(/1/9/9/3/) sho w that for man y other datasets/, the accuracy ofsingle feature rules and decision trees is often comp etitiv e with the accuracy of more complex learnedmo dels/./1/8/. In con trast/, the proso dic features phrasal comp osition and accen t w ere previously kno wn to b e usefulin c onjunction with eac h other and with phrasal p osition /(Hirsc h b erg /& Litman/, /1/9/9/3/)/, while part/-of/-sp eec h w as kno wn to b e useful only in conjunction with orthograph y /(Hirsc h b erg /& Litman/, /1/9/9/3/)/.Length/, adjacen t cue phrases/, and succeeding p osition w ere not used in either of the man ually deriv edmo dels /(Hirsc h b erg /& Litman/, /1/9/9/3/) /(although length and adjacen t cue phrases w ere sho wn to b e useful/- again only in conjunction with other proso dic and textual features /- in Exp erimen t Set /2/)/./8/6Cue Phrase Classifica tion Using Ma chine Learningtok enized mo dels/) to additionally sp ecify preceding and succeeding orthograph y /, part/-of/-sp eec h/, and adjacen t cue phrases that are appropriate for discourse usages/.Finally /, if the results of this pap er could b e fully automated/, they could also b e used innatural language understanding systems/, b y enhancing their abilit y to recognize discoursestructure/. The results obtained b y Litman and P assonneau /(/1/9/9/5/) and P assonneau andLitman /(in press/) suggest that algorithms that use cue phrases /(in conjunction with otherfeatures/) to predict discourse structure outp erform algorithms that do not tak e cue phrasesin to accoun t/. In particular/, Litman and P assonneau dev elop sev eral algorithms that exploreho w features of cue phrases/, proso dy and referen tial noun phrases can b e b est com binedto predict discourse structure/. Quan titativ e ev aluations of their results sho w that the b estp erforming algorithms all incorp orate the use of discourse usages of cue phrases /(where cuephrases are classi/\fed as discourse using only phrasal p osition/)/. As discussed in Section /1/,discourse structure is useful for p erforming tasks suc h as anaphora resolution and planrecognition/. Recen t w ork has also sho wn that if discourse structure can b e recognized/, itcan b e used to impro v e retriev al of text /(Hearst/, /1/9/9/4/) and sp eec h /(Sti/\reman/, /1/9/9/5/)/.Although the proso dic features w ere man ually lab eled b y Hirsc h b erg and Litman/, thereare recen t results suggesting that at least some asp ects of proso dy can b e automaticallylab eled directly from sp eec h/. F or example/, Wigh tman and Ostendorf /(/1/9/9/4/) dev elop analgorithm that is able to automatically recognize proso dic phrasing with /8/5/-/8/6/% accuracy/(measured b y comparing automatically deriv ed lab els with hand/-mark ed lab els/)/; this accu/-racy is only sligh tly less than h uman/-h uman accuracy /. Recall that the exp erimen tal resultsof this pap er sho w that mo dels learned from the single feature p osition in intonationalphr ase /- whic h could b e automatically computed giv en suc h an automatic proso dic phras/-ing algorithm /- p erform at least as w ell as an y other learned proso dic mo del/. Similarly /,accen ting v ersus deaccen ting can b e automatically lab eled with /8/8/% accuracy /(Wigh tman/& Ostendorf/, /1/9/9/4/)/, while a more sophisticated lab eling sc heme that distinguishes b et w eenfour t yp es of accen t classes /(and is somewhat similar to the proso dic feature ac c ent/* usedin this pap er/) can b e lab eled with /8/5/% accuracy /(Ostendorf /& Ross/, in press/)/. Recall fromExp erimen t Set /3 that the tok enized mo dels learned using ac c ent/* also classify cue phraseswith go o d results/.Although the textual features w ere automatically extracted from a transcript/, the tran/-script itself w as man ually created/. Man y natural language understanding systems do notdeal with sp eec h at all/, and th us b egin with suc h textual represen tations/. In sp ok en lan/-guage systems the transcription pro cess is t ypically automated using a sp eec h recognitionsystem /(although this in tro duces further sources of error/)/./6/. Related W orkThis pap er has b oth compared the results obtained using mac hine learning to previouslyexisting man ually/-obtained results/, and has also used mac hine learning as a to ol for dev el/-oping theories giv en new linguistic data /(as in the mo dels resulting from Exp erimen t Set /3/,where the new feature token w as considered/)/. Siegel /(/1/9/9/4/) similarly uses mac hine learning/(in particular/, a genetic learning algorithm/) to classify cue phrases from a previously un/-studied set of textual features/: a feature corresp onding to token /, as w ell as textual featurescon taining the lexical or orthographic item immediately to the left of and in the /4 p ositions/8/7Litmanto the righ t of the example/. Siegel/'s input consists of one judge/'s non/-am biguous examplestak en from the data used b y Hirsc h b erg and Litman /(/1/9/9/3/) as w ell as additional examples/;his output is in the form of decision trees/. Siegel rep orts a /2/1/% estimated error rate/, withhalf of the corpus used for training and half for testing/. Siegel and McKeo wn /(/1/9/9/4/) alsoprop ose a metho d for dev eloping linguisticall y viable rulesets/, based on the partitioning ofthe training data pro duced during induction/.Mac hine learning has also b een used in sev eral other areas of discourse analysis/. F or ex/-ample/, learning has b een used to dev elop rules for structuring discourse in to m ulti/-utterancesegmen ts/. Grosz and Hirsc h b erg /(/1/9/9/2/) use the classi/\fcation and regression tree systemcar t /(Brieman et al/./, /1/9/8/4/) to construct decision trees for classifying asp ects of discoursestructure from in tonational feature v alues/. Litman and P assonneau /(/1/9/9/5/) and P assonneauand Litman /(in press/) use the system C/4/./5 to construct decision trees for classifying utter/-ances as discourse segmen t b oundaries/, using features relating to proso dy /, referen tial nounphrases/, and cue phrases/. In addition/, C/4/./5 has b een used to dev elop anaphora resolutionalgorithms/, b y training on corp ora tagged with appropriate discourse information /(Aone /&Bennett/, /1/9/9/5/)/. Similarly /, McCarth y and Lehnert /(/1/9/9/5/) use C/4/./5 to learn decision treesto classify pairs of phrases as coreferen t or not/. So derland and Lehnert /(/1/9/9/4/) use themac hine learning program ID/3 /(a predecessor of C/4/./5/) to supp ort corpus/-driv en kno wledgeacquisition in information extraction/. Mac hine learning often results in algorithms thatoutp erform man ually deriv ed alternativ es /(Litman /& P assonneau/, /1/9/9/5/; P assonneau /& Lit/-man/, in press/; Aone /& Bennett/, /1/9/9/5/; McCarth y /& Lehnert/, /1/9/9/5/)/, although statisticalinference is not alw a ys used to ev aluate the signi/\fcance of the p erformance di/\u000berences/.Finally /, mac hine learning has also b een used with great success in man y other areas ofnatural language pro cessing/. As discussed ab o v e/, the w ork of most researc hers in discourseanalysis has concen trated on the direct application of existing sym b olic learning approac hes/(e/.g/./, C/4/./5/)/, and on the comparison of learning and man ual metho ds/. While researc hersin other areas of natural language pro cessing ha v e also addressed these issues/, they ha v ein addition applied a m uc h wider v ariet y of learning approac hes/, and ha v e b een concernedwith the dev elopmen t of learning metho ds particularly designed for language pro cessing/. Arecen t surv ey of learning for natural language /(W erm ter/, Rilo/\u000b/, /& Sc heler/, /1/9/9/6/) illustratesb oth the t yp e of learning approac hes that ha v e b een used and mo di/\fed /(in particular/,sym b olic/, connectionist/, statistical/, and h ybrid approac hes/)/, as w ell as the scop e of theproblems that ha v e pro v ed amenable to the use of learning tec hniques /(e/.g/./, grammaticalinference/, syn tactic disam biguation/, and w ord sense disam biguation/)/./7/. ConclusionThis pap er has demonstrated the utilit y of mac hine learning tec hniques for cue phraseclassi/\fcation/. Mac hine learning supp orts the automatic generation of linguistically viableclassi/\fcation mo dels/. When compared to man ually deriv ed mo dels already in the literature/,man y of the learned mo dels con tain new linguistic insigh ts and p erform with at least ashigh /(if not higher/) accuracy /. In addition/, the abilit y to automatically construct classi/\f/-cation mo dels mak es it easier to comparativ ely analyze the utilit y of alternativ e featurerepresen tations of the data/. Finally /, the ease of retraining mak es the learning approac hmore scalable and extensible than man ual metho ds/./8/8Cue Phrase Classifica tion Using Ma chine LearningA /\frst set of exp erimen ts w ere presen ted that used the mac hine learning programscgrendel /(Cohen/, /1/9/9/2/, /1/9/9/3/) and C/4/./5 /(Quinlan/, /1/9/9/3/) to induce classi/\fcation mo delsfrom the preclassi/\fed cue phrases and their features that w ere used as training data b yHirsc h b erg and Litman /(/1/9/9/3/)/. These results w ere then ev aluated with the same testing dataand metho dology used b y Hirsc h b erg and Litman /(/1/9/9/3/)/. A second group of exp erimen tsused the metho d of cross/-v alidation to b oth train and test from the testing data used b yHirsc h b erg and Litman /(/1/9/9/3/)/. A third set of exp erimen ts induced classi/\fcation mo delsusing the new feature token /. A fourth set of exp erimen ts induced classi/\fcation mo delsusing the new classi/\fcation unknown /.The exp erimen tal results indicate that sev eral learned classi/\fcation mo dels /(includingextremely simple one feature mo dels/) ha v e signi/\fcan tly lo w er error rates than the mo delsdev elop ed b y Hirsc h b erg and Litman /(/1/9/9/3/)/. One p ossible explanation is that the hand/-built classi/\fcation mo dels w ere deriv ed using v ery small training sets/; as new data b ecamea v ailable/, this data w as used for testing but not for up dating the original mo dels/. In con/-trast/, mac hine learning in conjunction with cross/-v alidation /(Exp erimen t Set /2/) supp ortedthe building of classi/\fcation mo dels using a m uc h larger amoun t of the data for training/.Ev en when the learned mo dels w ere deriv ed using the same small training set /(Exp erimen tSet /1/)/, the results sho w ed that the learning approac h help ed guard against o v er/\ftting onthe training data/.While the proso dic classi/\fcation mo del dev elop ed b y Hirsc h b erg and Litman demon/-strated the utilit y of com bining phrasal p osition with phrasal comp osition and accen t/, theb est p erforming proso dic mo dels of Exp erimen t Sets /1 and /2 demonstrated that phrasalp osition w as in fact ev en more useful for predicting cue phrases when used b y itself/. Theother high p erforming classi/\fcation mo dels of Exp erimen t Set /2 also demonstrated the util/-it y of classifying cue phrases based on the proso dic feature length and the textual featurepr e c e ding cue phr ase /, in com bination with other features/.Just as the mac hine learning approac h made it easy to retrain when new training ex/-amples b ecame a v ailable /(Exp erimen t Set /2/)/, mac hine learning also made it easy to retrainwhen new features b ecome a v ailable/. In particular/, when the v alue of the feature tokenw as added to all the represen tations in Exp erimen t Set /2/, it w as trivial to relearn all of themo dels /(Exp erimen t Set /3/)/. Allo wing the learning programs to treat cue phrases individ/-ually further impro v ed the accuracy of the learned classi/\fcation mo dels/, and added to theb o dy of linguistic kno wledge regarding cue phrases/. Exp erimen t Set /3 demonstrated thatwhile not useful b y themselv es for classifying all cue phrases/, the proso dic features basedon phrasal length/, phrasal comp osition/, and accen t/, and textual features based on adjacen tcue phrases/, succeeding p osition/, and part/-of/-sp eec h/, w ere in fact useful when used only inconjunction with the feature token /.A /\fnal adv an tage of the mac hine learning approac h is that the ease of inducing classi/\fca/-tion mo dels from man y di/\u000beren t sets of features supp orts an exploration of the comparativ eutilit y of di/\u000beren t kno wledge sources/. This is esp ecially useful for understanding the trade/-o/\u000bs b et w een the accuracy of a mo del and the set of features that are considered/. F orexample/, it migh t b e w orth the e/\u000bort to co de a feature that is not automatically obtainableor that is exp ensiv e to automatically obtain if adding the feature results in a signi/\fcan timpro v emen t in p erformance/./8/9LitmanIn sum/, the results of this pap er suggest that mac hine learning is a useful to ol forcue phrase classi/\fcation/, when the amoun t of data precludes e/\u000bectiv e h uman analysis/,when the /\rexibilit y a/\u000borded b y easy retraining is needed /(e/.g/./, due to additional trainingexamples/, new features/, new classi/\fcations/)/, and//or when an analysis goal is to gain a b etterunderstanding of the di/\u000beren t asp ects of the data/.Sev eral areas for future w ork remain/. First/, there is still ro om for p erformance impro v e/-men t/. The error rates of the b est p erforming learned mo dels/, ev en though they outp erformthe man ually deriv ed mo dels/, p erform with error rates in the teens/. Note that only thefeatures that w ere co ded or discussed b y Hirsc h b erg and Litman /(/1/9/9/3/) w ere considered inthis pap er/. It ma y b e p ossible to further lo w er the error rates b y considering new t yp esof proso dic and textual features /(e/.g/./, other con textual textual features /(Siegel/, /1/9/9/4/)/, orfeatures that ha v e b een prop osed in connection with the more general topic of discoursestructure/)/, and//or b y using di/\u000beren t kinds of learning metho ds/. Second/, Exp erimen t Set/4 /(and the previous literature/) sho w that as y et/, there are no mo dels for predicting whena cue phrase usage should b e classi/\fed as unknown /, rather than as disc ourse or sentential /.Again/, it ma y b e p ossible to impro v e the p erformance of the existing learned mo dels b yconsidering new features and//or learning metho ds/, or p erhaps p erformance could b e im/-pro v ed b y pro viding more training data/. Finally /, it is curren tly an op en question whetherthe textual mo dels dev elop ed here/, whic h w ere based on transcripts of sp eec h/, are applicableto written texts/. T extual mo dels th us need to b e dev elop ed using written texts as trainingdata/. Mac hine learning should con tin ue to b e a useful to ol for helping to address theseissues/.App endix A/. C/4/./5 Results for Exp erimen t Sets /2 and /3T ables /1/3/, /1/4 and /1/5 presen t the C/4/./5 error rates for Exp erimen t Sets /2 and /3/. The C/4/./5results for Exp erimen t Set /2 are sho wn in the /\\Non/-T ok enized/\" columns/. A comparison ofT ables /1/3 and /5 sho ws that except for A in the larger test set/, the C/4/./5 proso dic error ratesfall within the cgrendel con/\fdence in terv als/. A similar comparison of T ables /1/4 and /6sho ws that except for O/-P in the larger test set/, the C/4/./5 textual error rates fall within thecgrendel con/\fdence in terv als/. Finally /, a comparison of T ables /1/5 and /7 sho ws that theC/4/./5 error rate of sp e e ch/-text falls within the cgrendel con/\fdence in terv al/. The fact thatcomparable cgrendel and C/4/./5 results are generally obtained suggests that the abilit y toautomate as w ell as to impro v e up on man ual p erformance is not due to the sp eci/\fcs ofeither learning program/.The C/4/./5 results for Exp erimen t Set /3 are sho wn in the /\\T ok enized/\" columns of T a/-bles /1/3/, /1/4 and /1/5/. Comparison with T ables /8/, /1/0 and /1/2 sho ws that the error rates of C/4/./5and cgrendel are not as similar as in Exp erimen t Set /2/. Ho w ev er/, the error rates rep ortedin the tables use the default C/4/./5 and cgrendel options when running the learning pro/-grams/. Comparable p erformance b et w een the t w o learning programs can in fact generallyb e ac hiev ed b y o v erriding one of the default C/4/./5 options/. As detailed b y Quinlan /(/1/9/9/3/)/,the default C/4/./5 approac h /{ whic h creates a separate subtree for eac h p ossible feature v alue/{ migh t not b e appropriate when there are man y v alues for a feature/. This situation c har/-acterizes the feature token /. When the C/4/./5 default option is c hanged to allo w feature v aluesto b e group ed in to one branc h of the decision tree/, the problematic C/4/./5 error rates do/9/0Cue Phrase Classifica tion Using Ma chine LearningMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)Non/-T ok enized T ok enized /(/+/) Non/-T ok enized T ok enized /(/+/)P/-L /3/2/./5 /3/1/./7 /3/2/./2 /3/1/./4P/-P /1/6/./2 /1/8/./4 /1/8/./8 /1/9/./0I/-L /2/5/./6 /2/6/./8 /2/5/./6 /2/5/./6I/-P /2/5/./9 /2/6/./3 /1/9/./4 /1/8/./8I/-C /3/6/./5 /3/6/./6 /3/5/./8 /3/2/./8A /4/0/./7 /4/0/./7 /2/9/./6 /2/9/./2A/* /2/8/./3 /2/6/./7 /2/8/./8 /3/1/./2proso dy /1/6/./0 /1/5/./2 /1/9/./4 /1/6/./0hl/9/3features /3/0/./2 /2/9/./0 /1/8/./8 /1/8/./8phrasing /1/5/./9 /1/5/./2 /1/8/./0 /1/7/./4length /2/4/./8 /2/4/./4 /2/6/./2 /2/4/./2p osition /1/8/./1 /1/8/./0 /1/9/./6 /1/7/./6in tonational /1/6/./8 /1/6/./6 /1/8/./8 /1/9/./8in termediate /2/1/./2 /2/2/./3 /2/1/./6 /1/8/./4T able /1/3/: Error rates /(/%/) of the C/4/./5 proso dic classi/\fcation mo dels/, testing data/. /(T rainingand testing w ere done from the m ultiple cue phrase corpus using cross/-v alidation/./)Mo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)Non/-T ok enized T ok enized /(/+/) Non/-T ok enized T ok enized /(/+/)C/-P /4/0/./7 /3/9/./3 /3/9/./2 /3/3/./6C/-S /4/0/./7 /3/9/./9 /3/9/./2 /3/9/./2O/-P /4/0/./7 /3/5/./7 /1/8/./6 /1/4/./6O/-P/* /1/8/./4 /2/0/./3 /1/7/./2 /1/5/./0O/-S /3/5/./0 /3/1/./6 /3/1/./8 /3/1/./8O/-S/* /3/4/./4 /3/2/./5 /3/1/./0 /3/2/./4POS /4/0/./7 /3/4/./7 /4/1/./8 /3/1/./8text /1/9/./0 /2/0/./6 /2/0/./0 /1/5/./0adjacency /4/0/./9 /3/9/./4 /4/0/./6 /4/3/./6orthograph y /1/8/./9 /1/9/./3 /1/7/./8 /1/8/./0preceding /1/8/./7 /1/9/./3 /1/9/./2 /1/6/./0succeeding /3/4/./1 /3/2/./9 /3/0/./0 /3/1/./8T able /1/4/: Error rates /(/%/) of the C/4/./5 textual classi/\fcation mo dels/, testing data/. /(T rainingand testing w ere done from the m ultiple cue phrase corpus using cross/-v alidation/./)indeed impro v e/. F or example/, the A/+ error rate for the classi/\fable non/-conjuncts c hangesfrom /2/9/./2/% /(T able /1/3/) to /1/1/%/, whic h is within the /1/2/./8/% /\u0006 /3/./1/% cgrendel con/\fdencein terv al /(T able /8/)/.Ac kno wledgemen tsI w ould lik e to thank William Cohen and Jason Catlett for their helpful commen ts regardingthe use of cgrendel and C/4/./5/, and Sandra Carb erry /, Reb ecca P assonneau/, and the threeanon ymous JAIR review ers for their helpful commen ts on this pap er/. I w ould also lik e to/9/1LitmanMo del Classi/\fable Cue Phrases /(N/=/8/7/8/) Classi/\fable Non/-Conjuncts /(N/=/4/9/5/)Non/-T ok enized T ok enized /(/+/) Non/-T ok enized T ok enized /(/+/)sp eec h/-text /1/5/./3 /1/3/./6 /1/6/./8 /1/7/./6T able /1/5/: Error rates /(/%/) of the C/4/./5 proso dic//textual classi/\fcation mo del/, testing data/./(T raining and testing w ere done from the m ultiple cue phrase corpus using cross/-v alidation/./)thank William Cohen/, Ido Dagan/, Julia Hirsc h b erg/, and Eric Siegel for commen ts on apreliminary v ersion of this pap er /(Litman/, /1/9/9/4/)/."}
{"category": "abstract", "text": "Cue phrases ma y b e used in a disc ourse sense to explicitly signal discourse structure/, butalso in a sentential sense to con v ey seman tic rather than structural information/. Correctlyclassifying cue phrases as discourse or sen ten tial is critical in natural language pro cessingsystems that exploit discourse structure/, e/.g/./, for p erforming tasks suc h as anaphora reso/-lution and plan recognition/. This pap er explores the use of mac hine learning for classifyingcue phrases as discourse or sen ten tial/. Tw o mac hine learning programs /( cgrendel andC/4/./5/) are used to induce classi/\fcation mo dels from sets of pre/-classi/\fed cue phrases andtheir features in text and sp eec h/. Mac hine learning is sho wn to b e an e/\u000bectiv e tec hniquefor not only automating the generation of classi/\fcation mo dels/, but also for impro vingup on previous results/. When compared to man ually deriv ed classi/\fcation mo dels alreadyin the literature/, the learned mo dels often p erform with higher accuracy and con tain newlinguistic insigh ts in to the data/. In addition/, the abilit y to automatically construct classi/-/\fcation mo dels mak es it easier to comparativ ely analyze the utilit y of alternativ e featurerepresen tations of the data/. Finally /, the ease of retraining mak es the learning approac hmore scalable and /\rexible than man ual metho ds/./1/. In tro ductionCue phr ases are w ords and phrases that ma y sometimes b e used to explicitly signal discoursestructure in b oth text and sp eec h/. In particular/, when used in a disc ourse sense/, a cuephrase explicitly con v eys structural information/. When used in a sentential sense/, a cuephrase instead con v eys seman tic rather than structural information/. The follo wing examples/(tak en from a sp ok en language corpus that will b e describ ed in Section /2/) illustrate samplediscourse and sen ten tial usages of the cue phrases /\\sa y/\" and /\\further/\"/"}
{"category": "non-abstract", "text": "/: /: /; m g /. No w/, w e cande/\fne a state abstraction mapping /\u000b\n/0/0/( s /) /= /\u000b\n/0/( /\u000b /( s /)/) and a sequence abstraction mapping/\f\n/0/0/( j /) /= /\f /( /\f\n/0/( j /)/)/. It is easy to see/, that /\u000b\n/0/0is a w ell de/\fned state abstraction mapping/( s /\u0013 s\n/0/) /\u000b /( s /) /\u0013 /\u000b /( s\n/0/) /) /\u000b\n/0/( /\u000b /( s /)/) /\u0013 /\u000b\n/0/( /\u000b /( s\n/0/)/)/) and that /\f\n/0 /0is a w ell de/\fned sequenceabstraction mapping /( /\f /( /\f\n/0/(/0/)/) /= /0 /; /\f /( /\f\n/0/( m /)/) /= /\f /( k /) /= n /; u /< v /, /\f\n/0/( u /) /< /\f\n/0/( v /) /,/\f /( /\f\n/0/( u /)/) /< /\f /( /\f\n/0/( v /)/)/)/. F urthermore it follo ws /\u000b\n/0/0/( s\nc/\f\n/0/0/( j /)\n/) /= /\u000b\n/0/( /\u000b /( s\nc/\f /( /\f\n/0/( j /)/)\n/)/) /= /\u000b\n/0/( s\n/0/\f\n/0/( j /)\n/) /= s\naj\n/,leading to the conclusion that C/\u0017\nis an abstraction of C/0\nwith resp ect to /( Dc\n/, Da\n/)/. /2Theorem /8 /(Correctness and completeness of the P abs algorithm/) If a c omplete SLD/-r efutation pr o c e dur e is use d in the P abs algorithm/, then Case Ca\nis an abstr action of c ase Ccwith r esp e ct to /( Dc\n/; Da\n/) and the generic the ory A /, if and only if Ca\n/2 P ABS/( hDc\n/; Da\n/; Ai /; Cc\n/) /.Pro of/:Corr e ctness /(/\\ /\u0012 /\"/)/: If Ca\nis returned b y P abs /, then h /( o\na/1\n/; /: /: /: /; o\nak\n/) /; /\u000b\n/\u0003/\u0003/; /\f i /2 P aths holds\n/1/9in phase/-IV/. W e can de/\fne a state abstraction mapping /\u000b /( s /) /:/= f e /2 /\u000b\n/\u0003/\u0003jRc\n/[ A /[ s /` e g /,whic h/, together with the sequence abstraction mapping /\f will lead to the desired conclusion/.F or ev ery op erator o\nai\n/, w e kno w b y construction of phase/-IV/, that h /\f /( i /BnZr /1/) /; /\f /( i /) /; o\nai\n/; /\u001c i /2 Gholds/. By construction of phase/-I I I/, w e can conclude that s\na/\f /( i /BnZr /1/)\n/[ Ra\n/` P r eo\nai\nholds andthat consequen tly /\u001cE\n/[ Ra\n/` P r eo\nai\nalso holds for the resp ectiv e execution of the b o dy of thewhile/-lo op in phase/-IV/. Since /\u001cE\n/\u0012 /\u000b\n/0/\u0012 /\u000b\n/\u0003/\u0003holds and /` is a monotonic deriv ation op erator/,it is ob vious that /\u000b /( s\nc/\f /( i /)\n/) /[ Ra\n/` P r eo\nai\n/. F urthermore/, the /`if for all/'/-test/, whic h is executedb efore the extension of the path/, ensures that /( s\na/\f /( i /BnZr /1/)\n/\\ /\u000b\n/\u0003/\u0003/)\no\nai/BnZr /! /( s\na/\f /( i /)\n/\\ /\u000b\n/\u0003/\u0003/) holds/. T ogetherwith the ful/\fllmen t of the precondition of the op erator w e ha v e /\u000b /( s\nc/\f /( i /BnZr /1/)\n/)\no\nai/BnZr /! /\u000b /( s\nc/\f /( i /)\n/)/.Th us/, w e ha v e sho wn/, Ca\nis correct abstraction with resp ect to De/\fnition /5/.Completeness /(/\\ /\u0013 /\"/)/: Assume/, case Ca\n/= hh s\na/0\n/; s\nam\ni /; /( o\na/1\n/; /: /: /: /; o\nam\n/) i is an abstraction of Ccbased on a deductiv ely justi/\fed state abstraction mapping/. Then there exists a state ab/-/1/9/. Note that /\u000b\n/\u0003/\u0003refers to the set /\fnally constructed after termination of the while/-lo op/. W e use /\u000b\n/\u0003todenote the resp ectiv e set during the construction in this lo op/./1/1/1Ber gmann /& Wilkestraction mapping /\u000b and a sequence abstraction mapping /\f suc h that /\u000b /( s\nc/\f /( i /BnZr /1/)\n/)\no\nai/BnZr /! /\u000b /( s\nc/\f /( i /)\n/)holds for all i /2 f /1 /; /: /: /: /; m g /. Since /\u000b is deductiv ely justi/\fed b y A /, it follo ws b y constructionof phase/-I I/, that /\u000b /( s\nci /BnZr /1\n/) /\u0012 s\nai /BnZr /1\n/. Since /` is a monotonic deriv ation op erator/, the precondi/-ton of o\nai\nis also ful/\flled in s\na/\f /( i /BnZr /1/)\n/. F urthermore/, the addlist of the op erator is ful/\flled in/\u000b /( s\nc/\f /( i /)\n/) and is consequen tly also ful/\flled in s\nai\n/. By the construction of phase/-I I I/, it is no wguaran teed/, that h /\f /( i /BnZr /1/) /; /\f /( i /) /; o\nai\n/; /\u001c i /2 G /. No w/, w e w ould lik e to sho w/, that in phase/-IV/:/\u000f there exists a sequence of assignmen ts to the v ariable P aths /, suc h that h /(/) /; /\f/0\n/; /\u000b\n/\u0003/0\ni /2P aths /, h /( o\na/1\n/) /; /\f/1\n/; /\u000b\n/\u0003/1\ni /2 P aths /, /: /: /: /, h /( o\na/1\n/; /: /: /: /; o\nam\n/) /; /\fm\n/; /\u000b\n/\u0003m\ni /2 P aths /,/\u000f /\fk\n/( /\u0017 /) /= /\f /( /\u0017 /) for /\u0017 /2 f /0 /; /: /: /: /; k g/\u000f /( /\u000b\n/\u0003k\n/\\ s\nal\n/) /\u0012 /\u000b /( s\ncl\n/) for l /2 f /1 /; /: /: /: /; n g and/\u000f /\u000b\n/\u0003k\n/\u0013\nSkl /=/1\nAddo\nal\n/.The pro of is b y induction on i/. The induction basis is ob vious due to the initializationof the P aths v ariable/. No w/, assume that h /( o\na/1\n/; /: /: /: /; o\nak\n/) /; /\fk\n/; /\u000b\n/\u0003k\ni /2 P aths /(with k /< m /)at some state of the execution of phase/-IV/. Since/, h /\f /( k /) /; /\f /( k /+ /1/) /; o\nak /+/1\n/; /\u001c i /2 G holds asargued b efore/, and /\f /( k /) /= /\fk\n/( k /) b y induction h yp othesis/, the selected op erator sequenceis tried to b e extended b y o\na/= o\nak /+/1\nin the b o dy of the while/-lo op/. Additionally /, w ekno w/, that /\u001cE\ncon tains exactly those sen tences whic h are required to pro of the preconditionof o\nak /+/1\n/. Note/, that since the SLD/-resolution pro cedure is assumed to b e complete ando\nak /+/1\nis applicable in /\u000b /( s\nck\n/)/, /\u001cE\nis required to pro of the preconditition of o\naif and only if/\u001cE\n/\u0012 /\u000b /( s\nc/\f /( k /)\n/)/. Since /\u000b is deductiv ely justi/\fed/, /8 e /2 /\u001cE\n/; /8 l /2 f /1 /; /: /: /: /; m g holds/: e /2 /\u000b /( s\nc/\f /( l /)\n/)if s\nc/\f /( l /)\n/[ Rc\n/[ A /` e /. By construction of the s\nal\n/, /8 e /2 /\u001cE\n/; /8 l /2 f /1 /; /: /: /: /; m g holds/: e /2 /\u000b /( s\nc/\f /( l /)\n/)if e /2 s\nal\n/. Consequen tly /, /\u001cE\n/\\ s\nal\n/\u0012 /\u000b /( s\ncl\n/) for all l /2 f /1 /; /: /: /: /; m g /. On the other hand/, w ealso kno w that o\nak /+/1\nleads to /\u000b /( s\nc/\f /( k /+/1/)\n/)/. Consequen tly /, Addo\nak /+/1\n/\u0012 /\u000b /( s\nc/\f /( k /+/1/)\n/)/. F ollo wingthe same argumen tation as ab o v e/, w e can conclude that /( Addo\nak /+/1\n/\\ s\nal\n/) /\u0012 /\u000b /( s\ncl\n/) for alll /2 f /1 /; /: /: /: /; m g /. Consequen tly /, for /\u000b\n/0/= /\u000b\n/\u0003k\n/[ /\u001cE\n/[ Addo\nak /+/1\nholds that /\u000b\n/0/\\ s\nal\n/\u0012 /\u000b /( s\ncl\n/)/. No w/,w e can conclude that P aths is extended b y o\nak /+/1\nas follo ws/. Since /\u000b /( s\nc/\f /( /\u0017 /BnZr /1/)\n/)\no\na/\u0017/BnZr /! /\u000b /( s\nc/\f /( /\u0017 /)\n/)holds and that Addo\na/\u0017\n/2 /\u000b\n/0and /( /\u000b\n/0/\\ s\na/\f /( /\u0017 /)\n/) /\u0012 /\u000b /( s\nc/\f /( /\u0017 /)\n/)/, w e can immediately follo w that/( /\u000b\n/0/\\ s\na/\f /( /\u0017 /BnZr /1/)\n/)\no\na/\u0017/BnZr /! /( /\u000b\n/0/\\ s\na/\f /( /\u0017 /)\n/)/. Consequen tly /, h /( o\na/1\n/; /: /: /: /; o\nak\n/; o\nak /+/1\n/) /; /\u000b\n/\u0003k /+/1\n/; /\fk /+/1\ni /2 P aths with/\u000b\n/\u0003k /+/1\n/= /\u000b\n/0and /\fk /+/1\n/( /\u0017 /) /= /\fk\n/( /\u0017 /) /= /\f /( /\u0017 /) for /\u0017 /2 f /1 /; /: /: /: /; k g and /\fk /+/1\n/( k /+ /1/) /= /\f /( k /)/. So/,the induction h yp othesis is ful/\flled for k /+ /1/. Thereb y /, it is sho wn that Ca\nis returned b yP abs /. /2Ac kno wledgemen tsThe authors w an t to thank Agnar Aamo dt/, Jaime Carb onell/, P adraig Cunningham/, Sub/-barao Kam bhampati/, Mic hael M/. Ric h ter/, Man uela V eloso/, as w ell as all mem b ers of ourresearc h group for man y helpful discussions and for remarks on earlier v ersions of this pa/-p er/. P articularly /, w e w an t to thank P adraig Cunningham for carefully pro of/-reading the/1/1/2Building and Refining Abstra ct Planning Casesrecen t v ersion of the pap er/. W e are also greatly indebted to the anon ymous JAIR review/-ers who help ed to signi/\fcan tly impro v e the pap er/. This researc h w as partially supp ortedb y the German /\\Sonderforsc h ungsb ereic h/\" SFB/-/3/1/4 and the Commission of the Europ eanComm unities /(ESPRIT con tract P/6/3/2/2/, the Inreca pro ject/)/. The partners of Inreca areAc knoSoft /(prime con tractor/, F rance/)/, tecInno /(German y/)/, Irish Medical Systems /(Ireland/)and the Univ ersit y of Kaiserslautern /(German y/)/."}
{"category": "abstract", "text": "Planning Casesb y Change of Represen tation LanguageRalph Bergmann ber gmann/@inf orma tik/.un i/-kl/.deW olfgang Wilk e wilke/@inf orma tik/.uni/-kl/.deCentr e for L e arning Systems and Applic ations /(LSA/)University of Kaiserslautern/, P/.O/./-Box /3/0/4/9/, D/-/6/7/6/5/3 Kaiserslautern/, GermanyAbstractAbstraction is one of the most promising approac hes to impro v e the p erformance of problemsolv ers/. In sev eral domains abstraction b y dr opping sentenc es of a domain description /{ asused in most hierarc hical planners /{ has pro v en useful/. In this pap er w e presen t exampleswhic h illustrate signi/\fcan t dra wbac ks of abstraction b y dropping sen tences/. T o o v ercomethese dra wbac ks/, w e prop ose a more general view of abstraction in v olving the change ofr epr esentation language /. W e ha v e dev elop ed a new abstraction metho dology and a relatedsound and complete learning algorithm that allo ws the complete change of r epr esentationlanguage of planning c ases from concrete to abstract/. Ho w ev er/, to ac hiev e a p o w erfulc hange of the represen tation language/, the abstract language itself as w ell as rules whic hdescrib e admissible w a ys of abstracting states m ust b e pro vided in the domain mo del/.This new abstraction approac h is the core of P aris /( P lan A bstraction and R e/\fnemen tin an I n tegrated S ystem/)/, a system in whic h abstract planning cases are automaticallylearned from giv en concrete cases/. An empirical study in the domain of pro cess planningin mec hanical engineering sho ws signi/\fcan t adv an tages of the prop osed reasoning fromabstract cases o v er classical hierarc hical planning/./1/. In tro ductionA bstr action is one of the most c hallenging and also promising approac hes to impro v e complexproblem solving and it is inspired b y the w a y h umans seem to solv e problems/. A t /\frst/, lessrelev an t details of a giv en problem are ignored so that the abstracted problem can b esolv ed more easily /. Then/, step b y step/, more details are added to the solution b y taking anincreasingly more detailed lo ok at the problem/. Thereb y /, the abstract solution constructed/\frst is re/\fned to w ards a concrete solution/. One t ypical c haracteristic of most w ork onhierarc hical problem solving is that abstraction is mostly p erformed b y dr opping sentenc esof a domain description /(Sacerdoti/, /1/9/7/4/, /1/9/7/7/; T enen b erg/, /1/9/8/8/; Unruh /& Rosen blo om/,/1/9/8/9/; Y ang /& T enen b erg/, /1/9/9/0/; Knoblo c k/, /1/9/8/9/, /1/9/9/4/; Bacc h us /& Y ang/, /1/9/9/4/)/. A secondcommon c haracteristic is that a hierarc hical problem solv er usually deriv es an abstractsolution from scratc h/, without using exp erience from previous problem solving episo des/.Giunc higlia and W alsh /(/1/9/9/2/) ha v e presen ted a comprehensiv e formal framew ork forabstraction and a comparison of the di/\u000beren t abstraction approac hes from theorem pro ving/(Plaisted/, /1/9/8/1/, /1/9/8/6/; T enen b erg/, /1/9/8/7/)/, planning /(New ell /& Simon/, /1/9/7/2/; Sacerdoti/, /1/9/7/4/,/1/9/7/7/; T enen b erg/, /1/9/8/8/; Unruh /& Rosen blo om/, /1/9/8/9/; Y ang /& T enen b erg/, /1/9/9/0/; Knoblo c k/,/1/9/8/9/, /1/9/9/4/)/, and mo del based diagnosis /(Mozetic/, /1/9/9/0/)/. F or hierarc hical planning/, Korf /'smo del of abstraction in problem solving /(Korf/, /1/9/8/7/) allo ws the analysis of reductions inc/\r /1/9/9/5 AI Access F oundation and Morgan Kaufmann Publishers/. All righ ts reserv ed/.Ber gmann /& Wilkesearc h caused b y single and m ultiple lev els of abstraction/. He has sho wn that in the optimalcase/, abstraction can reduce the exp ected searc h time from exp onen tial to linear/. Knoblo c khas dev elop ed an approac h to construct a hierarc h y of abstraction spaces automaticallyfrom a giv en concrete/-lev el problem solving domain /(Knoblo c k/, /1/9/9/0/, /1/9/9/3/, /1/9/9/4/)/. Theseso called ordered monotonic abstraction hierarc hies /(Knoblo c k/, T enen b erg/, /& Y ang/, /1/9/9/1b/)ha v e pro v en useful in man y domains/. Recen tly /, Bacc h us and Y ang /(/1/9/9/4/) presen ted animpro v ed metho d for automatically generating abstraction hierarc hies based on a moredetailed mo del of searc h costs/.All these abstraction metho ds/, ho w ev er/, rely on abstraction b y dropping sen tences ofthe domain description whic h is a kind of homomorphic abstr action /(Holte et al/./, /1/9/9/4/,/1/9/9/5/)/. It has b een sho wn that these kinds of abstractions are highly represen tation de/-p enden t /(Holte et al/./, /1/9/9/4/, /1/9/9/5/)/. F or t w o classical planning domains/, di/\u000beren t /\\natural/\\represen tations ha v e b een analyzed and it turns out that there are sev eral represen tationsfor whic h the classical abstraction tec hniques do not lead to signi/\fcan tly impro v ed problemsolv ers /(Knoblo c k/, /1/9/9/4/; Holte et al/./, /1/9/9/5/)/. Ho w ev er/, it is w ell kno wn that normally man ydi/\u000beren t represen tations of the same domain exist as already p oin ted out b y Korf /(/1/9/8/0/)/,but up to no w no theory of represen tation has b een dev elop ed/. In particular/, there is notheory of represen tation for hierarc hical problem solving with dropping sen tences/.F rom a kno wledge/-engineering p ersp ectiv e/, man y di/\u000beren t asp ects suc h as simplicit y /,understandabilit y /, and main tainabilit y m ust b e considered when dev eloping a domain rep/-resen tation/. Therefore/, w e assume that represen tations of domains are giv en b y kno wledgeengineers and rely on represen tations whic h w e consider most /\\natural/\" for certain kinds ofproblems/. W e will demonstrate t w o simple example problems and related represen tations/,in whic h the usual use of abstraction in problem solving do es not lead to an y impro v emen t/.In the /\frst example/, no impro v emen t can b e ac hiev ed b ecause abstraction is restricted todropping sen tences of a domain/. In the second example/, the abstract solution computedfrom scratc h do es not decomp ose the original problem and consequen tly do es not cut do wnthe searc h space at the next detailed lev el/. W e do not w an t to argue that the examplescan nev er b e represen ted in a w a y that standard hierarc hical problem solving w orks w ell/.Ho w ev er/, w e think it w ould require a large e/\u000bort from a kno wledge engineer to dev elop anappropriate represen tation and w e b eliev e that it is often imp ossible to dev elop a represen/-tation whic h is appropriate from a kno wledge/-engineering p ersp ectiv e and whic h also allo wse/\u000ecien t hierarc hical problem solving based on dropping sen tences/.W e tak e these observ ations as the motiv ation to dev elop a more general mo del of ab/-straction in problem solving/. As already p oin ted out b y Mic halski /(/1/9/9/4/)/, abstraction/, ingeneral/, can b e seen as switc hing to a completely new represen tation language in whic h thelev el of detail is reduced/. In problem solving/, suc h a new abstract represen tation languagem ust consist of completely new sen tences and op erators and not only of a subset of thesen tences and op erators of the concrete language/. T o our kno wledge/, Sipe /(Wilkins/, /1/9/8/8/)is the only planning system whic h curren tly allo ws the c hange of represen tation languageacross di/\u000beren t lev els of abstraction/. Ho w ev er/, a general abstraction metho dology whic hallo ws e/\u000ecien t algorithms for abstraction and re/\fnemen t has not y et b een dev elop ed/. W ew an t to prop ose a metho d of abstraction whic h allo ws the c omplete change of r epr esenta/-tion language of a problem and a solution from concrete to abstract and vice v ersa/, if theconcrete and the abstract language are giv en/. Additionally /, w e prop ose to use exp erienc e/5/4Building and Refining Abstra ct Planning Casesfrom previously solv ed problems/, usually a v ailable as a set of c ases /, to come to abstractsolutions/. The use of exp erience has already pro v en useful in v arious approac hes to sp eed/-up learning suc h as explanation/-based learning /(Mitc hell/, Keller/, /& Kedar/-Cab elli/, /1/9/8/6/;DeJong /& Mo oney /, /1/9/8/6/; Rosen blo om /& Laird/, /1/9/8/6/; Min ton/, /1/9/8/8/; Min ton/, Carb onell/,Knoblo c k/, Kuokk a/, Etzioni/, /& Gil/, /1/9/8/9/; Sha vlik /& O/'Rork e/, /1/9/9/3/; Etzioni/, /1/9/9/3/; Min ton/& Zw eb en/, /1/9/9/3/; Langley /& Allen/, /1/9/9/3/; Kam bhampati /& Kedar/, /1/9/9/4/)/, and analogical orcase/-based reasoning /(Carb onell/, /1/9/8/6/; Kam bhampati /& Hendler/, /1/9/9/2/; V eloso /& Carb onell/,/1/9/9/3/; V eloso/, /1/9/9/4/)/.As the main con tribution of this pap er/, w e presen t an abstraction metho dology and arelated le arning metho d in whic h b ene/\fcial abstr act planning c ases are automatically deriv edfrom giv en concrete cases/. Based on a giv en c oncr ete and abstr act language /, this learningapproac h allo ws the complete c hange of the represen tation of a case from the concrete tothe abstract lev el/. Ho w ev er/, to ac hiev e suc h an unconstrained kind of abstraction/, theset of admissible abstractions m ust b e implicitly prede/\fned b y a generic abstr action the ory /.Compared to approac hes in whic h abstraction hierarc hies are generated automatically /, moree/\u000bort is required to sp ecify the abstract language/, but w e feel that this is a price w e ha v eto pa y to mak e planning more tractable in certain situations/.This approac h is fully implemen ted in P aris /( P lan A bstraction and R e/\fnemen t in anI n tegrated S ystem/)/, a system in whic h abstract cases are learned and organized in a c aseb ase /. During no v el problem solving/, this case/-base is searc hed for a suitable abstract casewhic h is further r e/\fne d to a concrete solution to the curren t problem/.The presen tation of this approac h is organized as follo ws/. The next section presen ts ananalysis of hierarc hical problem solving in whic h the shortcomings of curren t approac hesare illustrated b y simple examples/. Section three argues that a p o w erful case abstractionand re/\fnemen t metho d can o v ercome the iden ti/\fed problems/. F urthermore/, w e presen t theP aris approac h informally /, using a simple example/. The next three sections of the pap erformalize the general abstraction approac h/. After in tro ducing the basic terminology /, Sec/-tion /5 de/\fnes a new formal mo del of case abstraction/. Section /6 con tains a v ery detaileddescription of a correct and complete learning algorithm for case abstraction/. Section /7explains the re/\fnemen t of cases for solving new problems/. Section /8 giv es a detailed de/-scription of the domain of pro cess planning in mec hanical engineering for the pro ductionof rotary/-symmetric w orkpieces on a lathe and demonstrates the prop osed approac h on ex/-amples from this domain/. Section /9 rep orts on a detailed exp erimen tal ev aluation of P arisin the describ ed domain/. Finally /, w e discuss the presen ted approac h in relation to similarw ork in the /\feld/. The app endix of the article con tains the formal pro ofs of the prop ertiesof the abstraction approac h and the related learning algorithm/. Additionally /, the detailedrepresen tation of the mec hanical engineering domain used for the exp erimen tal ev aluationis giv en in Online App endix /1/./2/. Analysis of Hierarc hical Problem SolvingThe basic in tuition b ehind abstraction is as follo ws/. By /\frst ignoring less relev an t featuresof the problem description/, abstraction allo ws problems to b e solv ed in a coarse fashion withless e/\u000bort/. Then/, the deriv ed abstract /(sk eletal/) solution serv es as a problem decomp ositionfor the original/, more detailed problem/. Korf /(/1/9/8/7/) has sho wn that hierarc hical problem/5/5Ber gmann /& Wilkesolving can reduce the required searc h space signi/\fcan tly /. Assume that a problem requiresa solution of length n and furthermore assume that the a v erage branc hing factor is b /,i/.e/./, the a v erage n um b er of states that can b e reac hed from a giv en state b y applying asingle op erator/. The w orst/-case time complexit y for /\fnding the required solution b y searc his O /( b\nn/)/. No w/, supp ose that the problem is decomp osed b y an abstract solution in tok subproblems/, eac h of whic h require a solution of length n/1\n/; /"}
