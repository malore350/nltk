# -*- coding: utf-8 -*-
"""nlp1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvHINzur0yDsXqszjXvew_Ytavs7I_eq
"""

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BartTokenizer, BartForConditionalGeneration, AdamW
import json

# Load the BART tokenizer and model
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Define a custom dataset class
class TextDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text, summary = self.data[index]
        input_ids = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=1024).squeeze(0).to(device)
        output_ids = tokenizer.encode(summary, return_tensors='pt', truncation=True, max_length=512).squeeze(0).to(device)
        return input_ids, output_ids

# Initialize empty lists for abstract and non-abstract texts
abstract_texts = []
non_abstract_texts = []

# Read the JSON file
with open(r"C:\Users\kamra\DataspellProjects\nltk_test\data.json", "r", encoding = 'utf8') as json_file:
    for line in json_file:
        # Parse each line as a JSON object
        data = json.loads(line)

        # Extract the category and text
        category = data.get("category")
        text = data.get("text")

        # Add the text to the corresponding list based on the category
        if category == "abstract":
            abstract_texts.append(text)
        elif category == "non-abstract":
            non_abstract_texts.append(text)

# Define the training data
train_data = []
min_len = min(len(abstract_texts), len(non_abstract_texts))
for i in range(min_len):
    train_data.append((non_abstract_texts[i], abstract_texts[i]))

# Define the training dataset and dataloader
train_dataset = TextDataset(train_data)
train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)

import re
from pypdf import PdfReader

# Derive the main text for testing
# Read in the research paper data
pdf_file = open(r'C:\Users\kamra\DataspellProjects\nltk_test\file2.pdf', 'rb')

# Create a PDF reader object
pdf_reader = PdfReader(pdf_file)
num_pages = len(pdf_reader.pages)

# Loop through all the pages in the PDF document and extract the text
text = ""
for i in range(num_pages):
    # Get the page object
    page = pdf_reader.pages[i]

    # Extract the text from the page
    page_text = page.extract_text()

    # Append the text to the document text
    text += page_text

# Close the PDF file
pdf_file.close()


# Define a regular expression to match the word "References" and everything that follows it
references_pattern = re.compile(r'References(.*)', re.DOTALL)

# Use the regular expression to remove the text after "References"
text = references_pattern.sub('', text)

# Define a regular expression to match equations
equations_pattern = re.compile(r'.*?=.+')


updated_text = re.sub(r".*Abstract[^:]+:", "", text, flags=re.DOTALL)
print(updated_text)

# Define the optimizer and learning rate
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

# Define the training loop
def train(epochs):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for i, (input_ids, output_ids) in enumerate(train_dataloader):
            # Zero the gradients
            optimizer.zero_grad()

            # Generate the summary using the BART model
            summary_ids = model.generate(input_ids, max_length=512, num_beams=6, early_stopping=True)

            # Compute the loss and backpropagate
            loss = model(input_ids, labels=output_ids)[0]
            loss.backward()
            optimizer.step()

            # Accumulate the total loss
            total_loss += loss.item()

        # Print the average loss at the end of each epoch
        avg_loss = total_loss / len(train_dataloader)
        print(f'Epoch {epoch}, Average Loss {avg_loss}')

# Train the model for 15 epochs
train(epochs=15)

# Save the trained model
model.save_pretrained("model")

# Load the saved model
model = BartForConditionalGeneration.from_pretrained("model")
model.to(device)

# Test the model on a string
input_text = updated_text
input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=1024).to(device)
summary_ids = model.generate(input_ids, max_length=1024, num_beams=2)
summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(f"Summary text: {summary_text}")

