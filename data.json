{"category": "non-abstract", "text": "+81-75-705-1694, e-mail: aogaki@cc.kyoto-su.ac.jp).\nI. Moritani is with the Department of Information and Communication\nSciences, (tel.: +81-75-705-1694, e-mail: i654168@cc.kyoto-su.ac.jp).\nT. Sugai is with the Department of Information and Communication\nSciences, (tel.: +81-75-705-1694, e-mail: sugai@cc.kyoto-su.ac.jp).\nF. Takeutchi is with the Department of Computer Sciences, (tel.:\n+81-75-705-1694, e-mail: takeut@ksuvx0.kyoto-su.ac.jp).\nF.M. Toyama is with the Department of Information and Communication\nSciences, (tel.: +81-75-705-1898, e-mail: toyama@cc.kyoto-su.ac.jp).situation an observed image g(x,y)can be modeled as the\nconvolution of a true image f(x,y)and a blur image\nh(x,y),i.e.,\ng(x,y)=f(x,y)*h(x,y), (1)\nwhere the blur h(x,y)i sa s s u m e dt ob ec a u s e db yal i n e a r\nshift system. Our aim is to devise a mathematical tool for\nautomatically finding the zero-values of the z-transform of\nh(x,y). Throughout this paper, the sizes of the given image\ng(x,y)and the blur h(x,y)are denoted by M\u0001Nandm\u0001n,\nrespectively. The z-transform H(u,v)ofh(x,y)is written as\nH(u,v)=1\nmnh(x,y)uxvy\ny=0n\u00011\n\u0002\nx=0m\u00011\n\u0002 , (2)\nwhere uand vare complex variables. We first consider\nzero-values of v,which are the solutions of H(u,v)=0for a\ngiven uand denoted by \u0002i(i=1, 2 ,\u0001,n';n'\u0003n\u00011).Putting\nu=\u0003ue\u0001i\u0002u(\u0002u,\u0001u: real parameters), \u0001iare given as a\nfunction of \u0001uand\u0001u,i.e.,\u0001i=\u0001i(\u0003u,\u0002u).\nThe zero-values \u0001isatisfy the following mnequations,\nH(\u0004ue\u0001i\u0003u,\u0002i)=0,\nd\nd\u0005uH(\u0004ue\u0001i\u0003u,\u0002i)=0,\nd2\nd\u0005u2H(\u0004ue\u0001i\u0003u,\u0002i)=0,\n\u0001\ndmn\u00011\nd\u0005umn\u00011H(\u0004ue\u0001i\u0003u,\u0002i)=0,(3)\nwhere\u0001ustands for \u0001uor\u0001u.Equation (3) is mn\nsimultaneous linear equations for h(x,y).Coefficients of\nh(x,y)in each equation of (3) are given in terms of\n\u0001u,\u0001i,\u0003\u0001i/\u0003\u0002u,\u00032\u0001i/\u0003\u0002u2,\u0001 ,and\u0004mn\u00011\u0002i/\u0004\u0003umn\u00011.As\nthe matrix Ccomposed of the coefficients of h(x,y)is\ncomplex in general we relegate it to Appendix A, where we\ngive the explicit form of Cfor only 2\u00013blurs, as a\npreparation to the discussion in the following.\nWe can construct a generator for the CE for m\u0001nblurs in\nterms of the matrix C. The generator is given as C=0,\nwhich is actually the condition for obtaining nontrivial\nsolutions for h(x,y)in (3). As the explicit forms of the CEs\nfor blurs of large sizes are very complex, in the following weConditional Expressions for Blind\nDeconvolution: Derivative form\nS. Aogaki, I. Moritani, T. Sugai, F. Takeutchi, and F.M. Toyama, Kyoto Sangyo University,\nKyoto-603, Japan\nL2discuss with the CE for 2\u00013blurs, as a preparation to the\nnext section.\nFrom C=0we obtain the following CE for 2\u00013blurs,\nE2\u00043\u0003u(\u0002i)\u0005\u0001135\u0002i(2)6+80\u0002i(1)3\u0002i(3)3+15\u0002i(1)4\u0002i(4)2\n\u000160\u0002i(1)3\u0002i(2)\u0002i(3)\u0002i(4)\u000112\u0002i(1)4\u0002i(3)\u0002i(5)\n+18\u0002i(1)3\u0002i(2)2\u0002i(5)+270\u0002i(1)\u0002i(2)4\u0002i(3)\n\u0001180\u0002i(1)2\u0002i(2)2\u0002i(3)2=0,(4)\nwhere\u0001i(k)\u0004\u0003k\u0001i/\u0003\u0002uk.Equation (4) was obtained by taking\n\u0002u=\u0001u.If we take as \u0002u=\u0001uwe obtain another expression\nE2\u00033\u0002u(\u0001i)of the CE for 2\u00013blurs. Although the form\nE2\u00033\u0002u(\u0001i)so obtained is more complex than E2\u00033\u0002u(\u0001i),t h e ya r e\nin fact identical to each other in the sense that the two\nexpressions are related as E2\u00043\u0002u(\u0001i)=\u0003u12E2\u00043\u0003u(\u0001i).This can be\ndirectly verified with the relations between derivatives of\n\u0001iwith respect to \u0001uand\u0001u,\u0005\u0002i/\u0005\u0003u=\u0001i\u0004u\u0005\u0002i/\u0005\u0004u () (the\nrelations between higher degree derivatives are obtained from\nthis relation). Therefore, in the following we discuss only the\nexpression Em\u0003n\u0002u(\u0001i).The CE E2\u00033\u0002u(\u0001i)of (4) is evaluated for\nany single point of \u0001uand\u0001u.In (3) we may use any higher\ndegree derivatives of H(u,v)with respect to \u0001u.However, it\nonly makes the CE more complex. Equation (4) is the simplest\nversion of the CE for 2\u00013blurs.\nThe E2\u00033\u0002u(\u0001i)of (4) has been derived for 2\u00013blurs.\nHowever, E2\u00033\u0002u(\u0001i)implicitly includes the CEs for blurs of the\nsizes smaller than 2\u00013,i.e., E2\u00032\u0002u(\u0001i), E1\u00032\u0002u(\u0001i)and\nE1\u00033\u0002u(\u0001i).This is because E2\u00033\u0002u(\u0001i)can be expressed as a\nlinear combination of E2\u00032\u0002u(\u0001i)orE1\u00033\u0002u(\u0001i).There exists no\nCE of vfor i\u00011(i=1,\u0001,M)blurs. (Note that there are\nmany versions of the CE Em\u0003n\u0002u(\u0001i)in general.)\nFurther, E2\u00032\u0002u(\u0001i)can be expressed as a linear combination of\nE1\u00032\u0002u(\u0001i). Eventually, it is seen that E2\u00033\u0002u(\u0001i)includes the CEs\nfor2\u00012,1\u00012,a n d 1\u00013blurs implicitly. We relegate the\nexplicit decomposition of E2\u00033\u0002u(\u0001i)into E2\u00032\u0002u(\u0001i)to [3]. Here,\nIt is noted that there exists an additional case. The E2\u00033\u0002u(\u0001i)\nalso includes E1\u0003k\u0002u(\u0001i)(k=3+1,\u0001,N)for1\u0001kblurs, which\nare partially larger than 2\u00013.For such blurs, all zero-values\n\u0001i(i=1,\u0001,N';N'\u0002N\u00011)are constants. Hence, any degree\nderivatives of \u0001iare zero. This causes E2\u00033\u0002u(\u0001i)=0.This is\nwhy E2\u00033\u0002u(\u0001i)includes E1\u0003k\u0002u(\u0001i)implicitly. This feature of the\nCE holds for any Em\u0003n\u0002u(\u0001i)in general. This is generally\nproven by basic manipulations for the determinant C .This\nfeature is very useful because this enables us to automatically\ndetect all zero-values of any blurs convolved in the givenimages, if we construct the CE for sufficiently large mandn.\nWe have derived the CE by assuming the\nm\u0001nblur\nh(x,y).When we apply the CE to a given image, we need to\nevaluate derivatives of \u0001iwith respect to \u0001u(i.e.,\u0001uor\u0001u)by using a given image g(x,y)because we have no prior\nknowledge of the blur h(x,y).This is possible as follows. For\nthe zero-values of the blur \u0001i,H(\u0004ue\u0001i\u0003u,\u0002i)=0.Hence, the\nderivatives \u0003\u0001i/\u0003\u0002u,\u00032\u0001i/\u0003\u0002u2,\u0001,a n d\u0004mn\u00011\u0002i/\u0004\u0003umn\u00011\nare respectively expressed as polynomial forms of \u0001ifrom\ndH (\u0004ue\u0001i\u0003u,\u0002i)/d\u0005u=0,d2H(\u0004ue\u0001i\u0003u,\u0002i)/d\u0005u2=0,\u0001,a n d\ndmn\u00011H(\u0004ue\u0001i\u0003u,\u0002i)/d\u0002umn\u00011=0.On the other hand, the\nz-transform G(u,v)of g(x,y) is factorized as\nG(u,v)= g(x,y)uxvy\ny=0N\u00011\u0002x=0M\u00011\u0002 =F(u,v)H(u,v)where F(u,v)\nis the z-transform of the true image f(x,y).I f\u0001iis a\nzero-value of H(\u0004ue\u0001i\u0003u,\u0002i),then H(\u0004ue\u0001i\u0003u,\u0002i)=0and\nF(\u0004ue\u0001i\u0003u,\u0002i)\u00050. Accordingly, for \u0001iof the\nblurs dkG(\u0004ue\u0001i\u0003u,\u0002i)/d\u0005uk=0 (k=1,\u0001,mn\u00011) is\nrespectively equivalent to dkH(\u0004ue\u0001i\u0003u,\u0002i)/d\u0005uk=0.Hence,\nthe analytical forms of \u0004k\u0002i/\u0004\u0003uk(k=1,\u0001,mn\u00011)needed\nfor the CE are actually given in terms of G(\u0004ue\u0001i\u0003u,\u0002i)that is\nthe z-transform of the given image. Thus, the derivatives of \u0001i\nneeded for the CE are all given as polynomials of \u0001i\nthemselves and the given image g(x,y).W eg i v es o m e\nexplicit forms of the derivatives in [3].\nWhen we apply the CE to a given image, first we solve\nG(\u0003ue\u0001i\u0002u,v)=0 for v numerically to obtain\n\u0002i(i=1, 2 ,\u0001,N';N'\u0003N\u00011). Next we substitute them into\nthe CE. As we mentioned above, the CE has been derived by\nusing the z-transform H(u,v)ofh(x,y).Nevertheless, we\ncan evaluate the CE by using a given image g(x,y).This fact\nmakes the CE a very valuable tool. For any \u0001ithat are solved\nfor the given image g(x,y)numerically at any single point of\nuwe can automatically judge whether or not they are of the\nzero-values of the blurs without performing any tough analysisof the zero-sheets of the given image.\nIn order to restore a true image by eliminating blurs from\nt h eg i v e ni m a g ew en e e dt or e p e a tt h es a m ep r o c e d u r ea l s of o r\nzero-values\n\u0002i(i=1, 2,\u0001,M\u00011)of variable u.The CE for \u0001i\nare obtained in the similar manner by putting v=\u0003ve\u0001i\u0002vin (2).\nFor blurs of m=n,the form of the CE for \u0001iis the same as\nthat for\u0001i.Therefore, once we obtain the CE for \u0001iwe can\nobtain the CE for \u0001iby the following replacements \u0001i\u0003\u0002i,\n\u0001u\u0002\u0001v,\u0001u\u0002\u0001v,x\u0001yandm\u0001nin the CE for \u0001iand\nrelevant equations, but g(x,y)must be left as it is. However,\nfor blurs of m\u0001n,the form of the CE for \u0001iis different from\nthat of\u0001i.As a preparation to the next section, for the 2\u00013\ncase we give the explicit form of the CE for \u0001i,i.e.,\u0001\nE2\u00043\u0003v(\u0002i)\u0005\u000140\u0002i(3)3+60\u0002i(2)\u0002i(3)\u0002i(4)\u000115\u0002i(1)\u0002i(4)2\n\u000118\u0002i(2)2\u0002i(5)+12\u0002i(1)\u0002i(3)\u0002i(5)=0,(5)\nwhere\u0001i(k)\u0004\u0003k\u0001i/\u0003\u0002uk.3An advantage of the CEs is that for any zero-values\nnumerically obtained at any single point ofuwe can judge\nwhether or not they are of blurs\u2019 ones by only substituting\nthem into the CEs. Thus, the CEs are useful tools for an\nautomatic blind deconvolution. On the other hand, thecomputational complexity becomes large as the size of thegiven image becomes large because many higher degree\nderivatives of the zero-values\n\u0001iand\u0001iare needed to\nevaluate the CEs.\nIII. I LLUST\u0004ARTION\nFig.1 shows model images used for the illustration in this\nsection. Fig. 1(a) shows a 40\u000140model image that we\nregard as a true image, which has been often used in manyreferences to demonstrate image restorations [4]. Figs. 1(b),1(c), 1(d), and 1(e) represent blur images of the sizes\n1\u00012,\n2\u00011,2\u00012,and 2\u00013,respectively. We convolved these four\nblurs into the true image of Fig. 1(a). Fig. 1(f) shows the\nconvolved image, of which size is 43\u000144.\nWe illustrate how E2\u00033\u0002u(\u0001i)of (4) and E2\u00033\u0002v(\u0001i)of (5)\ndetect zero-values of the blurs. For the parameter \u0001uand\u0001v\nwe took them to be \u0001u=\u0001v=1.We can take any values for\n\u0001uand\u0001vin principle. However, in order to accomplish the\nimage-restoration successfully we have to choose optimal\nvalues for \u0001uand\u0001v.Fig. 2 shows the results of numerical\nevaluations of the CEs for the convolved image shown in Fig.\n1(f). We carried out the evaluation at \u0001u=2\u0002j/4 3\n(j=0,\u0001,4 2) and\u0001v=2\u0002k/4 4 ( k=0,\u0001,4 3 ) . Note that\nwhen we restore the image by the inverse Fourier transform\nwe need the representation of G(u,v)at those points of \u0001u\nand\u0001v.In Fig. 2 we plotted log[| E2\u00033\u0002u(\u0001i)|+1]and\nlog[| E2\u00033\u0002v(\u0001i)|+1]for only four points of each \u0001uand\u0001v.As\nwe have stressed in the preceding section, both E2\u00033\u0002u(\u0001i)and\nE2\u00033\u0002v(\u0001i)include CEs for blurs of smaller sizes\n1\u00012,2\u00011,2\u00012,and 2\u00013implicitly . Therefore, E2\u00033\u0002u(\u0001i)\nmust detect totally four (=1+1+2)zero-values \u0001i.When\nthere exists a degenerate zero-value in the 2\u00013blur, it is\nthree (=1+1+1).On the other hand, the number of\nzero-values \u0001ithat should be detected by E2\u00033\u0002v(\u0001i)is just\nthree (=1+1+1).\nAs seen in Fig. 2(a), for \u0001u=0,four zero-values \u00011,\u000113,\n\u000120,and\u000121are detected by E2\u00033\u0002u(\u0001i). Also at other \u0001ufour\nzero-values are well detected by E2\u00033\u0002u(\u0001i).Here, note that the\nsolution-numbers of the detected zero-values are different at\neach\u0001u.In the evaluation of the CE we solved \u0001i\nnumerically by Mathematica , separately at each \u0001u.\nAccordingly, the order of the numerical solutions\n(zero-values) is at random at each \u0001u.For\u0001v=0,as seen in\u0001\nFig. 1. (a): True image of 40\u000140size that we took from [4]. (b): Blur\nimage of 1\u00012size. (c): Blur image of 2\u00011size. (d): Blur image of 2\u00012\nsize. (e): Blur image of 2\u00013size. (f): The image that was obtained by\nconvolving the four blurs of (b)-(e) into the true image of (a). The size of the\nconvolved image is 43\u000144.\nFig. 2(b), three zero-values \u00011,\u00013,and\u000112are detected by\nE2\u00033\u0002v(\u0001i), as expected. Also at other points \u0001vthree\nzero-values are well detected by E2\u00033\u0002v(\u0001i). As we mentioned\nearlier, E2\u00033\u0002u(\u0001i)and E2\u00033\u0002v(\u0001i)must detect the zero-values of\nblurs of not only the size 2\u00013but also the sizes 1\u00012, 2\u00011,\nand 2\u00012,all at once. The results of Fig. 2 verify that the\ndetection of the zero-values was done successfully.\nIn Fig. 3 we show the restored image by removing four\nzero-values \u0001iinvand three zero-values \u0001iinu.T h e\nrestored image is perfectly the same as the true image of Fig.\n1(a). Thus, we could confirm that E2\u00033\u0002u(\u0001i)and\nE2\u00033\u0002v(\u0001i)certainly detect the zero-values of blurs convolved in\nthe given image. The results show that the CEs would be very\npowerful mathematical tools for the LB\u2019s blind deconvolution.4\nFig. 2. Results of the evaluations of E2\u00033\u0002u(\u0001i)and E2\u00033\u0002v(\u0001i)of (4) and (5). In\n(a) and (b), log[| E2\u00033\u0002u(\u0001i)|+1]and log[| E2\u00033\u0002v(\u0001i)|+1]are plotted for the\nzero-value number i\u0001and i\u0001.We took \u0001uand\u0001vas\u0001u=\u0001v=1.5\nFig. 3. Restored image by removing the four and three zero-values that were\nrespectively detected by E2\u00033\u0002u(\u0001i)and E2\u00033\u0002v(\u0001i).\u0001\nIV. S UMMARY AND CONCLUTION\nWe have derived the CEs for finding blurs convolved in a\ngiven image \u0002in a situation where any noise is absent. The\nCEs are given in terms of the derivatives of the zero-values of\nthe z-transform of the given image. The derivatives of the\nzero-values are given as polynomials of the zero-values\nthemselves. Hence, the CE can be evaluated with only thezero-values evaluated at any single point. This is a great\nadvantage for the CEs. By using the CEs we can avoid a\ntough analysis of the zero-sheets of the given image and\nreduce the processing time.\nThe CEs constructed for m\u0001nblurs can actually detect any\nblurs of the sizes smaller than m\u0001nall at once. Therefore,\nwhen we apply the CEs to the given image, it is desirable to\nconstruct the CEs for blurs of sufficiently large size, although\nwe have to take account of the computational complexity of\nthe image restoration process. The CEs would be very useful\nto make the LB\u2019s blind deconvolution more practical one.\nThe CEs are very complex for blurs of large sizes. This\ncauses a big computational load in the image restoration. In\nour second paper [2], we give the other version of the CEs that\nwe devised to solve this problem. As we mentioned above the\nCEs detect multiple blur all at once. Instead the sizes of the\ndetected blurs cannot be determined, except for a single blur.\nIn our third paper [5] we present a simple method for finding\nonly a single blur of a specified size m\u0001n.\nAPPENDIX A: GENERATOR MATRIX C\nAs an example, for a 2\u00013blur we show the matrix Ccomposed of the coefficients of h(x,y)in (3). The Matrix Cis given\nas\nC=e2i\u0002u1\u0001i \u0001i2\u0003u\u0003u\u0001i \u0003u\u0001i2\n0\u0001i(1)2\u0001i\u0001i(1)1\u0001i+\u0003u\u0001i(1)\u0001i2+2\u0003u\u0001i\u0001i(1)\n0\u0001i(2)2(\u0001i(1)2+\u0001i\u0001i(2))0 2 \u0001i(1)+\u0003u\u0001i(2)2(2\u0001i\u0001i(1)+\u0003u\u0001i(1)2+\u0003u\u0001i\u0001i(2))\n0\u0001i(3)2(3\u0001i(1)\u0001i(2)+\u0001i\u0001i(3))0 3 \u0001i(2)+\u0003u\u0001i(3) 2(3\u0001i(1)2+3\u0001i\u0001i(2)+3\u0003u\u0001i(1)\u0001i(2)\n+\u0003u\u0001i\u0001i(3))\n0\u0001i(4)2(3\u0001i(2)2+4\u0001i(1)\u0001i(3)+\u0001i\u0001i(4))0 4 \u0001i(3)+\u0003u\u0001i(4) 2(12\u0001i(1)\u0001i(2)+4\u0001i\u0001i(3)+3\u0003u\u0001i(2)2\n+4\u0003u\u0001i(1)\u0001i(3)+\u0003u\u0001i\u0001i(4))\n0\u0001i(5)2(10\u0001i(2)\u0001i(3)+5\u0001i(1)\u0001i(4)+\u0001i\u0001i(5))05\u0001i(4)+\u0003u\u0001i(5)2(15\u0001i(2)2+20\u0001i(1)\u0001i(3)+5\u0001i\u0001i(4)\n+10\u0003u\u0001i(2)\u0001i(3)\n+5\u0003u\u0001i(1)\u0001i(4)+\u0003u\u0001i\u0001i(5))\u0004\n\u0006\u0005\n\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0007\n\t\b\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b,\n(A1)\nwhere we took as \u0002u=\u0001uand\u0001i(k)\u0004\u0003k\u0001i/\u0003\u0002uk.The CE E2\u00033\u0002u(\u0001i)of (4) is obtained from C=0.\nACKNOWLEDGMENT\nThis work was supported in part by the Science Research\nPromotion from the Promotion and Mutual Aid Corporationfor private Schools of Japan, and a grant from Institute forComprehensive Research, Kyoto Sangyo University.\nR\nEFERENCES\n[1] R.G. Lane and R.H. Bates, \"Automatic multidimensional\ndeconvolution,\" J. Opt. Soc. Am , vol. A4, 1987, pp. 180-188.[ 2 ] S .A o g a k i ,I .M o r i t a n i ,T .S u g a i ,F .T a k e u t c h i ,a n dF . M .T o y a m a ,\"\nConditional expressions for blind beconvolution: Multi-point form, \"\nICITA2007, submitted.\n[ 3 ] S .A o g a k i ,I .M o r i t a n i ,T .S u g a i ,F .T a k e u t c h i ,a n dF . M .T o y a m a ,\"\nConditional expressions for blind beconvolution, \" J. Opt. Soc. Am ,t o\nbe submitted.\n[4] http://www.imageprocessingplace.com/; Rafael C. Gonzalez, Richard\nE. Woods, Digital image processing, (R.C. Gonzalez, R.E. Woods),\n329-341, Second Edition, 2 002.\n[5] S. Aogaki, I. Moritani, T. Sugai, F. Takeutchi, and F.M. Toyama,\n\"Simple method to eliminate blur based on Lane and Bates\nalgorithm \", ICITA2007, submitted."}
{"category": "abstract", "text": "\u2014We developed novel conditional expressions (CEs)\nfor Lane and Bates\u2019 blind deconvolution. The CEs are given interm of the derivatives of the zero-values of the z-transform of\ngiven images. The CEs make it possible to automatically detectmultiple blur convolved in the given images all at once withoutperforming any analysis of the zero-sheets of the given images.We illustrate the multiple blur-detection by the CEs for a model\nimage.\nIndex Terms -- Deconvolution, Image restoration \u0003\n\u0003\nI. INTRODUCTION\nane and Bates\u2019 (LB) blind deconvolution is based on the\nzero-sheets of the z-transform of given images [1]. It\nenables us to uniquely remove blurs convolved in the given\nimages. However, due to its advanced analytical method the\ncomputational complexity of the image processing is large.\nWe developed novel and useful tool for the LB\u2019s blind\ndeconvolution, i.e., conditional expressions (CEs) that make it\npossible to automatically find zero-values of blurs convolved\nin the given images. The CEs enable us to eliminate the blurs\nin the given images without performing any tough analysis of\nthe zero-sheets of the images. We devised two versions of the\nCEs. One (version 1) is the CE that is given in terms of\nderivatives of zero-values evaluated at a single point. The\nother (version 2) is the CE that is given with zero-values\nevaluated at multi point. In this paper we present only the\nversion 1 of the CEs and we relegate the version 2 to our\nsecond paper [2].\nThe version 1 of the CEs makes the LB\u2019s blind\ndeconvolution perfectly automatic one. It can detect multiple\nblur all at once. In this paper we present the CE and illustratethe blur-detection by the CE for a model image.\nIn Sec. II we discuss the derivation of the CE. In Sec. III we\nillustrate the blur detection by the CE. Section IV is for the\nsummary.\nII. C\nONDITIONAL EXPRESSIONS FOR MULTIPLE BLURS\nWe consider a situation where any noise is absent. In the\nS. Aogaki is with the Department of Information and Communication\nSciences, (tel."}
{"category": "non-abstract", "text": "schlei@LANL.gov\n\u2020E. Mail: prasad@LANL.gov\n1The notion of shapeis intimately related to the notion of boundary orcontour, in that\nthe contour delineates the shape from its exterior, and converse ly the shape is de\ufb01ned in\nextent by its contour.\nIn digital image processing contour extraction is important for the purpose of enclosing\nand characterizing a set of pixels belonging to an object within a given image. In the\nfollowing, we shall work only with bilevel images, i.e., images that contain only pixels\nof two spectral values, e.g., black and white pixels. Without loss of ge nerality let us\nassume that we have therefore only black and white pixels in a given ima ge, where the\nwhite pixels represent pixels of objects, and black pixels represent pixels of holes and the\nuniverse, respectively. Fig. 1.a shows an example of a bilevel image.\n(a)\n (b)\nFigure 1: (a) Bilevel image of a person. The shape consists of two blo bs of which one of\nthem contains a few holes. (b) Contour set of edges, which separa tes white pixels from\nblack ones. We note, that the edges are not connected yet at this stage of the contour\nextraction.\nAs a \ufb01rst step, we shall construct a set of edges, which separate s white pixels from\nblack ones. In order to do this, we note that a given white pixel ihas four corners\nAi,Bi,Ci,Di(cf.Fig. 2). From the pixels corner points one can then construct its fo ur\nedgesAiBi,BiCi,CiDi,DiAi. LetAiBi\u2261ei\n1,BiCi\u2261ei\n2,CiDi\u2261ei\n3andDiAi\u2261ei\n4,\nrespectively. Then we can always tell for a given edge, ei\nj(j= 1,2,3,4), the side where\nthe parent pixel is present. Thus, we always know for a given edge, on which sides the\ninside and the outside of the corresponding shape is located.\n2i\nA\ni\nB\ni\nD\ni\nC\ni\ne\n1\ni\ne\n3\ni\ne\n4\ni\ne\n2\ni\nFigure 2: Corners and edges of the i-th pixel (see text for more detail).\nA pixelican contribute to the set of edges, which separates white pixels fr om black ones,\neither with no, one, two, three or four edges. If two neighboring p ixels are white, the\ncommon edge they share, i.e., ei\n1=ej\n3orei\n2=ej\n4(i/negationslash=j), willnotcontribute to our edge\nlist. Hence, our desired edge list will only consist of edges which have a multiplicity of\none. For the bilevel image in Fig. 1.a we show the corresponding (but n ot yet connected)\nedge set in Fig. 1.b.\nWe would like to emphasize, that all given white pixels could be treated s imultaneously,\nwhen their lower, left, upper, and right edges are examined for can didates in the contour\npixel edge set. Thus, here, an excellent opportunity for the gene ration of parallel com-\nputer code is given by our above described technique.\nIn the second step, we construct connected loops from our cont our edge set which we\nhave generated so far. In doing so, we \ufb01rst enumerate the edges of the contour edge set.\nThen, a new set is created, which is the set of end points (i.e., their co ordinate pairs)\nof the edges themselves. Every point is listed only once, but for eac h point we also list\nwhich edges (their identities are given through their enumeration) a re connected to a\ngiven point. The reader can convince himself easily, that there are a lways either two or\nfour edges connected to a point in the point list.\nAt this stage, all the knowledge for connecting the edges is available . To be speci\ufb01c,\nwe shall generate contours, which are oriented counterclockwise about the objects they\nenclose, and which are oriented clockwise when they enclose holes wit hin the objects,\nrespectively.\nInitially, all edges of the contour edge set are labeled as \u201cunused\u201d. We take the \ufb01rst\n3element of the contour edge set to construct our \ufb01rst contour lo op. If the edge is of type\nei\n1(ei\n2(ei\n3(ei\n4))) we note point Ai(Bi(Ci(Di))) as our \ufb01rst point of the \ufb01rst loop in\norder to ensure its correctorientation. We label our \ufb01rst edge as \u201cused\u201d and then look\nup the end point list to examine to which other edge our current edge is connected at\nthe other point Bi(Ci(Di(Ai))) of our used edge. At this point, we have to distinguish\nbetween two cases. In the \ufb01rst case, only two edges are connect ed to point Bi(Ci(Di\n(Ai))), wherein we just consider the as yet unused edge as our next e dge. In the second\ncase, four edges are connected to point Bi(Ci(Di(Ai))). Then we choose as the next\nedge the unused edge belonging to the current pixel i. In doing so, we always ensure a\nuniquechoiceforbuildingthecontours. Furthermore, weweakent heconnectivity between\npixels, that touch each other only in one point. In fact, as we shall s ee below, our dilated\nversions of the contours will lead to a total separation between tw o pixels that only share\nonecommon point. After having found our next edge j, we shall add its \ufb01rst point to\nour contour point chain list, and then we label that new edge as \u201cuse d\u201d. We reiterate\nthe above described procedure, until we encounter our \ufb01rst \u201cus ed\u201d element of the contour\nedge set. In order to avoid double accounting, we shall notadd the \ufb01rst point of the \ufb01rst\nedge to the contour point chain again. This, then, concludes the co nstruction of the \ufb01rst\ncontour. While creating the contour, we may count the number of e dges, which we have\nused so far and compare it to the total number of edges present in our contour edge set.\nIn case there are still \u201cunused\u201d edges present in our contour edg e set we shall scroll\nthrough our edge list, until we \ufb01nd the \ufb01rst next \u201cunused\u201d edge an d use it as the \ufb01rst\nedge of our next contour.\nWe repeat the above described algorithm to create the new contou r. This is done until\neventually all edges have been used. The result will be a set of point c hains, of which\neach one of them represents a closed contour.\nAt this point, we may note that the smallest contours consist of only four edge points.\nThe latter result is valid for all isolated white pixels, i.e., those, which ha ve no other white\npixels as 4-neighbors.\nFig. 3.a shows the contours for the bilevel image given in Fig. 1.a. We st ress, that in our\nset of point chains each point chain is a list of the \ufb01rst points of our pix el edges. If we\nreplace these points by the middle points of the edges without chang ing their connectivity,\nwe get modi\ufb01ed contours, which are a dilation of the centers of the b oundary pixels. The\ndilated contour version derived from the example given in Fig. 3.a is sho wn in Fig. 3.b.\nIn particular, in places where some of the contours touch themselv es due to two white\nneighboring pixels, which are in contact in only one point, we obtain a de \ufb01nite separation\nof the contour sections.\nWewould like tomention, that another very popularcontour extrac tion algorithmisgiven\nby Ref. [1]. But this algorithm is very slow compared to our algorithm, s ince it explores\nthe 8-neighborhood of the image pixels in a sequential raster scan, whereas we use only\nthe 4-neighborhood of the pixels and all pixels at the same time. Furt hermore, we have\nconvinced ourselves that the algorithm presented in Ref. [1] canno t account for nested\nregions in the image very easily and reliably.\n4(a)\n (b)\nFigure 3: (a) Connected contour set of edges, which separates w hite pixels from black\nones in Fig. 1.a. (b) Dilated contours.\nIn summary, we have presented a partially parallel and otherwise line ar algorithm to\nconstruct dilated contours from bilevel images. We stress, that t he generated contours\nare always nondegenerate, i.e., they always enclose an area larger t han zero, and they\nnever cross or overlap each other. Furthermore, our contours are oriented with respect to\nthe objects and their possible holes."}
{"category": "abstract", "text": "We describe a simple, but e\ufb03cient algorithm for the generati on of dilated con-\ntours from bilevel images. The initial part of the contour ex traction is explained to\nbe a good candidate for parallel computer code generation. T he remainder of the\nalgorithm is of linear nature.\n\u2217E. Mail"}
{"category": "non-abstract", "text": "presenting signs, shortness of \nbreath, electrocardiographic findings, arterial blo od gas \nabnormalities and chest X-ray changes [1]. In addit ion, a \nnumber of more specific imaging studies such as spi ral \ncomputed tomography, pulmonary angiography and \nVentilation/Perfusion (V/Q) scanning are employed.  Obvious \ncases such as normal lungs and multiple large pulmo nary \nemboli are usually easy to diagnose.  The difficult  diagnoses \nare those intermediate between the two extremes. \n \nThe accurate diagnosis of PE is a challenging probl em, but \none which radiographers make on a daily basis. The role of an \nautomated system should at present not be thought o f as a \ndecision support system, and not as replacement to an \nexperienced radiographer. The intrinsic characteris tics of an \nautomated tool would allow for a decrease in the \ninter-observer and intra-observer variability. \n \nThis paper is organized as follows, in Section II, a brief \noverview of VQ scanning is given. Section III deals  with data \n \nS. J. Scurell is with the School of Electrical and Information Engineering \nat the University of Witwatersrand, Johannesburg, S outh Africa; e-mail: \ns.scurell@ee.wits.ac.za).  \nProf. T. Marwala is Carl Emily Fuchs Chair of Syste ms and Control at the \nSchool of Electrical and Information Engineering at  the University of \nWitwatersrand, Johannesburg, South Africa; e-mail: \nt.marwala@ee.wits.ac.za ). \nProf. D. Rubin is professor at the School of Electr ical and Information \nEngineering at the University of Witwatersrand, Joh annesburg, South Africa; \ne-mail: d.rubin@ee.wits.ac.za ). collection. Sections IV and V deals with the implem entation \nand training of the system. The results and conclus ion in \ndiscussed in Sections VI and VII respectively.  \nII.  V/Q SCANNING  \nFor a long time V/Q scanning has been the imaging p rotocol \nof choice for the evaluation of patients with suspe cted PE. The \nprocedure has two parts: Perfusion and ventilation which \nenables visualization the blood and air flow in the  lungs. \n1) Perfusion:  The patient is injected with 5mCi \nTechnetium-99m MAA (macroaggregated albumin). This \nallows the radiologist to visualise the flow of blo od in the \npatients lungs. \n2)  Ventilation:  The patient is asked to inhale 20mCi of \nTechnetium-99m DTPA (diethylenetriamine-pentaacetic  \nacid) labelled aerosol which indicates airflow in t he patients \nlungs. In some instances the patient may swallow th e gas \ninstead of inhaling it. Due to the deposition of th e aerosol in \nthe stomach and throat, artifacts in the image appe ar which \nneed to be removed before diagnosis. \n \nThe scan consists of capturing images from six diff erence \nviews, name the anterior, posterior, left lateral, right lateral, \nleft posterior oblique and right posterior oblique.  \nIII.  DATA COLLECTION  \nRetrospective General Hospital, Baragwanath Hospita l and \nthe Donald Gordon Medical Center. All of these inst itutions \nare located in the Johannesburg area in South Afric a. \n \nEach case consisted of 12 images, six for ventilati on and six \nfor perfusion which represent the views discussed e arlier in \nSection II. All images were exported from APEX View  in \nPCX file format. The images were then imported into  \nMATLAB\u00a9 and stored in a single structure. The V/Q s cans \nwhere obtained using the following protocol:  \n\u2022 Ventilation: 20mCi Tc-99m DTPA \n\u2022 Perfusion: 5mCi Tc-99m MAA \n \nIn total 179 cases where collected and studied retr ospectively.  \nIV.  IMPLEMENTATION  \nThere are several stages the images need to go thro ugh before \na diagnosis can be made. Fig. 1 shows the block dia gram for \nthe system. Different sizes of lung images where te sted, \nincluding 64x64, 32x32 and 16x16. \n Automatic Detection of Pulmonary Embolism using Com putational \nIntelligence \nSimon J. Scurell, Tshilidzi Marwala, and David Rubin \nP  \n  \n \nFig. 1. System Block Diagram \nA.  Image Segmentation \nAll the lung images must at first be segmented. Thi s removes \nunwanted background noise which may include areas o f \nradiation in the stomach and trachea. \n \nThe registration of medical images can be achieved using \nseveral methods, such as using iterative processes that \nestimate affine transformations [2] between a refer ence image \nand a target image. There are two methods which des cribed \nthe use of neural networks [3], [4] in image regist ration. \nMulti-layer perceptron (MLP) methods have been used  to find \nthe edges of the lung from an MRI image [3]. The \ndisadvantage of this method is it semi-automated, s o would \nneed human assistance. Another method describes the  input \nimages being represented by Gabor Wavelets to provi de \nfeature vectors [4]. \n \nOnce the images have been registered, relevant feat ures need \nto be extracted, which describe adequately the imag e \ninformation. Several methods for this are described  in [5], \nsuch as The Karhunen-Lo\u00e9ve Transform, Principle \nComponent Analysis, Discrete Time Fourier Transform  \n(DFT), Hadamard Transform, Haar Transform and Discr ete \nTime Wavelet Transform (DTWT). \n \n1)  Hotspot Removal:  Hot spots are generally caused by the \nradiopharmaceutical getting trapped in a small area  of the lung \ndue to obstructive lung disease or the poor technic al quality \n[6] of the radio nucleotides. Hot spot removal prov ides a \nreduction in areas of high intensity relative to th e surrounding \narea in the image. The equation for determining a h otspot is \ngiven in (1) and is taken from [6]. Generally, hots pot removal \nis only applied to ventilation images. \nqsVyxVCyx HS v \u2265\u2212\u2208 =\u02c6\u02c6),(; ),(           (1) \n2)  Full Scale Histogram Stretch:  A Full Scale Histogram \nStretch (FSHS) is applied to each image; this greatly increases \nthe contrast in the image and allows for more accurate contour \ndetection.  The equation for the FSHS is given below in (2) . \n\uf8f7\n\uf8f8\uf8f6\uf8ec\n\uf8ed\uf8eb\n\u2212\u2212\u00d7 =ABKyxVyxVB o1),( ),(            (2) \nIn this implementation K is set to 256, while A and B are the \nminimum and maximum image intensity respectively. F ig. 2 \nshows the effects of performing a FSHS on an anteri or lung  \n \nFig. 2. Full Scale Histogram Stretch. The left imag es represent \nthe images before the FSHS is performed, notice the  poor \ncontrast in the histogram and the lungs are hardly visible. The \nright images represent the images after a FSHS is p erformed \nshowing an increase in contrast. \n \n \nscan. The histogram in the top left is typical of a n image \nexhibiting low contrast. This is confirmed in the i mage in Fig. \n2 in which the lung is barely visible. The histogra m on the \nright shows much better contrast, which can be seen  in the \nlung image after the FSHS. \n \n3)  Filtering and Contour Extraction:  The image is then \nfiltered to reduce noise in the image and provide s moother \ncontours. Iso-contours are calculated and the lung is \nsegmented from the background. An example of an ant erior \nlung image with overlayed iso-contours can be seen in Fig. 3, \nwhile Fig. 4 shows the lung images before and after  \nsegmentation. The views, from left to right, are an terior, \nposterior, left lateral, right lateral, left poster ior oblique and \nright posterior oblique. \n \nFig. 3. Anterior lung with iso-contours overlayed. \n \n4)  Dethroathing and Destomach:  Dethroating involves the \nremoval of throat artifacts from the ventilation im ages. Throat \nartifacts, caused by the radio pharmaceutical getti ng trapped \nin the patients trachea, leads to areas of high ima ge intensity.   \n  \n \nFig. 4. Images before segmentation. From left to ri ght the \nimages are Anterior, Posterior, Left Lateral, Right  Lateral, \nLeft Posterior Oblique and Right Posterior oblique.  Top row \nshows perfusion scans while the bottom row shows ve ntilation \nscans. \n \n \nFig. 5. Image dethroating. The left hand image show s the lung \nbefore dethroating. The right image shows the lungs  after a \ndethroating is performed. \n \n \nStomach artifacts are caused by the patient swallow ing some \nthe radio pharmaceutical instead of inhaling it. Th e radio \npharmaceutical then settles in the stomach and show s up on \nthe image. These artifacts must be removed for the image \nalignment and classification to be most effective. The entire \nprocedure is show in Fig. 5. The areas shaded in re d indicate \nwhich areas are to be removed from the image. \n \nB.  Image Alignment \nAs pulmonary embolism is identified by matched defe cts in \nperfusion and ventilation images, each VQ pair (the re are six \npairs) must be aligned before subtraction. More spe cifically \nareas where ventilation is present and perfusion ab sent are \nregarded as probable pulmonary emboli. To reduce th e effects \nof lung defects on the alignment, the segmented ima ges are \nconverted to binary images. The alignment is accomp lished \nusing multi-variable (scale, rotation, x-translatio n, \ny-translation) genetic algorithms. \n \nFig. 6 shows the results of an image alignment algo rithm using \nthe GA with Schepp-Logan digital phantom images. Th e \nreference image represents the image that needs to be \noptimised, in other words the origin of the optimis ation  \n \nFig. 6. Result of image alignment using Schepp-Loga n digital \nphantoms. Summary of results can been found in Tabl e I. \n \nTABLE I \nSUMMARY OF GA OPTIMISATION \n \nParameter Actual Found Error  (%)  \nScale 0.9 0.88 2.3 \nRotation 6.5 o 7.15 o 9.1 \nX-translation 25 27 7.4 \nY-translation 15 16 6.25 \n \nproblem. The target image represents the destinatio n of the \noptimisation. The 4 parameters, namely scale, rotat ion, \nx-translation and y-translation provide a transform ation \nbetween the reference image and the target image. T he \ntransformation image represents the reference image , after it \nhas been transformed with the optimized parameters.  Table I \nshows a summary of the parameters found using the G A. \nC.  Image Subtraction \nAfter the images all aligned the ventilation and pe rfusion \nimages are subtracted. The algorithm subtracts the ventilation \nimage from the perfusion image, areas with intensit y values \nless than 0 indicate that there is more ventilation  than \nperfusion in that specific area. The severity of th e defect can \nthen be quantified by taking a magnitude of pixel i ntensity in \nthe subtraction image. \nD.  Feature Extraction \nPCA (principle component analysis) was performed on  the \nimages, from 16x16 to 64x64. As the image size gets  smaller, \nfor the same retained variability (VR), the number of required \neigenvectors decreases. Conversely, for the same nu mber of \neigenvectors, the retained variability increases by  \napproximately 10% for every half reduction in image  size. \nThis trend is most likely caused by a certain amoun t of \nvariability being lost when reducing the image size . \nE.  Feature Extraction \nAfter the number of inputs has been reduced, it can  be further \nminimised by using input selection methods. For thi s the \nstatistical overlay function (SoF) is employed, tak en from [7]. \nThe SoF equation is given in (3). \n22 12 1\n\u03c3 \u03c3\u00b5 \u00b5\u03b4+\u2212=                  (3) \nEach input to the system can be said to have a dist ribution of \npossible values. The goal of input selection is to choose those   \n  \n \nFig. 7. Illustration of the Statistical Overlay Fun ction ( \u03b4) \n \n \ninputs whose distributions show the greatest amount  of \nseparation across the different classes of output. This concept \nis illustrated in Fig. 7. The greater the value of \u03b4, the more \nseparation there is between the two inputs. So \u03b4 between \u00b51, \u03c31 \nand \u00b53, \u03c33 would be greater than the \u03b4 between \u00b51, \u03c31 and \u00b52, \n\u03c32. The number of selected inputs was varied between 10, 20 \nand 30 for comparative purposes. \nV.  SYSTEM TRAINING  \nThe Bayesian MLP was trained in Matlab\u00a9 using the \nNETLAB toolbox ([8]). The network consisted of a si ngle \ninput, hidden and output layer. The number of input  nodes was \nvaried between 10, 20 and 30. The hidden layer cont ained 5 \nnodes while the output layer contained 1 node. \n \nOf the 179 cases, 125 (70%) were used for training (49 \nnegative, 56 intermediate, 20 high), while 54 (30%)  were used \nfor validation (27 negative, 20 intermediate, 7 hig h). The \nnetwork was trained using the Hybrid-Monte-Carlo me thod, \nresulting in a committee of 250 neural networks. \n \nThe scaled PCA inputs were fed into each of the 250  networks \nand the final classification was based upon the mea n value of \nthe committee output. Because each member of the co mmittee \ngives an individual output, 95% confidence levels c an be \ncalculated by taking into account the standard devi ations ( \u03c3) \nof the output distributions. So outputs that exhibi t a small \u03c3 \nimply a high confidence, while outputs exhibiting a  large \u03c3 \nimply a low confidence. \nVI.  MAIN RESULTS  \nThe VR, chosen during the PCA analysis is a paramet er which \nwas varied. A steep increase in training performanc e is gained \nbetween a VR of 70% and 75%. There also appears to be a \ngradual increase in validation performance with inc reasing \nVR. Validation performance also increased with inpu t size. \n Overall the negative cases are shown to be diagnose d with the \nhighest degree of accuracy, followed by the high pr obability \ncases and lastly the intermediate probability cases . This would \nconfirm what was mentioned earlier in that the inte rmediate \ncases are the most difficult to diagnose. The valid ation \nperformance increases with image size, and the numb er of \ninputs to the neural network. Table II shows the se nsitivities, \nspecificities, positive and negative predictive val ues for the \ndifferent output. The specificity and NPV show the highest \nmeans, indicating that PE can be reasonably exclude d if the \nprediction is negative. \n \nTABLE II \nTABLE SHOWING SENSITIVITY, SPECIFICITY, POSITIVE \nPREDICTIVE VALUE (PPV) AND NEGATIVE PREDICTIVE VALU E \n(NPV) USING THE VALIDATION DATA SET \n \n High Intermediate Negative \nSensitivity \n[%]  max 71.43 55.00 70.37 \n min 14.29 0.00 18.52 \n mean 46.83 30.84 48.35 \nSpecificity \n[%]  max 95.92 94.44 77.14 \n min 68.11 68.00 61.37 \n mean 81.03 82.01 69.01 \n max 71.43 62.50 61.54 \nPPV [%] min 6.25 0.00 33.33 \n mean 25.95 41.24 50.97 \n max 95.92 79.07 77.14 \nNPV [%] min 88.68 62.96 55.10 \n mean 92.71 71.44 66.42 \n \n \nThe best system performance is achieved by using a network \nwith 30 inputs, an image size of 64x64 and a VR of 90-90%. \n \nIn order to compare these results with those of pre vious \nauthors, a receiver operating characteristic (ROC) curve is \nneeded. As most of the previous works only consider ed a \n2-class classifier, it was decided that for compara tive purposes \nthe intermediate probability cases would be grouped  together \nwith the high probability cases to form a new \"posi tive\" class. \nThe ROC curve, shown in Fig. 8, was generated from a \nnetwork using an image size of 64x64, 30 inputs and  a VR of \n95%. \n \nThe area under curve (AUC) is 0.64. This compares t o an \nAUC of 0.86 achieved in [9]. An interesting compari son is to \nthe work done in [6] where an AUC of 0.85 was achie ved \nwhen the gold standard was angiography, while an AU C of \n0.67 was achieved when the gold standard was the co nsensus \nopinion of nuclear medicine physicians. This compar es \nfavourably with the AUC of 0.64 in this study, wher e the gold \nstandard was also the opinion of a nuclear medicine  physician. \nVII.  CONCLUSION  \nAlthough performance is below that of previous work s, it has \nbeen shown that it is feasible to diagnose PE autom atically \nusing a Bayesian Neural Network. The most critical areas in \nthe process are those of image segmentation, featur e   \n  \n \nFig. 8. ROC curve for Bayesian Classifier. 64x64 im age size, \nusing 30 inputs and a VR of 95\\%. The AUC is 0.64. \nIntermediate and high probability cases are grouped  together. \n \n \nextraction and input normalisation. Without correct  scaling of \nthe input data, the network simply does not \"work\".  The image \nsegmentation routines proved to be very effective i n most \ncircumstances. There were however a few instances w here \nmanual touch-up was needed. This was mostly done on  \nventilation images where the scans contained large amounts of \nnoise. \n \nFrom the results, it would appear that the best sys tem \nperformance is achieved by having a network with 30  inputs \nusing an image size of 64x64 and a VR of 90-95%. \nIntermediate cases were shown to be the most diffic ult to \ndiagnose correctly, while specificity calculations show that PE \ncan reasonably be excluded if the prediction is not  positive. \n \nThis research could be furthered by the collection of more \ncases to provide a more balanced data set including  equal \nnumbers of each class for both training and validat ion. It is \nlikely that a large increase in available cases wil l greatly \nimprove system performance. \n \nIf possible, using a different Gold Standard , angiography for \ninstance, may improve the performance of the Bayesi an \nclassifier. \nACKNOWLEDGMENT  \nI would like to thank the staff of the Chris Hani B aragwanath \nHospital, Johannesburg General Hospital and the Don ald \nGordon Medical Centre for their assistance in obtai ning the \nimaging data. A special thanks must go to Dr Carlos  Liebhabe. \n \nThis work was supported by DENEL and the Ledger Pro ject. \n REFERENCES  \n[1]  M. Rodger and P. Wells, \u201cDiagnosis of pulmonary emb olism,\u201d \nThrombosis Research , vol. 103, pp. 224\u2013238, 200. \n[2]  K. Cios, L. Goodenday, and J. Sacha, \u201cBayesian lear ning for cardiac \nSPECT image interpretation,\u201d Artificial Intelligence in Medicine , vol. \n26, no. 1-2, pp. 109\u2013143, September-October 2002. \n[3]   R. Damper and I. Middleton, \u201cSegmentation of magne tic resonance \nimages using a combination of neural networks and a ctive contour \nmodels,\u201d Medical Engineering & Physics , vol. 26, no. 1, pp. 71\u201386, \nJanuary 2003. \n[4]  G. Coppini, S. Diciotti, and G. Vialli, \u201cMatching o f medical images by \nself-organizing neural networks,\u201d Pattern Recognition Letters , vol. 25, \nno. 3, pp. 341\u2013352, February 2004. \n[5]  K. Koutroumbas and S. Theodoridis, Pattern Recognition . Academic \nPress, 1999. \n[6]  A. Frigyesi, \u201cAn automated method for the detection  of pulmonary \nembolism in V/Q scans,\u201d Medical Image Analysis , vol. 7, no. 7, pp. \n341\u2013349, 2003. \n[7]  T. Marwala, \u201cFault identification using neural netw orks and vibration \ndata,\u201d Ph.D. dissertation, University of Cambridge,  2001. \n[8]  I. T. Nabney, \u201cNETLAB,\u201d www, June 2002. \n[9]  A. Ericsson, A. Huart, A. Ekefjrd, K. strm, H. Hols t, E. Evander, P. \nWollmer, and L. Edenbrandt, \u201cAutomated interpretati on of \nventilation-perfusion lung scintigrams for the diag nosis of pulmonary \nembolism using Support Vector Machines,\u201d in 13 th  Scandinavian \nConference on Image Analysis . SCIA, June 2003."}
{"category": "abstract", "text": "\u2014 This article describes the implementation of a \nsystem designed to automatically detect the presenc e of \npulmonary embolism in lung scans. These images are firstly \nsegmented, before alignment and feature extraction using PCA. \nIn total 179 cases were collected from various medi cal \ninstitutions. Of the 179 cases, 125 (70%) were used  for training \n(49 negative, 56 intermediate, 20 high), while 54 ( 30%) were \nused for validation (27 negative, 20 intermediate, 7 high). The \nnetwork was trained using the Hybrid-Monte-Carlo me thod, \nresulting in a committee of 250 neural networks. Th e best system \nperformance is achieved by using a network with 30 inputs, an \nimage size of 64x64 and a VR of 90-90% which result ed in an \nAUC of 0.64. This compares favourably with previous  work in \nwhich an AUC of 0.67 was achieved. \nI.  INTRODUCTION  \nULMONARY embolism (PE) is a potentially \nlife-threatening condition, yet it is frequently tr eatable. \nThe diagnosis of PE is based on a number of relativ ely \nnon-specific findings including"}
{"category": "non-abstract", "text": "image grading, wavelet and curvelet transforms, moments, vari-\nance, skewness, kurtosis.\n1arXiv:0802.3528v1  [cs.CV]  24 Feb 20081 Image Grading as a Content-Based Image Re-\ntrieval Problem\nPhysical sieves are used to classify crushed stone based on size and granularity.\nThen mixes of aggregate are used. We directly address the problem of classifying\nthe mixtures, and we assess the algorithmic potential of this approach which\nhas considerable industrial importance.\nThe success of content-based image \fnding and retrieval is most marked\nwhen the user's requirements are very speci\fc. An example of a speci\fc appli-\ncation domain is the grading of engineering materials. Civil engineering con-\nstruction aggregate sizing is carried out in the industrial context by passing the\nmaterial over sieves or screens of particular sizes. Aggregate is a 3-dimensional\nmaterial (images are shown later in this article) and as such need not neces-\nsarily meet the screen aperture size in all directions so as to pass through that\nscreen. The British Standard and other speci\fcations suggest that any single\nsize aggregate may contain a percentage of larger and smaller sizes, the mag-\nnitude of this percentage depending on the use to which the aggregate is to be\nput. An ability to measure the size and shape characteristics of an aggregate\nor mix of aggregate, ideally quickly, is desirable to enable the most e\u000ecient use\nto be made of the aggregate and binder available. This area of application is\nan ideal one for image content-based matching and retrieval, in support of au-\ntomated grading. Compliance with mixture speci\fcation is tested by means of\nmatch against an image database of standard images, leading to an automated\n\\virtual sieve\". Previous work includes Murtagh et al. (2005a, 2005b).\nIn this work we do not seek to discriminate as such between particles of\nvarying sizes and granularities, but rather to directly classify mixtures. Our\nwork shows the extent to which we can successfully address this more practical\nand operational problem. As a \\virtual sieve\" this classi\fcation of mixtures\nis far more powerful than physical sieving which can only handle individual\ncomponents in the mixtures.\nIn section 2 we will review the properties of multiresolution transform co-\ne\u000ecients, given our planned use of these coe\u000ecients to discriminate between\nimages and thus support image retrieval and/or grading. In section 3 we carry\nout a detailed study to assess the moments of multiscale transforms, wavelet\nand curvelet transforms, and use of images of di\u000berent smoothness and edginess\ncharacteristics. In section 4 we carry this work further into a practical domain\nof application of image grading, viz. that of the civil engineering construction\nmaterials.\n2 Distributions of Multiresolution Coe\u000ecients\n2.1 Gaussian Distribution of Wavelet Coe\u000ecients\nNoise \fltering from a wavelet transformed image, based on a Gaussian model, is\nhighly developed in theory and practice. The monographs Starck et al. (1998a)\n2and Starck and Murtagh (2006) study in great detail wavelet-based noise \flter-\ning. Unlike e.g. smoothness criteria in noise \fltering (Donoho and Johnstone,\n1995), our perspective in this work has been towards, or based on, de\fnable\nnoise models in the data. Such noise \fltering is generalized there for the Poisson\ncase, through variance stabilization. Alternative noise thresholding approaches\nare studied for other distributions, including small count Poisson and Rayleigh.\nThe noise \fltering is also studied in the perspective of optimal image restoration\n(see Starck et al., 1998b), and is incorporated into image deconvolution.\nThe Gaussian statistical model for images is particularly appropriate for the\ncase of CCD (charge coupled device) image detectors, where Gaussian read-out\nnoise is dominant.\nThe Gaussian model leads to a close relationship between multiscale (Shan-\nnon) entropy, wavelet energies, and variances (Starck et al., 1998b). This lends\nweight to the use of the second order moment as an important multiscale feature.\nThe conclusion here is the following: when data is Gaussian distributed,\nthen the modeling is very well understood.\n2.2 Variance or Energy of Multiresolution Coe\u000ecients\nThe analysis of texture has used Markov modeling of spatial context (Cross and\nJain, 1983) and a wavelet transform provides another way to incorporate local\nspatial relationships. Unser (1995) used co-occurrence matrices and concluded\nthat second order statistics may be best for segmentation of microtextures. The\nuse of a wavelet transform for this purpose was \frst proposed by Mallat (1989).\nScheunders et al. (1998) discuss multiband features, which they use with 3-band\ncolor data. In Livens et al. (1996), energy in di\u000berent bands is used. This does\nnot provide image rotational invariance and for this a sum of energies over bands\nat a given resolution scale is proposed. Fatemi-Ghomi (1997) uses window size\nrelated to resolution scale within which to de\fne features, and she discusses\nadaptive window sizes.\n2.3 Long Tailed Distribution of Wavelet Coe\u000ecients\nIn the general use of multiresolution transforms, it is well known that wavelet\ncoe\u000ecients can be of long tailed distribution (Belge et al., 2000; Buccigrossi\nand Simoncelli, 1999; Murtagh and Starck, 2003). Long tailed distributed data\ninclude data characterized by long range interactions, long memory processes,\nfractal or multifractal or self-similar processes, multiplicative noise regimes (An-\nteneodo and Tsallis, 2003), and signals with burstiness, abrupt changes, and\nspikes (Bezerianos et al., 2003). Applications to thresholding are in Murtagh\nand Starck (2003) and Wang and Chung (2005). The L\u0013 evy distribution char-\nacterizes many such cases: L(x)/\u0000\n1\u0000(1\u0000q)x\n\u0015\u00011=(1\u0000q). When parameter\nq!1, this power law approaches an exponential law: exp\u0000\n\u0000x\n\u0015\u0001\n, which typi\fes\nBoltzmann-Gibbs thermodynamics and Gaussian statistics.\n32.4 Multiresolution Tsallis Entropy\nStarck et al. (1998b) and Starck and Murtagh (2006) used multiscale Shannon\nentropy for image \fltering, showing how it is clearly related to the second order\nmoment in the Gaussian case.\nIn 1928 Hartley developed the entropy of equiprobable events, and in 1948\nthis was generalized by Shannon and widely applied as a basis for the theory of\ncommunications. As opposed to the coding objectives based on events, a statis-\ntical mechanics objective based on system states was developed, such that the\nsame Boltzmann-Gibbs-Shannon entropy results. In 1960 R\u0013 enyi generalized the\nrecursive rather than linear estimation. From 1988 onwards Tsallis developed a\ngeneralized form of entropy, which happens to di\u000ber from Shannon and R\u0013 enyi\nentropies in being non-logarithmic, to cater for fractal and self-similar systems,\ni.e. systems where invariance across resolution scales is of importance. (See\nKaniadakis and Lissia, 2004.) The roots as such of Tsallis entropy go back to\n1970 (Abe and Rajagopal, 2000). The Shannon or Boltzmann-Gibbs-Shannon\nentropy is: S=\u0000P\nipilnpi. In the thermodynamics perspective, as opposed\nto the event space view, piis the probability that the system is found in the ith\ncon\fguration.\nThe non-extensive or Tsallis entropy, with parameter q, a positive real, is\ngiven byST\nq=\u00001\nq\u00001(P\ni(pq\ni\u0000pi)) =\u00001\nq\u00001(P\nipq\ni\u00001) As for the Shannon\nentropy we may consider a constant of proportionality here, the Boltzmann\nconstant, which we have set to 1.\nThe Tsallis entropy has been proposed for long tailed data. Tsallis entropy of\nwavelet transformed data was used in Rosso et al. (2002). An image thresholding\napproach was related to Tsallis entropy in Portes de Albuquerque et al. (2004).\nSporring and Weickert (1999) considered R\u0013 enyi entropy in scale spaces: Tsallis\nentropy, as we have suggested above, may be more appropriate. Tsallis entropy\nwas used on scale space transformed data by Tanaka et al. (1999). In Costa et al.\n(2002) Tsallis entropy is applied to one-dimensional signals on a regular discrete\nrange of resolution scales, and plotted, in order to characterize biomedical data.\n2.5 Distribution of Wavelet Coe\u000ecients in Practice: a\nCase Study\nIn this section we will take an empirical standpoint, using a batch of images.\nWe ask: How do we assess Gaussianity or long tailedness of wavelet coe\u000ecients?\nThe image shown in Figure 1 is from the application which motivated this\nwork. It is characterized by textured signal, and the image's distributional\nmodel may be useful to us for handling irregular variation in texture. The\ndistributions of wavelet scales 1, 2, 3, 4 and 5, furnished by a B 3spline \u0012 a\ntrous wavelet transform, were determined using, in each case, a histogram with\n100 bins. Figure 2 shows these distributions. Wavelet scales 1, 2 and 5 look\nsomewhat long tailed; on the other hand wavelet scales 3 and 4 look more\nsymmetric.\nWe tested the distributions shown in Figure 2 (Markwardt, 2004), using the\n4Figure 1: An image of construction aggregate. The properties of the aggregate\nare de\fned by size for larger pieces, and by granularity for \fner pieces.\n020 40 60 8010002000 6000 10000\n100 histogram binsCountsImage Dsc02004_a\nFigure 2: Distributions of wavelet scales of the image shown in Figure 1. The\nhigher peaked curves are from wavelet scales 1, 3, and 2, respectively, from left\nto right. The second lowest peaked curve, and the lowest one, are from wavelet\nscales 4 and 5, respectively.\n5Table 1: MSEs of \fts. For scales 1, 2, and 5, the \ft by a Lorentzian outperforms\nthe \ft by a Gaussian. However, for scales 3 and 4, a Gaussian \ft is better.\nWavelet scale Lorentzian \ft Gaussian \ft\n1 1.24 4.90\n2 0.07 29.55\n3 1411.76 594.46\n4 73890.6 50390.1\n5 156515.0 271948.0\nmean square error (MSE) of the \fts by a Lorentzian (also called Cauchy, a long\ntailed distribution) and a Gaussian to the wavelet scales. Results are shown\nin Table 1. We conclude that both Gaussian and Lorentzian distributions may\nwell be relevant to practical image analysis situations of the sort considered.\nTaking the 6 images to be discussed below (Figure 8), we looked at the \fts\nof Lorentzian and Gaussian distributions. Table 2 shows the results. For each\nof the selected images we look at Lorentzian versus Gaussian peak \fts to 5\nwavelet coe\u000ecient levels. Sometimes the long tailed Lorentzian gives a better\nresult, maybe even a spectacularly better result. However this is not always the\ncase. Sometimes the Gaussian \ft is far better, and we also note that high MSE\nvalues may indicate that neither distribution is particularly good.\n2.6 Role of Multiresolution Coe\u000ecient Moments\nWavelet coe\u000ecients are often of long tailed (hence not Gaussian) distribution\nbut we \fnd them also sometimes to be close to Gaussian. The Shannon entropy\nis appropriate for Gaussian distributed data, whereas the Tsallis non-extensive\nentropy is associated with power law, long tailed data. In practice, distribution\nmixtures, at di\u000berent wavelet resolution scales, of long tailed and Gaussian\ndistributed data must be handled.\nWe \fnd that both Gaussian/Shannon and long-tailed/Tsallis perspectives\nare potentially useful in practice. While we can proxy the former with the second\norder moment (Starck et al., 1998b, Starck and Murtagh, 2006), the situation is\nnot the same for the latter because of the many possible distributions, and the\nimportance of higher order moments. (A Gaussian is completely characterized\nby moments of order 1 and 2.)\nFor general analysis where multiresolution coe\u000ecients follow a mixture of dis-\ntribution families, a convenient and practical way to carry out the analysis is by\nusing higher order moments of the multiresolution coe\u000ecients as proxies of the\nunknown, underlying distributions. In the absence of a clear distribution hold-\ning for the multiresolution scales, we are better o\u000b keeping to multiresolution\ncoe\u000ecient moments for reasons of e\u000bectiveness, practicality and convenience.\n6Table 2: Image sequence number chosen: these are the images shown (in succes-\nsion, from upper left) in Figure 8. For each image, 5 wavelet resolution scales\nare studied. 2D Lorentzian and Gaussian \fts are shown: MSE (mean square\nerror) used. An asterisk indicates whether Lorentzian or Gaussian \ft is better.\nSeq. no. Scale Lorentzian Gaussian\n1 1 * 31.9 43.3\n2 1397.2 * 9.1\n3 * 2982.0 10404.7\n4 * 77135.4 122607.0\n5 * 192195.0 276682.0\n60 1 37.6 * 28.7\n2 * 18.7 134.8\n3 * 22180.5 26668.1\n4 * 37069.2 44615.1\n5 * 859.6 875.7\n120 1 * 3.3 5.6\n2 * 2.7 8.1\n3 * 23.8 214.8\n4 2.0 * 0.0\n5 86422.3 * 1.4\n180 1 49.1 * 6.6\n2 * 0.6 5.4\n3 9817.3 * 74.0\n4 7739.2 * 5.5\n5 * 51196.0 75436.2\n240 1 * 0.5 0.8\n2 * 0.3 23.4\n3 88.0 * 5.8\n4 * 591.3 46947.3\n5 * 3315.3 85459.2\n300 1 * 3.8 12.2\n2 2506.9 * 10.3\n3 39793.6 * 48.3\n4 13137.1 * 108.6\n5 * 211860.0 243913.0\n72.7 Third and Fourth Order Moments as Features\nThe use of higher order moments, beyond the \frst and second, for texture\nanalysis is well-established (Tsatsanis and Giannakis, 1992; Chandran et al.,\n1997; Avil\u0013 es-Cruz et al., 2005). Spatial modeling is used in Popovici and Thiran\n(2004). We use spatial models as part and parcel of the multiscale transforms.\nOther, less typical, applications of texture analysis using higher order moments\ninclude Kim and Strauss (1998), who apply this approach to the \\textures\" of\npoint pattern distributions.\nWe have motivated higher order moments in the context of long tailed distri-\nbutions of multiscale transform coe\u000ecients. In the next section we will illustrate\nexperimentally the potential usefulness of higher order moments in the multi-\nscale transform context.\n3 Selection of Multiresolution Features: Setting\nthe Context\n3.1 Wavelet Transform and Curvelet Transform\nWith the B 3spline \u0012 a trous redundant wavelet transform (Starck et al., 2007),\nthere are no aliasing e\u000bects due to decimation, and the wavelet function (similar\nto a Mexican hat function) is symmetric, and within the limits of separability\nof use in horizontal and vertical image directions it approximates an isotropic\nfunction. See Starck et al. (1998b), Starck and Murtagh (2002), and Starck\net al. (2006). The (pixelwise additive) decomposition of the image was, for\nall experiments described below, 5 wavelet resolution scales plus the smooth\ncontinuum image.\nThis wavelet transform uses a particular set of basis functions, which are\nde\fned by roughly isotropic functions present at all scales and locations. Hence\nthis wavelet transform is appropriate for isotropic features or mildly anisotropic\nfeatures. To move beyond the wavelet transform, a range of other basis function\nsets have been used, with properties relating to alignments, elongations, edges,\nand indeed curved features. Non-wavelet multiresolution transforms therefore\ntarget the detection and characterization of non-Gaussian signatures in the im-\nage data.\nIn Starck et al. (2004, 2005) the kurtosis (fourth order moment) was used\nto understand the nature of complex non-isotropic features in cosmology. The\nskewness and variance (third, second order moments) were also discussed. Con-\nsequently we wished to investigate the use of these moments in our work.\nThe ridgelet transform uses wavelet-like functions which are constant along\nlinesx1cos\u0012+x2sin\u0012= Const., where a \fxed set of angles \u0012is used; and\nx1;x2are related to scaling through a dyadic multiplicity factor. The ridgelet\ntransform can be shown to be the application of a 1-dimensional wavelet trans-\nform to constant angle slices of the Radon transform. The ridgelet transform\nis a good pattern matcher for sheets at varying scales and positions, whereas\n8the redundant \u0012 a trous wavelet transform targets (isotropic or near isotropic)\nclusters.\nTo \fnd curved features the curvelet transform is used. The curvelet trans-\nform \frst decomposes the image into a set of wavelet bands. Then each band\nis analyzed with a local (i.e., blockwise) ridgelet transform. See Starck et al.\n(2002). The curvelet transform is an e\u000bective tool for curve \fnding at multiple\nresolution levels. The command curstat in the MR package (MR, 2004) was\nused for the curvelet transform. Six scales were used, with a ridgelet block size\nof 16. This gave a total of 19 curvelet coe\u000ecient bands.\n3.2 Data\nIn this section we will present, using a simple procedure, how we can show that\n(i) multiscale transforms other than wavelet transforms, and (ii) higher order\nmoments, may provide the most discriminating features, This is a \\proof of\nconcept\" demonstration, based on a simple but non-trivial image classi\fcation\nproblem.\nWe took four images with a good quantity of curved edge-like structure for\ntwo reasons: \frstly, due to a similar mix of smooth, but noisy in appearance,\nand edge-like regions in our construction images; and secondly, in order to test\nthe curvelet as well as the wavelet transforms. To each image we added three\nrealizations of Gaussian noise of standard deviation 10, and three realizations\nof Gaussian noise of standard deviation 20. Thus for each of our four images,\nwe had seven realizations of it. In all, we used these 28 images.\nExamples are shown in Figure 3. The images used were all of dimensions\n512\u0002512. The images were the widely used test images Lena and Landscape, a\nmammogram, and a satellite view of the city of Derry (Londonderry) and River\nFoyle in Northern Ireland.\nWe expect the e\u000bect of the added noise to make the image increasingly\nsmooth at the more low (i.e., smooth) levels in the multiresolution transform.\nFigure 3: Four images used (top row), and (bottom row) each with added\nGaussian noise of standard deviation 20.\n9The data used therefore was the set of multiresolution transform coe\u000ecients\nfor each of the 7 images relating to one of our four test images. What we will seek\nto do is to \fnd very clear similarity between the 7 images that are all derived\nfrom one initial image. So we will seek a very clear discrimination between the\nfour clusters of image, each cluster having 7 images.\nOur analysis aims at distinguishing between clusters of images, and deter-\nmining the most useful features for this. We address these analyses in an inte-\ngrated way, taking all data into account simultaneously.\nThe 28 images are each characterized by:\n\u000fFor each of 5 wavelet scales resulting from the \u0012 a trous wavelet transform,\nwe determined the 2nd, 3rd and 4th order moments at each scale (hence:\nvariance, skewness and kurtosis). So each image had 15 features.\n\u000fFor each of 19 bands resulting from the curvelet transform, we again deter-\nmined the 2nd, 3rd and 4th order moments at each band (hence: variance,\nskewness and kurtosis). So each image had 57 features.\nThe 28 images were therefore characterized by 72 features, taking spatial\nand frequency resolution scale into account. We did not normalize the images,\nnotwithstanding the varying pixel means. This decision was made in order to\navoid the choice of any ad hoc way of doing this. For the analysis of feature\nimportance, and of how well we can cluster our 28 images into four clusters,\nan important requirement ensues: we must only use relative values, or what we\ncan term a pro\fle of values, and not the absolute values.\nThis issue of relative values is very adroitly handled by correspondence anal-\nysis (Murtagh, 2005). Just as with principal components analysis, the inherent\ndimensionality of both our 28 images in a 72-dimensional space, and our 72\nfeatures in a 28-dimensional space, must be the minimum of 28 and 72. Call\nthe value on feature jfor imageixij, and convert it to a fraction bounded by 0\nand 1:fij=xij=xwherex=P\niP\njxij. Correspondence analysis forms pro-\n\fles both by row (image) and by column (feature): xij=xjfor each row, where\nxjis the column sum; and xij=xifor each column, where xiis the row sum.\nAssumefij\u00150. Given the Gaussian noise, this was not always the case: our\nsole modi\fcation was to enforce a mass, fiorfjto be\u00150.\nThe\u001f2metric is used, de\fned for two rows iandi0as:P\nj1=fj(fij=fi\u0000\nfi0j=fi0)2. A new set of coordinate axes are found to best \ft the data in both\nfeature (28-dimensional) and image (72-dimensional) spaces. This output fac-\ntorspace is endowed with the Euclidean metric, allowing visualization. Unlike\nprincipal components analysis, the scales of both feature and image spaces are\nthe same, so that both rows and columns can be displayed in the output repre-\nsentation.\nThe percentage inertia explained by the \frst factor (tantamount to the over-\nall information content explained by this factor) was 86.9%, indicating a highly\none-dimensional underlying manifold in the dual spaces of images and of fea-\ntures. Note again that while the absolute input values varied greatly in ac-\ncordance with originating image, the use of pro\fles in correspondence analysis\n10guarantees that this very pronounced one-dimensionality is a characteristic of\nthe data.\nFigure 4 shows both images and features on the principal factor plane. The\nimages in cluster 3 (the mammogram ones) are completely superimposed. The\nimages in cluster 4 (the Derry ones) are close. The images in the two other\nclusters (cluster 1: Lena; and cluster 2: landscape) are arrayed somewhat diag-\nonally. In all cases there is clear distinction between image clusters.\nTo assess in\ruence of features, we can avail of the contributions , de\fned as\nsum of mass times projected distance squared, of the features relative to the \frst\n(predominant) factor. For feature j, its mass is fj. Let its projected value on\nfactor 1 be F1(j). Then its contribution to this factor is fjF2\n1(j). Contributions\nare commonly used in correspondence analysis to interpret the results (Murtagh,\n2005). Just two features are found to be important. These correspond to the\ntwo \\blips\" with contribution values 0.116 and 0.437 in the histogram shown in\nFigure 5. These features relate to the curvelet transform in both cases. Firstly\nband 12 and secondly band 16 are at issue. In both cases it is a matter of the\n4th order moment.\nThat these contributions are so pronounced should manifest itself in image\ncluster low dimension projected locations: if anything, the use of these two\nfeatures alone should make our image clusters even more compact. We see this\nin Figure 6. Many labels of images are superimposed there. So we extensively\njittered the points displayed there in Figure 7. This is an erroneous display but\nit helps to understand Figure 6.\nIt is obvious that for a given collection of images, some multiresolution fea-\nture, or set of features, may do just the right job in providing a best discrim-\nination. Our assessment framework has found that two curvelet, 4th order\nmoments, are far and away the best for the image collection used.\n4 Application to Image Grading\nThe image grading problem related to construction materials and involving dis-\ncrimination of aggregate mixes, is exempli\fed in Figure 8. The data capture\nconditions included (i) constant height of camera above the scene imaged, and\n(ii) a constant and soft lighting resulting from two bar lamps, again at \fxed\nheight and orientation. It may be noted that some of the variables we use,\nin particular the variance, would ordinarily require prior image normalization.\nThis was expressly not done in this work on account of the relatively homo-\ngeneous image data capture conditions. In an operational environment such a\nstandardized image capture context would be used.\nThe British Standard speci\fcation sets out nominal proportions of con-\nstituent materials in a mix, which we call a class, in terms of sieve size. Classes,\nin such constituent property space, are overlapping. Our \frst approach was\ntherefore as follows. With di\u000berent feature sets we carried out extensive testing\nof the discrimination properties, initially with training sets from class boundary\nregions, and testing on images from the central regions of the classes. But with\n11\u22121.0 \u22120.5 0.0\u22120.4 \u22120.2 0.0 0.2 0.4\nFactor 1, 86.9% of inertiaFactor 2, 6.7% of inertia1\n111\n1112\n222\n2223333333\n4444\n444*******\n**\n*\n**\n* ****\n**\n**\n*\n****\n*****\n*****\n**\n**\n***\n*\n**** ****\n*Figure 4: Principal factor plane. Clusters of images are displayed with 1 for\n\frst cluster images, 2 for second, and so on. Features are displayed with an\nasterisk.\n12Figure 5: Histogram of values of contributions by features (abscissa). In looking\nfor features with strong contributions to the factor, we \fnd just two here, with\ncontributions of 0.116 and 0.437.\n1320 40 60 80100 150 200 250 300 350 400\nCurvelet band 12, 4th momentCurvelet band 16, 4th moment1234567 89101112131415161718192021\n22232425262728Figure 6: Clusters of images labeled 1{7, 8{14, 15{21, and 22{28 shown as\ncurvelet transform band 12 and band 16, 4th order moment (in both cases) of\ncoe\u000ecients. See Figure 7 for application of jitter to expose the superimposed\npoints.\n1420 40 60 80100 150 200 250 300 350 400\nCurvelet band 12, 4th momentCurvelet band 16, 4th moment12\n345\n6 7 8910\n1112131415161718\n192021\n222324252627 28Figure 7: Identical to Figure 6 but with extensive jitter applied to image labels\nto avoid the superimposed display.\n15Figure 8: Sample images from classes 1 through 6, in sequence from upper left.\n16overlapping classes of irregular and unknown morphologies in any constituent\nproperty space, this was not a productive approach. We instead therefore used\ntraining images from central regions of the classes, and test images from class\nboundaries. Our training data consisted of 12 classes of 50 images, and we se-\nlected 3 classes (classes 2, 4 and 9, spanning the 12 classes) each of 100 images\nas test data.\nAs before we used 5 wavelet scales from a B 3spline \u0012 a trous redundant wavelet\ntransform, and for each scale we determined the wavelet coe\u000ecients' variance,\nkurtosis and skewness. Similarly, using the curvelet transform with 19 bands,\nfor each band we determined the curvelet coe\u000ecients' variance, kurtosis and\nskewness. As before we used therefore 72 features. Our training set comprised\nthree classes, each of 50 images. Our test set comprised three classes, each of\n100 images.\nOur features are diverse in value, and require some form of normalization.\nHowever we know from the discussion in section 2 that, for instance, reduction\nto unit variance would be inappropriate if the feature values are non-Gaussian.\nSo we again use a correspondence analysis of all the data available to us, a\nsuperset of the data so far described, { in all 12 classes (incorporating the 3\nclasses de\fning our training data) of 50 images each, and the 300 test images.\nThe correspondence analysis was carried out on 900 images, each characterized\nby 72 features. One important aim was to map the data, both images and\nfeatures, into a Euclidean space as a preliminary step prior to using k-nearest\nneighbors discriminant analysis.\nThe \frst axis was very dominant (75.7% of the inertia was explained by it),\nand again a curvelet coe\u000ecient feature was the most dominant in the de\fnition\nof this factor: it related to the 16th band in the curvelet transform, and (again)\nthe 4th order moment, or skewness.\nThe assignments of test data (three classes, called here classes 2, 4 and\n9, each of 100 images) to the training data (these three classes, each of 50\nimages) was assessed using k-nearest neighbors. This supervised classi\fcation\napproach was used in view of the di\u000eculty level of this data (we looked at\nlow dimensional displays resulting from the correspondence analysis) and the\nnonlinear properties provided by k-NN. We used k = 1. We assessed:\n\u000fThe 72-dimensional feature space.\n\u000fThe 71-dimensional factor space. (71, because of a linear dependency\nthrough centering the factor space cloud; if there are nrows andm\ncolumns, then this Euclidean embedding dimensionality is min( n\u00001,\nm\u00001)).\n\u000fThen we explored alllow dimensionality spaces, using the ordering of\nfactors. This would make no sense if we did not have a coordinate system\nwith an ordering (of \\importance\", provided by the percentage inertia\nexplained) of the coordinates.\nThe assignments found are shown in Table 3. Justi\fcation for the choice of\nthe 7-dimensional best Euclidean reduced-dimensionality embedding is derived\n17Table 3: Assignments, to classes labeled 2, 4 and 9, for the successive sets of 100\nimages in the test set. In total, there are 300 images in the test set. Discrim-\nination with the 7-dimensional data is far purer than with the 72-dimensional\ndata.\nOriginal data, 72-dimensional\n(found) class 2 4 9\n(real) class 2 22 51 27\n4 6 85 9\n9 5 11 84\n7-dimensional factor space: factors 1{7\n(found) class 2 4 9\n(real) class 2 60 19 21\n4 1 97 2\n9 2 7 91\nfrom Figure 9. For the original, full 72-dimensionality data (Table 3), the correct\nassignments were respectively for the three classes 22, 85 and 84, all out of 100\nimages. For the best Euclidean embedding, viz. the 7-dimensional one, furnished\nby correspondence analysis, the correct assignments were respectively for the\nthree classes 60, 97 and 91, all out of 100 images.\nAs can be seen, the analysis of the low dimensional, correspondence analysis\nresult is impressive relative to analysis of the input data. In this 7-dimensional\nfactor space, we ask next what are the predominant features. Looking at his-\ntograms of the contributions (i.e., sum of mass times projected distance squared\nfrom the origin) by features to factors 1 through 7, a threshold of 0.1 is either\na natural one, or else is a reasonable choice. This furnishes the following pre-\ndominant features as follows:\n\u000fwavelet scale 5, 4th order moment\n\u000fcurvelet band 1, 2nd order moment\n\u000fcurvelet band 7, 3rd and 4th order moments\n\u000fcurvelet band 8, 4th order moment\n\u000fcurvelet band 11, 4th order moment, for two of the factors\n\u000fcurvelet band 12, 4th order moment\n\u000fcurvelet band 16, 4th order moment, for two of the factors\n\u000fcurvelet band 19, 2nd and 4th order moments, in the case of the 4th for\ntwo of the factors\n180 10 20 30 40 50 60 70120 140 160 180 200 220 240\nFactor space dimensionality (from 2 to 71)Totals of correctly assigned test images (out of 300)Figure 9: Totaled correct assignments for best \ft Euclidean (factor) subspaces,\nin dimensions 2 to 71.\n19What is apparent here is that the 4th order moment has clear discriminatory\npower, although it is not unique in this capability. It is also apparent that the\ncurvelet transform is very powerful in furnishing discriminatory features.\n5 Conclusions\nSecond order moment, or energy, has traditionally been used in image retrieval,\nincluding retrieval supported by multiple resolution transforms. For example,\nsee Fatemi-Ghomi (1997), who uses energy at multiple scales; Kubo et al. (2003),\nusing energy and standard deviation at multiple scales; and Kokare et al. (2005),\nusing up to second order autocorrelations, again at multiple scales. In Starck et\nal. (1998b) it was shown how, under a Gaussian model assumption, the second\norder moment could be viewed as a Shannon entropy. For some types of imagery,\nthe second order moment is a useful discriminator. But not for all, and while\nwavelet coe\u000ecients may be long-tailed they are not { as we have shown in this\narticle { always so.\nWe have shown that taking the second, third and fourth moments as features,\nat multiple resolution scales, may enhance discrimination between images in the\nimage set used. Clearly the \frst order moment is of no use to us in the context\nof such transforms.\nOur results point very clearly towards the importance of 4th order moments\nof curvelet transform coe\u000ecients.\nThese moments provide a proxy or substitute for the appropriate entropy to\ncharacterize the information, from among the mixture of appropriate entropies.\nStarck et al. (2004, 2005) take this work in the direction of characterizing non-\nGaussian signatures (e.g., degree of clustering, \flamentarity, sheetedness and\nvoidness) in the data. For this, we use a battery of multiresolution transforms\n(discussing, inter alia, the product of kurtosis values yielded by di\u000berent mul-\ntiresolution transforms at given scales or bands).\nIn this article we have applied this perspective to new classes of image and\nfound excellent results in doing so."}
{"category": "abstract", "text": "We show the potential for classifying images of mixtures of aggregate,\nbased themselves on varying, albeit well-de\fned, sizes and shapes, in or-\nder to provide a far more e\u000bective approach compared to the classi\fcation\nof individual sizes and shapes. While a dominant (additive, stationary)\nGaussian noise component in image data will ensure that wavelet coe\u000e-\ncients are of Gaussian distribution, long tailed distributions (symptomatic,\nfor example, of extreme values) may well hold in practice for wavelet coef-\n\fcients. Energy (2nd order moment) has often been used for image char-\nacterization for image content-based retrieval, and higher order moments\nmay be important also, not least for capturing long tailed distributional\nbehavior. In this work, we assess 2nd, 3rd and 4th order moments of mul-\ntiresolution transform { wavelet and curvelet transform { coe\u000ecients as\nfeatures. As analysis methodology, taking account of image types, mul-\ntiresolution transforms, and moments of coe\u000ecients in the scales or bands,\nwe use correspondence analysis as well as k-nearest neighbors supervised\nclassi\fcation.\nKeywords"}
{"category": "non-abstract", "text": "Shape, Skeleton, Support Vector Machine, Graph Kernel\n1 Introduction\nThe skeleton of a 2 Dshape is de\ufb01ned as the location of the singularities of the\nsigned distance function to the border of the shape. This structu re has several\ninteresting properties: it is thin, homotopic to the shape, invariant under rigid\ntransformationsoftheplaneandmostimportantlyithasanatural interpretation\nas a graph. The representation of a shape by a skeletal (or shock ) graph has\nbecome popular owing the good properties of this representation in herited from\nthe properties of the skeleton. However, beside all this good prop erties, the\nskeletonization is not continuous and small perturbations of the bo undary insert\nstructural noise within the graph encoding the shape.\nSeveral graph based methods have been proposed to compute a d istance be-\ntween shapes robust to such a structural noise. Sharvit et al. [1] propose a graph\nmatching method based on a graduated assignment algorithm. Siddiq i [2] pro-\nposes to transform the shock graph into a tree and then applies a t ree matching\nalgorithm. Pellilo [3] uses the same tree representation but transfo rms the tree\nmatching problem into a maximal clique problem within a speci\ufb01c associat ion\ngraph.\n\u22c6This work is performed in close collaboration with the labor atory Cyc\u00b4 eron and is\nsupported by the CNRS and the r\u00b4 egion Basse-Normandie.2 Fran\u00b8 cois-Xavier Dup\u00b4 e and Luc Brun\nAll the above graph methods operate directly on the space of grap hs which\ncontains almost no mathematical structure. This lack of mathemat ical structure\nforbids the use of basic statistical tools such as the mean or the va riance. Graph\nkernels provide an elegant solution to this problem. Using appropriat e kernels,\ngraphs can be mapped either explicitly or implicitly into a vector space w hose\ndot product corresponds to the kernel function. All the \u201cnatur al\u201d operations\non a set of graphs which were not de\ufb01ned in the original graph space are now\npossible into this transformed vector space. In particular, graph kernels may be\ncombined with the kernelised version of robust classi\ufb01cation algorith ms such as\nthe Support Vector Machine (SVM).\nA Graph kernel used within the shape representation framework s hould\ntake into account the structural noise induced by the skeletoniza tion process.\nBunke [4] proposes to combine edit distance and graph kernels by us ing a set\nofnprototype graphs {g1,...,g n}. Given a graph edit distance d(.,.), Bunke\nassociates to each graph gthe vector\u03c6(g) = (d(g,g1)...,d(g,gn)). The kernel\nk(g1,g2) between the two graphs g1andg2is then de\ufb01ned as the dot product\n<\u03c6(g1),\u03c6(g2)>.\nNeuhaus [5] proposes a similar idea by de\ufb01ning for a prototype graph g0, the\nkernel:kg0(g,g\u2032) =1\n2/parenleftbig\nd2(g,g0)+d2(g0,g\u2032)\u2212d2(g,g\u2032)/parenrightbig\n, whered(.,.) denotes the\ngraph edit distance. Several graph prototypes may be incorpora ted by summing\nor multiplying such kernels. Using both Neuhaus [5] and Bunke [4] kern els two\nclose graphs should have close edit distance to the di\ufb00erent graph p rototypes.\nThe metric induced by such graph kernels is thus relative both to the weights\nused to de\ufb01ne the edit distance and to the graph prototypes. This explicit use\nof prototype graphs may appear as arti\ufb01cial in some application. Mo reover, the\nde\ufb01nite positive property of these kernels may not in general be gu aranteed.\nSuard [6] proposes to use the notion of bag of paths of \ufb01nite length for shape\nmatching. This method associates to each graph all its paths whose length is\nlower than a given threshold. The basic idea of this approach is that t wo close\nshapes should share a large amount of paths. A kernel between th ese sets should\nthus re\ufb02ect this proximity. However, small perturbations may dra stically reduce\nthe number of common paths between two shapes (Section 3.2). Mo reover, the\nstraightforward de\ufb01nition of a kernel between set of paths does not lead to a\nde\ufb01nite positive kernel (Section 2.1).\nThis paper proposesa new de\ufb01nite positive kernelbetween set of p aths which\ntakes into account the structural noise induced by the skeletoniz ation process.\nWe \ufb01rst present in Section 2 the bag of paths approach for shape s imilarity. Our\ncontributions to this \ufb01eld are then presented in Section 3. The e\ufb00ec tiveness of\nour method is demonstrated through experiments in Section 4.\n2 Kernels on bag of paths\nLet us consider a graph G= (V,E) whereVdenotes the set of vertices and\nE\u2282V\u00d7Vthe set of edges. As mentioned in Section 1 a bag of paths Pof\nlengthsassociated to Gcontains all the paths of Gof length lower than s. WeHierarchical bag of paths for kernel based shape classi\ufb01cat ion 3\ndenote by |P|the number of paths inside P. Let us denote by Kpatha generic\nkernel between paths. Given two graphs G1andG2and two paths h1\u2208P1\nandh2\u2208P2of respectively G1andG2,Kpath(h1,h2) may be interpreted as a\nmeasure ofsimilarity between h1andh2and thus as a local measureof similarity\nbetween these two graphs. The aim of a kernel between bags of pa ths consits to\nagregate all these local measures between pairs of paths into a glo bal similarity\nmeasure between the two graphs.\n2.1 The max kernel\nThis \ufb01rst method, proposed by Suard [6], uses the kernel Kpathas a measure of\nsimilarityand computes for eachpath h1\u2208P1the similaritywith its closestpath\ninP2(maxhj\u2208P2Kpath(h1,hj)). A \ufb01rst global measure of similarity between P1\nandP2is then de\ufb01ned as:\n\u02c6Kmax(G1,G2) =\u02c6Kmax(P1,P2) =1\n|P1|/summationdisplay\nhi\u2208P1max\nhj\u2208P2Kpath(hi,hj).(1)\nThe function \u02c6Kmax(G1,G2) is however not symmetric according to G1and\nG2. Suard obtains a symmetric function interpreted as a graph kerne l by taking\nthe mean of \u02c6Kmax(G1,G2) and\u02c6Kmax(G2,G1):\nKmax(G1,G2) =1\n2/bracketleftBig\n\u02c6Kmax(G1,G2)+\u02c6Kmax(G2,G1)/bracketrightBig\n. (2)\nThis kernelis not positive de\ufb01nite in general.Howeveras shownby Ha asdonk\n[7], SVM with inde\ufb01nite kernels have in some cases a geometrical interp retation\nas the maximization of distances between convex hulls. Moreover, e xperiments\n(section 4, and [6]) show that this kernel usually leads to valuable res ults.\n2.2 The matching kernel\nThe non de\ufb01nite positiveness of the kernel Kmaxis mainly due to the max oper-\nator. Suard [6] proposes to replace the kernel Kpathby a kernel which decreases\nabruptly when the two paths are di\ufb00erent. The resulting kernel is d e\ufb01ned as:\nKmatching(G1,G2) =Kmatching(P1,P2) = (3)\n1\n|P1|1\n|P2|/summationdisplay\nhi\u2208P1/summationdisplay\nhj\u2208P2exp/parenleftBigg\n\u2212d2\npath(hi,hj)\n2\u03c32/parenrightBigg\n.\nwheredpathis the distance associated to the kernel Kpathand de\ufb01ned by:\nd2\npath(h1,h2) =Kpath(h1,h1)+Kpath(h2,h2)\u22122Kpath(h1,h2).\nThe resulting function de\ufb01nes a de\ufb01nite positive kernel. This kernel relies\non the assumption that using a small value of \u03c3, the couple of paths with the\nsmallest distance will predominate the others in equation 3. This kern el may\nthus lead to erroneous results if the distance are of the same orde r of magnitude\nthan\u03c3or if several couples of paths have nearly similar distances.4 Fran\u00b8 cois-Xavier Dup\u00b4 e and Luc Brun\n(a) Sets on the unit\nsphere(b) original (c) edge protru-\nsion(d) node\ninsertion\nFig.1.Separating two sets using one-class SVM (a). The symbols ( w1,\u03c11) and (w2,\u03c12)\ndenote the parameters of the two hyperplanes which are repre sented by dashed lines.\nIn\ufb02uence of small perturbations on the bag of paths ((b), (c) and (d))\n2.3 The change detection kernel\nDesobry [8] proposed a general approach for the comparison of t wo sets which\nhas straightforward applications in the design of a kernel between bags (sets)\nof paths. Desobry models the two sets as the observation of two s ets of random\nvariables in a feature space and proposes to estimate a distance be tween the two\ndistributions without explicitly building the pdf of the two sets.\nThe feature space considered by Desobry is based on the normalise d kernel\n(K(h,h\u2032) =Kpath(h,h\u2032)//radicalbig\n(Kpath(h,h)Kpath(h\u2032,h\u2032))). Using such a kernel we\nhave/bardblh/bardbl2\nK=K(h,h) = 1 for any path. The image in the feature space of our set\nof paths lies thus on an hypersphere of radius 1 centered at the or igin (Fig. 1).\nDesobry de\ufb01nes a region on this sphere by using a single class \u03bd-SVM. This\nregion corresponds to the density support estimate of the unkno wn pdf of the\nset of paths [8].\nUsing Desobry\u2019s method, two set of vectors are thus map onto two regions\nof the unit sphere and the distance between the two regions corre sponds to a\ndistance between the two sets. Several kernels based on this map ping have been\nproposed:\n1. Desobry proposed[8] to de\ufb01ne the distance between the two sp hericalarcs as\na contrast measure de\ufb01ned by: d2\nDesobry(P1,P2) =arccos\u201cw1K1,2w2\n/bardblw1/bardbl/bardblw2/bardbl\u201d\narccos\u201c\u03c11\n/bardblw1/bardbl\u201d\n+arccos\u201c\u03c12\n/bardblw2/bardbl\u201d.\nThis distance is connected to the Fisher ratio (see [8, Section IV]). H owever,\nthe de\ufb01nite positiveness of the Gaussian RBF kernel based on this d istance\nremains to be shown.\n2. Suard [6] proposedthe followingkernel: KSuard(G1,G2) =KSuard(P1,P2) =\n\u03c11\u03c12/summationtext\nhi\u2208P1/summationtext\nhj\u2208P2\u03b11,iKpath(hi,hj)\u03b12,jwithw1= (\u03b11,1,...,\u03b1 1,|P1|) and\nw2= (\u03b12,1,...,\u03b1 2,|P2|).\nThis kernel is de\ufb01nite positive, but does not correspond to any str aightfor-\nward geometric interpretation.Hierarchical bag of paths for kernel based shape classi\ufb01cat ion 5\n2.4 Path kernel\nAll the kernels between bags of paths de\ufb01ned in Section 2 are based on a generic\nkernelKpathbetween paths. A kernel between two paths h1= (v1,...,v n) and\nh\u2032= (v\u2032\n1,...,v\u2032\np) is classically [9] built by considering each path as a sequence of\nnodes and a sequence of edges. This kernel denoted Kclassicis then de\ufb01ned as 0\nif both paths have not the same size and as follows otherwise:\nKclassic(h,h\u2032) =Kv(\u03d5(v1),\u03d5(v\u2032\n1))|h|/productdisplay\ni=2Ke(\u03c8(evi\u22121vi),\u03c8(ev\u2032\ni\u22121v\u2032\ni))Kv(\u03d5(vi),\u03d5(v\u2032\ni))\n(4)\nwhere\u03d5(v) and\u03c8(e) denote respectively the vectors of features associated to\nthe nodevand the edge e. The terms KvandKedenote two kernels between\nrespectively nodes and edge\u2019s features. For the sake of simplicity, we have used\nGaussian RBF kernels between the attributes of nodes and edges ( Section 4).\n3 Hierarchical kernels\nSince the mainfocus ofthis paperis anewkernelmethod forshapec lassi\ufb01cation,\nthe construction of skeletal graphs from shapes has been adres sed using classical\nmethods. We \ufb01rst build a skeleton using the method proposed by Sidd iqi [2].\nHowever the graph we build from the skeleton does not correspond to the shock\ngraph proposed by Siddiqi. Indeed, this graph provides a precise de scription\nof the shape but remains sensitive to small perturbations of the bo undary. We\nratherusetheconstructionschemeproposedbySuard[6]andRu berto[10]which\nconsists to select as node all the pixels of the skeleton which corres pond to end\npoints or junctions. These nodes are then connected by edges, e ach edge being\nassociated to one branch of the skeleton. Given a skeletal graph Gwe valuate\neach of its edge by an additive weight measure and we consider the ma ximal\nspanning tree TofG. The bag of path associated to Gis built on the tree T.\nNote that, the skeletonization being homotopic we have G=Tif the 2Dshape\ndoes not contain any hole.\n3.1 Bag of path kernel\nNone of the bag of path kernels proposed by Desobry or Suard (Se ction 2) is\nboth de\ufb01nite positive and provides a clear geometrical interpretat ion. We thus\npropose a new kernel based on the following distance:\nd2\nchange(P1,P2) = arccos/parenleftbiggw1K1,2w2\n/bardblw1/bardbl/bardblw2/bardbl/parenrightbigg\n. (5)\nThis distance corresponds to the angle \u03b1between the two mean vectors w1and\nw2of each region (Fig. 1). Such an angle may be interpreted as the geo desic6 Fran\u00b8 cois-Xavier Dup\u00b4 e and Luc Brun\ndistance between two points on the sphere and has thus a clear geo metrical\ninterpretation. Based on this distance we use the Gaussian RBF ker nel:\nKchange(G1,G2) =Kchange(P1,P2) = exp/parenleftBigg\n\u2212d2\nchange(P1,P2)\n2\u03c32/parenrightBigg\n.(6)\nThis kernel is de\ufb01nite positive since the normalized scalar product is p ositive\nde\ufb01nite and arccos is bijective on [0 ,1]. The Gaussian RBF kernel based on this\ndistance is thus de\ufb01nite positive (see [11] for further details).\n3.2 Hierarchical kernel between paths\nA mentioned in Section 1, the use of kernels between bags of paths w ithin the\nshape matching framework relies on the assumption that the graph s associated\nto two similar shapes share a large amount of similar paths. This assum ption\nis partially false since a small amount of structural noise may have imp ortant\nconsequences on the set of paths. Let us for example, consider t he small defor-\nmation ofthe square(Fig. 1(b)) representedon Fig. 1(c). This sm all deformation\ntransforms the central node in Fig. 1(b) into an edge (Fig. 1(c)). Consequently\ngraphs associated to these two shapes only share two paths of len gth 2 (the ones\nwhich connect the two corners on the left and right sides). In the s ame way,\na small perturbation of the boundary of the shape may add branch es to the\nskeleton(Fig. 1(d)). Such additional branches i) split existing edge s into two sub\nedges by adding a node and ii) increase the size of the bag of path eith er by\nadding new paths or by adding edges within existing paths.\nThein\ufb02uence ofsmallperturbationsofthe shapeontoanexistings et ofpaths\nmay thus be modeled by node and edge insertions along these paths. In order\nto get a path kernel robust against structural noise we associat e to each path a\nsequence of successively reduced paths, thus forming a hierarch y of paths. Our\nimplicit assumption is that, if a path has been elongated by structura l noise one\nof its reduced version should corresponds to the original path.\nThe reduction of a path is performed either by node removal or edg e con-\ntraction along the path. Such a set of reduction operations is comp atible with\nthe taxinomy of topological transition of the skeleton compiled by Gib lin and\nKimia[12]. Note that, since all verticeshavea degreelowerthan 2 alon gthe path\nthese operations are well de\ufb01ned. In order to select both the typ e of operation\nand the node or the edge to respectively remove or contract we ha ve to associate\na weight to each node and edge which re\ufb02ects its importance accord ing to the\nconsidered path and the whole graph.\nLet us consider a skeletal graph G, its associated maximal spanning tree T\nand a path h= (v1,...,v n) withinT. We valuate each operation on has follows:\nNode removal : Let us denote by vi,i\u2208 {2,...,n\u22121}the removed node of\nthe pathh. The node vihas a degree greater than 2 in Tby construction.\nOur basic idea consists to valuate the importance of viby the total weight\nof the additional branches which justify its existence within the pat hh. ForHierarchical bag of paths for kernel based shape classi\ufb01cat ion 7\n1 2 3 4 567\n910\n0.5 0.5 0.2 0.40.10.4\n0.60.1\n1 3 4 5910\n1.5 0.2 0.40.60.1\n1 2,3 4 567\n910\n0.660.260.4\n0.60.1\n0.36 0.4\n(a)original skeleton (b)reduction ofnode 2 (c)reduction ofedgee2,3\nFig.2.Di\ufb00erent reductions of a path (in gray) within a skeletal tre e.\neach neighbor vofvinot equal to vi\u22121norvi+1we compute the weight W(v)\nde\ufb01ned as the addition of the weight of the tree rooted on vinT\u2212{eviv}\nand the weight of eviv. This tree is unique since Tis a tree. The weight of\nthe nodevi(and the cost of its removal) is then de\ufb01ned as the sum of weight\nW(v) for all neighbors vofvi(excludingvi\u22121andvi).\nAfter the removal of this node the edges evi\u22121viandevivi+1are concatenated\ninto a single edge in the new path h\u2032. The weight of this new edge is de\ufb01ned\nas the sum of the weight of the edges evi\u22121vi,evivi+1and the weight of the\nnodevi(Fig. 2(a) and (b)).\nEdge contraction : The cost of an edge contraction is measured by the rele-\nvance of the edge which is encoded by its weight. Let us denote by evivi+1,\ni<nthe contracted edge of the path h= (v1,...,v n). In order to preserve\nthe total weight of the tree after the contraction, the weight of the edge\nevivi+1is equally distributed among the edges of Tincident to viandvi+1:\n\u2200e\u2208\u03b9(vi)\u222a\u03b9(vi+1)\u2212{evivi+1}w\u2032(e) =w(e)+w(evivi+1)\nd(vi)+d(vi+1)\u22122\nwhere\u03b9(v) andd(v) denote respectively the set of edges incident to vand\nthe cardinal of this set (the vertex\u2019s degree). The symbol w(e) denotes the\nweight of the edge e.\nFor example, the contraction of the edge e2,3in Fig. 2(a) corresponds to a\ncost ofw(e2,3) =.5. The contraction of this edge induces the incrementation\nof the edge\u2019s weights w(e2,1),w(e2,6),w(e3,4) by.5/3\u2248.16\nAny additive measure encoding the relevance of a branch of the ske leton may\nbe used as a weight. We choose to use the measure de\ufb01ned by Torse llo [13] which\nassociates to each branch of the skeleton (an thus to each edge) the length of\nthe boundaries which contributed to the creation of this branch. S uch a measure\ninitially de\ufb01ned for each pixel of the skeleton is trivially additive.\nLet us denote by \u03bathe function which applies the cheapest operation on a\npath. The successive applications of the function \u03baassociate to each path ha\nsequence of reduced paths ( h,\u03ba(h),...,\u03baD(h)) whereDdenotes the maximal\nnumber of reductions. Using Kclassicfor the path comparison, we de\ufb01ne the\nkernelKeditas the mean value of kernels between reduced paths of equal lengt h.\nGiven two paths handh\u2032, this kernel is thus equal to 0 if ||h|\u2212|h\u2032||>D. Indeed,\nin this case the maximal reduction of the longuest path remains longu er than8 Fran\u00b8 cois-Xavier Dup\u00b4 e and Luc Brun\nthe shortest one. Otherwise, ||h|\u2212|h\u2032|| \u2264D, andKedit(h,h\u2032) is de\ufb01ned as:\nKedit(h,h\u2032) =1\nD+1D/summationdisplay\nk=0D/summationdisplay\nl=0Kclassic(\u03bak(h),\u03bal(h\u2032)) (7)\nThis kernel is proportional (by a factor D+ 1) to a sum of R-convolution\nkernels [14, Lemma 1] and is thus de\ufb01nite positive.\nSinceKclassicis equal to 0 for paths of di\ufb00erent lengths, Keditis indeed equal\nto a sum of kernels between reduced paths of equal length. For ex ample, given\ntwopathshandh\u2032whoserespective length isequal to 4and 3 wehavefor D= 2:\nKedit(h,h\u2032) =1\n3/bracketleftbig\nKclassic(\u03ba(h),h\u2032)+Kclassic(\u03ba2(h),\u03ba(h\u2032))/bracketrightbig\n4 Experiments\nWe used the following features for our experiments: Each node is we ighted by its\ndistance to the gravity center of the shape and each edge is assoc ated to a vector\nof two features: The \ufb01rst feature corresponds to the edge\u2019s we ight (section 3.2).\nThe second feature is the angle between the straight line passing th rough the\ntwo nodes of the edge and the principal axis of the shape. These ex periments\nare based on the LEMS [15] database which consists of 99 objects d ivided into\n9 classes.\nWe de\ufb01ned three kernelsfor these experiments: The kernel Kmax,classic based\non a conjoint use of the kernels Kmax(equation 2) and Kclassic(equation 4) has\nbeenintroducedbySuard[6].Thekernel Kchange,classic basedonaconjointuseof\nthekernelsKchange(equation6)and Kclassicallowstoevaluatetheperformances\nof the kernel Kchangecompared to the kernel Kmax. Finally, the kernel Knewis\nbased on a conjoint used of the two kernels KeditandKchangeproposed in this\npaper. The kernel Kclassicis de\ufb01ned by the two parameters \u03c3edgeand\u03c3vertex\nrespectively used by the Gaussian RBF kernels on edges and vertice s. The kernel\nKmax,classic does not require additional parameterswhile Kchange,classic is based\non a\u03bd-SVM and requires thus the parameter \u03bd. It additionally requires the\nparameter \u03c3classic\nchangeused by the RBF kernel in equation 6. The kernel Kedit\nrequires the two parameters \u03c3edgeand\u03c3vertexused byKclassictogether with the\nmaximal number of edition ( D). Finally, the kernel Knewrequires as Kchange\nthe twoadditionalparameters \u03bdand\u03c3new\nchange(equation 6). Theseparametershave\nbeen \ufb01xed to the following values in the experiments described below: D= 2,\n\u03c3edge=\u03c3vertex= 0.1,\u03bd= 0.9,\u03c3new\nchange= 0.3 and\u03c3classic\nchange= 1.0. The parameters\n\u03c3edgeand\u03c3vertexare common to all kernels. The remaining parameters have\nbeen been set in order to maximize the performances of each kerne l on the\nexperiments below.\nOur \ufb01rst experiment compares the distance induced by each kerne lkand\nde\ufb01ned asd2(x,x\u2032) =k(x,x)+k(x\u2032,x\u2032)\u22122k(x,x\u2032). The mean number of matches\nfor each class is de\ufb01ned as follows: For each shape of the selected c lass we sort\nall the shapes of the database according to their distances to the selected shapeHierarchical bag of paths for kernel based shape classi\ufb01cat ion 9\n(a) hands,tools,dudes(1)\n \u03b7= 7\n(2)\n \u03b7= 7\n(3)\n \u03b7= 9\n(b) sorted distances to the hand\nFig.3.Five representative shapes of the classes hands, tools and d udes of the LEMS\ndatabase (a), and (b) the 10 closest shapes from an hand using the distances induced\nby the kernels Kmax,classic (1),Kchange,classic (2) andKnew(3).\nusingan ascendingorder.The numberofgoodmatchesofthe input shapeisthen\nde\ufb01ned as the number of shapes ranked before the \ufb01rst shape wh ich belongs to\na di\ufb00erent class than the selected one. For example, the 10 neares t neighbors of\na hand sorted in an ascending order are represented in Fig. 3(b), t he number\nof good matches of each shape is indicated on the right of the \ufb01gure . Note\nthat the greater number of good match being obtained for the ker nelKnew.\nThe mean number of good matches of a class is de\ufb01ned as the mean va lue of\nthe number of good matches for each shape of the class. The di\ufb00er ent values\nrepresented in Tab. 1(a) represent the mean values of these num ber of good\nmatches for the classes: hands, tools and dudes (Fig. 3(a)). As in dicated by\nTab. 1(a), the kernel Kmax,classic provides stable results but is sensitive to slight\nperturbations of the shapes as the ones of the class dudes and ca nnot handle the\nsevere modi\ufb01cations of the hands. The kernel Kchange,classic leads to roughly\nsimilar results on the di\ufb00erent classes. Though not presented here , the kernel\nKmatching,classic (equation 3) gives worst results than the others kernels. This\nresult may be explained by the drawbacksofthis kernel (Section 2.2 ). The kernel\nKnewalways provides the best results with a good robustness to pertur bation\non dudes and hands.\nOur second experiment evaluates performances of each kernel w ithin a clas-\nsi\ufb01cation framework. To this end, we have trained a SVM on 5 shapes of each of\nthe three classes: dudes, hands, tools on one side and one model o f each of the\n6 remaining classes on the other side. The SVM margin parameter was selected\nin order to maximize the number of true positive while having no false po sitive.\nTab. 1(b) shows the number of well classi\ufb01ed shapes for each class . The kernel\nHandsToolsDudes\nKmax,classic 4.816.186.36\nKchange,classic 5.275.456.36\nKnew 7.099.826.36\n(a) Mean number of good\nmatches.HandsToolsDudes\nKmax,classic 71110\nKchange,classic 71010\nKnew 91111\n(b) Number of recognized shapes in\none class.\nTable 1. Kernels evaluation based on distance (a) and classi\ufb01cation (b) criteria.10 Fran\u00b8 cois-Xavier Dup\u00b4 e and Luc Brun\nKnewgives the best performances especially for the hands where the tw o missing\nshapes are the more perturbed ones. The two others kernels pre sent good results\nand are competitive when shapes are not strongly deformed. This e xperiment\ncon\ufb01rms the robustness of our kernel against perturbed shape s.\n5 Conclusion\nThe bag of path approach is based on a decomposition of the complex graph\nstructure into a set of linear objects (paths). Such an approach bene\ufb01ts of recent\nadvances in both string and vectors kernels. Our graph kernel ba sed on a hier-\narchy of paths is more stable to small perturbations of the shapes than kernels\nbased solely on a bag of paths. Our notion of path\u2019s hierarchy is relat ed to the\ngraph edit distance through the successive rewritings of a path. O ur kernel is\nthus related to the ones introduced by Neuhaus and Bunke."}
{"category": "abstract", "text": ". Graph kernels methods are based on an implicit embedding\nof graphs within a vector space of large dimension. This impl icit embed-\nding allows to apply to graphs methods which where until rece ntly solely\nreserved to numerical data. Within the shape classi\ufb01cation framework,\ngraphs are often produced by a skeletonization step which is sensitive\nto noise. We propose in this paper to integrate the robustnes s to struc-\ntural noise by using a kernel based on a bag of path where each p ath\nis associated to a hierarchy encoding successive simpli\ufb01ca tions of the\npath. Several experiments prove the robustness and the \ufb02exi bility of our\napproach compared to alternative shape classi\ufb01cation meth ods.\nKey words"}
{"category": "non-abstract", "text": "sreechakra@gmail.com, akray@smst.iitkgp.ernet.in\nJ. Acharya is with the Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA - 92093, USA\nEmail: jacharya@ucsd.edu .\nJ. C. Goswami is Principal Engineer at Schlumberger Technology Center, Sugar Land, TX - 77478, USA. He was a Professor at IIT Kharagpur.\nEmail: jcgoswami@ieee.org\nNovember 1, 2018 DRAFTarXiv:0712.4015v1  [cs.CV]  24 Dec 20072\non the histogram of the image. Kwon [6] proposed a threshold selection method based on cluster analysis. The method proposed\na criterion function that involved not only the histogram of the image but also the information on spatial distribution of pixels.\nThis criterion function optimizes intra-class similarity to achieve the most similar class and inter-class similarity to con\ufb01rm that\nevery cluster is well separated. The similarity function uses all pixels in the two clusters as denoted by their coordinates.\nMultilevel thresholding is a process that segments a gray-level image into several distinct regions. The method works very\nwell for objects with colored or complex backgrounds, on which bilevel thresholding fails to produce satisfactory results. Reddi\net al. [7] proposed an iterative form of Otsu\u2019s method, so as to generalize it to multilevel thresholding. Ridler and Calward\nalgorithm [8] uses an iterative clustering approach. An initial estimate of the threshold is made (e.g., mean image intensity) and\npixels above and below are assigned to the white and black classes respectively. The threshold is then iteratively re-estimated\nas the mean of two class means. The most dif\ufb01cult task is to determine the appropriate number of thresholds automatically.\nPapamarkos and Gatos [13] specify the multithreshold values as the global minima of the rational functions which approximate\nthe histogram segments by using hill clustering technique to determine the peak locations of image histogram. Arora et al. [9]\npropose a simple multilevel thresholding using means and variances. Here we also point out that the extension of various\nbinarization methods become extremely complex for multilevel segmentation due to the curse of dimensionality. We will discuss\nthis with respect to Otsu\u2019s method. Ari\ufb01n and Asano [11] propose a hierarchical clustering method, which is shown to be more\nef\ufb01cient than some existing methods. In their method, they use agglomerative clustering (using a dendrogram of gray levels)\nbased on a similarity measure which involves the interclass variance of the clusters to be merged and the intra-class variance of\nthe newly merged cluster.\nWe use unbiased estimators of variances for optimization and \ufb01nd a recursive formulation which improves computational\nef\ufb01ciency of the algorithm. We then compare our results with these well known global thresholding methods and \ufb01nd that the\nproposed approach is better than most existing techniques on the criteria that we consider.\nII. A PPROACH AND ALGORITHM\nIn the proposed approach, we will optimize the ratio of unbiased estimators of within and between class variances of the\nimage to \ufb01nd thresholds for segmenting the image into multiple levels. The method is agglomerative hierarchical where we\nmerge two classes at each level, and replace the pixels belonging to either class by their weighted mean gray-level. Thus we\nreduce the number of classes by one at each iteration. This procedure can be easily imagined in terms of a dendrogram. A\nsimilar approach has been used for petrophysical data classi\ufb01cation in [14] and by Acharya et al. in [15]. Let Gbe the number\nof levels of the gray scale image, so that the number of classes initially is G. Also let Mbe the a priori known desired number\nof classes. Let tibe the iththreshold at a given stage and Ckbe the kthclass, de\ufb01ned asfgjtk\u00001< g\u0014tkg, where g\ndenotes a gray-level between 1andG. The thresholds t0andtMare de\ufb01ned to be 0 and G, respectively.\nA histogram h(g)is the function which shows the number of occurrences of the gray-level gin the image. If Nis the total\nnumber of pixels in the image then, N=PG\ng=1h(g)\nK(i)= the number of classes after the ithrecursion of the algorithm. Thus K(0) = G.\nnk(i)= the number of pixels in the kthclass after the ithrecursion\nak(i)= the amplitude of the kthclass after ithrecursion,\na= the grand weighted mean gray-level of all the pixels in the image. Note that ak(i)refers to the weighted pixel value (or\ngray-level) of the pixels in class Ck(i), i.e., class Ckat the ithrecursion.\nNovember 1, 2018 DRAFT3\nxkm(i)= the mthpixel in the kthclass after the ithrecursion. Note that wherever iis not de\ufb01ned, it refers to the recursion\nnumber.\nWe de\ufb01ne the unbiased estimator of within-class variance v(i)and that of between-class variance w(i)at the ithrecursion\nby the following equations:\nv(i) =1\nN\u0000K(i)K(i)X\nk=1nk(i)X\nm=1[xkm(i)\u0000ak(i)]2(1)\nw(i) =1\nK(i)\u00001K(i)X\nk=1nk(i)[ak(i)\u0000a]2(2)\nThe criterion function is given by q(i)\nq(i) =v(i)\nw(i)(3)\nwhere q(i)is the value of the function to be minimized after the ithiteration.\nAn exhaustive search which calculates the objective function for every pair of connected classes eligible to be merged is\ncomputationally cumbersome. We derive a recursive formulation which avoids this. Suppose we are in the (i\u00001)thstage and\nthe pair of classes to be combined are Cl(i\u00001)andCl+1(i\u00001). From equation 1, we have\nv(i\u00001) =1\nN\u0000K(i\u00001)K(i\u00001)X\nk=1nk(i\u00001)X\nm=1[xkm(i\u00001)\u0000ak(i\u00001)]2\n=1\nN\u0000K(i\u00001)nK(i\u00001)X\nk=1;k6=l;l+1\u0010nk(i\u00001)X\nm=1[xkm(i\u00001)\u0000ak(i\u00001)]2\u0011\n+nl(i\u00001)X\nm=1[xlm(i\u00001)\u0000al(i\u00001)]2\n+nl+1(i\u00001)X\nm=1[xl+1;m(i\u00001)\u0000al+1(i\u00001)]2o\nAfter Cl(i\u00001)andCl+1(i\u00001)are merged, the number of classes will be reduced by one ( K(i) =K(i\u00001)\u00001) and the\nlast two terms above will be changed. Writing v(i)in terms of the parameters in the (i\u00001)threcursion, we get\nv(i) =1\nN\u0000K(i\u00001) + 1nK(i\u00001)X\nk=1;k6=l;l+1\u0010nk(i\u00001)X\nm=1[xkm(i\u00001)\u0000ak(i\u00001)]2\u0011\n+nl(i\u00001)X\nm=1[xlm(i\u00001)\u0000al0(i)]2+nl+1(i\u00001)X\nm=1[xl+1;m(i\u00001)\u0000al0(i)]2o\nwhere,\nal0(i) =nl(i\u00001)al(i\u00001) +nl+1(i\u00001)al+1(i\u00001)\nnl(i\u00001) +nl+1(i\u00001)(4)\nis the combined mean gray-level of the classes Cl(i\u00001)andCl+1(i\u00001)after they merge.\nComparing the above few equations, and doing some algebraic manipulations, we ultimately get\nv(i) =N\u0000K(i\u00001)\nN\u0000K(i)v(i\u00001) +1\nN\u0000K(i)dl(i\u00001)2(5)\nwhere\nNovember 1, 2018 DRAFT4\ndl(i\u00001)2=nl(i\u00001)nl+1(i\u00001)\nnl(i\u00001) +nl+1(i\u00001)[al(i\u00001)\u0000al+1(i\u00001)]2(6)\nSimilarly, we obtain\nw(i) =K(i\u00001)\u00001\nK(i)\u00001w(i\u00001)\u00001\nK(i)\u00001dl(i\u00001)2(7)\nThe equations 5, 7 give a recursive formulation to calculate the within and between class variances. We see that the only term\ncontrolling the increment of v(i)and the decrement of w(i)is the term dl(i\u00001)2, and thus minimizing dl(i\u00001)2helps us in\noptimizing q(i).\nA. Algorithmic Implementation\nAs above, let Mbe the number of classes desired after segmentation. The stepwise implementation of the above algorithm\nis given below:\n\u000fInitialize the number of classes by the number of gray-levels ( K(0) = G).\n\u000fCalculate d2\nkfor all pairs of adjacent classes for the 0thstage.\n\u000fFor each stage i i+ 1, until K(i)\u0015M, Do:\n1) Combine the lthand(l+ 1)thsegments, where lis the index for which d2\nlyields its minimum value.\n2) Update the necessary as follows:\nK(i) = K(i\u00001)\u00001\nal(i) =nl(i\u00001)al(i\u00001) +nl+1(i\u00001)al+1(i\u00001)\nnl(i\u00001) +nl+1(i\u00001)\nnl(i) = nl(i\u00001) +nl+1(i\u00001)\nal\u0000m(i) = al\u0000m(i\u00001); m= 1;2; : : : l\u00001\nal+m(i) = al+m+1(i\u00001); m= 1;2; : : : K (i)\u0000l\nnl\u0000m(i) = nl\u0000m(i\u00001); m= 1;2; : : : l\u00001\nnl+m(i) = nl+m+1(i\u00001); m= 1;2; : : : K (i)\u0000l\ndl\u00001(i)2=nl\u00001(i)nl(i)\nnl\u00001(i) +nl(i)[al\u00001(i)\u0000al(i)]2\ndl(i)2=nl(i)nl+1(i)\nnl(i) +nl+1(i)[al(i)\u0000al+1(i)]2\ndl\u00001\u0000m(i)2=dl\u00001\u0000m(i\u00001)2; m= 1;2; : : : l\u00002\ndl+m(i)2=dl+1+m(i\u00001)2; m= 1;2; : : : K (i)\u0000l\u00001\n3) The cumulative sums of the nk\u2019s yield us the values of thresholds.\nIII. M ULTILEVEL THRESHOLDING AND COMPUTATIONAL ADVANTAGE OVER OTSU\nThe algorithm proposed can be used for multilevel image thresholding as well as binarization. For having a particular number\nof classes, we have to stop at a stage when the number of levels equals the number of classes desired. This method has the\nadvantage of being computationally much more ef\ufb01cient than Otsu\u2019s, in spite of having a very similar criterion function. We also\nNovember 1, 2018 DRAFT5\nknow that of all methods possible, Otsu\u2019s technique will have the minimum Peak-Signal-to-Noise-Ratio (PSNR) value because\nPSNR acts like the criterion function for Otsu\u2019s algorithm. We theoretically evaluate the performance of the proposed algorithm\nand compare it with conventional Otsu\u2019s method for multilevel thresholding. For segmenting an image with Ggray-levels using\nnthresholds, Otsu\u2019s exhaustive search method searches\u0000G\nn\u0001\ndifferent combinations of thresholds, which can be approximated\ntoGnforn << G . Thus the time complexity is exponential in the number of thresholds.\nClaim : The proposed method can calculate all the possible thresholds in the multilevel case in O(G2)time.\nProof : We can divide the algorithm into three parts:\n1) Finding the minima of d2\nl, which takes O(K(i))steps for the ithiteration.\n2) Updating the parameters, which can be done in O(1)time.\n3) Writing down the thresholds will take at most O(K(i))time. We can also store the values ef\ufb01ciently by storing only the\nclasses being merged at each stage.\nThus, if we run the entire algorithm until we are left with two classes ( G\u00001iterations), it will take O(G2)time, thus providing a\ntremendous edge in terms of computational time over the exponentially complex Otsu\u2019s thresholding. We also point out here that\nmost of the hierarchical thresholding algorithms (for example [11]) will also consume O(G2)time, but the proposed method,\nbecause of its unique recursive formulation, incurs low costs even in terms of the actual number of computations.\nIV. R ESULTS AND OBSERVATIONS\nFor evaluating the performance of the proposed algorithm, we have implemented the method on a wide variety of images\nfor both binarization and multilevel thresholding. The image set we used for comparing the performance of our algorithm with\nother methods for the bilevel case is shown in Fig. 1. Along with the results obtained using the proposed technique, those of\nfour other methods have been shown - Otsu\u2019s [4], Kwon\u2019s [6], Kittler-Illingworth\u2019s [12] and Ari\ufb01n-Asano\u2019s [11]. The results are\n(a) Bacteria\n (b) Coins\n (c) Fish\n (d) Things\n (e) Rice\nFig. 1. Various Images we use for comparison.\ncompared with the ground truth images generated by manual thresholding (See Figs. 2, 3, 4, 5, 6) We observe that our method\nworks at least at par with the other methods, and in most of the cases produces a superior classi\ufb01cation.\nIn order to compare the quality of the thresholded images quantitatively, we have evaluated the performance of the proposed\nmethod against the four different methods, using the criteria of misclassi\ufb01cation error (ME) and relative foreground area error\n(RAE) [2].\nNovember 1, 2018 DRAFT6\nME is de\ufb01ned in terms of the correlation of the images with human observation. It corresponds to the ratio of background\npixels wrongly assigned to foreground, and vice versa. ME can be simply expressed as:\nME = 1\u0000jBOT\nBTj+jFOT\nFTj\nBO+FO(8)\nwhere background and foreground are denoted by BOandFOfor the original image (the ground truth image), and by BT\nandFTfor the test image, respectively.\nRAE measures the number of discrepancies in the thresholded image with respect to the reference image, taking the area of\nthe foreground into account. It is de\ufb01ned as\nRAE =(\nAO\u0000AT\nAO; A O> AT\nAT\u0000AO\nAT; A T\u0015AO(9)\nwhere AOis the area of the reference image, and ATis the area of thresholded image.\n(a) Ground\n (b) Ari\ufb01n\n (c) Otsu\n (d) KI\n (e) Kwon\n (f) Proposed\nFig. 2. Bacteria Image results for various methods\n(a) Ground\n (b) Ari\ufb01n\n (c) Otsu\n (d) KI\n (e) Kwon\n (f) Proposed\nFig. 3. Coins Image results for various methods\nIn Table I we show the ME and RAE for the various images under consideration.\nIn this section, we also consider another measure called the Peak Signal-to-Noise Ratio (PSNR) of the segmented image. Let\nSbe the source image and Tbe the image obtained after thresholding ( Smeans S(i; j)andTmeans T(i; j)).Nis the total\nnumber of pixels in the image. Let us assume for the time being that the number of gray-levels is 256. Then, the mean square\nerror (MSE) and the PSNR are given by:\nNovember 1, 2018 DRAFT7\n(a) Ground\n (b) Ari\ufb01n\n (c) Otsu\n (d) KI\n (e) Kwon\n (f) Proposed\nFig. 4. Rice Image results for various methods\n(a) Ground\n (b) Ari\ufb01n\n (c) Otsu\n (d) KI\n (e) Kwon\n (f) Proposed\nFig. 5. Fish Image results for various methods\n(a) Ground\n (b) Ari\ufb01n\n (c) Otsu\n (d) KI\n (e) Kwon\n (f) Proposed\nFig. 6. Things Image results for various methods\nMSE =P\n[S\u0000T]2\nN(10)\nPSNR = 10 log10\u0012\n2552\nMSE\u0013\n; (11)\nwhere the summation in MSE is over all the pixels in the image and the PSNR value obtained above is in dB (decibels).\nPSNR is used to determine the overall quality and error of thresholded image. Several test images were taken and PSNR was\ncalculated for thresholded images for multiple levels using the proposed technique. The results are shown in Table II. We can\nsee that the value of PSNR grows in general with the number of classes, and this combined with the computational advantage\nNovember 1, 2018 DRAFT8\nImage Ari\ufb01n Otsu KI Kwon Proposed\nME\nBacteria 0.0857 0.0432 0.1253 0.3316 0.0459\nRice 0.0589 0.0741 0.0924 0.1719 0.0609\nFish 0.1745 0.0673 0.0356 0.2860 0.0263\nCoins 0.0163 0.0135 0.0076 0.1887 0.0066\nThings 0.1759 0.0432 0.1772 0.2669 0.0515\nRAE\nBacteria 0.4629 0.1893 0.7027 0.6394 0.2200\nRice 0.0318 0.1495 0.2311 0.2973 0.0325\nFish 0.7981 0.2929 0.1276 0.5675 0.0859\nCoins 0.0514 0.0426 0.0200 0.3726 0.0095\nThings 0.1912 0.0448 0.1923 0.3356 0.0204\nTABLE I\nCOMPARISON OF ME AND RAE FOR VARIOUS IMAGES ON THE METHODS\nImage 2 level 3 level 5 level 10 level 25 level\nLena 20.3 23.6 27.0 33.4 41.1\nBaboon 20.1 24.1 27.8 32.8 40.6\nPeppers 19.2 21.6 26.4 32.1 39.8\nRice 21.4 23.6 28.5 34.1 41.8\nFish 21.3 26.6 30.8 36.2 43.4\nThings 18.7 21.2 26.7 31.6 39.5\nTABLE II\nPSNR V ALUES FOR VARIOUS LEVELS OF IMAGE THRESHOLDING BY PROPOSED METHOD\ndiscussed above makes the process valuable. The table of PSNR values gives us an idea that the method can be used for \u201cfast\u201d\nlossy compression in real time in a general setting owing to high PSNR values. We give examples of natural and smooth pictures\nin addition to the images considered above because the objective here is compression and not object recognition.\nWe show the original and thresholded images for some well known images in Figs. 7, 8, 9 obtained using the proposed\nmethod. Note that for the bilevel case, we show images with gray-levels equal to the means of the two classes.\n(a) Gray\n (b) 2 Level\n (c) 3 Level\n (d) 5 Level\n (e) 10 Level\n (f) 25 Level\nFig. 7. Lena image thresholding Image results for various levels.\nNovember 1, 2018 DRAFT9\n(a) Gray\n (b) 2 Level\n (c) 3 Level\n (d) 5 Level\n (e) 10 Level\n (f) 25 Level\nFig. 8. Baboon image thresholding Image results for various levels.\n(a) Gray\n (b) 2 Level\n (c) 3 Level\n (d) 5 Level\n (e) 10 Level\n (f) 25 Level\nFig. 9. Peppers image thresholding Image results for various levels.\nV. C ONCLUSION\nWe have proposed a novel image segmentation technique based on unbiased variance estimators. The proposed method is\ncompared with other global histogram-based thresholding methods. The various results show the ef\ufb01ciency of the proposed\nmethod with respect to object recognition and also for compression owing to a high PSNR associated with it. The method is\nalso computationally very advantageous because of its polynomial running time.\nACKNOWLEDGMENT\nWe would like to thank Dr. Denis Heliot of Schlumberger Technology Corporation, under whose supervision a part of the\nwork began. We also thank Prof. Somnath Sengupta of IIT Kharagpur for his valuable suggestions.\nREFERENCES\n[1] Z. Chi, H. Yan, T. Pham, \u201cFuzzy Algorithms: With applications to images processing and pattern recognition,\u201d World Scienti\ufb01c, Singapore,\n1996..\n[2] M. Sezgin and B. Sankur, \u201cSurvey over image thresholding techniques and quantitative performance evaluation\u201d, Journal of Electronic\nImaging , 13(1), 146-165, 2004.\n[3] W. Niblack, \u201cAn Introduction to Digital Image Processing,\u201d Englewood Cliffs, N. J.: Prentice Hall, 115-116, 1986.\n[4] N. Otsu, \u201cA threshold selection using gray level histograms,\u201d IEEE Trans. Systems Man Cybernetics , 9, 62-69, 1979.\n[5] A. S. Abutaleb, \u201cAutomatic thresholding of gray level pictures using two-dimensional entropy,\u201d Comput. Vision Graphics Image Process.\n47, 22-32, 1989.\n[6] S. H. Kwon, \u201cThreshold selection based on cluster analysis,\u201d Pattern Recognition Letters , 25, 10451050, 2004.\nNovember 1, 2018 DRAFT10\n[7] S. S. Reddi, S. F. Rudin, H. R. Keshavan, \u201cAn Optical Multiple Threshold Scheme for Image Segmentation\u201d, IEEE Trans. System Man and\nCybernetics , 14, 661-665, 1984.\n[8] T. W. Ridler, S. Calward, \u201cPicture Thresholding Using an Iterative Selection Method\u201d, IEEE Trans. Systems, Man and Cybernetics , 8,\n630-632, 1978.\n[9] S. Arora, J. Acharya, A. Verma, P. K. Panigrahi, \u201cMultilevel Thresholding for Image Segmentation through a Fast Statistical Recursive\nAlgorithm,\u201d Pattern Recognition Letters , 29, 119125, 2008.\n[10] P. K. Sahoo, S. Soltani, A. K. C. Wong, \u201cSURVEY: A survey of thresholding techniques\u201d, Comput. Vision Graphics Image Process , 41,\n233-260, 1988.\n[11] A. Z. Ari\ufb01n, A. Asano, \u201cImage segmentation by histogram thresholding using hierarchical cluster analysis,\u201d Pattern Recognition Letters ,\n27, 1515-1521, 2006.\n[12] J. Kittler, J. Illingworth, \u201cMinimum error thresholding,\u201d Pattern Recognition , 19(1), 41-47, 1986.\n[13] N. Papamarkos, B. Gatos, \u201cA new approach for multilevel threshold selection,\u201d Graphics Models Image Process, 56, 357-370, 1996.\n[14] A. Moghaddamjoo, \u201cOptimum Well-Log Signal Segmentation,\u201d IEEE Transactions on Geoscience and Remote Sensing , 27(5), 633 - 641,\n1989.\n[15] J. Acharya, G. Sreechakra, J. C. Goswami, D. Heliot, \u201cHierarchical Zonation Technique to Extract Common Boundaries of a Layered\nEarth Model,\u201d Proceedings of the IEEE Antenna and Propagation Society Symposium , Hawaii, 2007.\nNovember 1, 2018 DRAFT"}
{"category": "abstract", "text": "This paper proposes a novel method for segmentation of images by hierarchical multilevel thresholding. The method is global,\nagglomerative in nature and disregards pixel locations. It involves the optimization of the ratio of the unbiased estimators of within\nclass to between class variances. We obtain a recursive relation at each step for the variances which expedites the process. The\nef\ufb01cacy of the method is shown in a comparison with some well-known methods.\nIndex Terms\nMultilevel Thresholding, Image Segmentation, Histogram, Recursion, Unbiased Estimator\nI. I NTRODUCTION\nThresholding is an important technique for image segmentation and object extraction. The aim of an effective segmentation is\nto separate objects from the background and to differentiate pixels having nearby values for improving the contrast. The use of\nbinary images reduces the computational cost of the succeeding steps and increases the ease of manipulation compared to using\ngray-level images. Image thresholding is a well-researched \ufb01eld and there exist many algorithms for determining an optimal\nthreshold of the image. Many surveys of thresholding methods and their applications exist in literature [1], [2].\nThresholding techniques can be divided into bilevel and multilevel categories, depending on the number of classes. In bilevel\nthresholding, the image is segmented into two different regions. The pixels with gray-level values greater than a certain value T\nare classi\ufb01ed as object pixels, and the others with values lesser than T are classi\ufb01ed as background pixels or vice versa. Otsu\u2019s\nmethod [4] chooses optimal thresholds by maximizing the between class variance and minimizing within class variance. Sahoo\net al. [10] found that in global thresholding, Otsu\u2019s method is one of the better threshold selection methods for general real\nworld images with regard to uniformity and shape measures. However, inef\ufb01cient formulation of between class variance makes\nthe method time consuming. Abutaleb [5] used two-dimensional entropy to calculate the threshold. Kittler and Illingworth [12]\nsuggested a minimum error thresholding method. They assume that an image is characterized by a mixture distribution with both\nobject and background classes having a Gaussian distribution. The probability density function for each class is estimated based\nG. Sreechakra and A. K. Ray are with the Department of Electronics and Electrical Communication Engineering, Indian Institute of Technology,\nKharagpur, WB - 721302, India. Email"}
{"category": "non-abstract", "text": "edrosten@lanl.gov \u0001loveland@lanl.gov\nEdward Rostem\nDepartment of Engineering\nUniversity of Cambridge, Cambridge, UK\nE-mail: er258@cam.ac.uk\nRohan Loveland\nDepartment of Engineering Science\nUniversity of Oxford, Oxford, UK\nE-mail: rohan@robots.ox.ac.uk1 Introduction\n1.1 Camera Calibration and Distortion Correction\nCamera calibration addresses the problem of \ufb01nding the pa-\nrameters necessary to describe the mapping between 3-D\nworld coordinates and 2-D image coordinates. This can be\ndivided into the determination of extrinsic and intrinsic pa-\nrameters. The extrinsic parameters are necessary to relate\nan arbitrary 3-D world coordinate system to the internal 3-\nD camera coordinate system, where the z-axis is typically\ntaken to be along the optical axis. The intrinsic parameters\nare independent of the camera position, and typically in-\nclude a focal length, an offset relating pixel coordinates cen-\nter to the optical center, a scale factor describing the pixel\naspect ratio, and one or more parameters describing non-\nlinear radial and tangential distortions. Detailed examples\nof this process can be found in [13,16], among others.\nIn this work we address the determination of the non-\nlinear distortion parameters, with the assumption that the\nother intrinsic parameters, usually addressed assuming a pin-\nhole camera model, can be found after the initial non-linear\ndistortion correction. In order to do this a model must be\nselected, as well as a method of estimating the model pa-\nrameters.\n1.2 Summary of Related Work\n1.2.1 Distortion Correction Models\nA general equation for distortion correction is given by\nI1(x0) =I0(x);x0\u0011D(x); (1)\nwhere I1is the output (corrected) image, I0is the input (dis-\ntorted) image, and xis a pixel location in the input imagearXiv:0810.4426v2  [cs.CV]  4 Jan 20092\nthat is mapped to location x0in the output image by distor-\ntion correction function D(x):R2)R2.\nIf the distortion is assumed to be isotropic, and strictly\nradial, then D(x)can be written as\nD(x) =f(r)\u02c6r+c; (2)\nwhere cis the center or radial distortion, the radius vector\nr= [r1r2]T=x\u0000c, the radius r=krk, the normalized ra-\ndius vector \u02c6r=r=randfis a scalar function of r.\nSeveral models for distortion correction have been used\npreviously. In terms of f, the models include the polynomial\nmodel [13,2], where\nf(r) =r(1+k1r2+k2r4+k3r6+:::); (3)\nand the Harris model [27], where\nf(r) =rp\n1+gr2: (4)\nMore complex models such as [14] include additional\ntangential distortion\nD(x) =\u0014p1(r2+2r2\n1)+2p2r1r2\n2p1r1r2+p2(r2+2r2\n2)\u0015\n+f(r)\u02c6r+c; (5)\nto account for decentering of the optical system (where p1\nandp2are the parameters). More recently the rational func-\ntion model, where polynomial and perspective transforms\nare combined into a rational polynomial [5,19] has been pro-\nposed.\nAll of these models can have adequate performance de-\npending on the characteristics of the camera optics. In the\nfollowing we limit our discussion and implementation to\na single model in order to bound the scope of the paper,\nthough it should be noted that the method we present is ex-\ntensible to any model. The Harris model is chosen for its\nreasonable performance, single parameter, and ease of in-\nversion (achieved by simply negating g).\n1.2.2 Parameter Estimation Methods\nA variety of methods exist for estimating camera distortion\ncorrection model parameters. Earlier efforts relied on im-\nagery with arti\ufb01cially created structure, either in the form of\na test-\ufb01eld, populated with objects having known 3-D world\ncoordinates, or using square calibration grids with lines at\nconstant intervals [13,16,2]. Alternative approaches do not\nrequire arti\ufb01cially created structure, but used multiple views\nof the same scene. The calibration technique makes use of\nconstraints due to known camera motion (for instance rota-\ntion) [23], known scene geometry such as planar scenes [21]\nor general motion and geometry constrained with the epipo-\nlar constraint [24,1,5].\nThese approaches required access to the camera in or-\nder to perform a speci\ufb01c operation, such as acquiring viewsfrom multiple positions or views of a particular scene. This\nis problematic in instances where access is no longer avail-\nable to the camera, but only to the resulting imagery.\nAs a result, a number of methods have been developed\nwhich can operate on single views but which make use of\ncommon structure in images such as vanishing points [3],\nhigher-order correlations in the frequency domain [11] and\nstraight lines [4].\nThe methods relying on the existence of straight lines,\nthe \u201cplumb-line\u201d methods, were pioneered by Brown [4].\nBrown used a test-\ufb01eld with actual strung plumb-lines, but\nin general these methods do not require knowledge of the\nlocations of the lines. In particular, white plumb-lines were\nstrung across a black background, with the plumb-bobs im-\nmersed in oil for stability. The photographic plate was ex-\nposed twice with the camera rotated about the optic axis by\n90\u000e, giving a nominally square grid of lines. A number of\npoints along the lines recorded on the photographic plate\nwere measured using a microscope (a Mann comparator).\nCalibration was then performed by minimizing the least-\nsquares error in the image between distorted straight lines\nand the measures points on the photographed plumb-lines.\nPlumb-line methods are applicable to many image types,\nbecause nearly all scenes containing man-made structures\nhave a large number of straight lines. Plumb-line methods\ngenerally rely on the process of optimizing the distortion\ncorrection parameters to make lines that are curved by ra-\ndial distortion straight in the corrected imagery. The lines\ncan be manually selected, as in [26], or they can be found\nautomatically, as in [7,25,6] by detecting edgels and linking\nthem in to line segments.\nThe objective function for optimization can be formu-\nlating by undistorting the line segments and measuring the\nstraightness by \ufb01tting a straight line [26,7]. Alternatively,\nthe distortion model can be chosen so that straight lines be-\ncome speci\ufb01c family of curves, such as conics [6] or cir-\ncles [25]. The distortion can then be found by \ufb01tting these\ncurves to the distorted line segments.\n2 Algorithm Overview\nWe propose a method that is simple and robust to high levels\nof noise, as shown in the results section. In our algorithm we\ncalculate all image edgels, and then transform these into a\none-dimensional Hough space representation of angle. This\ncreates an orientation histogram of the edgel angles. In this\nform, curved lines will be represented at a variety of angles,\nwhile straight lines will be found only at one. Therefore, we\noptimize the model distortion parameters which minimize\nthe entropy (or spread) of the Hough space angular repre-\nsentation. The individual steps are:\n1. Find salient image edgels, with normal vectors.3\n(a) (b) (c)\nFig. 1 Salient edge detection using tensor voting. (a) The original im-\nage, (b) the gradient magnitude, (c) the edge saliency.\n2. Perform a distortion correcting transformation to the edgels.\n3. Compute the 1-D angular Hough transform.\n4. Compute an objective function de\ufb01ned as the spread (en-\ntropy) of the 1-D Hough transform.\n5. Optimize the transformation parameters to minimize the\nentropy/spread based objective function, iterating steps\n2\u20134,\n6. Use the optimized transform parameters to map the input\nimage to a corrected output image.\nNote that step 1, \ufb01nding the edgels, is only required\nonce, due to the fact that the edgels, rather than the underly-\ning image, can be transformed. This, and the other steps of\nthe process, are described in further detail in the following\nrespectively enumerated subsections.\n2.1 Salient edge extraction\nThe structure that we are making use of in this paper con-\nsists of long, straight edges, which appear as long, smoothly\ncurved edges in the input image.\nSimply performing an edge detection does not result in\nthese features being dominant, so to enhance the long edges,\nwe use tensor voting [15] on the dense gradient image. The\ngradient is produced by \ufb01nite differences, and voting is made\nin proportion to the gradient magnitude. We use the stan-\ndard kernel for smooth, circular curves, speci\ufb01cally the ker-\nnel given by the equations on page 60 of [15].\nFor edges, each pixel is represented by a 2 \u00022 positive\nsemi-de\ufb01nite matrix with eigenvalues l1andl2. We wish to\n\ufb01nd points which are suf\ufb01ciently \u2018edgey\u2019 so we use the edge\nsaliency function f, given by:\nf=maxl1;l2\u0000eminl1;l2: (6)\nThis detects points where the edge in the main orientation is\nsigni\ufb01cantly brighter than other edges. We have found that\ne=2 produces good results. An example of these are shown\nin Figure 1. The normal of these edgels is given by the eigen-\nvector corresponding to the largest eigenvalue.\nThe \ufb01nal stage is to discard the non-salient edges by\nthresholding at f=0. For computational ef\ufb01ciency of the\nlater stages, the set of edges passing the threshold is subsam-\npled. Typically, we keep 100,000 edgels. In order to prevent\nthe result being dominated by one region with strong edges,we split the image up in to a regular grid, and perform sub-\nsampling independently within each grid cell, so that each\ncell contains the same number of edgels.\n2.2 Distortion correction model\n2.2.1 Anisotropic extension\nAs mentioned previously, for the basic radial distortion cor-\nrection model, we have based our distortion correction model\non the Harris model de\ufb01ned in Equation 4. In order to extend\nthe model\u2019s \ufb02exibility, we add an anisotropic component, so\nthat the model becomes:\nD(x) =\u02c6rf(r)(1+g(q))+c; (7)\nwhere f(:)andg(:)are the distortion functions.\nIn particular, we de\ufb01ne the anisotropy function, g(q)to\nbe:\ng(q) =a1sin(q+y1)+a2sin2(q+y2)+:::\n=b1sinq+b2cosq+(b3sinq+b4cosq)2+:::; (8)\nwhich can be rearranged as a Fourier series. This general-\npurpose formulation has no discontinuity as the angle wraps\naround and conveniently avoids the use of trigonometric func-\ntions, since cos q=r1=r, etc.\n2.2.2 Edgel transformation\nIn order to evaluate the cost function it is necessary to de-\ntermine the edgels\u2019 orientation, as discussed in the previous\nsection. If it were necessary to regenerate these by comput-\ning a completely new image for each new set of distortion\nmodel parameters, however, the optimization process (in-\nvolving numerous evaluations of different sets of model pa-\nrameters) would be prohibitively slow. Furthermore, errors\ndue to resampling would be introduced when the image was\nundistorted at each iteration.\nFortunately, the edgels need to be determined only once,\nfrom the input image, due to the fact that the distortion cor-\nrecting transformations can be applied directly to the edgels,\nprovided that the Jacobians of the transformations are known.\nIn order to see this, consider an edgel in the input image as\npart of a parameterised curve l(t)inR2. The distortion cor-\nrection transformation, D, can be applied to this space to\ncreate a new line:\nm(t) =D(l(t)): (9)\nThe line tangents are then found by differentiating to be:\n\"\n\u00b6m1\n\u00b6t\u00b6m2\n\u00b6t#\n=J\"\n\u00b6l1\n\u00b6t\u00b6l2\n\u00b6t#\n; (10)4\nDistorted lines 0 5 10 15 20 25 30 3500.050.10.150.2Probability\nBin number\nStraight lines 0 5 10 15 20 25 30 3500.20.40.60.8Probability\nBin number\nFig. 2 Images of curved and straight line images and the correspond-\ning 1-D Hough transforms.\nwhere the Jacobian, J, is given by:\nJ=\"\u00b6D1\n\u00b6ll\u00b6D1\n\u00b6l2\u00b6D2\n\u00b6ll\u00b6D2\n\u00b6l2#\n: (11)\nThe edgels include normals nde\ufb01ned at discrete points. The\ntransformed edgel normals, h, can then be found by rotating\nthe normals by 90\u000e(making them tangents), transforming\nthe tangents by Jand rotating the transformed tangents back\nto be normals:\nh=\u00140 1\n\u00001 0\u0015\nJ\u00140\u00001\n1 0\u0015\nn (12)\nNote that we do not parameterise the line with a func-\ntion. The line and its normal is known (and used) only at a\ndiscrete set of points, speci\ufb01cally where the edgels are de-\ntected. This means that l(t)andn(t)can be evaluated at ev-\nery value of twe require. Since the edgel detection process\nalso provides the normals, Jis only a function of the distor-\ntion model, and is therefore computed analytically from the\nde\ufb01nition of D. The derivation of Jfor the Harris model is\ngiven in Appendix A.\n2.3 1-D Angular Hough transform\nThe Hough transform is a technique for \ufb01nding lines in im-\nages [9]. It is suf\ufb01ciently well-known so that only a trun-\ncated explanation will be given here. The basis of the tech-\nnique is the transform of a line to a point in \u201cHough\u201d space.\nBy way of example, if a line is de\ufb01ned by y=mx+b,\nthen it can be represented by a single point in a 2-D Hough\nspace of [m]\u0002[b]. An edgel, which contains both a normal\nvector (and thus the slope of a line) and a discrete point\n(which a corresponding line would pass through), can alsobe mapped to a single point in Hough space. In practice the\nmx+bformulation is unwieldy, so\nr=xcosq+ysinq (13)\nis used, where ris the shortest distance from the line to the\norigin, and qis de\ufb01ned by the vector normal to the line.\nThe Hough space can be quantized into discrete bins,\nand each edgel can be assigned to a particular bin. Given a\nset of edgels, support for the existence of a particular line\nis then indicated by the accumulation of a large number of\nedgels in the corresponding bin. The edgels from a curve\nend up in adjacent bins, resulting in a diffuse cluster of non-\nzero bins, while edgels that are all on the same line end up\nin exactly the same bin. This important fact motivates the\nde\ufb01nition of our objective function, as explained in the next\nsection.\nIn general we anticipate the presence of a number of\nparallel lines in the scene, and would like to utilize their\nreinforcement of each other. We therefore marginalize the\n[r]\u0002[q]Hough space into a 1-D qspace by summing over\nthervalues for each q. An example of some images with\ncurved lines and straight lines, and their corresponding 1-D\nHough transforms, are shown in Fig. 2. Note that the two\nprincipal line directions are mapped to two corresponding\npeaks in Hough space.\n2.4 Entropy-based objective function\nThe radial distortion correction method presented here is\nmotivated by the observation that curved lines map to spread\nout peaks in Hough space, while straight lines map to a sin-\ngle bin. Therefore, it is desirable to have an objective func-\ntion that measures this spread. In information theory this\nquality is represented by entropy [22]. We have therefore\nnormalized the 1-D Hough representation, and treat it as a\nprobability distribution. The objective function is then:\nC(H)\u0011\u0000B\n\u00e5\nb=1p(Hb)log2(p(Hb)); (14)\nwhere His the discretized 1-D angular Hough transform of\nthe edgels of an input image for a given set of model pa-\nrameters, p(Hb)is the value of a given normalized Hough\nbin, and Bis the number of bins. Minimizing Ctherefore\nminimizes the spread.\n2.5 Optimization\nThe cost function has many local minima, so to optimize\nit effectively a reasonable strategy is needed. As a broad\noverview, we have found that the MCDH (Monte-Carlo down-\nhill) strategy works best. Essentially, starting parameters are5\nselected at random, and the downhill simplex algorithm [17]\n(speci\ufb01cially the variation given in [18]) is used to optimize\nthe cost from the selected starting points. The best result\nis then selected. The downhill simplex algorithm is iterated\nuntil the residual drops below a threshold (10\u000015), or 1000\niterations occur, whichever is sooner. The residual is com-\nputed as:\nr=ksbest\u0000sworstk2\nksbestk2;\nwhere sbestis the location of the best (smallest) point on the\nsimplex, and sworst is the location of the worst (largest point)\non the simplex.\nIn order to implement this method effectively, several\ntechniques are needed. Parameters computed in the Hough\ntransform will not lie exactly at the bin centres. If the param-\neter is simply placed in the closest bin, then the quantization\ncaused by this causes \ufb02at areas to exist in the cost function.\nThe \ufb02at areas can easily confound the optimizer, so instead,\nthe neighbouring bins are incremented using linear interpo-\nlation.\nThe optimizer is most effective when the parameters be-\ning optimized have similar orders of magnitude, since other-\nwise roundoff error will cause errors in the computation of\nnew simplexes. In this case, a simple scale based on a priori\nknowledge of approximate parameter values suf\ufb01ces.\nFor radial distortion, cis roughly in the middle of an\nimage and is of the order 102to 104pixels. Given Equa-\ntion 4, it is reasonable to expect gto be approximately in the\nrange\u0006r\u00002\nmax, where rmax\u0019j\u02dccjand\u02dccis the centre of the im-\nage. Therefore, in the optimization, we solve for b, where\nb=100gr2\nmax, which brings bin to the same range as c.\nSimilarly, we have found that the values of the anisotropic\ncoef\ufb01cients, b1;:::are of order 10\u00003, so we instead solve\nfordi, where di=105bi. The resulting optimization is per-\nformed over the vector:\n[c1;c2;b;d1;d2;d3;d4;d5;d6]: (15)\nThe random selection of the starting parameters must be\nbased around some knowledge of the values. The centre of\nradial distortion is usually near to the centre of the image, so\nwe draw the initial centres from a normal distribution cen-\ntred at \u02dccwith a standard deviation ofj\u02dccj\n20. Given the approxi-\nmate values of g, and therefore b, we draw initial values of\nbfromN(0;100). The initial anisotropy parameters are set\nto zero. For our application and these parameters, we have\nfound that 120 MCDH iterations is suf\ufb01cient.\n2.6 Generating the output image\nIn order to generate the output (corrected) image the follow-\ning procedure is needed:\n70% Random noise\n70% Correlated noise00.20.40.60.8101234x 10\u22125g\nProportion of noise edgelsUncorrelated noise\n  \nIsotropic\nAnisotropic\n00.20.40.60.8101234x 10\u22125g\nProportion of noise edgelsCorrelated noise\n  \nIsotropic\nAnisotropicFig. 3 (Left) Illustration of the simulated data (shown with 100 points\nper line). (Right) Performance of the algorithm on the simulated data,\nwithg=10\u00005andg=2\u000210\u00005. The median value of gamma is shown\nwith error bars at the 10thand 90thpercentile.\n1. For each point, x0, in a grid de\ufb01ned on the output image\nI1, \ufb01nd the corresponding point in the distorted input im-\nageI0given by D\u00001(x0).\n2. Copy the pixel back, performing the assignment:\nI1(x0):=I0(D\u00001(x0)):\nSince images are discrete, interpolation will be required\nto \ufb01nd I1(D\u00001(x0)). Bicubic interpolation has been used to\ngenerate the results shown in the following section.\n3 Results\n3.1 Synthetic tests\nIn order to compute the sensitivity to various kinds of noise,\nwe create a number of synthetic images, using known radial\ndistortion levels, and compute the resulting radial distortion\ncorrection parameters using our technique. We investigate\nthe two correction models, namely the strictly radial cor-\nrection of the unmodi\ufb01ed Harris model, and the anisotropic\nextension discussed previously. The images are designed to\ntest the performance of the algorithm in the presence of mea-\nsurement error in the plumb-lines, and both uncorrelated and\ncorrelated non-plumb-line data.\nThe 250\u0002250 pixel test images consist of \ufb01ve good\nquality lines (no point can be within 60 pixels of the centre\nof the image), consisting of 10 points on each line, drawn\nwith barrel distortion and with random orientation errors.\nThen (in the case of uncorrelated noise), a number of ran-\ndomly placed, randomly oriented points are added. In the\ncase of correlated noise, a number of randomly placed, sized6\nand oriented ellipses (with the same approximate point spac-\ning as the lines) are added. Our algorithm is then applied to\nthe test image to estimate the parameters.\nFor every selected noise proportion for the simulation,\n200 test images are created for each of two different values\nofg. The results of these, and an illustration of the simulated\ndata is shown in Figure 3. As can be seen, the technique is\nvery robust to noise, producing good results with up to 70%\ncontamination and interestingly it does not perform much\nworse in the case of correlated noise. Furthermore, although\nthe anisotropic extension (in this case) introduces six extra\nparameters to the optimization, this has no signi\ufb01cant effect\non the stability of the technique in high noise situations.\n3.2 Example images\nThe results on two real images are shown in Figure 4. In the\ncorrected images, real-world straight lines have been anno-\ntated with lines to show that there is no signi\ufb01cant amount\nof curvature. The upper image shows the technique operat-\ning on an image where the lines belong to vanishing points\n(note that some distortion was arti\ufb01cially added to this im-\nage in addition to the camera distortion present). Note the\npresence of strong curved edges along the boundaries of the\ncars and parts of the building.\nThe lower image shows the results on an aerial image\nof a city. Although there are two principal directions in this\nimage, the contrast on the principal edges is low compared\nto many of the other features. Additionally, the anisotropy\nextension was required to correct the very strong distortion\nin the lower left corner.\n3.3 Comparison to other techniques\nTo gauge the accuracy of our method, we have compared it\nto a technique based on structured scenes. In particular we\nhave used the technique of [10], which uses multiple images\nof a planar grid of squares to determine the intrinsic and\nextrinsic camera calibration parameters. This system is able\nto calibrate cubic, quintic, Harris and Harris with unit aspect\nratio camera models. We use the last listed model, since this\nmatches the camera model in the paper. In particular, we use\nthis program to optimize the model:\n\u0014xi\nyi\u0015\n=\u0014u\nv\u0015\n+\u0014f0\n0f\u0015\u0014xc\nyc\u00151p\n1+a(x2c+y2c); (16)\nwhere (u;v)is the optic axis, (xi;yi)is a coordinate in the\nimage, and (xc;yc)is a coordinate in ideal, normalized cam-\nera\u2019s image plane. Since the technique is a structured scene\ntechnique, it also estimates the focal length of the camera,\nf. To translate from one model to the other, set g=a\nf2. Notethat this assumes that the centre of radial distortion is at the\noptic axis of the camera, which is a reasonable assumption.\nIn order to measure the quality of the camera calibra-\ntion, we localise a known 3D model in an image, and mea-\nsure the root mean square (RMS) error between the control\npoints on the model and the measured points in the image.\nThe model consists of 13 by 9 black and white squares in a\nchequerboard pattern on a \ufb02at plane. The model is warped\nusing a homography (the parameters of which must be de-\ntermined), then by the calibrated radial distortion function\nand rendered in to the image. We then search normal to the\nrendered edges, looking for edges in the image. We de\ufb01ne\nthe error to be the distance offset in pixels between the ren-\ndered model edge and the measured edge position in the im-\nage The parameters of the homography are then adjusted\nusing the downhill-simplex algorithm to minimize the RMS\nerror. In particular, we search 15 pixels in either direction,\nand take the point with the highest gradient, measured us-\ning the kernel [\u00001\u00002\u00001 0 1 2 1 ]. If the highest gradient\ndoes not exceed some threshold, or is not a local maxima,\nthen the edge search is discarded and does not contribute to\nthe RMS error. Quadratic interpolation is then used to \ufb01nd\nthe edge position to subpixel accuracy. The search distance\nis suf\ufb01ciently small that data association is trivial, and ro-\nbust \ufb01tting is not required. A more detailed treatment of very\nclosely related systems for aligning models is given in [12,\n8].\nThe results of this test are shown in Figure 5. As ex-\npected, the technique which uses many (in this case approxi-\nmately 100) images of a structured scene performs best. The\nperformance of our method varies somewhat with the con-\ntents of the scene, but performs well on certain images. One\ninteresting property of our technique is well illustrated. The\nproperty is that edgels from multiple, unrelated images can\nbe aggregated to provide more straight edges. In the case\nshown, the resulting calibration is superior to the calibra-\ntion generated from a single image of a carefully constructed\ngrid.\n4 Discussion and conclusions\nIn this paper, we have presented a new, simple and robust\nmethod for determining the radial distortion of an image us-\ning the plumb-line constraint. The technique works by \ufb01rst\nextracting salient edgels and then minimizing the spread of\na 1D angular Hough transform of these edgels. The tech-\nnique is simple and because no edge \ufb01tting is performed, the\ntechnique is very robust to the presence of noise. Further-\nmore, the technique is more generally applicable than other\nplumb-line techniques in that the lines used do not need to\nbe continuous. The technique works on textures with prin-\ncipal directions, as illustrated by the aerial image of a city,7\nFig. 4 (Left) Original, distorted images. (Right) Images undistorted using our technique. Some lines which are straight in the world have been\nannotated with straight lines in the undistorted image. The images are of a building, showing strong vanishing points and an aerial image of a city.\nwhere the salient edge detection results in a large number of\nrelatively small edge fragments.\nThe proposed algorithm has a number of parameters: the\nparameters of the tensor voting kernel, the number of bins\nand the parameters of the optimization. In practice, the se-\nlection of these parameters are not critical, and indeed the\nsame set of parameters was used for the simulated data, the\nexample images and the test images shown.\nOur method is \ufb02exible in that it does not impose con-\nstraints beyond the presence of one or more straight edges:\nit is not a requirement that the edges share vanishing points,\nor structure of any particular kind. It is not even a require-\nment that the edgels belong to a related set of images. The\ntechnique can be equally applied to edgels from multiple\nimages of unrelated scenes taken with the same camera pa-\nrameters. Finally, our method is widely applicable because\nit is, in terms of RMS error, able to produce a calibration\nto within three percentage points of a technique requiring\naccess to the camera and structured scenes."}
{"category": "abstract", "text": "In this paper we present a simple and robust method\nfor self-correction of camera distortion using single images\nof scenes which contain straight lines. Since the most com-\nmon distortion can be modelled as radial distortion, we il-\nlustrate the method using the Harris radial distortion model,\nbut the method is applicable to any distortion model. The\nmethod is based on transforming the edgels of the distorted\nimage to a 1-D angular Hough space, and optimizing the dis-\ntortion correction parameters which minimize the entropy\nof the corresponding normalized histogram. Properly cor-\nrected imagery will have fewer curved lines, and therefore\nless spread in Hough space. Since the method does not rely\non any image structure beyond the existence of edgels shar-\ning some common orientations and does not use edge \ufb01tting,\nit is applicable to a wide variety of image types. For instance,\nit can be applied equally well to images of texture with weak\nbut dominant orientations, or images with strong vanishing\npoints. Finally, the method is performed on both synthetic\nand real data revealing that it is particularly robust to noise.\nKeywords Radial distortion\u0001Camera distortion \u0001Plumb-\nline constraint\nEdward Rosten\u0001Rohan Loveland\nLos Alamos National Laboratory\nLos Alamos, New Mexico, USA\nE-mail"}
{"category": "non-abstract", "text": "0805.1854v2  [cs.CV]  16 May 2008Figure 1. Objects as parts: in this de\ufb01nition,\nan object might be a whole scene subdivided\ninto parts (buffalos, background), or a single\nentity (a person) subdivided into meaningful\nparts (face, clothing, hair). All parts are de-\n\ufb01ned by the user through traces over the re-\ngions of interest, and each color represents\nan object label.\nevaluating a match between an input edge connecting two\ninput vertices which represent adjacent oversegmented re-\ngions related to two distinct objects and a model edge con-\nnecting model vertices which represent the the same previ-\nous objects.\nIn this paper, we propose a novel algorithm for the graph\nmatching step. Each possible matching from an input ARG\nvertex to a model ARG vertex is seen as a deformation of\nthe model graph (\ufb01g. 2), expressed by the introduction of\nthedeformation ARG, which represents an altered version\nof the model ARG that preserves its topological proper-\nties while entailing attributes from the given input vertex.\nThis new interpretation addresses the problem of matching\ntwo topologically different structures and results in a signif-\nicantly faster segmentation method.\nThis paper is organized as follows. Section 2 presents our\nformal de\ufb01nition of attributed relational graphs for image\nrepresentation. Section 3 gives an overview of all the steps\nof the segmentation method proposed herein. Section 4\ndescribes the problem of image segmentation as a graph\nmatching task, whereas Section 5 introduces the proposed\nsegmentation algorithm based on an optimization technique\nfor matching the input and model graphs. Finally, experi-\nmental results are discussed in Section 6 and a few conclu-\nsions, as well as suggestions for future work, are the topic\nof Section 7.\nFigure 2. Matching of two topologically dif-\nferent attributed relational graphs versus two\ntopologically equal ones under a deformation\npoint of view.\n2. Graph-based representation\nThe representation of images using graphs and the mat-\nter of graph matching applied to pattern recognition and im-\nage processing have been explored in a variety of situations,\nsuch as those reported in the works of Bunke [2] and Conte\net al [5], as well as in the method proposed by Felzenzswalb\nand Huttenlocher [8], and the classic work of Wilson and\nHancock [17], among others.\nIn this paper, an attributed relational graph (ARG)\nis a directed graph formally expressed as a tuple\nG= (V;E;\u0016;\u0017 ), whereVstands for its set of ver-\ntices andE\u0012V\u0002V, its set of edges. Typically, a vertex\nrepresents a single image region (subset of image pix-\nels) and an edge is created between vertices representing\ntwo image regions. \u0016:V!LVassigns an object at-\ntribute vector to each vertex of V, whereas\u0017:E!LEas-\nsigns a relational attribute vector to each edge of E.jVjde-notes the number of vertices in V, whilejEjdenotes the\nnumber of edges in E.\nFor color images, the object attribute vector is composed\nof the three average RGB values which characterize the cor-\nresponding image region, i.e. \u0016(v) = (Rv;Gv;Bv). When\ndealing with gray-scale images, \u0016(v) = (g(v)), whereg(v)\ndenotes the average gray-level of the image region asso-\nciated to vertex v2V. Each component of \u0016(v)is nor-\nmalized between 0 and 1 with respect to the minimum and\nmaximum possible gray-levels. Similarly, the relational at-\ntribute of an edge e= (v;w)2E,v;w2V, is de\ufb01ned as\n\u0017(e) = (pw\u0000pv)=(2dmax), wherepvandpware the cen-\ntroids of their respective corresponding image regions. The\nfactordmax is the largest distance between any two points\nof the input image region. Other attributes may easily be\nemployed, since the methodology presented herein does not\nimpose any restriction on the nature of \u0016and\u0017.\nFor the purpose of the segmentation method, three in-\nstances of such ARGs are considered: an input ARGGi=\n(Vi;Ei;\u0016i;\u0017i), derived from the input image, a model ARG\nGm= (Vm;Em;\u0016m;\u0017m), representing the objects of in-\nterest selected by the user, and a deformation ARGGd=\n(Vd;Ed;\u0016d;\u0017d), used as an auxiliary data structure for mea-\nsuring deformations implied in the model when matching a\nvertexv2Vito anotherw2Vm.\nSubscripts shall be used to denote the correspond-\ning graph, e.g. vi2Videnotes a vertex of Gi, whereas\n(vi;wi)2Eidenotes an edge of Gi. Similar nota-\ntions are used for GmandGdas well.\n3. Methodology overview\nThe segmentation process is depicted step-by-step in\n\ufb01g. 3. Given an input image to be segmented, the user \ufb01rst\npoints out the target objects by placing traces over the in-\nput, thus creating a model image in which each color iden-\nti\ufb01es an object of interest. Next, an oversegmentation is per-\nformed using the watershed algorithm to obtain a partition\nimage where the real contours of each object are present.\nThis oversegmented image is used to create both an in-\nput ARGGiand a model ARG Gm. The \ufb01rst is obtained in\nthe following fashion: each watershed region gives rise to\na vertex and its attributes, whereas adjacent regions devise\nan edge and its respective attributes. Gmis obtained simi-\nlarly, but only those watershed regions which intercept the\nuser-de\ufb01ned traces result in a model vertex. Clearly, the in-\nput and model ARGs present different topologies and this\nfact must be accounted for when using structure as a seg-\nmentation guide.\nSince the topological discrepancy is due to the overseg-\nmentation caused by the watershed, the \ufb01nal segmentation\nshould be a mapping of all vi2Visuch that input vertices\nrelated to image regions corresponding to the same model\nFigure 3. Overview of the methodology steps.\nThe ARGs are depicted only with their ver-\ntices for better visualization.\nobject are assigned to the same model vertex. This is equiv-\nalent to merging regions of an oversegmented object into\na single region. The mapping of vertices from Gito those\ninGmcharacterizes a graph matching problem [2, 5]. Al-\nthough many mappings are possible, a desirable solution\nshould correspond to an image partition as similar to that\nde\ufb01ned by the model as possible. Thus, to ensure that the \ufb01-\nnal mapping follows the model ARG topology, the defor-\nmation ARGGdis introduced. This graph is initialized as\na copy ofGmand it is used to help evaluate the local de-\nformation effect that a given assignment between a vertex\nvi2Viand another vm2Vminduces in the model. The\npursued solution is one that minimizes such effects. In the\nnext section, we discuss how these deformations are com-\nputed and how they \ufb01t in the graph matching problem solu-\ntion.\n4. The graph-matching algorithm for model-\nbased image segmentation\nA segmentation of the input image according to the\nmodel under the graph-based representation is a solution for\nthe graph matching problem between GiandGm, charac-terized as a mapping f:Vi!Vm. This implies \ufb01nding\na corresponding model vertex to each input vertex. Clearly,\nthere arejVmjpossible assignments for each input vertex\nand the decision of which to choose depends on an opti-\nmization procedure.\nLetGdbe an ARG initially equal to Gm, i.e.,Vd=Vm,\nEd=Em,\u0016d=\u0016m, and\u0017d=\u0017m. Let alsovdandvmbe\ntwo corresponding vertices in GdandGmrespectively. Sup-\npose that an assignment from a vertex vi2Vito a vertex\nvmis under consideration. The quality of such an assign-\nment may be assessed by computing the deformation which\noccurs in the model when this new vertex is merged with\nvd, causing the attributes of \u0016d(vd)and\u0017d(vd)to be fused\nwith those from vi. After such merge, Gdbecomes a dis-\ntorted version of the model in which:\n\u0016d(vd) = ((Rvd+Rvi)\n2;(Gvm+Gvi)\n2;(Bvm+Bvi)\n2)\n(1)\nand\n\u0017d(e) = (pwd\u0000pvd)=(2dmax) (2)\n8e2Ed(vd) =fe2Ed:e= (vd;wd)ore=\n(wd;vd);wd2Vdgand withpvd=pvm+pvi\n2.\nThe impact of such deformation is then measured ac-\ncording to the following cost function :\nf(Gd;Gm) =\u000bcV(vd;vm) +(1\u0000\u000b)\njEd(vd)jX\ne2Ed(vd)cE(e;em)\n(3)\nThe termcV(vd;vm)is a measure of the deformation be-\ntween the object attributes of vdandvmand it is de\ufb01ned as:\ncV(vd;vm) =sX\nC=R;G;B(Cvd\u0000Cvm)2 (4)\nAlthough the RGB color space was chosen to describe\nthe color appearance feature of an object, other color spaces,\nsuch as the Lab, might as well be used with appropriate met-\nrics adaptation.\nSimilarly, if e2Ed(v(d))andem2Emis its corre-\nsponding edge, then cE(e;em)is a measure of the deforma-\ntion between both edges de\ufb01ned as:\ncE(e;em) =\rEjcos(\u0012)\u00001j\n2+(1\u0000\rE)jk\u0017(e)k\u0000k\u0017(em)kj\n(5)\nThe value\u0012is the angle between \u0017(e)and\u0017(em),\nwhereas, the parameter \rE,0\u0014\rE\u00141, controls\nthe weights of the modulus and angular dissimilari-\nties. Thus, the total impact caused on the edges directlyconnected by vdis computed as the modulus and angu-\nlar differences between the relational attribute vectors of\neach pair (e;em).\nTherefore, the cost function measures how the merging\nof a vertexviwith a copy vdof a model vertex affects the\nlocal structure of the graph, as well as the appearance at-\ntributes it holds. The parameter \u000b,0\u0014\u000b\u00141, controls the\nimportance of the appearance or structural effects of all ver-\ntex mappings.\n5. The optimization algorithm\nTo mapGitoGmand estimate the adequacy of each ver-\ntex assignment using Gd, the following algorithm was de-\nvised:\n1. de\ufb01ne\u000b\n2. de\ufb01ne\r\n3. for each vertex vi2Vi\n4.fmin 1\n5.minlbl \u00001\n6. for each vertex vd2Vd\n7.Gd Gm\n8.\u0016(vd)(Rvd+Rvm\n2;Gvd+Gvm\n2;Bvd+Bvm\n2)\n9. for each e2Ed((vd)s.t.e= (vd;w)ore=\n(w;vd),w2Vd\n10.\u0017(e) vector between centroids of vdandw\n11. compute the value fof the cost function (eq. 3)\nf(Gd;Gm)\n12. iff <f min\n13.fmin f\n14.minlbl vd\n15. label( vi) minlbl\nAll possible assignments for each vertex vi2Viare\nevaluated and the \ufb01nal label of vicorresponds to the model\nvertex which was affected the least by the deformation\ncaused by the introduction of vi. At each iteration, a ver-\ntex fromVireceives a label. fmin is the minimum defor-\nmation cost obtained so far and minlbl is the correspond-\ning model vertex to which viwas mapped resulting in such\nminimal deformation. The output of the algorithm is a map-\nping of all vertices of Vito vertices of Vm.Figure 4. Sample segmentation results: orig-\ninal images (left col.), images with user-input\nstrokes de\ufb01ning regions of interest (center\ncol.) and corresponding resulting partitions\n(right col.).\n6. Experimental results\nIn order to test the presented technique, a Java applica-\ntion was designed and implemented1. Its interface allows\nthe user to load images to be segmented, de\ufb01ne a model\naccording to traces drawn over different regions of inter-\nest, and choose the \u000band\rparameters of the cost function\n(eq. 3), therefore specifying how to favor structure against\nappearance features.\nFigure 4 shows a few segmentation results for the appli-\ncation of the methodology to natural color images obtained\nfrom the Berkeley Image Segmentation Database2. All re-\nsulting images depict the \ufb01nal regions labelled according to\nthe color of the traces de\ufb01ned by the user. Transparency was\nused over the labelled segmented images in order to visual-\nize more precisely what areas have been classi\ufb01ed as a given\nobject. Although certain image regions, such as the moun-\ntains and the face, present high variability due to textures or\ndifferent objects grouped as one in the model, the \ufb01nal seg-\nmentation remains accurate and robust thanks to the struc-\n1 Please refer to the accompanying demo video.\n2 http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/tural constraints embedded in the model.\nTests for the reusability and robustness of the model were\nalso performed on sample frames of a moving head video\n(\ufb01g. 5 top) from the XM2VTS Database3and on a set of\nsimilar images retrieved randomly from the web (\ufb01g. 5 bot-\ntom). Each model was de\ufb01ned once by the user in the \ufb01rst\nimage of each set and then applied to the other similar im-\nages. The latter step is interactively accomplished as fol-\nlows: once the user draws the traces over the \ufb01rst image,\na minimum enclosing rectangle of the strokes is automati-\ncally de\ufb01ned. This rectangle, called a stamp , can later be ap-\nplied by the user to other images and segmentation can be\nperformed within such area.\nNote that the model ARG is derived only once for the\n\ufb01rst image and then used in the segmentation process of all\nother input images. It is important to notice that simply ap-\nplying the same strokes to the other images would not pro-\nduce the same model as the one obtained for the \ufb01rst time,\nas \ufb01g. 6 depicts. This also shows that the model is robust\nenough to treat small variances presented by the input im-\nages under analysis. For each image set, the overall struc-\nture remained similar and the segmentation was once again\nsatisfactory even though the model was derived from an im-\nage with different appearance features.\nThe new algorithm proposed for the graph matching step\npresents faster performance than that of the algorithm re-\nported in [4]. While the present algorithm runs in time pro-\nportional to O(jVijjVmj), the other is bounded by a func-\ntion\n(jVijjVmj2). Besides this, the optimization algorithm\ndoes not depend on the order in which vertices from the in-\nput are labelled, since each vertex is treated separately when\nanalysed during the graph matching step.\n7. Conclusion\nThis paper proposed a novel algorithm for performing in-\nteractive model-based image segmentation using attributed\nrelational graphs to represent both model and input images.\nThis approach allows the usage of information ranging from\nappearance features to structural constraints. Topological\ndifferences between graphs are dealt with by means of a\ndeformation ARG, a structure which allowed the design of\nan optimization algorithm for graph matching that evaluates\npossible solutions according to local impacts (or deforma-\ntions) they determine on the model. The faster performance\nof the algorithm in comparison with the one proposed in [4],\nthe reusability of the model graph when segmenting sev-\neral images, as well as the satisfying quality of the results\ndue to the adequate use of structural information, character-\nize the main contributions of the method.\n3 http://www.ee.surrey.ac.uk/CVSSP/xm2vtsdb/Figure 5. Sample segmentation results after\napplying the same user-de\ufb01ned stroke model\n(top left image in each set) to different im-\nages.\nOur ongoing work is devoted to reducing interaction\nwhen reusing the model to segment various images. For\nnow, it is required that the user places the stamp over the\narea of interest of the image. In the future, we hope to be\nable to apply the model ARG without the need of this inter-\nactive positional information. This shall be accomplished\nthrough the investigation of MAP-MRF methods applied\nwithin this framework in order to make more robust mod-\nels and improve segmentation quality under different con-\nditions such as object translation and rotation. Furthermore,\nwe intend to perform a quantitative study to compare the ac-\ncuracy of our results with those of other related methods.\nFigure 6. Replication of the model: simply\nreusing the user-de\ufb01ned strokes over dif-\nferent images does not guarantee that the\nmodel to be derived is always consistent,\nsince the strokes could fall over distinct ob-\njects from one image to another and the \ufb01nal\nsegmentation would be compromised."}
{"category": "abstract", "text": "This paper proposes a novel algorithm for the prob-\nlem of structural image segmentation through an interac-\ntive model-based approach. Interaction is expressed in the\nmodel creation, which is done according to user traces\ndrawn over a given input image. Both model and input are\nthen represented by means of attributed relational graphs\nderived on the \ufb02y. Appearance features are taken into ac-\ncount as object attributes and structural properties are ex-\npressed as relational attributes. To cope with possible topo-\nlogical differences between both graphs, a new structure\ncalled the deformation graph is introduced. The segmenta-\ntion process corresponds to \ufb01nding a labelling of the in-\nput graph that minimizes the deformations introduced in\nthe model when it is updated with input information. This\napproach has shown to be faster than other segmentation\nmethods, with competitive output quality. Therefore, the\nmethod solves the problem of multiple label segmentation in\nan ef\ufb01cient way. Encouraging results on both natural and\ntarget-speci\ufb01c color images, as well as examples showing\nthe reusability of the model, are presented and discussed.\n1. Introduction\nSemi-automated, or interactive, image segmentation\nmethods have successfully been used in different appli-\ncations, whenever human knowledge may be provided as\ninitial guiding clues for the segmentation process. Exam-\nples of such methods are the region-growing technique,\nmarker-based watersheds [16], the IFT [7], graph-cuts\nand Markov-random \ufb01elds [1, 14, 15], amongst oth-\ners.\nAnother source of a priori information for segmentation\nare image models, which consist of representative instances\nof desired objects, conveying different types of features\n(e.g. color, shape, geometry, relations, etc.) that describesuch entities. Approaches guided by models are widely used\nfor a variety of image processing purposes such as medical\nimaging [9, 11, 12, 13], face recognition and tracking [3, 6],\nand OCR [10].\nThough the aforementioned interactive approaches have\nestablished remarkable contributions to the image segmen-\ntation domain, most of them have not attempted to consider\nimage structure to aid the segmentation procedure. An at-\ntributed relational graphs (ARG) is a particularly useful\nrepresentation not only for embedding structural informa-\ntion when modelling a problem, but also for expressing ap-\npearance features.\nRegarding the segmentation issue, the present paper pro-\nposes a new algorithm for segmenting color images using\nboth interactive cues and a model using an ARG-based rep-\nresentation. An object (\ufb01g. 1 left) is considered to be a set of\nparts (subset of pixels of an image) and their relations. An\nobject model image is de\ufb01ned by a user according to traces\ndrawn over the input (\ufb01g. 1 right). The input and model im-\nages are then represented by means of attributed relational\ngraphs, in which objects and their relations are represented,\nrespectively, as vertices and edges. Under this formulation,\nthe segmentation problem is viewed as a graph matching\nprocedure.\nThe introduced algorithm substantially improves the ap-\nproach described in [4], in which structure was taken into\naccount when segmenting an input gray-scale image ac-\ncording to a model, but the structures under comparison,\nrepresented by ARGs, often presented different topologies.\nSuch differences imply dif\ufb01culties to determine a suitable\nmapping between the input and model graphs, as well as\nhigh computational cost. As \ufb01g. 2 shows, the graph match-\ning problem allows many possible solutions and therefore\nthe optimization procedure not only has to consider ver-\ntex similarities, but it also has to evaluate various structural\nmatch con\ufb01gurations in order to rule out those which are not\nplausible for a \ufb01nal segmentation. However, this might be\nmisleading when both topologies are distinct and cause the\nmethod to look down on potential solutions, such as whenarXiv"}
{"category": "non-abstract", "text": "Camera calibration, Radial distortion, Radial\nundistortion, Polynomial Models, Rational Models.\nI. Introduction\nA. Camera Calibration\nTo address the problem of radial distortion, the prob-\nlem of camera calibration needs to be addressed \ufb01rst,\nsince the radial distortion is one step in the camera cal-\nibration procedures. Depending on what kind of calibra-\ntion object used, there are mainly two categories of cal-\nibration methods: photogrammetric calibration and self-\ncalibration. Photogrammetric calibration refers to those\nmethods that observe a calibration object whose geome-\ntry in 3-D space is known with a very good precision [1],\n[2], [3]. Self-calibration does not need any calibration ob -\nject. It only requires point matches or correspondences\nfrom image sequence. In [4], it is shown that it is possible\nto calibrate a camera just by pointing it to the environ-\nment, selecting points of interest and then tracking them\nin the image as the camera moves. The obvious advantage\nof the self-calibration method is that it is not necessary\nto know the camera motion and it is easy to set up. The\ndisadvantage is that it is usually considered unreliable [5 ].\nA four step calibration procedure is proposed in [6] where\nthe calibration is performed with a known 3-D target. The\nfour steps in [6] are: linear parameter estimation, nonlin-\near optimization, correction using circle/ellipse, and im age\ncorrection. But for a simple start, linear parameter es-\ntimation and nonlinear optimization are enough. In [7],\na plane-based calibration method is described where the\ncalibration is performed by \ufb01rst determining the absolute\nconicB=A\u2212TA\u22121, where Ais a matrix formed by the\ncamera\u2019s intrinsic parameters. In [7], the parameter \u03b3(a\nparameter describing the skewness of the two image axes)\nis assumed to be zero and it is observed that only the rel-\native orientations of planes and camera are of importancein avoiding singularities because the planes that are par-\nallel to each other provide exactly the same information.\nThe camera calibration method in [2] focuses on the desk-\ntop vision system and lies between the photogrammetric\ncalibration and the self-calibration, because 2-D metric i n-\nformation is used rather than 3-D. The key feature of the\ncalibration method in [2] is that the absolute conic Bis\nused to estimate the intrinsic parameters and the parame-\nter\u03b3can be considered. The proposed technique in [2] only\nrequires the camera to observe a planar pattern at a few\n(at least 3, if both the intrinsic and the extrinsic param-\neters are to be estimated uniquely) di\ufb00erent orientations.\nEither the camera or the calibration object can be moved\nby hand as long as they cause no singularity problem and\nthe motion of the calibration object or camera itself needs\nnot to be known in advance.\nAfter estimation of camera parameters, a perspective\nprojection matrix Mcan directly link a point in the 3-\nD world reference frame to its projection (undistorted) in\nthe image plane. That is\n\u03bb\uf8ee\n\uf8f0u\nv\n1\uf8f9\n\uf8fb=M\uf8ee\n\uf8ef\uf8f0Xw\nYw\nZw\n1\uf8f9\n\uf8fa\uf8fb=A[R|t]\uf8ee\n\uf8ef\uf8f0Xw\nYw\nZw\n1\uf8f9\n\uf8fa\uf8fb=A\uf8ee\n\uf8f0Xc\nYc\nZc\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0u\nv\n1\uf8f9\n\uf8fb=A\uf8ee\n\uf8f0Xc\nZc\nYc\nZc\n1\uf8f9\n\uf8fb=\uf8ee\n\uf8f0\u03b1 \u03b3 u 0\n0\u03b2 v 0\n0 0 1\uf8f9\n\uf8fb\uf8ee\n\uf8f0x\ny\n1\uf8f9\n\uf8fb. (1)\nThe matrix Afully depends on the 5 intrinsic parameters\nwith their detail descriptions in Table I, where some other\nvariables used throughout this paper are also listed.\nThe calibration method used in this work \ufb01rst estimates\nthe projection matrix and then uses the absolute conic to\nestimate the intrinsic parameters [2]. The detail procedur es\nare summarized below:\n\u2022Linear Parameter Estimation,\n\u2013Estimation of Intrinsic Parameters;\n\u2013Estimation of Extrinsic Parameters;\n\u2013Estimation of Distortion Coe\ufb03cients;\n\u2022Nonlinear Optimization.\nB. Radial Distortion\nVirtually all imaging devices introduce certain amount\nof nonlinear distortion, where the radial distortion is theTABLE I\nList of Variables\nVariable Description\nPw= [Xw, Yw, Zw]T3-D point in the world frame\nPc= [Xc, Yc, Zc]T3-D point in the camera frame\n[R|t] Pc=RPw+t\n(ud, vd) Distorted image points in pixel\n(u, v) Undistorted image points in pixel\n(xd, yd) [xd, yd,1]T=A\u22121[ud, vd,1]T\n(x, y) [x, y,1]T=A\u22121[u, v,1]T\n(Xu, Yu)/bracketleftBigXu\nYu/bracketrightBig\n=f/bracketleftbigg\nXc\nZc\nYc\nZc/bracketrightbigg\n=f/bracketleftBigx\ny/bracketrightBig\nr r2=x2+y2\nf Focal length\n(\u03b1, \u03b2, \u03b3, u 0, v0) 5 intrinsic parameters\nk Distortion coe\ufb03cients\nJ Objective function\nA Camera intrinsic matrix\nmost severe part [3], [8]. Radial distortion causes an in-\nward or outward displacement of a given image point from\nits ideal location. The negative radial displacement of the\nimage points is referred to as the barrel distortion, while\nthe positive radial displacement is referred to as the pin-\ncushion distortion [9].\nThe removal or alleviation of radial distortion is com-\nmonly performed by \ufb01rst applying a parametric radial dis-\ntortion model, estimating the distortion coe\ufb03cients, and\nthen correcting the distortion. Most of the existing works\non radial distortion models can be traced back to an early\nstudy in photogrammetry [10] where the radial distortion\nis governed by the following polynomial equation [2], [11],\n[12], [13]:\nF(r) =r f(r) =r(1 +k1r2+k2r4+k3r6+\u00b7 \u00b7 \u00b7),(2)\nwhere k1, k2, k3, . . .are the distortion coe\ufb03cients and [2],\n[11]\nr2=x2+y2, (3)\nor [12], [13]\nr2=X2\nu+Y2\nu=f2(x2+y2). (4)\nBoth of the above two formulae of rare in the camera frame\nand they are basically the same, since the resulting distor-\ntion coe\ufb03cients have one-to-one relations with each other,\nwhere one set of distortion coe\ufb03cients are proportional to\nthe other set by a series of scalars ( f2, f4, . . .).\nUntil recently, the most commonly used radial distortion\nmodels are still in the polynomial form of (2), though other\nmodels, such as the division model [14] and the \ufb01sh-eye\nradial distortion models (the Fish Eye Transform [3] and\nthe Field-Of-View [15]), are available in the literature.\nFor the polynomial radial distortion model in (2) and\nits variations, the distortion is especially dominated by t he\n\ufb01rst term and it has also been found that too high an or-\nder may cause numerical instability [2], [8], [11]. In thispaper, at most three terms of radial distortion are consid-\nered. When using two coe\ufb03cients, the f(r) in (2) becomes\nf(r) = 1 + k1r2+k2r4. (5)\nThe relationship between the distorted and the undistorted\nimage points becomes [2]\nud\u2212u0= (u\u2212u0)(1 + k1r2+k2r4),\nvd\u2212v0= (v\u2212v0)(1 + k1r2+k2r4).(6)\nThe polynomial function in (5) has one main drawback,\nthat is, the inverse of the polynomial function in (6) is\ndi\ufb03cult to perform analytically but can be obtained nu-\nmerically via an iterative scheme. In [16], for practical\npurpose, only one distortion coe\ufb03cient k1is used. To over-\ncome the inversion problem, another polynomial radial dis-\ntortion model using also two terms is proposed as [17]\nf(r) = 1 + k1r+k2r2, (7)\nwhose main appealing feature lies in its satisfactory ac-\ncuracy as well as the existence of an easy analytical radial\nundistortion formula. The two polynomial radial distortio n\nmodels in (5) and (7) act as benchmarks for evaluating the\nperformance of the rational distortion models presented in\nSec. III.\nIn this work, a new class of rational radial distortion\nmodels are proposed. To compare the performance of our\nnew models with the existing polynomial approximation\nmodels, the calibration procedures presented in [2] are ap-\nplied, while being aware that the usage of other calibration\nmethods, such as the image registration method in [18] and\nthe plumb-line algorithm in [19], are also feasible.\nIn [2], the estimation of radial distortion is done after\nhaving estimated the intrinsic and the extrinsic parame-\nters just before the nonlinear optimization step. So, for\ndi\ufb00erent radial distortion models, we can reuse the esti-\nmated intrinsic and extrinsic parameters. To compare the\nperformance of di\ufb00erent radial distortion models, the valu e\nof optimization function Jis used, where the initial guess\nforkis chosen to be 0. The objective function used for\nnonlinear optimization is [2]:\nJ=N/summationdisplay\ni=1n/summationdisplay\nj=1/bardblmij\u2212\u02c6m(A, k1, k2,Ri,ti, Mj)/bardbl2,(8)\nwhere \u02c6 m(A, k1, k2,Ri,ti, Mj) is the projection of point Mj\nin the ithimage using the estimated parameters and Mjis\nthejth3-D point in the world frame with Zw= 0. Here, n\nis the number of feature points in the coplanar calibration\nobject and Nis the number of images taken for calibration.\nThe rest of the paper is organized as follows. Sec. II\ndescribes the two polynomial radial distortion models (5)\nand (7) in detail, where the inverse problem of (5) and the\nanalytical undistortion formula of (7) are also described.\nThe new class of rational radial distortion models and the\ncomparison with the existing polynomial models are pre-\nsented in Sec. III. Finally, some concluding remarks are\ngiven in Sec. IV.II. Polynomial Radial Distortion Models and\nTheir Undistortion Functions\nFrom (6), the radial distortion can be resulted in one of\nthe following two ways:\n\u2022Transform from the camera frame to the image plane,\nthen perform distortion in the image plane\n/bracketleftbigg\nx\ny/bracketrightbigg\n\u2192/bracketleftbigg\nu\nv/bracketrightbigg\n\u2192/bracketleftbigg\nud\nvd/bracketrightbigg\n;\n\u2022Perform distortion in the camera frame, then transform\nto the image plane\n/bracketleftbigg\nx\ny/bracketrightbigg\n\u2192/bracketleftbigg\nxd\nyd/bracketrightbigg\n\u2192/bracketleftbigg\nud\nvd/bracketrightbigg\n,\nwhere\nxd=xf(r), y d=y f(r). (9)\nThis is because\uf8ee\n\uf8f0u\nv\n1\uf8f9\n\uf8fb=A\uf8ee\n\uf8f0x\ny\n1\uf8f9\n\uf8fb=\uf8ee\n\uf8f0\u03b1 \u03b3 u 0\n0\u03b2 v 0\n0 0 1\uf8f9\n\uf8fb\uf8ee\n\uf8f0x\ny\n1\uf8f9\n\uf8fb.\nThen, (6) becomes\nud= (u\u2212u0)f(r) +u0\n=\u03b1 xf(r) +\u03b3 yf(r) +u0\n=\u03b1 xd+\u03b3 yd+u0, (10)\nvd= (v\u2212v0)f(r) +v0\n=\u03b2 yd+v0.\nTherefore,\n\uf8ee\n\uf8f0ud\nvd\n1\uf8f9\n\uf8fb=A\uf8ee\n\uf8f0xd\nyd\n1\uf8f9\n\uf8fb.\nThus, the distortion performed in the image plane can be\nunderstood from a new point of view: introducing distor-\ntion in the camera frame and then transform to the image\nplane. The radial undistortion can be perfomed by extract-\ning (x, y) from ( xd, yd).\nA. Radial Undistortion of Model (5)\nThe following derivation shows the problem when trying\nto extract ( x, y) from ( xd, yd) using (5). According to (9),\nxd=xf(r) =x[1 +k1(x2+y2) +k2(x2+y2)2],\nyd=yf(r) =y[1 +k1(x2+y2) +k2(x2+y2)2].(11)\nIt is obvious that xd= 0 i\ufb00 x= 0. When xd/ne}ationslash= 0, by letting\nc=yd/xd=y/x, we have y=cxwhere cis a constant.\nSubstituting y=cxinto the above equation gives\nxd=x[1 +k1(1 +c2)x2+k2(1 +c2)2x4]. (12)\nNotice that the above function is an odd function. An\nintuitive understanding of (12) is that the radial distorti on\nfunction is to approximate the relationship between xdand\nx, which, in ideal cases, is xd=x. The analytical solution\nof (12) is not a trivial task, which is still an open problem\n(of course, we can use numerical method to solve it).B. Radial Undistortion of Model (7)\nFor (7), in a similar manner as for (5), we have\nxd=x[1 +k1/radicalbig\n1 +c2sgn(x)x+k2(1 +c2)x2],(13)\nwheresgn(x) gives the sign of x. To extract xfromxdin\n(13), the following algorithm can be applied:\n1)x= 0 i\ufb00 xd= 0,\n2)Assuming that x >0, (13) becomes\nxd=x+k1/radicalbig\n1 +c2x2+k2(1 +c2)x3.\nUsingsolve , a Matlab Symbolic Toolbox function, we can\nget three possible solutions for the above equation denoted\nbyx1+,x2+, and x3+respectively. To make the equations\nsimple, let y=xd,p=k1\u221a\n1 +c2andq=k2(1 +c2). The\nthree possible solutions for y=x+px2+qx3are\nx1+=1\n6qE1+2\n3E2\u2212p\n3q,\nx2+=\u22121\n12qE1\u22121\n3E2\u2212p\n3q+\u221a\n3\n2(1\n6qE1\u22122\n3E2)j,(14)\nx3+=\u22121\n12qE1\u22121\n3E2\u2212p\n3q\u2212\u221a\n3\n2(1\n6qE1\u22122\n3E2)j,\nwhere\nE1={36pq+ 108 yq2\u22128p3\n+12\u221a\n3q/radicalbig\n4q\u2212p2+ 18pqy+ 27y2q2\u22124yp3}1/3,(15)\nE2=p2\u22123q\nqE1,j=\u221a\n\u22121.\nFrom the above three possible solutions, we discard those\nwith imaginary parts not equal to zero. Then, from the\nremaining, discard those solutions that con\ufb02ict with the\nassumption that x >0. Finally, we get the candidate solu-\ntionx+by choosing the one closest to xdif the number of\nremaining solutions is greater than 1.\n3)Assuming that x <0, there are also three possible so-\nlutions for\nxd=x\u2212k1/radicalbig\n1 +c2x2+k2(1 +c2)x3, (16)\nwhich can be written as\ny=x+ (\u2212p)x2+qx3. (17)\nThe three solutions for (17) can thus be calculated from\n(14) and (15) by substituting p=\u2212p. With a similar\nprocedure as described in the case for x >0, we will have\nanother candidate solution x\u2212.\n4)Choose among x+andx\u2212for the \ufb01nal solution of xby\ntaking the one closest to xd.\nThe basic idea to extract xfromxdin (13) is to choose\nfrom several candidate solutions, whose analytical formul ae\nare known. The bene\ufb01ts of using this new radial distortion\nmodel are as follows:\n\u2022Low order \ufb01tting, better for \ufb01xed-point implementation;\n\u2022Explicit or analytical inverse function with no numerical\niterations, which is important for real-time vision applic a-\ntions.TABLE II\nDistortion Models\nModel #f(r) xd=f(x)\n0 1 +k1r2+k2r4x\u00b7(1 +k1(1 +c2)x2+k2(1 +c2)2x4)\n1 1 +k r x\u00b7(1 +k\u221a\n1 +c2x sgn(x))\n2 1 +k r2x\u00b7(1 +k(1 +c2)x2)\n3 1 +k1r+k2r2x\u00b7(1 +k1\u221a\n1 +c2x sgn(x) +k2(1 +c2)x2)\n41\n1 +k rx\u00b71\n1 +k\u221a\n1 +c2x sgn(x)\n51\n1 +k r2x\u00b71\n1 +k(1 +c2)x2\n61 +k1r\n1 +k2r2x\u00b71 +k1\u221a\n1 +c2x sgn(x)\n1 +k2(1 +c2)x2\n71\n1 +k1r+k2r2x\u00b71\n1 +k1\u221a\n1 +c2x sgn(x) +k2(1 +c2)x2\n81 +k1r\n1 +k2r+k3r2x\u00b71 +k1\u221a\n1 +c2x sgn(x)\n1 +k2\u221a\n1 +c2x sgn(x) +k3(1 +c2)x2\n91 +k1r2\n1 +k2r+k3r2x\u00b71 +k1(1 +c2)x2\n1 +k2\u221a\n1 +c2x sgn(x) +k3(1 +c2)x2\nIII. Rational Radial Distortion Models\nTo be a candidate for radial distortion model, the func-\ntion must satisfy the following properties:\n1)This function is radially symmetric around the image\ncenter ( u0, v0) and it is expressible in terms of radius r\nonly;\n2)This function is continuous, hence F(r) = 0 i\ufb00 r= 0;\n3)The resultant approximation of xdis an odd function\nofx.\nBased on the above criteria, a new class of radial distortion\nmodels (model #4 ,5,6,7,8,9) are proposed and summa-\nrized in Table II, where the other four polynomial models\n(model #0 ,1,2,3) are also listed.\nNow, we want to compare the performance of our new\nradial distortion models with the four polynomial models in\nTable II based on the \ufb01nal value of objective function after\nnonlinear optimization by the Matlab function fminunc .\nUsing the public domain test images [20], the desktop cam-\nera images (a color camera in our CSOIS), and the ODIS\ncamera images [17] (the camera on ODIS robot built in\nour CSOIS), the \ufb01nal objective function J, the estimated\ndistortion coe\ufb03cients k, and the 5 estimated intrinsic pa-\nrameters ( \u03b1, \u03b2, \u03b3, u 0, v0) are shown in Tables III, IV, and\nV, respectively. The reason for listing ( \u03b1, \u03b2, \u03b3, u 0, v0) is\nto show that the estimated parameters after nonlinear op-\ntimization are consistent when using di\ufb00erent distortion\nmodels.\nTable III, IV, and V are of the same format. The \ufb01rst\ncolumn is the model number used in Table II. The second\ncolumn shows the values of objective function Jde\ufb01ned\nin (8). The third column, the rank, sorts the distortion\nmodels by Jin a [0-smallest, 9-largest] manner.\nAfter carefully examining Table III, IV, and V, we have\nthe following observations:\n1)Using the proposed rational models, we can achievecomparable, or even better, results compared with the\npolynomial models in Table II, where the best result is\nfrom model 0, model 8or model 9. The advantage of using\nthe last two models is that the inverse function is at most\nof order 3. The radial undistortion can thus be performed\nusing the procedures described in Sec. II-B;\n2)For each category of models, either polynomial or ratio-\nnal, they generally follow the trend that the more complex\nthe model, the more accurate the performance (the smaller\nthe objective function J);\n3)There is no general rule to decide at which point the\npolynomial models become better than the rational ones. It\nis dependent on the particular data set. However, the last\nthree rational models always give the best results among\nall the non-iterative models, model 1-9;\n4)When the distortion is signi\ufb01cant, the performance im-\nprovement using complex models is more obvious.\nTo make the results in this paper repeatable\nby other researchers for further investigation, we\npresent the options we use for the nonlinear op-\ntimization: options = optimset(\u2018Display\u2019, \u2018iter\u2019,\n\u2018LargeScale\u2019, \u2018off\u2019, \u2018MaxFunEvals\u2019, 8000, \u2018TolX\u2019,\n10\u22125, \u2018TolFun\u2019, 10\u22125, \u2018MaxIter\u2019, 120) . The raw\ndata of the extracted feature locations in the image plane\nare also available upon request.\nIV. Concluding Remarks\nThis paper proposes a new class of rational radial distor-\ntion models. The appealing part of these distortion models\nis that they preserve high accuracy together with easy an-\nalytical undistortion formulae. Performance comparisons\nare made between this class of new rational models and\nthe existing polynomial models. Experiments results are\npresented to show that this new class of rational distortion\nmodels can be quite accurate and e\ufb03cient especially whenTABLE III\nComparison Results using Microsoft Images\n# J Rank Final Values of k Final Values of (\u03b1, \u03b3, u 0, \u03b2, v0)\n0144.8802 2 -0.2286 0.1905 - 832.4860 0.2042 303.9605 832.5157 206.5811\n1180.5714 8 -0.0984 - - 845.3051 0.1918 303.5723 845.2628 208.4394\n2148.2789 7 -0.1984 - - 830.7425 0.2166 303.9486 830.7983 206.5574\n3145.6592 5 -0.0215 -0.1566 - 833.6508 0.2075 303.9847 833.6866 206.5553\n4185.0628 9 0.1031 - - 846.1300 0.1921 303.5070 846.0823 208.6944\n5147.0000 6 0.2050 - - 831.0863 0.2139 303.9647 831.1368 206.5175\n6145.4682 4 -0.0174 0.1702 - 833.3970 0.2071 303.9689 833.4324 206.5567\n7145.4504 3 0.0170 0.1725 - 833.3849 0.2068 303.9719 833.4198 206.5443\n8144.8328 1 1.6457 1.6115 0.4054 830.9411 0.2044 303.9571 830.9705 206.5833\n9144.8257 0 1.2790 -0.0119 1.5478 831.7373 0.2045 303.9573 831.7665 206.5925\nTABLE IV\nComparison Results using Desktop Images\n#J(\u00d7103)Rank Final Values of k Final Values of (\u03b1, \u03b3, u 0, \u03b2, v0)\n0 0.7790 0 -0.3435 0.1232 - 277.1449 -0.5731 153.9882 270.5582 119.8105\n1 1.0167 8 -0.2466 - - 295.5734 -0.8196 156.6108 288.8763 119.8528\n2 0.9047 7 -0.2765 - - 275.5953 -0.6665 158.2016 269.2301 121.5257\n3 0.8033 6 -0.1067 -0.1577 - 282.5642 -0.6199 154.4913 275.9019 120.0924\n4 1.2018 9 0.3045 - - 302.2339 -1.0236 160.5601 295.6767 120.7448\n5 0.7986 5 0.3252 - - 276.2521 -0.5780 154.7976 269.7064 120.3235\n6 0.7876 4 -0.0485 0.2644 - 279.5062 -0.5888 154.1735 272.8822 119.9564\n7 0.7864 3 0.0424 0.2834 - 279.3268 -0.5870 154.1168 272.7049 119.9214\n8 0.7809 2 0.5868 0.5271 0.5302 275.8311 -0.5735 153.9991 269.2828 119.8195\n9 0.7800 1 0.2768 -0.0252 0.6778 276.4501 -0.5731 153.9914 269.8850 119.8091\nTABLE V\nComparison Results using ODIS Images\n#J(\u00d7103)Rank Final Values of k Final Values of (\u03b1, \u03b3, u 0, \u03b2, v0)\n0 0.8403 2 -0.3554 0.1633 - 260.7658 -0.2741 140.0581 255.1489 113.1727\n1 0.9444 8 -0.2327 - - 274.2660 -0.1153 140.3620 268.3070 114.3916\n2 0.9331 7 -0.2752 - - 258.3193 -0.5165 137.2150 252.6856 115.9302\n3 0.8513 5 -0.1192 -0.1365 - 266.0850 -0.3677 139.9198 260.3133 113.2412\n4 1.0366 9 0.2828 - - 278.0218 -0.0289 139.5948 271.9274 116.2992\n5 0.8676 6 0.3190 - - 259.4947 -0.4301 139.1252 253.8698 113.9611\n6 0.8450 4 -0.0815 0.2119 - 264.4038 -0.3505 140.0528 258.6809 113.1445\n7 0.8438 3 0.0725 0.2419 - 264.1341 -0.3429 140.1092 258.4206 113.1129\n8 0.8379 0 1.2859 1.1839 0.7187 259.2880 -0.2824 140.2936 253.7043 113.0078\n9 0.8383 1 0.4494 -0.0124 0.8540 260.9370 -0.2804 140.2437 255.3178 113.0561\nthe actual distortion is signi\ufb01cant."}
{"category": "abstract", "text": "\u2014The common approach to radial distortion is by\nthe means of polynomial approximation, which introduces\ndistortion-speci\ufb01c parameters into the camera model and re -\nquires estimation of these distortion parameters. The task\nof estimating radial distortion is to \ufb01nd a radial distortio n\nmodel that allows easy undistortion as well as satisfactory\naccuracy. This paper presents a new class of rational radial\ndistortion models with easy analytical undistortion formu -\nlae. Experimental results are presented to show that with\nthis class of rational radial distortion models, satisfact ory\nand comparable accuracy is achieved.\nKey Words"}
{"category": "non-abstract", "text": "Gamma correction as a function of intensity.\n(solid)\u03b3= 1; (dashed) \u03b3= 0.45; (dotted) \u03b3= 2.22.\nNote how, for \u03b3 <1, the lower intensities are mapped onto\na largerrange.\nItturnsoutthataninvariantundergammacorrectioncan\nbe designed from \ufb01rst and second order derivatives. Ad-\nditionalinvarianceunderscalingrequiresthirdorderder iva-\ntives. Derivativesarebynaturetranslationallyinvarian t. Ro-\ntational invariance in 2-d is achieved by using rotationall y\nsymmetricoperators.\n2Martin [6] reports the settings of \u03b3= 0.45,0.50,0.60for the Kodak\nMegaplus XRC camera\n12 The Invariants\nThe key idea for the design of the proposed invariants is\nto form suitable ratios of the derivativesof the image func-\ntion such that the parameters describing the transformatio n\nof interest will cancel out. This idea has been used in [12]\ntoachieveinvarianceunderlinearbrightnesschanges,and it\ncan be adjusted to the context of gamma correction by \u2013 at\nleast conceptually\u2013 consideringthe logarithm of the image\nfunction. Forsimplicity,webeginwith1-dimagefunctions .\n2.1 Invarianceunder GammaCorrection\nLetf(x)be the image function, i.e. the original signal, as-\nsumed to be continuous and differentiable, and f\u03b3(x) =\npf(x)\u03b3the correspondinggammacorrectedfunction. Note\nthatf(x)is a special case of f\u03b3(x)where\u03b3=p= 1. Tak-\ningthe logarithmyields\n\u02dcf\u03b3(x) = ln(pf(x)\u03b3) = lnp+\u03b3lnf(x)(2)\nwith the derivatives \u02dcf\u2032\n\u03b3(x) =\u03b3f\u2032(x)/f(x), and\u02dcf\u2032\u2032\n\u03b3(x) =\n\u03b3(f(x)f\u2032\u2032(x)\u2212f\u2032(x)2)/f(x)2. We cannowde\ufb01nethe in-\nvariant\u039812\u03b3undergammacorrectiontobe\n\u039812\u03b3(f(x))=\u02dcf\u2032\n\u03b3(x)\n\u02dcf\u2032\u2032\u03b3(x)\n=\u03b3f\u2032(x)\nf(x)\n\u03b3f(x)f\u2032\u2032(x)\u2212f\u2032(x)2\nf(x)2\n=f(x)f\u2032(x)\nf(x)f\u2032\u2032(x)\u2212f\u2032(x)2(3)\nThe factor phasbeeneliminatedby takingderivatives,and\n\u03b3hascanceledout. Furthermore, \u039812\u03b3turnsouttobecom-\npletelyspeci\ufb01edintermsofthe originalimagefunctionand\nits derivatives,i.e. the logarithmactually doesn\u2019thave t o be\ncomputed. The notation \u039812\u03b3(f(x))indicates that the in-\nvariantdependson the underlyingimage function f(x)and\nlocationx\u2013 the invariance holds under gamma correction,\nnotunderspatial changesoftheimagefunction.\nA shortcoming of \u039812\u03b3is that it is unde\ufb01ned where the\ndenominatoriszero. Therefore,we modify \u039812\u03b3to becon-\ntinuouseverywhere:\n0iff f\u2032= 0\u2227f f\u2032\u2032\u2212f\u20322= 0\n\u0398m12\u03b3=f f\u2032\nf f\u2032\u2032\u2212f\u20322if|f f\u2032|<|f f\u2032\u2032\u2212f\u20322|\nf f\u2032\u2032\u2212f\u20322\nf f\u2032else(4)\nwhere, for notational convenience, we have dropped the\nvariablex. The modi\ufb01cation entails \u22121\u2264\u0398m12\u03b3\u22641.\nNote that the modi\ufb01cation is just a heuristic to deal with\npoles. If all derivativesare zero because the image functio n\nis constant, then differentials are certainly not the best w ay\ntorepresentthefunction.2.2 InvarianceunderGammaCorrectionand\nScaling\nIfscaling isa transformationthat hasto be considered,the n\nanother parameter \u03b1describing the change of size has to\nbe introduced. That is, scaling is modeled here as variable\nsubstitution [11]: the scaled version of f(x)isg(\u03b1x) =\ng(u). So wearelookingatthe function\n\u02dcf\u03b3(x) = ln(pf(x)\u03b3) = lnp+\u03b3lng(\u03b1x) = \u02dcg\u03b3(u)\nwhere the derivatives with respect to xare\u02dcg\u2032\n\u03b3(u) =\n\u03b3\u03b1g\u2032(u)/g(u),\u02dcg\u2032\u2032\n\u03b3(u) =\u03b3\u03b12(g(u)g\u2032\u2032(u)\u2212g\u2032(u)2)/g(u)2,\nand\u02dcg\u2032\u2032\u2032\n\u03b3(u) =\u03b3\u03b13(g\u2032\u2032\u2032(u)/g(u)\u22123g\u2032(u)g\u2032\u2032(u)/g(u)2+\n2g\u2032(u)3/g(u)3). Nowtheinvariant \u0398123\u03b3(g(u))isobtained\nby de\ufb01ning a suitable ratio of the derivativessuch that both\n\u03b3and\u03b1cancelout:\n\u0398123\u03b3(g(u))=\u02dcg\u2032\n\u03b3(u)\u02dcg\u2032\u2032\u2032\n\u03b3(u)\n\u02dcg\u2032\u2032\u03b3(u)2\n=g2g\u2032g\u2032\u2032\u2032\u22123gg\u20322g\u2032\u2032+2g\u20324\ng2g\u2032\u20322\u22122gg\u20322g\u2032\u2032+g\u20324(5)\nAnalogouslytoeq.(4),we cande\ufb01neamodi\ufb01edinvariant\n0 if cond1\n\u0398m123\u03b3=g2g\u2032g\u2032\u2032\u2032\u22123gg\u20322g\u2032\u2032+2g\u20324\ng2g\u2032\u20322\u22122gg\u20322g\u2032\u2032+g\u20324if cond2\ng2g\u2032\u20322\u22122gg\u20322g\u2032\u2032+g\u20324\ng2g\u2032g\u2032\u2032\u2032\u22123gg\u20322g\u2032\u2032+2g\u20324else(6)\nwhere condition cond1 is g2g\u2032g\u2032\u2032\u2032\u22123gg\u20322g\u2032\u2032+2g\u20324= 0\n\u2227g2g\u2032\u20322\u22122gg\u20322g\u2032\u2032+g\u20324= 0, and condition cond2 is\n|g2g\u2032g\u2032\u2032\u2032\u22123gg\u20322g\u2032\u2032+2g\u20324|<|g2g\u2032\u20322\u22122gg\u20322g\u2032\u2032+g\u20324|.\nAgain,thismodi\ufb01cationentails \u22121\u2264\u0398m123\u03b3\u22641.\n2.3 AnAnalytical Example\nIt is a straightforward albeit cumbersome exercise to verif y\nthe invariants from eqs. (3) and (5) with an analytical, dif-\nferentiablefunction. Asanarbitraryexample,we choose\nf(x) = 3xsin(2\u03c0x)+30\nThe \ufb01rst three derivatives are f\u2032(x) = 3sin(2 \u03c0x) +\n6\u03c0xcos(2\u03c0x),f\u2032\u2032(x) = 12\u03c0cos(2\u03c0x)\u221212\u03c02xsin(2\u03c0x),\nandf\u2032\u2032\u2032(x) =\u221236\u03c02sin(2\u03c0x)\u221224\u03c03xcos(2\u03c0x). Then,\naccording to eq. (3), \u039812\u03b3(f(x)) = (3 xsin(2\u03c0x) +\n30)(3xsin(2\u03c0x) + 6\u03c0xcos(2\u03c0x))/((3xsin(2\u03c0x) +\n30)(12\u03c0cos(2\u03c0x)\u221212\u03c02xsin(2\u03c0x))\u2212(3sin(2\u03c0x) +\n6\u03c0xcos(2\u03c0x))2).\nIfwenowreplace f(x)withagammacorrectedversion,\nsayf0.45(x) = 2551\u22120.45\u00b73xsin(2\u03c0x) + 30)0.45,\nthe \ufb01rst derivative becomes f\u2032\n0.45(x) = 2550.55\u00b7\n0.45(3sin(2 \u03c0x) + 30)\u22120.55(3sin(2\u03c0x) + 6\u03c0xcos(2\u03c0x)),\n2the second derivative is f\u2032\u2032\n0.45(x) = \u22122550.55\u00b7\n0.45\u00b70.55(3sin(2 \u03c0x) + 30)\u22121.55(3sin(2\u03c0x) +\n6\u03c0xcos(2\u03c0x))2+ 2550.55\u00b70.45(3xsin(2\u03c0x) +\n30)\u22120.55(12\u03c0cos(2\u03c0x)\u221212\u03c02xsin(2\u03c0x)), and the third\nisf\u2032\u2032\u2032\n0.45(x) = 2550.55\u00b70.45(3sin(2 \u03c0x) + 30)\u22120.55(1.55\u00b7\n0.55(3sin(2 \u03c0x) +30)\u22122(3sin(2\u03c0x) +6\u03c0xcos(2\u03c0x))3+\n(\u22123)\u00b70.55(3sin(2 \u03c0x) + 30)\u22121(3sin(2\u03c0x) +\n6\u03c0xcos(2\u03c0x))(12\u03c0cos(2\u03c0x)\u221212\u03c02xsin(2\u03c0x)) +\n(\u221236\u03c02sin(2\u03c0x)\u221224\u03c03xcos(2\u03c0x))). If we plug\nthese derivatives into eq. (3), we obtain an expression for\n\u039812\u03b3(f0.45(x))whichisidenticaltotheonefor \u039812\u03b3(f(x))\nabove. The algebraically inclined reader is encouraged to\nverifythe invariant \u0398123\u03b3forthesamefunction.\n56789050100\n56789050100\nFigure 2: An analytical example function. (left) f(x) =\n3xsin(2\u03c0x) + 30; (right) f\u03b3(x) =pf(x)\u03b3,\u03b3= 0.45.\n(\ufb01rst row) original functions (second row) \ufb01rst derivative s;\n(third row) second derivatives; (fourth row) third deriva-\ntives; (\ufb01fthrow) \u0398m12\u03b3; (sixthrow) \u0398m123\u03b3.\nFig. 2 shows the example function and its gamma cor-\nrected counterpart, together with their derivatives and th e\ntwo modi\ufb01ed invariants. As expected, the graphs of the in-\nvariants are the same on the right as on the left. Note thatthe invariants de\ufb01ne a many-to-one mapping. That is, the\nmappingisnotinformationpreserving,anditisnotpossibl e\ntoreconstructtheoriginalimagefromitsinvariantrepres en-\ntation.\n2.4 Extension to 2-d\nIf\u0398m12\u03b3or\u0398m123\u03b3are to be computed on images, then\neqs. (3) to (6) have to be generalized to two dimensions.\nThis is to be done in a rotationally invariant way in order\nto achieve invarianceunder similarity transformations. T he\nstandardwayistouserotationallysymmetricoperators. Fo r\nthe\ufb01rstderivative,wehavethewellknown gradientmagni-\ntude,de\ufb01nedas\n\u2207(x,y) =/radicalBig\nI2x+I2y:=I\u2032(7)\nwhereI(x,y)is the 2-d image function, and Ix,Iyare par-\ntial derivativesalong the x-axis and the y-axis. For the sec -\nondorderderivative,we canusethelinear Laplacian\n\u22072(x,y) =Ixx+Iyy:=I\u2032\u2032(8)\nHorn[5]alsopresentsanalternativesecondorderderivati ve\noperator,the quadraticvariation3\nQV(x,y) =/radicalBig\nI2xx+2I2xy+I2yy (9)\nSince theQV is nota linearoperatorandmoreexpensiveto\ncompute,we use theLaplacianforourimplementation. For\nthe third order derivative, we can de\ufb01ne, in close analogy\nwiththe quadraticvariation,a cubicvariation as\nCV(x,y) =/radicalBig\nI2xxx+3I2xxy+3I2xyy+I2yyy:=I\u2032\u2032\u2032\n(10)\nThe invariantsfromeqs. (3) to (6) remain valid in 2-d if\nwereplace f\u2032withI\u2032,f\u2032\u2032withI\u2032\u2032,andf\u2032\u2032\u2032withI\u2032\u2032\u2032. Thiscan\nbe veri\ufb01ed by going through the same argument as for the\n1-d functions. Recall that the critical observation in eq. ( 3)\nwasthat\u03b3cancelsout,whichisthecasewhenallderivatives\nreturnafactor \u03b3. Butsuchisalsothecasewiththerotation-\nally symmetric operatorsmentionedabove. For example, if\nwe apply the gradient magnitude operator to \u02dcI(x,y), i.e. to\nthe logarithmof a gammacorrectedimagefunction,we ob-\ntain\n\u2207=/radicalBig\n\u02dcI2x+\u02dcI2y=/radicalbigg\n(\u03b3Ix\nI)2+(\u03b3Iy\nI)2=\u03b3/radicalBig\nI2x+I2y\nI\nreturning a factor \u03b3, and analogously for \u22072, QV, and CV.\nAsimilarargumentholdsforeq.(5)wherewehavetoshow,\nin addition, that the \ufb01rst derivative returns a factor \u03b1, the\nsecondderivativereturnsafactor \u03b12,andthethirdderivative\nreturnsa factor \u03b13, whichisthe caseforour2-doperators.\n3Actually, unlike Horn, wehave taken the square root.\n32.5 Differential Operators\nWhile the derivatives of continuous, differentiable func-\ntions are uniquely de\ufb01ned, there are many ways to imple-\nment derivatives for sampledfunctions. We follow Schmid\nand Mohr [11], ter Haar Romeny [13], and many other re-\nsearchersinemployingthederivativesoftheGaussianfunc -\ntionas\ufb01lterstocomputethederivativesofasampledimage\nfunction via convolution. This way, derivation is combined\nwithsmoothing. The2-dzeromeanGaussianisde\ufb01nedas\nG=1\n2\u03c0\u03c32e\u2212x2+y2\n2\u03c32 (11)\nThepartialderivativesuptothirdorderare Gx=\u2212x/\u03c32G,\nGy=\u2212y/\u03c32G,Gxx= (x2\u2212\u03c32)/\u03c34G,Gxy=xy/\u03c34G,\nGyy= (y2\u2212\u03c32)/\u03c34G,Gxxx= (3\u03c32x\u2212x3)/\u03c36G,\nGxxy= (\u03c32y\u2212x2y)/\u03c36G,Gxyy= (\u03c32x\u2212xy2)/\u03c36G,\nGyyy= (3\u03c32y\u2212y3)/\u03c36G. They are shown in \ufb01g. 3. We\nused the parameter setting \u03c3= 1.0and kernel size 7\u00d77.\nWiththesekernels,eq.(3),forexample,isimplementedas\n\u039812\u03b3=I/radicalbig\n(I\u229bGx)2+(I\u229bGy)2\nI(I\u229bGxx+I\u229bGyy)\u2212((I\u229bGx)2+(I\u229bGy)2)\nat eachpixel (x,y), where\u229bdenotesconvolution.\nFigure3: Partialderivativesofthe2-dGaussian. (\ufb01rstrow )\nGx,Gy,Gxx; (second row) Gxy,Gyy,Gxxx; (third row)\nGxxy,Gxyy,Gyyy.3 Experimental Dataand Results\nWe evaluate the invariant \u0398m12\u03b3from eq. (4) in two differ-\nent ways. First, we measure how much the invariant com-\nputed on an image without gamma correction is different\nfrom the invariant computed on the same image but with\ngamma correction. Theoretical, this difference should be\nzero, but in practice, it is not. Second, we compare tem-\nplate matching accuracyon intensity images, again without\nandwithgammacorrection,totheaccuracyachievableifin-\nstead the invariant representation is used. We also examine\nwhethertheresultscanbeimprovedbypre\ufb01ltering.\n3.1 Absoluteand RelativeErrors\nA straightforwarderrormeasureisthe absoluteerror ,\n\u2206GC(i,j) =|\u0398GC(i,j)\u2212\u03980GC(i,j)|(12)\nwhere\u201d0GC\u201dreferstotheimagewithoutgammacorrection,\nand GC stands for either \u201dSGC\u201d if the gamma correction is\ndone synthetically via eq. (1), or for \u201dCGC\u201d if the gamma\ncorrection is done via the camera hardware. Like the in-\nvariant itself, the absolute error is computed at each pixel\nlocation(i,j)oftheimage,exceptfortheimageboundaries\nwhere the derivativesand thereforethe invariantscannot b e\ncomputedreliably.\nFigure 4: Exampleimage WoBA: (a) no gammacorrection,\n\u201c0GC\u201d; (b)gammacorrectionbycamera,\u201cCGC\u201d; (c)syn-\ntheticgammacorrection,\u201cSGC\u201d.\nFig. 4 shows an example image. The SGC image has\nbeen computed from the 0GC image, with \u03b3= 0.6. Note\nthat the gamma correction is done afterthe quantization of\nthe 0GC image, since we don\u2019t have access to the 0GC im-\nagebeforequantization.\nFig. 5 shows the invariant representations of the image\ndatafrom\ufb01g.4andthecorrespondingabsoluteerrors. Since\n\u22121\u2264\u0398m12\u03b3\u22641, wehave 0\u2264\u2206GC\u22642. Thedarkpoints\nin \ufb01g. 5, (c) and (e), indicate areas of large errors. We ob-\nservetwoerrorsources:\n\u2022Theinvariantcannotbecomputedrobustlyinhomoge-\nneous regions. This is hardly surprising, given that it\nis based on differentials which are by de\ufb01nition only\nsensitivetospatial changesofthesignal.\n4050100150200250\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(a)\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.8\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(b)\n00.20.40.60.811.21.41.61.8\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(c)\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.8\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(d)\n00.20.40.60.811.21.41.61.8\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(e)\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.8\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(f)\nFigure 5: Absolute errors for invariant \u0398m12\u03b3, no pre-\n\ufb01ltering. (a) image WoBA, 0GC; (b) \u03980GC; (c)\u2206CGC;\n(d)\u0398CGC; (e)\u2206SGC; (f)\u0398SGC.\n\u2022There are outliers even in the SGC invariant represen-\ntation,atpointsofveryhighcontrastedges. Theyarea\nbyproduct of the inherent smoothing when the deriva-\ntives are computed with differentials of the Gaussian.\nNote that the latter put a ceiling on the maximum gra-\ndientmagnitudethatiscomputableon8-bitimages.\nIn addition to computingthe absolute error,we can also\ncomputethe relativeerror,in percent,as\n\u03b4CGC(i,j) = 100\u2206 CGC(i,j)/\u03980GC(i,j)\n(13)\nThen we can de\ufb01ne the set RP\u01ebofreliable points , relative\ntosomeerrorthreshold \u01eb,as\nRP\u01eb={(i,j)|\u03b4(i,j)\u2264\u01eb} (14)\nandPRP\u01eb, thepercentageofreliablepoints,as\nPRP\u01eb= 100|RP\u01eb|/n (15)\n(a)\n (b)\n (c)\n(d)\n (e)\n (f)\nFigure6: Reliablepoints RP\u01ebforinvariant \u0398m12\u03b3,inblack,\nimageWoBA,withoutand withpre\ufb01ltering. (a) \u03c3pre=0,\u01eb=\n5.0; (b)\u03c3pre=0,\u01eb=10.0; (c) \u03c3pre=0,\u01eb=20.0; (d) \u03c3pre=1.0,\n\u01eb=5.0; (e) \u03c3pre=1.0,\u01eb=10.0; (f) \u03c3pre=1.0,\u01eb=20.0.\nwherenis the numberof valid, i.e. non-boundary,pixels in\nthe image. Fig. 6 shows, in the \ufb01rst row, the reliable points\nforthreedifferentvaluesofthethreshold \u01eb. Thesecondrow\nshowsthesetsofreliablepointsforthesamethresholdsifw e\ngently pre\ufb01lterthe 0GC and CGC images. The correspond-\ning data for the ten test images from \ufb01g. 11 is summarized\nintable1.\nimage 5.010.020.05.010.020.0\nBuild 13.324.943.816.029.549.3\nWoBA 15.629.048.219.035.758.9\nWoBB 16.528.747.121.437.758.1\nWoBC 18.533.653.524.041.465.3\nWoBD 13.023.941.916.732.655.6\nCycl 15.428.345.922.638.857.6\nSand 14.527.244.722.038.557.6\nToolA 5.610.720.17.414.727.1\nToolB 6.112.022.78.315.728.6\nToolC 5.611.120.87.915.128.3\nmedian 13.926.144.317.934.256.6\nmean 12.422.938.916.530.048.6\nTable1: Percentagesofreliablepointsfor \u0398m12\u03b3,CGCim-\nages, for \u01eb=5.0, 10.0, 20.0. The three numerical columns\non the left show PRP\u01ebwithout pre\ufb01ltering, the three right\ncolumnswithGaussian pre\ufb01ltering, \u03c3pre=1.0.\nDerivativesareknowntobesensitivetonoise. Noisecan\nbereducedbysmoothingtheoriginaldatabeforetheinvari-\nants are computed. On the other hand, derivatives should\nbe computed as locally as possible. With these con\ufb02icting\n5goalsto be considered,we experimentwith gentle pre\ufb01lter-\ning,usingaGaussian\ufb01lterofsize \u03c3pre=1.0. Thesize ofthe\nGaussiantocomputetheinvariant \u0398m12\u03b3issetto\u03c3der=1.0.\nNote that \u03c3preand\u03c3dercannotbe combined into just one\nGaussianbecauseofthenon-linearityoftheinvariant.\nWithrespecttothesetofreliablepoints,weobservethat\nafter pre\ufb01ltering, roughly half the points, on average, hav e\na relative errorof less than 20%. Gentle pre\ufb01lteringconsis -\ntently reduces both absolute and relative errors, but stron g\npre\ufb01lteringdoesnot.\n3.2 Template Matching\nTemplate matching is a frequently employed technique in\ncomputervision. Here,wewillexaminehowgammacorrec-\ntion affects the spatial accuracy of template matching, and\nwhether that accuracy can be improvedby using the invari-\nant\u0398m12\u03b3. An overview of the testbed scenario is given in\n\u0398\u03b3 \u0398\u03b3template\ntemplatesearch\n(correlation)search\n(correlation)0GC intensity CGC intensity\n0GC invariant CGC invariant\nFigure7: The template locationproblem: A querytemplate\niscut out fromthe 0GCintensity imageandcorrelatedwith\nthe correspondingCGC intensity image. We test if the cor-\nrelation maximum occursat exactly the same location as in\nthe0GCintensityimage. Thesameprocessisrepeatedwith\ntheinvariantrepresentationsofthe0GCandCGCimages.\n\ufb01g.7. Asmalltemplateofsize 6\u00d78,representingthesearch\npattern, is taken from a 0GC intensity image, i.e. without\ngamma correction. This query template is then correlated\nwith the corresponding CGC intensity image, i.e. the same\nscene but with gamma correction switched on. If the cor-\nrelation maximum occurs at exactly the location where the\n0GC querytemplate has been cut out, we call this a correct\nmaximumcorrelationposition ,orCMCP.\nThe correlationfunction s(x,y)employedhere is based\nona normalizedmeansquareddifference c(x,y)[3]:\ns= max(0 ,1\u2212c)\nc=/summationtext\ni,j((I(x+i,y+j)\u2212I)\u2212(T(i,j)\u2212T))2\n/radicalBig/summationtext\ni,j(I(x+i,y+j)\u2212I)2/summationtext\ni,j(T(i,j)\u2212T)2whereIis an image, Tis a template positioned at (x,y),\nI(x,y)isthemeanofthesubimageof Iat(x,y)ofthesame\nsizeasT,Tisthemeanofthetemplate,and 0\u2264s\u22641. The\ntemplatelocationproblemthenistoperformthiscorrelati on\nfor the whole image and to determine whether the position\nofthecorrelationmaximumoccurspreciselyat (x,y).\n050100150200250\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.81\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\nFigure 8: Matched templates, image WoBA: (left) intensity\ndata; (right)invariantrepresentation. Blackbox=queryt em-\nplate,whitebox=matchedtemplate.\n00.10.20.30.40.50.60.70.8\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n900.10.20.30.40.50.60.7\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\nFigure9: Correlationmatrices,image WoBA. (left)intensity\ndata; (right)invariantrepresentation.\nFig. 8 demonstrates the template location problem, on\nthe left for an intensity image, and on the right for its in-\nvariant representation. The black box marks the position of\nthe original template at (40,15), and the white box marks\nthe position of the matched template, which is incorrectly\nlocated at (50,64) in the intensity image. On the right, the\nmatched template (white) has overwritten the original tem-\nplate(black)atthesame,correctlyidenti\ufb01edposition. Fi g.9\nvisualizes the correlation function over the whole image.\nThewhiteareasareregionsofhighcorrelation.\nTheexamplefrom\ufb01gs.8and9dealswithonly onearbi-\ntrarily selected template. In order to systematically anal yze\nthetemplatelocationproblem,werepeatthecorrelationpr o-\ncess forall possible template locations. Thenwe can de\ufb01ne\nthecorrelation accuracy CA as the percentage of correctly\nlocatedtemplates,\nCAtn\u00d7tm= 100|CMCP tn\u00d7tm|/n (16)\nwheretn\u00d7tmis the size of the template, CMCP tn\u00d7tm\nis the set of correct maximum correlation positions, and n,\n620 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(a)\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(b)\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(c)\n20 40 60 80 100 12010\n20\n30\n40\n50\n60\n70\n80\n90\n(d)\nFigure 10: Binary correlation accuracy matrices, white\npixels=CMCP 6\u00d78, imageWoBA. (a) intensity image,\n\u03c3pre=0; (b) intensity image, \u03c3pre=1.0; (c) invariant rep-\nresentation, \u03c3pre=0; (d)invariantrepresentation, \u03c3pre=1.0.\nagain, is the number of valid pixels. We compute the cor-\nrelation accuracy both for un\ufb01ltered images and for gently\npre\ufb01ltered images, with \u03c3pre= 1.0. Fig. 10 shows the bi-\nnary correlation accuracy matrices for our example image.\nThe CMCP set is shown in white, its complement and the\nboundariesin black. We observe a higher correlation accu-\nracy for the invariant representation, which is improved by\nthepre\ufb01ltering.\nFigure 11: Test images: (a) Build; (b)WoBA; (c)WoBB;\n(d)WoBC; (e)WoBD; (f)Cycl; (g)Sand; (h)ToolA;\n(i)ToolB;(j)ToolC.image Int/0Int/1.0 Inv/0Inv/1.0\nBuild 85.078.0 85.8 89.5\nWoBA 55.545.0 75.7 80.4\nWoBB 39.331.0 52.7 57.6\nWoBC 67.258.3 68.9 78.7\nWoBD 31.629.2 48.0 67.4\nCycl 60.545.4 98.6 99.4\nSand 50.540.9 85.2 94.4\nToolA 41.735.3 60.2 68.0\nToolB 29.523.4 45.7 54.1\nToolC 42.127.8 42.5 48.4\nmedian 46.338.1 64.6 73.4\nmean 50.341.4 66.3 73.8\nTable2: CorrelationaccuraciesCA,templatesize 6\u00d78,left\ncolumns for intensity data, right columns for the invariant\nrepresentation,withoutandwithpre\ufb01ltering.\n123456789100102030405060708090100\n123456789100102030405060708090100\nFigure12: Correlationaccuracies, (top)templatesize 6\u00d78;\n(bottom) template size 10\u00d710. The entries at x=1 refer to\nBuild, at x=2 toWoBA, etc. (circles, lower line) intensity\nimages; (stars,centerline)invariantrepresentation, \u03c3pre=0;\n(diamonds, upper line) invariant representation, \u03c3pre=1.0.\nThe markers on the left hand side indicate the means, the\nmarkersontherighthandside themedians.\n7We have computed the correlation accuracy for all the\nimagesgivenin\ufb01g.11. Theresultsareshownintable2and\nvisualizedin\ufb01g. 12. We observethefollowing:\n\u2022The correlationaccuracyCA is higheron the invariant\nrepresentationthanontheintensityimages.\n\u2022Thecorrelationaccuracyishigherontheinvariantrep-\nresentation with gentle pre\ufb01ltering, \u03c3pre= 1.0, than\nwithout pre\ufb01ltering. We also observed a decrease in\ncorrelationaccuracyifweincreasethepre\ufb01lteringwell\nbeyond\u03c3pre= 1.0. By contrast, pre\ufb01ltering seems to\nbealwaysdetrimentalto theintensityimagesCA.\n\u2022The correlation accuracy shows a wide variation,\nroughlyin the range 30% ...90% for the un\ufb01ltered in-\ntensity imagesand 50% ...100%for pre\ufb01lteredinvari-\nant representations. Similarly, the gain in correlation\naccuracyrangesfromclose to zeroup to 45%. Forour\ntestimages,itturnsoutthattheinvariantrepresentation\nis always superior, but that doesn\u2019t necessarily have to\nbethecase.\n\u2022ThemediansandmeansoftheCAsoveralltestimages\ncon\ufb01rm the gain in correlation accuracy for the invari-\nantrepresentation.\n\u2022The larger the template size, the higher the correlation\naccuracy, independent of the representation. A larger\ntemplatesizemeansmorestructure,andmorediscrim-\ninatorypower.\n4 Conclusion\nWe have proposednovel invariants that combine invariance\nunder gamma correction with invariance under geometric\ntransformations. In a general sense, the invariants can be\nseen as trading off derivatives for a power law parameter,\nwhich makes them interesting for applications beyond im-\nage processing. The error analysis of our implementation\non real imageshas shown that, for sampled data, the invari-\nants cannot be computed robustly everywhere. Neverthe-\nless,thetemplatematchingapplicationscenariohasdemon -\nstrated that a performance gain is achievable by using the\nproposedinvariant.\n5 Acknowledgements\nBob Woodham suggested to the author to look into invari-\nanceundergammacorrection. Hismeticulouscommentson\nthisworkweremuchappreciated. JochenLanghelpedwith\ntheacquisitionofimagedatathroughtheACMEfacility[8]."}
{"category": "abstract", "text": "Thispaperpresentsinvariantsundergammacorrectionand\nsimilaritytransformations. Theinvariantsarelocalfeat ures\nbased on differentials which are implemented using deriva-\ntivesoftheGaussian. Theuseoftheproposedinvariantrep-\nresentationisshowntoyieldimprovedcorrelationresults in\natemplatematchingscenario.\n1 Introduction\nInvariants are a popular concept in object recognition and\nimage retrieval [1, 2, 7, 10, 14, 15]. They aim to provide\ndescriptionsthatremainconstantundercertaingeometric or\nradiometric transformations of the scene, thereby reducin g\nthe search space. They can be classi\ufb01ed into global invari-\nants, typically based either on a set of key points or on mo-\nments,andlocalinvariants,typicallybasedonderivative sof\nthe image function which is assumed to be continuous and\ndifferentiable.\nThe geometric transformations of interest often include\ntranslation, rotation, and scaling, summarily referred to as\nsimilarity transformations. In a previous paper [12], build-\ning on work done by Schmid and Mohr [11], we have pro-\nposeddifferentialinvariantsforthosesimilaritytransf orma-\ntions,plus linearbrightnesschange. Here,wearelookingat\nanon-linear brightnesschangeknownas gammacorrection .\nGamma correction is a non-linear quantization of the\nbrightness measurements performed by many cameras dur-\ningtheimageformationprocess.1Theideaistoachievebet-\nterperceptual resultsbymaintaininganapproximatelycon-\nstant ratio between adjacent brightness levels, placing th e\nquantization levels apart by the just noticeable difference .\nIncidentally, this non-linear quantization also precompe n-\nsates for the non-linearmappingfrom voltage to brightness\ninelectronicdisplaydevices[4, 9].\n1Historically,theparametergammawasintroduced todescri bethenon-\nlinearity of photographic \ufb01lm. Today, its main useis to impr ove the output\nof cathode ray tube based monitors, but the gamma correction indisplay\ndevices is of no concern to ushere.Gammacorrectioncanbeexpressedbytheequation\nI\u03b3=pI\u03b3(1)\nwhereIis the inputintensity, I\u03b3is the outputintensity,and\npisanormalizationfactorwhichisdeterminedbythevalue\nof\u03b3. For output devices, the NTSC standard speci\ufb01es \u03b3=\n2.22. Forinput deviceslike cameras, the parametervalueis\njust inversed, resulting in a typical value of \u03b3= 1/2.22 =\n0.45. The camera we used, the Sony 3 CCD color camera\nDXC 950, exhibited \u03b3\u22480.6.2Fig. 1 shows the intensity\nmappingof8-bitdatafordifferentvaluesof \u03b3.\n0 50 100 150 200 250050100150200250\nFigure 1"}
{"category": "non-abstract", "text": "Block Thresholding; Boundary Mismatch; Image Continuity; Image\nVariance\n1 Introduction\nApplications like document image analysis [Dawoud and Kamel (2001)], q ual-\nity inspection of materials, non-destructive testing [Sezgen and Sa nkur (2001)]\netc., require the concerned images to be thresholded. Numerous m ethods to\nperform image thresholding exist in the literature [Trier and Jain (199 5),\nSezgen and Sakur (2004), Sahoo and Soltani (1998), Huang et al. (2005)].\nThresholding algorithms can be classi\ufb01ed into the following main catego ries:\nHistogram shape [Rosennfeld and Torre (1983), Sezan (1985), Ra mesh et al.\n1E-mail: prasanta@prl.res.in\nPreprint submitted to Elsevier Science 6 November 2018(1995),Wangetal.(2002)],wheretheaimofthealgorithmisto\ufb01nda noptimal\nthreshold that separates two major peaks in the histogram, is imple mented\nby sending a smoothing \ufb01lter on the histogram and then a di\ufb00erence \ufb01 lter\nor by \ufb01tting the histogram with two Gaussians. But the main disadvan tage\nof histogram-based methods is their disregard of spatial informat ion. Image\nentropy based methods [Pun (1980), Kapur et al.(1985), Li and Le e(1993),Li\nand Tam (1998)] use the entropy of the image as a constraint for t hreshold\nselection. The two common ways in which this can be done is by the maxi-\nmization of entropy of the thresholded image or minimization of cross entropy\nbetween input image and the output binary image.\nGeneral image attributes [Tsai (1985), Hertz and Schafer (1998 ), Gorman\n(1994), Arora et al. (2005)] can also be e\ufb00ectively used, where th e threshold is\nselected based on some similarity measure between the original image and the\nbinarized version of the image. These can take the form of edges, s hapes, color\nor other suitable attributes like compactness or connectivity of th e objects,\nresulting from the binarization process or the coincidence of edge \ufb01 elds. But\nthe disadvantage of the above method lies in its complexity and the re latively\nlow image quality.\nClustering of gray level [Ridler and Calward (1978), Leung and Lam (1 996),\nKittler and Illingworth (1986), Pal and Pal (1989)], based methods aim to\n\ufb01nd two clusters in pixel distribution, a foreground cluster and a ba ckground\ncluster. Various algorithms exist for \ufb01nding these clusters. Spatia l informa-\ntion [Abutaleb (1989)] utilize information of objects and backgroun d pixels in\nthe form of context probabilities, correlation functions, co-occu rrence prob-\nabilities, local linear dependence models of pixels, two dimensional ent ropy\netc.\nLocallyadaptivethresholdingbasedmethods[WhiteandRohrer(198 3),Niblack\n(1986), Lindquist (1999), Trier and Taxt (1995)], are character ized by calcula-\ntion of threshold at every pixel. The value of the threshold depends upon some\nlocal parameters like mean, variance, and surface \ufb01tting paramet ers or their\nsuitable combinations. In one approach, the gray value of the pixel is com-\npared with the average of the gray values in some neighborhood; if t he pixel is\nsigni\ufb01cantly larger than the average, it is assigned as foreground, otherwise it\nis classi\ufb01ed as background. Another common method adapts the th resholding\naccording to local mean ( \u00b5) and standard deviation ( \u03c3) over the window size.\nThe threshold at every pixel ( i,j) is calculated as T(i,j) =\u00b5(i,j)+k.s(i,j),\nfor a suitable value of k. Niblack\u2019s method for thresholding is a well-kno wn\nexample of this class. The calculation of threshold at every pixel mak es this\ntechnique relatively time consuming.\nIn approaches based on global thresholds, which are faster as co mpared to\ntheir local counterparts, one calculates a single threshold value fo r the entire\n2image. A common example of this class is Otsu\u2019s (1979) method of thre shold-\ning; this is an iterative approach, which assumes that the gray level histogram\nis the sum of two normal intensity distributions. Since the threshold ing is done\nonce forthe whole image, onemay lose certainlocal characteristics . Hence, the\nthresholding of images based on local attributes have proved to be generally\nsuperior to the global thresholding methods in terms of \ufb01nal image q uality. A\nnumber of the above thresholding methods su\ufb00er from the problem of image\ncontinuity, which cannot be tolerated in applications pertaining to me dical\nimaging, remote sensing, optical character recognition etc., wher e image con-\ntinuity plays a crucial role.\nThe usual method of calculation of local threshold for every pixel, w ith the\nhelp of information present in a window de\ufb01ned around it, is computat ionally\nintensive. In this paper, we present a hybrid method, where the th reshold is\ncalculated only once in a window. This locally adaptive block thresholding\n(LABT) algorithm makes use of the threshold values of the neighbor ing sub-\nimages to calculate a range. Image continuity is obtained by choosing the\nthreshold value of the sub-image under consideration to lie within the range\nof values speci\ufb01ed by the algorithm.\nThe above algorithm is applied to a wide variety of images and it is observ ed\nthat the local details are preserved to a great extent. In addition to that, the\nalgorithm fared better in terms of time-complexity as compared to t he other\nthresholding techniques.\nBefore proceeding to the details of the technique, it is convenient t o de\ufb01ne the\nfollowing notations. In this text, Sm,ndenotes a sub-image, where mandn\ndenote the position of the sub-image in the matrix of sub-images. Th reshold\nchosen for the sub-image Sm,non application of an appropriate thresholding\ntechnique, is denoted by OTm,n(Original Threshold). Threshold value of sub-\nimageSm,nafter application of the present algorithm, is denoted by Tm,n. The\nrange of threshold values that Sm,ncan take, without violating its continuity\nwith the upper (left) sub-image, is denoted by URm,n(LRm,n).\n2 Procedure For Local Block Thresholding With Continuity Co n-\nstraint\nThegiven imageneeds tobedivided into anumber ofsub-imagesofsize m\u00d7n,\nwhere the values of mandncan be chosen on the basis of standard image\nattributes. In this paper image variance has been chosen as that a ttribute.\nThe image having larger variance is divided into more number of sub-ima ges\nin order to bring out \ufb01ner details, whereas an image with a lower varian ce is\ndivided into low number of sub-images in order to be computationally ine x-\n3pensive. The reason to divide a given image into sub-images based on s ome\nattribute (image variance in this case), is to balance between image q uality\nand time complexity by choosing the right sub-image size depending up on\nthe application under consideration and level of \ufb01ner details to be ex tracted\nfrom the image. In a number of cases e.g., optical character recog nition, the\nsub-image size is dictated by the image under consideration. The sub -image\nsize can also be left as a variable, to be determined by the desired amo unt of\nimage details. This may have usefulness to medical imaging. The numbe r of\nrows and columns of the image are then converted to multiples of mandn\nrespectively.\nOnce the division of image has been done, the sub-images are scanne d from\ntop-left to bottom-right. Constraint is then imposed on the thres hold selection\nof a sub-image by thresholds of upper and left sub-images, i.e., cont inuity is\nsought between Sm,nandSm\u22121,n, andSm,nandSm,n\u22121.\nAny threshold determination technique can be used to binarize the s ub-image\nSm,n, starting from S1,1. The thresholds Tm\u22121,nandTm,n\u22121of the neighboring\nsub-images are used to impose constraint of continuity on the thre sholdTm,n\nof the sub-image Sm,n. The choice of threshold Tm,nofSm,nis constrained\nto a range Rm,n. This range is determined using the threshold values of the\nneighboring sub-images and the bordering pixel values of the sub-im age under\nconsideration.Anyvalueintherange Rm,nwhenusedtothresholdthecolumns\n(rows) of Sm,n, that borders the adjacent sub-image Sm\u22121,n(Sm,n\u22121), classify\nthem into foreground or background, in the same way Tm\u22121,n(Tm,n\u22121) clas-\nsi\ufb01es the pixels of those borders. Stating in symbolic terms, if \u03c4Tm,ndenotes\nthresholding operation,i.e., classifying every pixel ofa givenarray/ matrixinto\nforeground or background, using Tm,n, the constraint is then stated as:\n\u03c4Tm,n(Sm,n(outer lining )) =\u03c4Tm\u22121,n(Sm,n(outer lining )) (1)\nif the outer lining is a row of pixels and,\n\u03c4Tm,n(Sm,n(outer lining )) =\u03c4Tm,n\u22121(Sm,n(outer lining )) (2)\nif the outer lining is a column of pixels\nThe range of threshold values Rm,n, that a sub-image can take while main-\ntaining image continuity with the upper and side block, is determined in t he\nfollowing manner. An array comprising of threshold Tm-1,n of the upp er sub-\nimage and pixel values of Rm,n\u2019s uppermost row (say, X), which borders the\nupper sub-image with maximum and minimum pixel values, is created. Pix el\nvalues in X, which are equal to Tm,n, are deleted before Xis added to the ar-\nray in order get values other than Tm\u22121,n. Appending minimum and maximum\n4pixel values ensures the presence of values, greater and less tha nTm\u22121,n. Let\nthe values that are immediately lower and greater than Tm\u22121,nin the array,\nbeRl1andRh1, respectively. Then the range dictated by the upper sub-image\nis\nURm,n= [Rl1+1,Rh1].\nThe classi\ufb01cation of foreground and background pixels is done, ass uming the\nde\ufb01nition of thresholding as X <threshold = > X= background, and X\u2265\nthreshold = > X= foreground.\nThe same procedure is applied to determine the range LRm,ndictated by the\nleft sub-image. Here, Xis the column of Sm,nwhich borders the left sub-image\nand the threshold to be added to the array is Tm,n\u22121.The e\ufb00ective range,\nwithin which the threshold of Sm,n, has to be selected to avoid discontinuity,\nis\nRm,n=URm,n\u2229LRm,n.\n3 Algorithm\nThe salient features of the proposed LABT algorithm can be stated in the\nfollowing manner:\n1) The image is divided into number of same-sized rectangular sub-ima ges\nbased on the variance of the whole image. Other attributes of the im age can\nalso be used for this purpose.\n2)Imageisthenmadeintoamultipleofthesub-image,byasuitableope ration.\n3) Starting from S1,1, operations are performed on the sub-images row-wise,\ni.e., the image is scanned from top-left to bottom right.\n4) An original threshold OTm,nofSm,n, is determined, using a suitable thresh-\nolding technique.\n5) The range Rm,nfor the sub-window under consideration is worked out,\nusingTm\u22121,nandTm,n\u22121with the help of above method.\n6) ForS1,1, the threshold T1,1=OT1,1. For the sub-images in the topmost\nrow (leftmost column), continuity is maintained only with the left (upp er)\nsub-images.\n5(a) (b) (c)\nFig. 1. Plots showing dependence of three important charact eristics on sub image\nsize (averaged over 35 images)\n7) In case OTm,nfalls out of Rm,n, it is brought to the nearest extreme of Rm,n,\nusing the above speci\ufb01ed procedure, and denoted by Tm,n.\nThe above algorithm thus ensures that continuity is maintained acro ss sub-\nimages. Sub-image size can be changed depending upon the purpose , i.e., a\nsmaller sub-image size can be taken to bring out \ufb01ner details, whenev er it is\nnecessary. Bigger size sub-images are advisable for document imag e thresh-\nolding, where fuzzy outlines of letters need to be made well de\ufb01ned. A bigger\nsub-image size will help in keeping the threshold almost constant, acr oss let-\nters, thereby providing a consistent cut-o\ufb00 for removing fuzzine ss.\n4 Results and Observations\nWe observed reduction in the average size of the range Rm,nwith increasing\nsub-image size. This canbe seen fromFig. 1a. This reduction in size is b ecause\nof the availability of elements nearer to Tm,nin the bordering array X, when\nthe sub-image size gets bigger. Since the algorithm starts from S1,1and prop-\nagates downwards, it is preferable to binarize S1,1with threshold obtained by\napplying threshold over the whole image.\nThe second plot(Fig.1b) shows thevariationofthefractionoftime s threshold\nexceeds the range constraint; with sub-image size averaged over 35 images.\nThe fraction of times OTm,nfalling outside Rm,ndecreases with increase in\nsub-image size. This is due to stabilization of threshold across sub-im ages,\nwhen the sub-image size is increased. This is as expected, given the la rge-\nscale homogeneity present in numerous images.\nWe are presenting a few images for which the familiar method of thres hold-\ning, i.e., area division of cumulative distribution function (ADCDF), is us ed\n6(a) (b) (c)\n(d) (e) (f)\nFig. 2. Comparison between the results of various methods, ( a) Original image,\n(b) Thresholded using ADCDF, (c) Thresholded using Otsu, (d ) Thresholded using\nNiblack, (e) Thresholded using ADCDF with LABT, (f) Thresho lded using Otsu\nwith LABT.\n(a) (b)\nFig. 3. (a) Original text image, (b) Thresholded using LABT.\n7Table 1\nQuantitative comparision of various thresholding methods\nto binarize the sub-images. We have also used Otsu\u2019s algorithm for th e same\npurpose. Superior thresholding methods, when used in conjunctio n with this\nalgorithm, will give far better results. For the purpose of illustratin g the e\ufb03-\ncacy of our procedure and comparison, we have also presented th e binarized\nimages, using Otsu (global) and Niblack (local) thresholding methods in Fig.\n2.Oneclearlyseesthatthepresent locallyadaptiveblockthreshold ing method\nclearly does well in terms of extracting local features as well as ret aining the\nvisual image quality. We have checked this property of LABT in a varie ty of\nimages.\nFor text images, thresholding followed by morphological operation lik e thin-\nning gives good results (Fig. 3a & Fig 3b). It is advisable to choose the sub-\nimage size to be more than the average object size in the image. This e nsures\nthe whole object, to be uniformly classi\ufb01ed as background or foreg round, and\navoids classi\ufb01cation of within-object variation.\nThe computational time and PSNR for di\ufb00erent thresholding techniq ues, im-\nplemented with and without the locally adaptive block thresholding (LA BT),\nhasbeenshownintable1.Itisquiteobviousfromtheresultsthatth estandard\n8(a) (b)\n(c) (d)\nFig. 4. (a) Original image, (b) Inverted threshold, (c) Thre sholded image, and (d)\nFinal image obtained after ORing.\nthresholding techniques fare better when applied in conjunction wit h our al-\ngorithm. The table also shows that the number of times threshold ex ceeds the\nrange, and number of times the range dictated by upper and side su b-image\ndoes not overlap for three di\ufb00erent thresholding algorithms are qu ite small.\nThis justi\ufb01es our assumption of exploiting the freedom of range for bringing\nout local details.\nTo avoid possible errors arising from the scanning of the image, row- wise from\ntoptobottom, onecanscan theimagein di\ufb00erent ways andperform anORing\noperation of the di\ufb00erent images, as speci\ufb01ed below:\n1) Image is thresholded in the usual way.\n2) Invert the original image upside down and threshold the image. Th en invert\nit back to the original state.\n3) Invert the given image right side left and threshold the image. The n invert\nit back.\n4) ORing operation is carried out on the above images to get the resu lting\nimage, which is equivalent to scanning the image in di\ufb00erent ways and OR ing\n9them. Not just scanning row-wise from top to bottom.\nThe results of the ORing operation thus give superior results as sho wn in Fig.\n4. One can see much clearer local details in the \ufb01nal image.\n5 Conclusion and Discussion\nIn this paper, a new locally adaptive block thresholding method has be en pro-\nposed, which acts as a hybrid between known local and global metho ds. It can\nalso be used in conjunction with other methods of binarization to brin g out\ndetails of an image. It should be emphasized that the same is accomplis hed\nwithout introducing too much of time complexity, an extremely desire d at-\ntribute of any binarization scheme. The present algorithm has been designed\nto ensure that the transitions between sub-windows are maintaine d continu-\nously. This maintains image continuity.\nThe e\ufb03cacy of the method has been demonstrated in the context o f a vari-\nety of images of di\ufb00erent types. This procedure may also be useful when a\nvariable window size is required. The portions of an image requiring det ailed\ninvestigations may be divided into \ufb01ner sub images, whereas other po rtions\nof the image can be divided into bigger box sizes. In this case, one nee ds to\nexplore the problem of boundary mismatch and continuity more care fully. The\nboundary mismatch can be possibly taken care by pushing the bound ary of\nthe block that created the mismatch, till the selected threshold fa lls within\nthe range. This problem is currently under investigation and will be re ported\nelsewhere."}
{"category": "abstract", "text": "We present an algorithm that enables one to perform locally a daptive block thresh-\nolding, while maintaining image continuity. Images are div ided into sub-images\nbased some standard image attributes and thresholding tech nique is employed over\nthe sub-images. The present algorithm makes use of the thres holds of neighboring\nsub-images to calculate a range of values. The image continu ity is taken care by\nchoosing the threshold of the sub-image under consideratio n to lie within the above\nrange. After examining the average range values for various sub-image sizes of a\nvariety of images, it was found that the range of acceptable t hreshold values is\nsubstantially high, justifying our assumption of exploiti ng the freedom of range for\nbringing out local details.\nKey words"}
{"category": "non-abstract", "text": "\u2022Creation frame: the frame in which the mode \ufb01rst was\ncreated. We refer to the difference between the current\nframe and the creation frame as the mode age.\n\u2022Hit count: the number of times that the mode has been\nmatched. This characteristic represents the activity of th e\nmode.\n\u2022Last matched frame: the frame that the mode was last\nmatched in.\n\u2022Potential removal frame: the frame in which a mode is\nscheduled for removal in, if no further matches to this\nmode occur.\nThebestmodematchforeachblockintheimageisdetermined\nby a DCT classi\ufb01er. The DCT classi\ufb01er also determines if\na new mode is created for a given DCT block. A spatial\nclassi\ufb01er, described in more detail in section IV-B, is then\napplied to improve the selection of matching modes based on\nthe temporal information in adjacent modes.\nThe temporal information and DCT coef\ufb01cient values of\nmatched modes are updated using an approximated median\n\ufb01lter as described in section IV-D.\nThe output of this system is a set of connected compo-\nnents that represent foreground objects. Flood \ufb01ll connect ed\ncomponent analysis is used, using the creation time of the\nmatchedmodeforeachblockasthemergingcriterion.Ablock\nis connected to an adjacent block if and only if the creation\nframe of both matched mode models is either greater than a\nthreshold value T, or smaller or equal to the threshold value\nT. Therefore, each connected component will be exclusivelyFigure 2. Detecting an abandoned bag: Input image (left), Ag es of matched\nmodes in a sequence (right)\ncomprised of blocks that are either entirely of age greater t han\nT or less than or equal to T. Figure 2 shows an example where\na droppedbaghasanintermediateage,comparedto theperson\nand the background.\nIV. SAMMI METHOD\nA. DCT classi\ufb01er\nEach block in each input image is compared to the mode\nmodels for that block in order to give the probability that th e\ninputimageblockmatchesthemodemodel.Ahighprobability\nof a match means that the input is likely to have been\nencountered before. If the highest probability mode match f or\na blockis below a thresholdT, the input blockis likely to hav e\nnot been encountered previously and a new mode is created.\nThe highest probability mode match can be used directly for\ncreating blob output. However, we employ further processin g\nsteps as described in sections IV-B and IV-C.\nAs an initial mode matching value, a weighted sum of the\nabsolute differences between each of the coef\ufb01cients of blo ck\nBand mode model Mmis calculated as follows:\n\u03ba(B,Mm) =7/summationdisplay\ni=0ai|ci| (1)\nwhereciis the difference between the i-th coef\ufb01cient of the\ninput block and the i-th coef\ufb01cient of the mode model Mmand\naiis the trained weight. An example of \u03ba(B,Mm)values for\nan image is shown in Figure 3.\nTrained weight aiis determined as follows. Naive Bayes\ntraining is used to determine the probability that a given se t of\ninput coef\ufb01cients match a given set of mode coef\ufb01cientsbase d\nupona setoftrainingdata.A logisticregressionisthenapp lied\nto determine coef\ufb01cients a0...a7. This yields ai=the linear\nregression coef\ufb01cient of log(P(match|ci)/P(nomatch |ci))\nfor alli.\nThe threshold Tfor creating a new mode must also be\ndetermined. That is, the threshold that determines when a\nprobability is not strong enough to warrant a mode match. We\nuse the criterion {true positive rate + false positive rate = 1}\nto determine the threshold level for \u03ba(B,Mm)where P(mode\nmatch) = 0.5. This can be determined and visualized by using\na Receiver Operating Characteristic (ROC) graph (Figure 4) .\nThis method is also used for spatial training as detailed in\nsection IV-B.Figure 4. ROC curve of DCT blocks for match versus no match cla ssi\ufb01cation\nFigure 5. Indoor scene: Empty scene (top left), system input (top right),\nsystem output for 0 spatial iterations (bottom left), syste m output for 3 spatial\niterations (bottom right)\nB. Spatial classi\ufb01er\nVideo sequences from network cameras often contain noise\nfrom camera refocusing, lighting changes and sensor noise.\nTo avoid accumulation of errors in the scene model, a spatial\nclassi\ufb01er is included in this system. The spatial classi\ufb01er takes\nnot only the current block into account, but also its neighbo rs.\nThe underlying assumption is that for a given DCT block at\na given point in time in an image sequence, a mode is more\nlikely to be a match if the adjacent DCT blocks match modes\nthat were created at a similar time to when that mode was\ncreated.\nAccumulation of noise in the scene model is particularly\ndetrimental to age information. We model the creation frame\nand hit count of each mode in each block of the sequence.\nIncorrect mode matching causes the age information to be\ninvalid. These errors can propagate with time due to modeFigure 3. Values for \u03bafor an example image. Scene model (left), input image(cente r), classi\ufb01er output (dark = matcibh, bright = no match) (rig ht)\nmodel updating and mode removal, see \ufb01gure 5 (bottom\nleft) for an example. The spatial classi\ufb01er corrects the DCT\nclassi\ufb01er result for each block in the sequence by using\ninformation from adjacent blocks.\nLet adjacent DCT blocks to a given DCT block in an image\nbe de\ufb01ned using 4-connectivity as the blocks directly above ,\nbelow, left and right of the given DCT block. For each mode\nmodelMin the scene model for a given DCT block, Ais the\nnumberof modemodelsof adjacentblocksthat are temporally\nsimilar to M. Temporal similarity is de\ufb01ned according to the\ndifferencein creationtime between Mand the adjacentblock\u2019s\nmode model, using threshold t. Value Aaffects the likelihood\nthatMis the best match.\nThe \ufb01nal mode match value for a block is \u03ba(B,Mm)+\n\u03bb(I,A), where\u03bb(I,A)accesses a lookup table for the I-th\niteration and the number of similar neighbors A. The lookup\ntable is constructed during an of\ufb02ine training stage. For ea ch\npossible value of MandI, an ROC graph is determined and\n\u03bb(I,A)is taken at the point onthe graphwherethe sum of the\ntrue positive rate and false positive rate equals 1. This met hod\nis similar to the method for determiningthe threshold level for\nthe DCT classi\ufb01er described in section IV-A. An example of\none of the training sequences, at different stages of iterat ion\nis shown in \ufb01gure 6.\nC. Active mode bonus\nMode persistence is used to improve classi\ufb01cation. While\nsome object detection applications focus on tracking movin g\nobjects, other applications have a greater need for stable\nand consistent detection of stationary objects. By includi ng\na probability measure thatis added to the match probability\nformodesseenwithinthe last few frames,thistrade-offcan be\nadjusted by users of the system. Low (or zero) contributions\nfrom mode persistence result in better detection of moving\nobjects. Increasing the mode persistence probability resu lts in\nmore stable and consistent stationary object detection, wh ich\nalso reduces the impact of noise eroding a stationary object .\nD. Approximated median \ufb01lter scene model updating\nThecommonlyusedmethodofusinganexponentialmoving\naverage (EMA) to model scene modes [6], [1] suffers from a\nnumber of problems. There is a noise increase in very dark\nand very light regions, due to sensor noise, lighting change sFigure 7. Exponential Moving Average versus Approximated M edian Filter\nand foreground objects perturbing the model. Further, the\namount of time required to return to the original state after\nan impulse signal is dependent upon the size of the impulse.\nIn addition, the method is computationally expensive due to\nthe multiplications and precision required.\nWe use an approximatedmedian \ufb01lter (AMF) [2] to address\nthe aforementioned issues:\nIfxn> yn+\u03b1amf:yn+1=yn+\u03b1amf\nIfxn> yn\u2212\u03b1amf:yn+1=yn\u2212\u03b1amf\nIfxn\u2264yn+\u03b1amfandxn\u2265yn\u2212\u03b1amf:yn+1=xn\nwherexnis the input coef\ufb01cient value, ynis the mode model\ncoef\ufb01cient value, and \u03b1amfis the learning parameter that\ndetermines how fast the mode model adapts.\nBiased noise is decreased in bright and dark regions, since\nthe recovery time is dependent upon the duration of the noise\nsignal but not upon the intensity of the noise, see \ufb01gure 8 for\nan example. Recovery time for impulsive noise is 1 frame,\nunlike the exponential moving average.\nE. Mode removal\nMode models have to be removed for two reasons. The \ufb01rst\nreason is practical: a system may run out of memory if too\nmany modes have been created. This is especially importantFigure 6. Training sequence images, from left to right: Scen e model (top left), foreground (top), 0 spatial iterations ( top right), 1 spatial iteration (bottom\nleft), 2 spatial iterations (bottom), 3 spatial iterations (bottom right)\nFigure 8. Response to impulse noise for a dark background pix el\nin systems where the system is allocated a \ufb01xed maximum\namountof memory,e.g.in the contexta biggersystem wherea\nlargenumberofcamerasissupported.Inaddition,moremode s\nmeans more processing power is needed. A maximum number\nof modes may be introduced to make the system performance\nfeasibleandpredictable.Thesecondreasonisregardlesso fthe\navailabilityof system resources.Modesmust be removedfro m\nthe system in order to reduce the probability of new objects\nbeing matched to unrelated mode models.\nDeterminingwhen to removea mode is a trade-offdecision.\nIf modes are removed too soon, objects that are occludedwill not match the scene model when they reappear. This is\nknown as the revealed background problem. Modes that are\nnot removed quickly enough have an increased likelihood of\nbeing matched to unrelated objects, see for example \ufb01gure 9.\nWe remove a mode model when it hasn\u2019t been matched to\nan incoming block for a number of frames. For each mode\nmodel, a potential removal frame is computed when the mode\nmodel is created. The value is updated each time the mode\nmodel is matched to an incoming block. The mode removal\nframefmris computed as follows:\nfmr=fcurrent+Cs+Cv\u2217HitCount ,\nwhereCsis a constant indicating the minimum survival\ntime of the mode model, and Cvis a constant indicating the\nminimumpercentageoftimethispartofthescenemustremain\nvisible to retain its temporal information.\nEach time a new frame is processed, the fmrvalue of a\nmodemodelis checked.Ifthe valuematchesthe currentframe\nnumberfcurrent, the mode model is removed regardless of\nthe total number of mode models for the block. Note that\nalways at least one mode model remains if Csis greater than\n0. In addition, each time a new mode model is created, and\nthe maximum number of modes per DCT block has been\nreached, the mode with the earliest mode removal frame value\nis removed.\nV. EVALUATION\nA. Experimental setup\nAn evaluation video data set was recorded using a net-\nwork camera providing a Motion-JPEG stream (resolution\n768x576). Although the SAMMI algorithm is not speci\ufb01c to\nany particular \ufb02avor of JPEG compression, by using a speci\ufb01c\ncamera we have full control over the recording circumstance s,Figure 9. Modes that should have been deleted earlier from fr equent traf\ufb01c in the hallway. Scene model (left), input imag e(center), age image (black = new,\ngreen = old), (right)\nand we avoid any in\ufb02uence of video transcoding. The eval-\nuation data set targets dif\ufb01cult, yet realistic scenarios. That\nis, the data set is typically not representative for the cont ent\nan object detection system may encounter in practice. The\nfocus of our evaluation is on indoor scenarios, where there i s\narti\ufb01cial lightingwith in some cases additionalnaturalli ghting\nthrough windows. The data set is biased towards stationary\nforeground objects, stressing the importance of abandoned\nobject detection and object removal applications. Each vid eo\nstarts with an empty scene to allow for initialization of the\nbackground model. See table I for details on the videos. The\nscreen savers present in some of the videos serve to stretch\nthe modeling in that area of the scene for research purposes,\nas they have more states than the maximum numberof modes.\nFor each of the 10 videos a bounding box ground truth\nwas created. In addition, for more precise evaluation for in\ntotal 10,185 frames in 6 videos, an object mask ground truth\nwas created, which is expected to be accurate within an error\nmargin of 4 pixels. Note that the creation of ground truth is a\nproblem in itself, and that different people or the same pers on\nat a different time may segment the same video differently.\nAs creation of the precise object mask ground truth is very\nexpensive manual labor, it was done for a subset of the videos\nonly.\nThe DCT classi\ufb01er and spatial classi\ufb01er are trained on 12\nframes acquired from 7 videos. The scenes and locations at\nwhich the training videos were recorded are different from t he\nscenes used in the test set.\nB. Evaluation criteria\nLike other object detection algorithms, the SAMMI algo-\nrithm has general applicability. Whether the produced outp ut\nis good in a relative or absolute sense depends on the context\nin which it is used.The requirementsforobject detectionin an\nintruder alert system are very different from those in a peop le\ncounting application. Similarly, a system that alerts a sec urity\nguard will give a higher penalty to false alarms than a system\nthatdoesevent-basedrecording.Weevaluatethesystemout put\nat two levels:\n\u2022Pixel-level accuracy: exact matching between ground\ntruth object masks and the detected objects. Note that the\noutput of the SAMMI method has a coarser granularitythan pixel, viz. 8x8 blocks. Hence, it is not possible for\nour method to score the maximum on this level, while\npixel-based algorithms could theoretically reach a score\nof 100%. Also, the problem of inconsistency in ground\ntruths mentioned before may not even allow a perfect\nsegmentation algorithm to score 100%.\n\u2022Tracking suitability: the impact of the segmentation re-\nsults on a tracker application.\nFor pixel-level accuracy, we determine a true positive coun t\nby an AND operation on the detection result and the ground\ntruth. Similarly, false positives and false negatives are f ound.\nAn F1-measure (harmonic mean of recall and precision) is\nthen computed.\nFor tracking suitability, the number of detected blobs is\nset out against the number of blobs that could be related\nto a bounding box. Only one blob is associated per ground\ntruth bounding box, resulting in a penalty for fragmentatio n\nofobjects.Becauseofthesimplicityofthemeasure,itdoes not\nallow us to draw any conclusions about the absolute results.\nHowever, the measure does tell to what extent fragmentation\nhas been reduced. Although tracking applications may be\nable to deal with fragmentation, lower fragmentation leads\nto lower computational demands and decreases the chance\nof tracking spurious objects. The tracking suitability mea sure\nshould always be used in combination with another object\ndetection measure, such as the pixel-level F1-measure, as i t\ndoes not penalize for missed detections.\nFinally, we measure the computational expense and the\nmemory usage of the scene model. As the foremost purpose\nof the SAMMI method is an increase in speed, it is important\nto measure the impact on resource usage. Although actual\nimplementations of the evaluated methods have not been\noptimizedforspeed,processingtimestillgivesanindicat ionof\nthe differencebetween methods. Experimentswere run on one\n2.16GHzIntelCoreDuoprocessor,andmeasurementsinclude\n\ufb01le I/O. Memory usage is estimated for the persistent scene\nmodel only, that is the memory usage in between processing\n2 frames.\nC. Results\nWe comparethe SAMMIobjectdetectionmethodfordiffer-\nent numbers of iterations to the Mixture of Gaussians method .Description Dif\ufb01culty Length\n(frames)\n1. Several people walk through reception area. Screen saver. Low contrast difference between some objects and\nbackground.3439\n[896]\n2. Person sits down to read newspaper. Stationary objects. Screen saver. Low contrast difference\nbetween some objects and background.1711\n3. Person sits down to read, shuf\ufb02es magazines, leaves a smal l\nobject behind, and returns to fetch it.Stationary objects. Screen saver. Low contrast difference\nbetween some objects and background.6600\n[6299]\n4. Person moves around trolley in reception. There is a visib le\ncomputer screen.Structural changes of background. 2939\n[700]\n5. Person sits in of\ufb01ce, moves existing chairs around. Illumination change. Stationary objects. Screen saver. 1658\n6. Person sits in of\ufb01ce, moves existing chairs around. Windo w blinds\nare swaying.Rapid illumination change. Stationary objects. 4159\n7. Person sits down, deposits object, moves existing papers . Structural changes of background. Screen saver. 1435\n[572]\n8. Adult and child walk in corridor, from far away towards the\ncamera.Low contrast difference between some objects and backgroun d.448\n[370]\n9. Multiple people walk through hallway, leave objects behi nd,\nopen/close boxes on the scene.Stationary objects. 2117\n10. Several people walk through corridor, enter/exit at var ious\npoints, stand still, abandon small objects.Stationary objects. 1984\n[1348]\nTable I\nDESCRIPTION OF VIDEOS USED IN THE TEST SET . THE NUMBER OF FRAMES USED IN OBJECT MASK GROUND TRUTH IS GIVEN I N SQUARE BRACKETS .\nForSAMMI,themaximumnumberofmodemodelswasset to\n5. Results are shown in table II. Per video, results for SAMMI\nusing 3 iterations are shown in \ufb01gure 10\nThe Mixture of Gaussian method suffers from the trade-\noff between detecting moving objects and detecting station ary\nobjects. Although it has a precision higher than SAMMI (0.79\nversus 0.69), its recall is signi\ufb01cantly lower (0.42 versus 0.95)\nindicating a large number of false negatives. The differenc e in\nperformance at pixel level is not signi\ufb01cant for the various\niterations. However, the larger number of iterations shows a\nsigni\ufb01cant increase in tracking suitability, correspondi ng to a\ndecrease in object fragmentation.\nSince our experimental setup does not focus on measuring\nprocessing time precisely, it is inappropriate to draw \ufb01nal\nconclusions about the difference between the methods. How-\never, the signi\ufb01cant difference between Mixture of Gaussia ns\nand our method supports the expectation that our method is\nsigni\ufb01cantly faster. Similarly, the difference in memory u sage\nfor scene modeling is signi\ufb01cant.\nVI. CONCLUSIONS\nComputationally inexpensive background modeling can be\ndone without a signi\ufb01cant penalty in accuracy. The use of\nDCT information without transforming image information\nto the pixel domain still allows for good accuracy while\nmaking signi\ufb01cant savings in resource usage. The use of a\nfast approximated median method makes the modeling robust\nto noise in bright and dark regions of a scene, while it is\nfaster than the conventional exponential moving average ap -\nproach.Fragmentationnoise is reducedbyseveral iteratio nsof\nneighbor adapted classi\ufb01cation based on temporal coherenc y\nof objects.\nAnother advantage of the SAMMI system is its con\ufb01g-\nurability. Users can con\ufb01gure the trade-off between detect ing\nnew moving objects and existing stationary objects using th e\nFigure 10. SAMMI performance, using 3 iterations, at pixel l evel and\ntracking level for videos for which an object mask ground tru th is available.\nactive mode bonus. Similarly, users can make trade-offs for\nremoving modes by specifying the minimum percentage of\ntime a part of the scene must remain visible to retain its\ntemporal information.\nThe spatial processing outlined in this paper allows for a\ngreater variability in the size of objects, particularly sm all\nobjects, that can be successfully detected. The \ufb01ltering of\nlocal noise in the image sequence that would otherwise cause\nspurious blobs to be detected is embedded within the scene\nmodeling process.\nThrough low resource usage while preserving acceptable\naccuracy,the lightweight object detection method present ed in\nthis paper increases the feasibility of deploying video ana lysis\nsystems in the real world.Method Mixture of\nGaussiansSAMMI\nNumber of iterations - 0 1 2 3\nF1 0.55 0.79 0.80 0.80 0.80\nTracking suitability 0.38 0.27 0.31 0.34 0.38\nProcessing time (frames per second) 5 30 29 27 25\nScene model memory usage (KB) 36,288 1,080 1,080 1,080 1,080\nTable II\nRESULTS FOR METHODS ACCORDING TO THE EVALUATION MEASURES .\nREFERENCES\n[1] B. Li and M. I. Sezan. Adaptive video background replacem ent. InProc\nInt Conf Multimedia Expo (ICME) , pages 385\u2013388, Aug 2001.\n[2] N. McFarlane and C. Scho\ufb01eld. Segmentation and tracking of piglets in\nimages.Machine Vision and Applications , 8(3):187\u2013193, 1995.\n[3] J. Migdal, T. Izo, and C. Stauffer. Moving object segment ation using\nsuper-resolution background models. In OMNIVIS , Oct 2005.\n[4] T. Nakanishi and K. Ishii. Automatic vehicle image extra ction based on\nspatio-temporal image analysis. In Proc ICPR , volume I, pages 500\u2013504,\nAug 1992.\n[5] W. K. Pratt. Digital image processing (2nd ed.) . John Wiley & Sons,\nInc., New York, NY, USA, 1991.\n[6] C. Stauffer and E. Grimson. Learning patterns of activit y using real-time\ntracking. IEEE Trans Pattern Analysis Machine Intelligence , 22(8):747\u2013\n757, 2000."}
{"category": "abstract", "text": "\u2014We present the SAMMI lightweight object detection\nmethod which has a high level of accuracy and robustness, and\nwhich is able to operate in an environment with a large number\nof cameras. Background modeling is based on DCT coef\ufb01cients\nprovided by cameras. Foreground detection uses similarity in\ntemporal characteristics of adjacent blocks of pixels, whi ch is a\ncomputationallyinexpensivewaytomake useofobjectcoher ence.\nScene model updating uses the approximated median method fo r\nimproved performance. Evaluation at pixel level and applic ation\nlevel shows that SAMMI object detection performs better and\nfaster than the conventional Mixture of Gaussians method.\nI. INTRODUCTION\nThe \ufb01eld of object detection has matured to the extent\nthat its results are suf\ufb01ciently robust to be deployed in the\nreal world. Systems using object detection assist in mitiga ting\nsecuritythreatsandgatheringmarketinginformation.Res earch\nfocus has shifted to further analysis making use of object\ndetection results, such as behavior analysis. However, wit h\nthe large number of surveillance cameras installed each yea r,\nscalability of analysis solutions becomes a problem when on e\nserver is connected to many cameras. Performing real-time\nvideo analysis is not possible without investing signi\ufb01can tly\nin extra infrastructure, such as additional servers or dedi cated\nanalysis boxes.\nAs much of the resource usage in a video analysis is spent\non the underlying object detection, it makes sense to revisi t\ndetection techniques and make them more suitable for de-\nployment in large operations. Object detection, and especi ally\nforeground/background separation, is the bottle neck in an\nanalysis system because the foreground separation needed f or\nobjectdetectionisperformedoneverypixelintheframe.Af ter\nobjectshavebeenfound,videoanalysiscanfocusat the high er\nlevel of objects. Obviously, there are many less objects in t he\naverage frame than there are pixels. The problem could be\naddressed by reducing the sampling. For example, analysis\ncould be done at a lower frame rate than the camera\u2019s capture\nrate, or processing could be performed on a selected region\nof interest only. However, such approaches are just shiftin g\nthe problem and do not make optimal use of the information\nprovided by the cameras.\nIn this paper, we present a lightweight object detection\ntechniquethat still hasa highlevelof accuracyand robustn ess.\nIn the context of this paper, we assume that the camera has a\n\ufb01xed \ufb01eld of view. Yet we do expect that an object detection\nmethod reinitialize quickly when the \ufb01eld of view changes in\nthe case of Pan-Tilt-Zoom (PTZ) cameras. Also, we consider\nobjects in the scene that are non-transient during an entire\nrecording, e.g. a parked car, to be part of the background.Transient objects are considered foreground. A foreground\nobject may be stationary for part of the recording, while the\nbackground may contain movement, e.g. a swaying tree.\nThe paper is organized as follows. In section II, previous\nworkinthe\ufb01eldisdescribed.InsectionIII,ageneraloverv iew\nof the system and context in which the spatio-activity based\nobject detection operates is given. In section IV, we presen t\nthe details of our SAMMI (Spatio-Activity Multi-Mode with\nIterations) method. Finally, in sections V and VI, we evalua te\nthe method and draw conclusions.\nII. PREVIOUS WORK\nThe general concept of background modeling that is the\nbasis for our approach is well known in literature. The basic\nassumption is that background is static and doesn\u2019t change,\nand that anything new must be foreground. Early approaches,\ne.g.[4],updatea referenceframeto berobusttochangesint he\nsceneortheappearanceofthescene,e.g.lightingchanges, and\ndo backgroundsubtractionwith incomingframes. Backgroun d\nmodeling was popularized by Stauffer and Grimson\u2019s Mixture\nofGaussiansapproach[6].TheMixtureofGaussiansapproac h\ndoes not maintain one model per pixel, but several models in\nthe formof a Gaussian.The modelsarenot statically classi\ufb01 ed\nas foreground or background, but are dynamically classi\ufb01ed\nafter updating the model with the information from a new\nframe. More than one model can be background, allowing for\nmodeling of multi-modal backgrounds, such as swaying trees .\nLi and Sezan [1] have used informationabout the neighbors\nof pixels being classi\ufb01ed in order to address the problem\nof spatial and temporal aliasing on cameras. They assume\nthat the probability that a pixel is background depends on\nthe classi\ufb01cation of neighboring pixels. The disadvantage of\nthis approach is that it results in fragmentation and erosio n\nof foreground objects. For this reason, Li and Sezan apply\nthe neighbor based probability classi\ufb01cation only after \ufb01n ding\nlargeconnectedcomponents.Thisapproachpreventsundesi red\nhole \ufb01lling, but as a trade-off it rejects small foreground\nobjects. The trade-off may be acceptable in the background\nreplacement application with close-up and medium shots tha t\nLi and Sezan are targeting. But the loss of small objects is\nnot desirable in security applications, especially if obje cts are\nsmall because of their distance to the camera. The ability to\ndetect objects with a greater range in sizes requires less zo om\nto successfully detect objects, thus fewer cameras need be\ninstalled to cover the same \ufb01eld of view [3]. The approach\nof Li and Sezan shows that spatial proximity of pixels assist s\nin classifying them as foreground or background, but it is no tFigure 1. Block diagram of the SAMMI system.\nsuf\ufb01cient without de\ufb01ning a further relationship between t he\npixels.\nThe most obvious relationship between pixels is based on\nthe visual characteristics of the pixels, such as color. Suc h\nrelationships are complex, e.g. because of texture, and als o\ncomputationally expensive. This approach depends very muc h\non the progress in still image segmentation.\nIII. SAMMI SYSTEM OVERVIEW\nThe SAMMI (Spatio-Activity Multi-Mode with Iterations)\nmethod presented in this paper, makes use of temporal charac -\nteristics to de\ufb01ne the relationship between neighboring pi xels,\nor blocks of pixels in our particular implementation. The\ntemporal characteristics are cheap to maintain and compare\nfrom a resource usage perspective, yet they are effective\nbecause they make use of the temporal coherence of moving\nobjects. In this section, we present a general overview of\nthe system, as depicted in \ufb01gure 1. The key components are\ndiscussed in more detail in section IV.\nThe system context has signi\ufb01cant impact on the SAMMI\nalgorithm. The starting point is not raw image data, but a\nMotion JPEG stream (intra-frame coding only) captured by\na camera and transmitted over a network. A departure from\nconventionalapproachesisthatwedon\u2019tdecompresstheJPE G\ninformation to pixel-level information in the spatial doma in.\nInstead, processingis doneon Discrete Cosine Transformat ion\n(DCT) coef\ufb01cients in the frequency domain and, as a con-\nsequence, at 8x8 pixels block-level. The primary motivatio n\nfor using the DCT coef\ufb01cients is the computational cost of\nthe inverse transform to the pixel domain. In addition, it\nallowsforreductioninmemoryusageandprocessingforscen emodeling. Although storing 192 DCT coef\ufb01cient values is no\ndifferent from storing 192 RGB pixels for an 8x8 block, the\ninformation carried by the DCT coef\ufb01cients is concentrated\nin just a few coef\ufb01cients. We found that using 8 of the 192\ncoef\ufb01cientsis suf\ufb01cient for acceptable detection accurac y.The\ndisadvantage of the block-level approach is that object edg es\nare less accurate than in a pixel-level approach. The impact\nof the blockiness of edges depends on the application, e.g.\nsecurity applications are invariant under blockiness, and on\nthe camera resolution.\nThe original input to our system is in the YCbCr color\nspace. Principal component analysis of the YCbCr component\nvaluesof typical imagesshowsthat Cb and Cr are morehighly\ncorrelated to each other than the alternative in-phase (I) a nd\nquadrature (Q) chrominance components in the YIQ color\nspace [5]. Improving the independence of DCT components\nimprovesthe results of the DCT classi\ufb01er described in secti on\nIV-A, allowing for greater invariance under changing light ing\nconditions such as shadows. The \ufb01rst 6 Y DCT coef\ufb01cients\nare used in conjunction with the I and Q DC coef\ufb01cients.\nUsing the input coef\ufb01cients, the scene is modeled on a per-\nblock basis. For each block, several mode models are main-\ntained,eachrepresentinga state ofthe image,representin gsta-\ntionaryobjectsandthe underlyingbackgroundsimultaneou sly.\nMode models contain DCT coef\ufb01cient values and temporal\ninformation. This temporal information allows the age and\nfrequency of encountering a mode to be determined. For each\nmode, the following temporal characteristics are recorded"}
{"category": "non-abstract", "text": "in the next section,\nwe brie\ufb02y introduce the reader to the primary spatial pro-\ncessing by the HVS and to the related literature. We de-\nscribeinsection3 theFourier-BesselTransform(FBT) and\nthe proposed algorithm in section 4. We introduce the face\ndatabaseand testingmethodsin Section 5. The experimen-\ntal results are presented in Section 6 and in the last section\nwediscusstheresultsandongoingwork.\n2 Background and previous work\nMost of the current face recognition and veri\ufb01cation al-\ngorithms are based on feature extraction from a Cartesian\nperspective,typicaltomostanaloganddigitalimagingsys -\ntems. On the other hand, the HVS is known to process vi-\nsual stimuli by fundamental shapes de\ufb01ned in polar coor-\ndinates. In the early stages the visual image is \ufb01ltered by\nneuronstunedtospeci\ufb01cspatialfrequenciesandlocationi n\na linear manner[4]. In furtherstages, these neuronsoutput\nis processed to extract global and more complex shape in-\nformation, such as faces [19]. Electrophysiological exper -\niments in monkey\u2019s visual cerebral areas showed that the\nfundamental patterns for global shape analysis are de\ufb01ned\n1Partial results based on a preliminary version of the system were sub-\nmitted in [30]inpolarandhyperboliccoordinates[11]. Globalpoolingof\norientation information was also shown by psychophysical\nexperiments to be responsible for the detection of angular\nand radial Glass dot patterns [27]. Thus, it is evident that\ninformationregardingthe globalpolar contentof imagesis\neffectivelyextractedbyandisavailabletotheHVS.In[31]\nweintroducedtherepresentationoffaceimagesinthepolar\nfrequencydomainbyglobaltwo-dimensionalFBTfeatures.\nHowever,oneofthedisadvantagesofglobalfeatureextrac-\ntionsis the roughrepresentationof peripheralregions. Th e\nHVS compensates this effect by eye saccades, moving the\nfovea from one point to the other in the scene. Here we\npropose to apply the FBT at strategic regions [13], namely\ntheeyesregion. Moreover,wealsointegratedfaceandeyes\ndetection algorithms, which makes the veri\ufb01cation system\ncompletelyautomatic.\n3 Fourier-Bessel analysis\nThe FB series [9] is useful to describe the radial and\nangular components in images. FBT analysis starts by\nconverting the coordinates of a region of interest from\nCartesian (x,y)to polar(r,\u03b8). Thef(r,\u03b8)function is\nrepresentedbythetwo-dimensionalFBseries, de\ufb01nedas\nf(r,\u03b8) =\u221e/summationdisplay\ni=1\u221e/summationdisplay\nn=0An,iJn(\u03b1n,ir)cos(n\u03b8)\n+\u221e/summationdisplay\ni=1\u221e/summationdisplay\nn=0Bn,iJn(\u03b1n,ir)sin(n\u03b8) (1)\nwhereJnis the Bessel function of order n,f(R,\u03b8) = 0\nand0\u2264r\u2264R. \u03b1 n,iis theithroot of the Jnfunction,\ni.e. the zero crossing value satisfying Jn(\u03b1n,i) = 0.Ris\ntheradialdistancetotheedgeoftheimage. Theorthogonal\ncoef\ufb01cients An,iandBn,iaregivenby\nA0,i=1\n\u03c0R2J2\n1(\u03b1n,i)\u03b8=2\u03c0/integraldisplay\n\u03b8=0r=R/integraldisplay\nr=0f(r,\u03b8)rJn\n(\u03b1n,i\nRr)drd\u03b8 (2)\nifB0,i= 0andn= 0;\n/bracketleftbiggAn,i\nBn,i/bracketrightbigg\n=2\n\u03c0R2J2\nn+1(\u03b1n,i)\u03b8=2\u03c0/integraldisplay\n\u03b8=0r=R/integraldisplay\nr=0f(r,\u03b8)rJn\n(\u03b1n,i\nRr)/bracketleftbiggcos(n\u03b8)\nsin(n\u03b8)/bracketrightbigg\ndrd\u03b8 (3)\nifn >0.4 Faceveri\ufb01cationusing FBT\nThe proposed algorithm is based on image registration\nand normalization, and two subsequent feature extraction\nsteps followed by a classi\ufb01er formation. After the \ufb01rst two\nsteps, we extract the FB coef\ufb01cients from the images, we\ncompute the pairwise Cartesian distance between all the\nFBT-representations and re-de\ufb01ne each object by its dis-\ntancetoallotherobjects. Inthelaststagewetrainapseudo\nFisher classi\ufb01er. We tested thisalgorithmon the wholeim-\nage(globalanalysis)andonthecombinationofthreefacial\nregions(localanalysis).\n4.1 Faceregistrationandnormalization\nFace representations requires prior image registration\nand usually a spatial and luminance normalization pre-\nprocessing. Assuming the sample images contain a single\nface,wedetectedtheheadwithacascadeofclassi\ufb01ers[26]\nandestimatedthelocationoftheeyesregionwithanActive\nAppearance Model algorithm [3]. Within this region, we\nused \ufb02ow \ufb01eld information[14] to determine the eyes cen-\nter. Using the eyes coordinates, we translated, rotated, an d\nscaledtheimagessothattheeyeswereregisteredatspeci\ufb01c\npixels. Next, the images were cropped to 130 x 150 pixels\nsizeandamaskwasappliedtoremovemostofthehairand\nbackground. The unmasked region was histogram equal-\nized and normalized to zero mean and a unitary standard\ndeviation.\n4.2 Spatialto polarfrequencydomain\nImagesweretransformedbyaFBTuptothe30thBessel\norder and 6throot with angular resolution of 3\u02da, thus ob-\ntaining 372 coef\ufb01cients. These coef\ufb01cients correspond to\na frequency range of up to 30 and 3 cycles/image of angu-\nlarandradialfrequency,respectively. Thisfrequencyran ge\nwas selected based on earlier tests [31] with the small-size\nORL face database [23]. We tested the FBT descriptors of\nthewholeimage,aswellasacombinationoftheupperright\nregion,uppermiddleregion,andtheupperleft region(Fig-\nure1).\n4.3 Polarfrequencyto dissimilarity domain\nWe built a symmetric dissimilarity matrix D(t,t)de-\n\ufb01ned as the Euclidean distance between all training FBT\nimagest. In this space, each object is represented by\nits dissimilarity to all objects. This approach is based on\nthe assumption that the dissimilarities of similar objects to\n\u201cother ones\u201d is about the same [6]. Among other advan-\ntages of this representation space, by \ufb01xing the number of\nfeatures to the number of objects, it avoids a well knownNormalized Right eye Betweeneyes Lefteye\nFigure 1. Sample of a normalized whole face\nimage and theregions thatwere used forthe\nlocal analysis\nphenomenon, where recognition performance is degraded\nas a consequence of small number of training samples as\ncomparedtothenumberoffeatures.\n4.4 Classi\ufb01er\nTestimageswereclassi\ufb01edbasedonapseudoFLDusing\na two-class approach. A FLD is obtained by maximizing\nthe (between subjects variation)/(within subjects variat ion)\nratio [10]. Here we used a minimum-squareerrorclassi\ufb01er\nimplementation [24], which is equivalent to the FLD for\ntwo-class problems [10]. In these cases, after shifting the\ndatasuchthatit haszeromean,theFLD canbede\ufb01nedas\ng(x) =/bracketleftbigg\nD(t,x)\u22121\n2(m1\u2212m2)/bracketrightbiggT\nS\u22121(m1\u2212m2)\n(4)\nwherexisaprobeimage, Sisthepooledcovariancematrix,\nandmistands for the mean of class i. The probe image x\nis classi\ufb01ed as correspondingto class-1 if g(x)\u22650and to\nclass-2 otherwise. However, as the number of training ob-\njects and dimensionsis the same in the dissimilarity space,\nthe sample estimation of the covariance matrix Sbecomes\nsingular, and the classi\ufb01er cannot be built. One solution to\nthe problemis to use a pseudo-inverseand augmentedvec-\ntors[24]. Thus,Eq. 6isreplacedby\ng(x) = (D(t,x),1)(D(t,t),I)(\u22121)(5)\nwhere(D(t,x),1)is the augmented vector to be classi-\n\ufb01ed and(D(t,t),I)is the augmented training set. The\ninverse(D(t,t),I)(\u22121)is the Moore-Penrose Pseudo-\ninverse which gives the minimum norm solution. The cur-\nrentL-classes problem can be reduced and solved by the\ntwo-classes solution described above. The training set was\nsplitintoLpairsofsubsets,eachpairconsistingofonesub-\nset with images from a single subject and a second subset\nformedfromall the otherimages. A pseudo-FLDwas built\nforeach pairof subsets. A probeimage was tested onall L\ndiscriminant functions, and a \u201cposterior probability\u201d sco re\nwas generated based on the inverse of the Euclidean dis-\ntancetoeachsubject.5 Database, preprocessing, and testing pro-\ncedures\nWe used the FERET database, due to its large number\nof individuals and rigid testing protocols that allow pre-\nciseperformancecomparisonsbetweendifferentalgorithm s\n[20]. Here we compare our algorithm performance with a\n\u201cbaseline\u201dalgorithmandwiththepublishedresultsofthre e\nsuccessfulapproaches[22]. Asabaselinealgorithmweim-\nplementeda standardPCA-based algorithm[25]. Theprin-\ncipal components were based on a set of 700 images se-\nlected randomly from the gallery subset. Not all 1196 im-\nages were used, due to the huge RAM memory that such\noperation requires. The \ufb01rst three principal components,\nthat encodebasically illuminationvariations[12], were e x-\ncluded before projecting of the training and test images.\nThe three other approaches are: Gabor wavelets combined\nwith elastic bunchgraph matching(EBGM) [28], localized\nfacialfeaturesextractionfollowedbyaLinearDiscrimina nt\nAnalysis (LDA) [8], and a Bayesian generalization of the\nLDAmethod[18].\nIn the FERET protocol,a galleryset ofone frontalview\nimagefrom1196subjectsisusedtotrainthealgorithmand\na different dataset is used as probe. All images are gray-\nscale 256 x 384 pixels size. We used the four probe sets,\ntermedFB,DupI,DupIIandFC. The FB dataset is con-\nstituted of a single image from 1195 subjects, taken from\nthe same subjects in the gallery set, after an interval of a\nfew seconds, but with a different facial expression. The\nDupIandDupIIdatasetsinclude722or234images,respec-\ntively. TheDupIimagesweretakenimmediatelyorupto34\nmonthsafter the galleryimages, while the images in DupII\nwere takenat least 18monthsafter thegalleryimages. The\nFC subset contains 194 images of subjects under different\nlightingconditions.\nThe eyes coordinates were extracted automatically, as\ndescribed in Section 4.1. Approximately 1% of the faces\nwere not localized, in which cases the eyes region coor-\ndinates were set to a \ufb01x value derived from the mean of\nthe located faces. The \ufb01nal mean error was 3.6 \u00b15.1\npixels. In order to estimate the system performance un-\nder minimal localization errors, we executed a second se-\nries of experimentsin which ground-truthinformation was\nused. Thefaceregistrationwasfollowedbyanormalization\nstep, as described in Section 4.1. The same pre-processing\nprocedure was used in previous algorithms, except for the\nGabor-EBGMsystemwhereaspecialnormalizationproce-\ndurewasused.\nThe performanceof the system was evaluated by veri\ufb01-\ncation tests according to the FERET protocol [20]. Given\na gallery image gand a probe image p, the algorithm ver-\ni\ufb01es the claim that both were taken from the same subject.\nThe veri\ufb01cation probability PVis the probabilityof the al-Age(1-34months) Age(18-34months)\nExpression Illumination\nFigure 2. ROC functions of the FBT, PCA, and previous algorit hms on the age, expression and\nilluminationsubsets.\ngorithm accepting the claim when it is true, and the false-\nalarm rate PFis the probability of incorrectly accepting a\nfalse claim. The algorithm decision depends on the poste-\nrior probability score si(k)given to each match, and on a\nthreshold c. Thus,aclaimiscon\ufb01rmedif si(k)\u2264candre-\njected otherwise. A plot of all the combinationsof PVand\nPFasafunctionof cisknownasareceiveroperatingchar-\nacteristic(ROC). PVandPFwerecalculatedasthenumber\nofcon\ufb01rmationsdividedbythe numberofcorrector incor-\nrectmatches,respectively. Thisprocedurewasrepeatedfo r\n100equallyspacedthresholdlevels. Trainingandtestswer e\ndonewiththePRToolstoolbox[5].\n6 Results\n6.1 Semi-automatic system\nFigure 2 shows the performance of the proposed algo-\nrithm in the veri\ufb01cation test. On the expression dataset theglobalandlocal FBT versionsperformedat aboutthe same\nlevel as the previous best and second-best algorithms, re-\nspectively. OnbothagedatasetstheFBTalgorithmsoutper-\nformedthepreviousalgorithms,withthelocalversionbein g\nslightlysuperior. Ontheilluminationdatasettheglobala nd\nlocal FBT algorithmswere equal or better than the second-\nbestpreviousalgorithm(PCA+LDA).\n6.2 Partialocclusion\nLocal approaches for face recognition are in general\nmore robust for occlusions (for e.g. [15, 17]) than global\nones. To evaluatethisaspect ofthe proposedalgorithm,we\noccluded all the normalized test images with a gray mask\nthat covered >50% of the total area. We tested two mask-\ning options: maskingofthe right-eyeand mouthregionsor\nmasking of the mouth and nose regions (Fig. 3). Figure\n4 shows the effect of occlusion on the performance of the\nglobal and local versions of the FBT algorithms. The se-vereocclusionsdidnotreducemuchtheperformanceofthe\nlocal algorithm on the expression and age subsets, but af-\nfectedsigni\ufb01cantlyperformanceontheilluminationsubse t.\nThe globalversionperformedmuch worse underocclusion\nconditionson all subsets. These results con\ufb01rm the advan-\ntageof the local overthe globalapproach,and demonstrate\nthehighrobustnessofthelocal-FBTunderstrongocclusion\nconditionscombinedwith expressionandagevariations.\nNoocclusion Eye+ mouth Mouth+ nose\nFigure3.Examples of imageocclusion.\n6.3 Automatic system\nFigure 5 shows the performance of the FBT algorithms\nwith ground-truthinformation and when the eyes were de-\ntected automatically. The localization errors introduced in\nthe latter case reduced the performance of the FBT algo-\nrithmsup to 20%, approximatelyas it affected the PCA al-\ngorithm,whichisknowntobesensitivetothistypeoferror\n[16]. The localization sensitivity of the proposed system\nis expected, considering the variance property of the FBT\nto translation [1]. It is interesting to notice, however, th at\nunder such conditions the advantage of the local over the\nglobalapproachwassigni\ufb01cantlyreduced.\n7 Discussion\nWe introduceda fully automatedbiologically-motivated\nlocal-basedsystemforfaceveri\ufb01cationtasks. Themainem-\npirical result of this study is the demonstration of the high\nperformanceofaveri\ufb01cationsystembasedonFBTdescrip-\ntors, especially when these are extracted locally. The sig-\nni\ufb01cant advantageof the FBT approachin the age variance\ntestsareanindicationoftherobustnessofthepolarfeatur es\nin realistic situations of face variationsthat exceeds sim ple\nfacial expression, like illumination and age. The superior\nbehavior of the local approach was especially strong w.r.t.\nrobustnessto occlusion. In the local version, the mouth re-\ngion is completely ignored, thus its occlusion or variation\n(ex. due to a new beard or a scarf) does not affect perfor-\nmance at all. Moreover, the local-FBT outperformed the\nglobal-FBT even when the occluded regions included face\nregionsthatwereanalyzedbythe localversion.Thepropertyofrobustnesstoocclusionoflocalanalysis\nwas explored by others. Local PCA was used in [15, 17]\nto detect occluded regions in face images. The test images\nwereclassi\ufb01edbycomparingtheunoccludedregionstocor-\nresponding regions in training images. However, the com-\nbinationofFBTfeaturesandlocalapproachhasseveralad-\nvantagesoverthatmethodbesidesperformance. Inourpro-\nposal there,is no needto detect the occludedregionsin the\ntest images. Furthermore,thereisnoneedforspecial train -\ning strategies [17] or for training of speci\ufb01c classi\ufb01ers fo r\neach testing image depending on the occluded region [15].\nFinally, there is no need for any classi\ufb01cation rule for the\ncombination of the local features; the FBT features form a\nsinglevector. It is hardto compareourperformanceresults\nwith those obtained by [15, 17], since their tests were per-\nformedonsubsetsoflessthan100images. Thetrainingand\ntest images also did not included variations of expression,\nillumination or age. The algorithm of [17] was adapted in\norderto deal with expressionvariationby weightingdiffer -\nently local areas and assuming that the facial expressionof\nthetrainingimagesisknown. Incontrast,hereweshowthat\nthe proposed system can deal simultaneously with expres-\nsion, illumination, and age variations, besides large scal e\nocclusions.\nPerformance gain of the automatic FBT method can be\nachievedbyimprovingthe eyeslocalizationalgorithm. For\nexample,[17]learnedthesubspacethatrepresentslocaliz a-\ntion errors within eigenfaces. This method can be adopted\nfor the FBT subspace, with the advantage of the option to\nexclude from the \ufb01nal classi\ufb01cation face regions that gives\nhighlocalizationerrors.\nThe relation of the present algorithm to human face\nrecognition was not directly evaluated here, but a few as-\nsociations can be done. As discussed in the Introduction,\nthereisclearevidencethattheHVSextractglobalradialan d\nangularshapeinformation,afactthatmightlookincompat-\niblewiththeinformativeadvantageofthelocalinformatio n\npooling showed here. However, only little is known about\nthe size of the global pooling area. A 1.2 visual degrees\npooling area was suggested for the detection of Glass pat-\nterns[27],butthespatiallocationsandscale regardingfa ce\nimagesremainasopenquestions.\nIn the proposed system, the classi\ufb01er operates in non-\ndomain-speci\ufb01c metric space whose coordinates are simi-\nlarityrelations. Thehighperformanceachievedbythisrep -\nresentation indicates that the \u201creal-world\u201d proximity rel a-\ntions between face images are preserved to a good extent\nin the constructedinternalspace. It is possiblethat human s\nalsouseananalogousspacetorepresentvisualobjects. Thi s\nhypothesiswas studiedby correlatingthe distance between\ndifferent shape objects by objective and perceptual param-\neters (see [7] for a review). Comparison of the two mea-\nsurements is usually done by a multidimensional scalingAge(1-34months) Age(18-34months)\nExpression Illumination\nFigure4.ROCfunctionsoftheFBTontheoccludedandnotoccl udedage,expressionand illumina-\ntionsubsets.\nanalysis (MDS), which projects objects as points in a two-\ndimensional space where the distance between the points\napproximate the Euclidean distance between the original\nobjects. For example, in one study [21] objective and per-\nceptualsortingoffaceimageswerehighlycorrelated,espe -\ncially when the objectivesorting used globalfeatures, suc h\nas age and weight of the persons in the images. Similar\nresults were obtained in a neurophysiologicalstudy [29] in\nwhich monkeys were presented with face images. It was\nfoundthattheMDSmapsobtainedfromtheoriginalimages\nandfromtheresponsepatternsofneuronsintheinferotem-\nporalcortexhadsimilarpatterns. Theseresultsindicatet hat\nrepresenting images in a dissimilarity space can be analo-\ngousto humanrepresentationmechanisms.\nIn conclusion, the proposed system combines high face\nveri\ufb01cation performancefor expression, age, and illumina -\ntion tests, and robustness to occlusion. Future investiga-\ntions, using psychophysical methods, should establish the\nlevelofits relationtobiologicalsystems.Acknowledgments\nY. Zana is grateful to FAPESP (03/07519-0) and to\nCNPq (478384/01-7). R. Cesar-Jr. is grateful to FAPESP\n(99/12765-2)andtoCNPq(300722/98-2and474596/2004-\n4). R. Barbosa is grateful to FAPESP (03/03506-0). The\nauthors are grateful to Rog\u00b4 erio S. Feris and Matthew Turk\nforusefulcommentsandcollaborationonthisresearch."}
{"category": "abstract", "text": "We present an automatic face veri\ufb01cation system in-\nspired by known properties of biological systems. In the\nproposed algorithm the whole image is converted from\nthe spatial to polar frequency domain by a Fourier-Bessel\nTransform (FBT). Using the whole image is compared to\nthe case where onlyface image regions(localanalysis) are\nconsidered. The resulting representations are embedded in\nadissimilarityspace,whereeachimageisrepresentedbyit s\ndistance to all the other images, and a Pseudo-Fisher dis-\ncriminator is built. Veri\ufb01cation test results on the FERET\ndatabase showed that the local-based algorithm outper-\nforms the global-FBT version. The local-FBT algorithm\nperformed as state-of-the-art methods under different tes t-\ningconditions,indicatingthattheproposedsystemishigh ly\nrobust for expression, age, and illumination variations. W e\nalso evaluatedthe performance of the proposed system un-\nder strong occlusion conditions and found that it is highly\nrobust for up to 50% of face occlusion. Finally, we auto-\nmated completely the veri\ufb01cation system by implementing\nface and eye detection algorithms. Under this condition,\nthe local approach was only slightly superior to the global\napproach.\n1 Introduction\nFace veri\ufb01cation and recognition tasks are highly com-\nplex due to the many possible variations of the same sub-\nject in differentconditions,like illumination,facial ex pres-\nsion, and age. Many developers of face recognition algo-\nrithms adopted a biologically inspired approach in solving\ntheseproblems(forareview,see[2]),thuscontributingbo th\nto understand human face processing and to build ef\ufb01cient\nfacerecognitiontechnologies.\nTheapproachdescribedinthepresentpaperwasinspired\nby developments in neurophysiology and cognitive psy-\nchology and its fundamentals were \ufb01rst described by [31].Itisbasedonimagerepresentationthatmaybeanalogousto\nthoseusedbythehumanvisualsystem(HVS).Inparticular,\nwe evaluated the performanceof a face veri\ufb01cation system\nwhose primary features were the magnitude of radial and\nangular componentsof faces images, and representation in\na dissimilarity space. The main contribution of this paper\nis the proposal to use local analysis approach, in contrast\nto the previouslyused global analysis approach1. We show\nthat a system based on the new method achieves state-of-\nthe-art performance level. Moreover, we demonstrate that\nthe proposed system is robust to typical variations in face\nimages,likefacialexpression,age,illumination,andpar tial\nocclusion.\nThe paper is organized as follows"}
{"category": "non-abstract", "text": "1) have we exhausted what can be learned from a\ngiven set of data? 2) have we reached the end of classi\ufb01er developm ent? and 3)\nwhat else can be done?\nObviouslywehavenotsolvedallthe classi\ufb01cationproblem inthe world\u2013 new\napplications bring new challenges, many of which are still beyond reac h. But do\nthese require fundamental breakthroughs in classi\ufb01cation resea rch? Or are these\n\u201cmerely engineering problems\u201d that will eventually be solved with more machine\npower, or more likely, more human power to \ufb01nd better feature ext ractors and\n\ufb01ne tune the classi\ufb01er parameters?\nTo answerthese questionswe need to know whether there exists a limit in the\nknowledge that can be derived from a dataset, and where this limit lies . That is,\nare the classes intrinsically distinguishable? And, to what extent are they distin-\nguishable? These questions are about the intrinsic complexity of a cla ssi\ufb01cation\nproblem, and the match between a classi\ufb01er\u2019s capability to a problem\u2019s intrinsic\ncomplexity. We believe that an understandingof these is the only way to \ufb01nd outabout the current standing of classi\ufb01cation research, and to obt ain insights to\nguide further developments. In this lecture we describe our recen t e\ufb00orts along\nthese lines.\n2 Sources of di\ufb03culty in classi\ufb01cation\nWe begin with an analysis of what makes classi\ufb01cation di\ufb03cult. Di\ufb03culties in\nclassi\ufb01cation can be traced to three sources: 1) class ambiguity, 2 ) boundary\ncomplexity, and 3) sample sparsity and feature space dimensionality .\nClass ambiguity\nClass ambiguity refers to the situation when there are cases in a clas si\ufb01cation\nproblem whose classes cannot be distinguished using the given featu res byany\nclassi\ufb01cation algorithm. It is often a consequence of the problem fo rmulation.\nClasses can be ambiguous for two reasons. It could be that the clas s concepts\nare poorly de\ufb01ned and intrinsically inseparable. An example for this is t hat the\nshapes of the lower case letter \u201cl\u201d and the numeral \u201c1\u201d are the sam e in many\nfonts (Figure 1(a)). Such ambiguity cannot be resolved at the clas si\ufb01er level, a\nsolution has to involve the application context.\nThere is another situation where the classes are well de\ufb01ned, but t he chosen\nfeatures are not su\ufb03cient for indicating such di\ufb00erences (Figure 1 (b)). Again,\nthere is no remedy at the classi\ufb01er level. The samples need to be repr esented by\nother features that are more informative about the classes. Clas s ambiguity can\noccur for only some input cases. Problems where the classes are am biguous for\nat least some cases are said to have nonzero Bayes error, which se ts a bound on\nthe lowest achievable error rate.\n(a) The shapes of the lower case let-\nter \u201cl\u201d and the numeral \u201c1\u201d are the\nsame in many fonts. They cannot be\ndistinguishedbyshape alone. Which\nclass asamplebelongs todependson\ncontext.(b) There may be su\ufb03cient features\nfor classifying the shells by shape,\nbut not for classifying by the time\nof the day when they were collected,\nor by which hand they were picked\nup.\nFig.1.Ambiguous classes due to (a) class de\ufb01nition; (b) insu\ufb03cien t features.Boundary complexity\nAmong the three sources of di\ufb03culties, boundary complexity is close st to the\nnotion of the intrinsic di\ufb03culty of a classi\ufb01cation problem. Here we cho ose the\nclass boundary to be the simplest (of minimum measure in the feature space)\ndecisionboundarythat minimizesBayeserror.With acompletesample ,theclass\nboundary can be characterized by its Kolmogorov complexity [10] [12 ]. A class\nboundary is complex if it takes a long algorithm to describe, possibly inc luding\na listing of all the points together with their class labels. This aspect o f di\ufb03culty\nis due to the nature of the problem and is unrelated to the sampling pr ocess.\nAlso, even if the classes are well de\ufb01ned, their boundaries may still b e complex\n(Figure 2). An example is a random labeling of a set of uniformly distribu ted\npoints, where each point has a de\ufb01nite label, but points of the same la bel are\nscattered over the entire space with no obvious regularity. The on ly way to\ndescribe the classes may be an explicit listing of the positions of the po ints with\nthe same label.\n(a) (b) (c) (d)\nFig.2.Classi\ufb01cation problems of di\ufb00erent geometrical complexit y: (a) linearly sepa-\nrable problem with wide margins and compact classes; (b) lin early separable problem\nwith narrow margins and extended classes; (c) problem with n onlinear class boundary;\n(d) heavily interleaved classes following a checker board l ayout.\nKolmogorov complexity describes the absolute amount of informatio n in a\ndataset, and is known to be algorithmically incomputable [13]. Thus we r esort\nto relative measures that depend on the chosen descriptors. Spe ci\ufb01cally, we can\nchoose a number of geometrical descriptors that we believe to be r elevant in\nthe context of classi\ufb01cation. We then describe the regularities and irregularities\ncontained in the dataset in terms of the chosen geometrical primitiv es. We refer\nto these descriptors as measures of the geometrical complexity of a dataset. This\nwould be su\ufb03cient for pattern recognition where most classi\ufb01ers ca n also be\ncharacterized by geometrical descriptions of their decision region s.\nSample sparsity and feature space dimensionality\nAn incomplete or sparse sample adds another layer of di\ufb03culty to a dis crimi-\nnation problem, since how an unseen point should share the class labe ls of the\ntraining samples in its vicinity depends on speci\ufb01c generalization rules. Without\nsu\ufb03cient samples to constrain a classi\ufb01er\u2019s generalization mechanism , the deci-\nsions on the unseen samples can be largely arbitrary. The di\ufb03culty is e speciallysevere in high dimensional spaces where the classi\ufb01er\u2019s decision regio n, or the\ngeneralization rule, can vary with a large degree of freedom. The di\ufb03 culty of\nworking with sparse samples in high dimensional spaces has been addr essed by\nmany other researchers [3] [15] [17].\nIn practical applications, often a problem becomes di\ufb03cult because of a mix-\nture of boundary complexity and sample sparsity e\ufb00ects. Sampling d ensity is\nmore critical for an intrinsically complex problem (e.g. one with many iso lated\nsubclasses) than an intrinsically simple problem (e.g. a linearly separab le prob-\nlem with wide margins), since longer boundaries need more samples to s pecify.\nIf the sample is too sparse, an intrinsically complex problem may appea r decep-\ntively simple, like when representative samples are missing from many is olated\nsubclasses.However,it can alsohappen that an intrinsically simple pr oblem may\nappeardeceptively complex. An example is a linearly separableproblem that ap-\npears to have a nonlinear boundary when represented by a sparse training set.\nThus, in lack of a complete sample, measures of problem complexity ha ve to\nbe quali\ufb01ed by the representativeness of the training set. We will re fer to the\nboundary complexity computed from a \ufb01xed training set as apparent complexity.\nWith a given, \ufb01xed training set, there is little one can do to \ufb01nd out how\nclose the apparent complexity is to the \u201ctrue\u201d complexity. But this d oes not\nprevent one to infer about the true complexity with some con\ufb01denc e, if some\nweak assumptions on the geometry of the class distributions can be made. Here\nwe distinguish such assumptions from the more commonly adopted as sumptions\non the functional form of class distributions (e.g. Gaussians) which can be overly\nstrong.Byweakassumptionsonclassgeometry,wemeanthosepr opertiessuchas\nlocalcompactnessofthe point sets,localcontinuity andpiecewise linearityofthe\nboundaries,all tobe constrainedbyparametersspecifying asmall neighborhood.\nWe believe that even with very conservative assumptions on the geo metrical\nregularity, better uses of limited training samples can be made, and m ore useful\nerror estimates can be obtained than those derived from purely co mbinatorical\narguments emphasizing the worst cases. One should be able do thes e without\ninvoking strong assumptions on the functional form of the distribu tions.\n3 Characterization of Geometrical Complexity\nAmong the di\ufb00erent sources of classi\ufb01cation di\ufb03culty, the geometr ical complex-\nity of class boundaries is probably most ready for detailed investigat ion. Thus\nin this lecture we focus on e\ufb00ective ways for characterizing the geo metrical com-\nplexity of classi\ufb01cation problems.\nWe assume that each problem is represented by a \ufb01xed set of trainin g data\nconsisting of points in a d-dimensional real space Rd, and that each training\npoint is associated with a class label. Furthermore, we assume that w e have a\nsparse sample, i.e., there are unseen points from the same source t hat follow the\nsame (unknown) probability distribution but are unavailable during cla ssi\ufb01er\ndesign. The \ufb01nite and sparse sample limits our knowledge about the bo undary\ncomplexity, thus we are addressing only the apparent geometrical complexity ofa problem based on a given training set. We discuss only two-class pro blems, be-\ncause most of the measures we use are de\ufb01ned only for two-class d iscrimination.\nAn n-class problem produces a matrix of two-class values for each c hosen mea-\nsure. To describe n-class problems, one needs a way to summarize s uch matrices.\nThere are many ways to do so, possibly involving cost matrices. We ac knowledge\nthat the summary by itself is a nontrivial problem.\nOne natural measure of a problem\u2019s di\ufb03culty is the error rate of a ch o-\nsen classi\ufb01er. However, since our eventual goal is to study behav ior of di\ufb00erent\nclassi\ufb01ers, we want to \ufb01nd other measures that are less dependen t on classi\ufb01er\nchoices. Moreover, measures other than classi\ufb01er error rates m ay give hints on\nhow the errors arise, which could lead to improvements in classi\ufb01er de sign, and\ngive guidance on collection of additional samples.\nEarly in our investigations it became clear that there are multiple aspe cts\nof a problem\u2019s complexity that cannot be easily described by a single kn own\nmeasure. Furthermore,while it is easy to constructdi\ufb00erent meas uresfor various\ncharacteristics of a dataset, an arbitrary measure may not nece ssarily correlate\nwell with any complexity scale of a reasonable notion. Such considera tions led\nus to an evaluation of many di\ufb00erent types of measures under a con trol study,\nwhere each measure is computed for a wide range of problems of kno wn levels\nof di\ufb03culty.\nWe constructed a feature (measurement) space for classi\ufb01catio n problems,\nwhere each feature dimension is a speci\ufb01c complexity measure, and e ach prob-\nlem, de\ufb01ned by a labeled training set, is represented by a point in this s pace.\nMost of the individual measures came from the literature of both su pervised\nand unsupervised learning, with a few others de\ufb01ned by us. All meas ures are\nnormalized as far as possible for comparability across problems. The measures\nwe investigated can be divided into several categories:\n1.Measures of overlaps in feaure values from di\ufb00erent classes .These\nmeasuresfocusonthee\ufb00ectivenessofasinglefeaturedimensionin separating\ntheclasses,orthecompositee\ufb00ectsofanumberofdimensions.Th eyexamine\nthe range and spread of values in the dataset w.r.t. each feature, and check\nfor overlaps among di\ufb00erent classes (Table 1).\n2.Measures of separability of classes. These measures evaluate to what\nextent two classes are separable by examining the existence and sh ape of\nthe class boundary. The contributions of di\ufb00erent feature dimens ions are\nsummarized in a distance metric rather than evaluated individually (Ta ble\n2).\n3.Measures of geometry, topology, and density of manifolds. These\nmeasures give indirect characterizations of class separability. The y assume\nthat a point class is made up of single or multiple manifolds which form\nthe support of the probability distribution of the given class. The sh ape,\nposition, and interconnectedness of these manifolds give hints on h ow well\ntwo classes are separated, but they do not describe separability b y design\n(Table 3).F1maximum Fisher\u2019s discriminant ratio\nFisher\u2019s discriminant ratio for one feature dimen-\nsion is de\ufb01ned as:\nf=(\u00b51\u2212\u00b52)2\n\u03c312+\u03c322\nwhere\u00b51,\u00b52,\u03c312,\u03c322are the means and vari-\nances of the two classes respectively, in that fea-\nture dimension. We compute ffor each feature\nand take the maximum as measure F1.For a multi-dimensional\nproblem, notnecessarily all\nfeatures have to contribute\nto class discrimination. As\nlong as there exists one\ndiscriminating feature, the\nproblem is easy. Therefore\nthe maximum fover all\nfeature dimensions is the\none most relevant to class\nseparability.\nF2volume of overlap region\nLet the maximum and minimum values of\neach feature fiin class cjbemax(fi,cj) and\nmin(fi,cj), then the overlap measure F2 is de-\n\ufb01ned to be\nF2 =/producttext\niMINMAX i\u2212MAXMIN i\nMAXMAX i\u2212MINMIN i\nwherei= 1,...,dfor ad-dimensional problem,\nand\nMINMAX i= MIN(max(fi,c1),max(fi,c2))\nMAXMIN i= MAX( min(fi,c1),min(fi,c2))\nMAXMAX i= MAX( max(fi,c1),max(fi,c2))\nMINMIN i= MIN(min(fi,c1),min(fi,c2))F2 measures the amount\nof overlap of the bounding\nboxes of two classes. It is a\nproduct of the per-feature\nratio of the size of the over-\nlapping region over the size\nof total occupied region by\nthe two classes. The vol-\nume is zero as long as there\nis at least one dimension in\nwhich the value ranges of\nthe two classes are distinct.\nF3maximum (individual) feature e\ufb03ciency\nIn a procedure that progressively removes unam-\nbiguous points falling outside the overlapping re-\ngion in each chosen dimension [4], the e\ufb03ciency\nof each feature is de\ufb01ned as the fraction of all re-\nmaining points separable by that feature. To rep-\nresent the contribution of the feature most useful\nin this sense, we use the maximum feature e\ufb03-\nciency(largest fraction of points distinguishable\nwith only one feature) as a measure (F3).This measure con-\nsiders only separating hy-\nperplanes perpendicular to\nthefeatureaxes.Therefore,\neven for a linearly separa-\nble problem, F3 may be\nless than 1 if the optimal\nseparatinghyperplanehap-\npens to be oblique.\nTable 1. Measures of overlaps in feaure values from di\ufb00erent classes .\nMany of these measures have been used before, in isolation, to cha racterize\nclassi\ufb01cation problems. But there have been little serious studies on their ef-\nfectiveness. Some are known to be good only for certain types of d atasets. For\ninstance, Fisher\u2019s discriminant ratio is good for indicating the separa tion be-\ntween two classes each following a Gaussian distribution, but not for two classes\nformingnon-overlappingconcentricrings one inside the other. It is ourhope that\nmore measures used in combination will provide a more complete pictur e about\nclass separation, which determines the di\ufb03culty of classi\ufb01cation.L1minimized sum of error distance by linear programming (LP)\nLinear classi\ufb01ers can be obtained by a linear pro-\ngramming formulation proposed by Smith [16]\nthatminimizes thesumofdistancesoferrorpoints\nto the separating hyperplane (subtracting a con-\nstant margin):\nminimize att\nsubject to Ztw+t\u2265b\nt\u22650\nwherea,bare arbitrary constant vectors (both\nchosen to be 1),wis the weight vector, tis an\nerror vector, and Zis a matrix where each column\nzis de\ufb01ned on an input vector x(augmented by\nadding one dimension with a constant value 1)\nand its class c(with value c1orc2) as follows:\n/braceleftbigg\nz= +xifc=c1\nz=\u2212xifc=c2.\nThe value of the objective function in this formu-\nlation is used as a measure (L1).The measure is zero for\na linearly separable prob-\nlem. Notice that this mea-\nsure can be heavily af-\nfected by outliers that hap-\npentobe onthewrongside\nof the optimal hyperplane.\nWe normalize this measure\nby the number of points in\nthe problem and also by\nthe length of the diagonal\nof the hyperrectangular re-\ngion enclosing all training\npoints in the feature space.\nL2error rate of linear classi\ufb01er by LP\nThis measure is the error rate of the linear clas-\nsi\ufb01er de\ufb01ned for L1, measured with the training\nset.With a small training set\nthis may be a severe under-\nestimate of the true error\nrate.\nN1fraction of points on boundary (MST method)\nThis method constructs a class-blind MST over\nthe entire dataset, and counts the number of\npoints incident to an edge going across the op-\nposite classes. The fraction of such points over all\npoints in the dataset is used as a measure.For two heavily interleaved\nclasses, a ma-\njority of points are located\nnext to the class boundary.\nHowever, the same can be\ntruefor alinearly separable\nproblem with margins nar-\nrower than the distances\nbetween points of the same\nclass.\nN2ratio of average intra/inter class NN distance\nWe \ufb01rst compute the Euclidean distance from\neach point toits nearest neighbor within the class,\nand also to its nearest neighbor outside the class.\nWe then take the average (over all points) of all\nthe distances to intra-class nearest neighbors, and\nthe average of all the distances to inter-class near-\nestneighbors.Theratioofthetwoaveragesisused\nas a measure.This compares the within-\nclass spread to the size of\nthe gap between classes. It\nis sensitive to the classes\nof the closest neighbors to\na point, and also to the\ndi\ufb00erence in magnitude of\nthe between-class distances\nand that of the with-class\ndistances.\nN3error rate of 1NN classi\ufb01er\nThis is simply the error rate of a nearest-neighbor\nclassi\ufb01er measured with the training set.The error rate is esti-\nmated by the leave-one-out\nmethod.\nTable 2. Measures of class separability.L3nonlinearity of linear classi\ufb01er by LP\nHoekstra and Duin [9] proposed a measure for the\nnonlinearity ofaclassi\ufb01er w.r.t. toagivendataset.\nGiven a training set, the method \ufb01rst creates a\ntest set by linear interpolation (with random coef-\n\ufb01cients) between randomly drawn pairs of points\nfrom the same class. Then the error rate of the\nclassi\ufb01er (trained by the given training set) on\nthis test set is measured. Here we use such a non-\nlinearity measure for the linear classi\ufb01er de\ufb01ned\nfor L1.This measure is sensitive\nto the smoothness of the\nclassi\ufb01er\u2019s decision bound-\nary as well as the over-\nlap of the convex hulls of\nthe classes. For linear clas-\nsi\ufb01ers and linearly separa-\nble problems, it measures\nthe alignment of the deci-\nsion surface with the class\nboundary. It carries the ef-\nfects of the training proce-\ndure in addition to those of\nthe class separation.\nN4nonlinearity of 1NN classi\ufb01er\nThis is the nonlinearity measure, as de\ufb01ned for\nL3, calculated for a nearest neighbor classi\ufb01er.This measure is for the\nalignment of the nearest-\nneighbor boundary with\nthe shape of the gap or\noverlap betweentheconvex\nhulls of the classes.\nT1fraction of points with associated adherence subsets retai ned\nThis measure originated from a work on describ-\ning shapes of class manifolds based on a notion\nofadherence subsets in pretopology [11]. Simply\nspeaking, it counts the number of balls needed to\ncover each class, where each ball is centered at a\ntraining point and grown to the maximal size be-\nfore it touches another class. Redundant balls ly-\ning completely in the interior of other balls are re-\nmoved. We normalize the count by the total num-\nber of points.Alistofsuchballs isacom-\nposite description of the\nshape of the classes. The\nnumber and size of the\nballs indicate how much\nthe points tend to cluster\nin hyperspheres or spread\ninto elongated structures.\nIn a problem where each\npoint is closer to points of\nthe other class than points\nof its own class, each point\nis covered by a distinctive\nball of a small size, result-\ning in a high value of the\nmeasure.\nT2average number of points per dimension\nThis is a simple ratio of the number of points\nin the dataset over the number of feature dimen-\nsions.This measure is included\nmostly for connection with\nprior studies on sample\nsizes. Since the volume of\na region scales exponen-\ntially with the number of\ndimensions, a linear ratio\nbetween the two is not a\ngood measure of sampling\ndensity.\nTable 3. Measures of geometry, topology, and density of manifolds.4 Datasets for Validating Complexity Measures\nWe evaluated the e\ufb00ectivenessofthe complexity measureswith two collectionsof\nclassi\ufb01cation problems. The \ufb01rst collection includes all pairwise discrim ination\nproblems from 14 datasets in the UC-Irvine Machine Learning Depos itory [2].\nThe datasets are those that contain at least 500 points with no miss ing values:\nabalone, car, german, kr-vs-kp, letter, lrs, nursery, pima , segmentation, splice,\ntic-tac-toe, vehicle, wdbc, and yeast . Categorical features in some datasets are\nnumerically coded. There are altogether 844 two-class discriminatio n problems,\nwith training set sizes varying from 2 to 4648, and feature space dim ensionality\nvarying from 8 to 480. Using the linear programming procedure by Sm ith [16]\n(as given in the description of the L1 measure in Table 2), 452 out of t he 844\nproblems are found to be linearly separable. The class boundary in ea ch of these\nproblems, as far as the training set is concerned, can be described entirely by the\nweight vector of the separating hyperplane, so by Kolmogorov\u2019sno tion these are\nsimple problems. Thus a valid complexity measure should place these pr oblems\nat one end of its scale.\nThe second collectionconsistsof 100arti\ufb01cial two-classproblemse achhaving\n1000 points per class. Problem 1 has one feature dimension, problem 2 has two,\nso forth and the last problem contains 100 features. Each featur e is a uniformly\ndistributed pseudorandom number in [0 ,1]. The points are randomly labeled,\nwith equal probability, as one of two classes. Therefore, these ar e intrinsically\ncomplex problems, and they are expected to locate at the other en d of any\ncomplexity scale.\nWe studied the complexity measureson the distribution ofthese thr ee groups\nof problems, namely, (1) UCI linearly separable, (2) UCI linearly nons eparable,\nand (3) random labelings. A single measure is considered useful for d escribing\nproblemcomplexityifthethreegroupsofproblemsareseparableon itsscale,and\na set of measures are considered useful if the groups of problems are separable\nin the space spanned by the set.\n5 Key Observations\nThedistributionofthethreegroupsofclassi\ufb01cationproblemsinthis 12-dimensional\ncomplexity space displays many interesting characteristics. A deta iled descrip-\ntion of the observations in this study can be found in [6]. Here we summ arize\nthe main \ufb01ndings.\n5.1 Continuum of problem locations in complexity space\nThe \ufb01rst remarkable observation in this study is that the datasets fall on a\ncontinuum of positions along many dimensions of the complexity space . Even\nthough there have been no special selection criteria imposed on the se naturally\narising datasets, we \ufb01nd that the problems cover a large range of v alues in al-\nmost all the chosen complexity scales. This reminds us of the challeng es in thepractice of pattern recognition: to pursue a good match of metho ds to prob-\nlems, we must make sure that the classi\ufb01er methodology we choose is robust to\nvariations in these problem characteristics, or we must understan d the nature of\nthe dependence of classi\ufb01er behavior on such variations. Without a ccomplishing\neither, applications of classi\ufb01ers to problems are nothing but a blind m atch, and\nthere is little hope of ensuring highest success.\nAmoreencouragingobservationisthatmanyofthereal-world(UCI )datasets\nare located far away from the random labelings, suggesting that th ese practical\nproblems do indeed contain some intrinsic, learnable structure.\nInterestingly, there is substantial spread among the random labe lings of dif-\nferent dimensionality. While there is no obvious explanation for how dim en-\nsionality a\ufb00ects their intrinsic di\ufb03culties, closer examination of the di\ufb00 erences\nsuggests that this is more an e\ufb00ect of di\ufb00erences in apparent comp lexity due to\ndi\ufb00erent sampling densities, since these datasets all have the same size while the\nvolume of the space increases exponentially with dimensionality.\n5.2 E\ufb00ectiveness of individual measures in separating prob lems of\nknown levels of di\ufb03culty\nThe concentrations of the three groups of datasets (UCI linearly separable, UCI\nlinearly nonseparable, and random labelings) in di\ufb00erent regions in the com-\nplexity space suggest that many of the measures can reveal their di\ufb00erences. As\na stand-alone scale of complexity, several measures (F1,F2,F3,L2,L 3) are espe-\ncially e\ufb00ective in separating at least two of the three groups, with th e easiest set\n(UCI linearly separable) and the most di\ufb03cult set (random labelings) o ccupying\ntwo opposite ends of the scale. However, none of the measures ca n completely\nseparate the three groups with no overlaps. Some measures, suc h as N4 and T2,\nare especially weak when used in isolation.\n5.3 Distorted nearest neighbor error rates of sparse datase ts\nThe nearest-neighbor related measures (N1,N2,N3) have almost th e same dis-\ncriminating power for the three groups, except for a few peculiar c ases where the\ntraining set consists of only 2 or 3 points. For those extremely spar se datasets,\nalthough the class boundary (for the training set) is linear, the nea rest neigh-\nbors are almost always in a wrong class, thus the nearest-neighbor error rate\nbecomes very high. This is an artifact of the leave-one-out estimat e. However, it\nalso suggests that a single error rate, even that of a simple and well- understood\nclassi\ufb01er, may tell a distorted story about the data complexity.\n5.4 Pairwise Correlations between complexity measures\nBivariate plots of the distributions show that some pairs of measure s, such as\nL2 and L3, or N1 and N3, are strongly correlated, while little correlat ion is seen\nbetween many other pairs (Table 4). The existence of many uncorr elated pairssuggests that there are more than one independent factors a\ufb00e cting a problem\u2019s\ncomplexity.\nAn examination of the correlation between L2 (linear classi\ufb01er error rate)\nand N3 (nearest neighbor error rate) and between each of these two measures\nand others suggests that these error rates are not perfectly c orrelated, nor are\nthey always predictable by an arbitrary measure. This recon\ufb01rms t he risk of\nrelying on simple classi\ufb01er error rates for complexity measurement. These two\nclassi\ufb01ers, operating on very di\ufb00erent principles (linearity versus p roximity),\nhave di\ufb03culties caused by di\ufb00erent characteristics of a problem (Fig ure 3).\nSome measures, while on their own are very weak in separating all thr ee\ngroups of problems, can reveal the group di\ufb00erences when used in combination\nwith other measures (Figure 4). This demonstrates the importanc e of examining\nthe multiple aspects of a problem\u2019s complexity jointly.\nThe measure T1, while on its own being a strong separator of the thr ee\ngroups, characterizes a very di\ufb00erent aspect of complexity from others as evi-\ndenced by its weak correlation with others. Inspection of the plots involving T1\nand others suggests that while the shapes of the classes can vary a lot across\ndi\ufb00erent problems, it is less relevant to classi\ufb01cation accuracy than the shapes\nof the class boundaries.\nF1 F2 F3 L1 L2 L3 N1 N2 N3 N4 T1 T2\nF11.00 -0.02 0.06 -0.01 -0.02 -0.02 0.07 0.01 0.14 -0.02 0.03 -0 .03\nF2 1.00 -0.53 0.07 0.91 0.91 0.69 0.71 0.61 0.17 0.28 0.19\nF3 1.00 -0.24 -0.65 -0.62 -0.39 -0.69 -0.29 -0.40 -0.68 -0.28\nL1 1.00 0.33 0.32 0.37 0.37 0.28 0.53 0.18 -0.10\nL2 1.00 1.00 0.78 0.81 0.67 0.47 0.37 0.16\nL3 1.00 0.78 0.81 0.67 0.46 0.35 0.16\nN1 1.00 0.76 0.96 0.49 0.39 -0.02\nN2 1.00 0.68 0.51 0.55 0.12\nN3 1.00 0.38 0.38 -0.04\nN4 1.00 0.28 0.30\nT1 1.00 0.17\nT2 1.00\nTable 4. Correlation coe\ufb03cients between each pair of measures.\n5.5 Principal components of the complexity space\nA principal component analysis using the distribution of the problems in the 12-\ndimensional space shows that there are six signi\ufb01cant components each explain-\ningmorethan5%ofthevariance.Amongthese,the\ufb01rstcomponen texplainsover\n50%ofthevariance,andcomprisesevencontributionsfromF2,L2,L 3,N1,N2,and\nN3. It is a combination of e\ufb00ects of linearity of class boundaries and p roximity0.00010.0010.010.11\n0.0001 0.001 0.01 0.1 1error of 1NN classifier\nerror of LPme classifier0.00010.0010.010.11\n0.0001 0.001 0.01 0.1 1nonlinearity of LPme classifier\nerror of LPme classifier\n0.00010.0010.010.11\n0.0001 0.001 0.01 0.1 1error of 1NN classifier\nfraction of pts on boundary0.00010.0010.010.11\n0.0001 0.001 0.01 0.1 1fraction of pts on boundary\nnonlinearity of LPme classifier\nFig.3.Error rates of linear and nearest-neighbor classi\ufb01ers, and the measures they are\nmost strongly correlated with. ( \u22c4: UCI linearly separable problems; +: UCI linearly\nnonseparable problems; \u2737: random labelings)\nbetween opposite class neighbors. The next 3 components explain 1 2%, 11% and\n9% of the variance respectively, and can be interpreted as (PC2:) a balance of\nwithin-class and between-class scatter, (PC3:) the concentratio n and orientation\nof class overlaps, and (PC4:) within-class scatter. For a more deta iled discussion\nof these components, as well as for the trajectory traced in the PC projection\nby an example series of problems with controlled class separation, we again refer\nreaders to [6].\n6 Studies of Problems and Classi\ufb01ers Using Complexity\nMeasures\nA complexity measurement space like this has many potentially interes ting uses.\nFor a particular application domain, the scales of complexity can help d etermine\nthe existenceofanylearnablestructure,whichcanbe used toset expectationson\nautomaticlearningalgorithms.Theycanalsobe usedtodetermine ifa particular\ndataset is suitable for evaluating di\ufb00erent learning algorithms.\nThe measurescan be used to comparedi\ufb00erent problem formulation s, includ-\ning class de\ufb01nitions, choice of features, and potential feature tr ansformations.\nThey can be used to guide the selection of classi\ufb01ers and classi\ufb01er co mbination0.00010.0010.010.11\n0.0001 0.01 1nonlinearity of LPme classifier\nvolume of overlap region0.010.1110100100010000\n0.0001 0.001 0.01 0.1 1avg no. of points per dim\nfraction of pts on boundary\nFig.4.Groups of problems that overlap heavily on an individual com plexity scale\nmay show clear separation in the interaction of the e\ufb00ects. ( \u22c4: UCI linearly separable\nproblems; +: UCI linearly nonseparable problems; \u2737: random labelings)\nschemes, or control the process of classi\ufb01er training. A use of th ese measures for\ncomparing two methods for decision forest construction is report ed in [7].\nRegions occupied by datasets on which classi\ufb01ers display homogeneo us per-\nformances can be used to outline the domain of competences of tho se classi\ufb01ers,\nwith the expectation that performances on new datasets falling in t he same re-\ngion can be predicted accordingly. Regions where no classi\ufb01ers can d o well may\nbe characterized in detail by the complexity measures, which could le ad to new\nclassi\ufb01er designs covering those blind spots. In [14] we report a stu dy of the\ndomain of competence of XCS, a genetic algorithm based classi\ufb01er.\nOne may wish to study the distribution of all classi\ufb01cation problems in t his\nspace. An empirical approach will be to seek a representation of th e distribution\nby a much larger collection of practical problems. A theoretical app roach will\nbe more challenging; it involves reasoning about regions in this space t hat are\npossible or impossible for any dataset to occupy. The identi\ufb01cation o f such re-\ngions will require a better understanding of constraints in high-dime nsional data\ngeometry and topology. The intrinsic dimensionality of the problem dis tribution\nwill give more conclusive evidence on how many independent factors c ontribute\nto a problem\u2019s di\ufb03culty.\n7 Conclusions\nWe describe some early investigationinto the complexity of a classi\ufb01ca tion prob-\nlem, with emphasis on the geometrical characteristics that can be m easured di-\nrectly from a training set. We took some well known measures from t he pattern\nrecognition literature, and studied their descriptive power using a c ollection of\nproblems of known levels of di\ufb03culty. We found some interesting spre ad among\nthe di\ufb00erent types of problems, and evidence of existence of indep endent factors\na\ufb00ecting a problem\u2019s di\ufb03culty. We believe that such descriptors of co mplexity\nare useful for identifying and comparing di\ufb00erent classes of proble ms, character-izing the domain of competence of classi\ufb01er or ensemble methods, an d in many\nways guiding the development of a solution to a pattern recognition p roblem.\nThese are our \ufb01rst steps towards developing elements of a languag e with\nwhich we can talk more precisely about properties of high dimensional datasets,\nespecially those aspects a\ufb00ecting classi\ufb01er performances. We belie ve this is nec-\nessary for classi\ufb01cation research to advance beyond the curren t plateau. Finally,\nwe believe that such abstract studies are best coupled with tools fo r interactive\nvisualization of a dataset, so that an intuitive understanding may be obtained\non how the complexity arises from a particular problem [8]."}
{"category": "abstract", "text": "Despiteencouragingrecentprogressesinensembleapproaches, classi\ufb01cation\nmethods seem to have reached a plateau in development. Further a dvances\ndepend on a better understanding of geometrical and topological charac-\nteristics of point sets in high-dimensional spaces, the preservatio n of such\ncharacteristics under feature transformations and sampling pro cesses, and\ntheir interaction with geometrical models used in classi\ufb01ers. We discu ss\nan attempt to measure such properties from data sets and relate them to\nclassi\ufb01er accuracies.\n1 Introduction\nAdvances in ensemble learning have produced a signi\ufb01cant rise in class i\ufb01cation\naccuracy from those achieved when only monolithic classi\ufb01ers are kn own. How-\never, after the past decade of development, most methods seem s to have reached\nmaturity, so that no signi\ufb01cant improvements are expected to res ult from incre-\nmental modi\ufb01cations. Often, for a certain benchmark problem, on e can see many\nmethods in close rivalry, producing more or less the same level of acc uracy. Al-\nthough continuous attempts are being made on interpreting existin g techniques,\ntesting known methods on new applications, or mix-matching di\ufb00eren t strate-\ngies, no revolutionary breakthrough appears to be in sight. It almo st seems that\na plateau has been reached in classi\ufb01cation research, and question s like these\nbegin to linger in our minds"}
{"category": "non-abstract", "text": "Camera Calibration\nDepending on whatkind ofcalibrationobjectused, there\nare mainly two categories of calibration methods: pho-\ntogrammetric calibration and self-calibration. Photogram-\nmetric calibration refers to those methods that observe a\ncalibration object whose geometry in 3-D space is known\nwith a very good precision [1]. Self-calibration does not\nneed any calibration object. It only requires point matches\nfrom image sequence. In [2], it is shown that it is possible\nto calibrate a camera just by pointing it to the environ-\nment, selecting points of interest and then tracking them\nin the image as the camera moves. The obvious advantage\nof the self-calibration method is that it is not necessary\nto know the camera motion and it is easy to set up. The\ndisadvantage is that it is usually considered unreliable [3].\nA four step calibration procedure is proposed in [4] where\nthe calibration is performed with a known 3D target. The\nfour steps in [4] are: linear parameter estimation, nonlin-\near optimization, correction using circle/ellipse, and image\ncorrection. But for a simple start, linear parameter es-\ntimation and nonlinear optimization are enough. In [5],\na plane-based calibration method is described where the\ncalibration is performed by \ufb01rst determining the absolute\nconicB=A\u2212TA\u22121, whereAis a matrix formed by the\ncamera\u2019s intrinsic parameters. In [5], the parameter \u03b3(aparameter describing the skewness of the two image axes)\nis assumed to be zero and it is observed that only the rel-\native orientations of planes and camera are of importance\nin avoiding singularities because the planes that are par-\nallel to each other provide exactly the same information.\nThe camera calibration method in [6], [7] is regarded as\na great contribution to the camera calibration. It focuses\non the desktop vision system and advances 3D computer\nvision one step from laboratory environments to the real\nworld. The proposed method in [6], [7] lies between the\nphotogrammetric calibration and the self-calibration, be-\ncause 2D metric information is used rather than 3D. The\nkey feature of the calibration method in [6], [7] is that the\nabsolute conic Bis used to estimate the intrinsic parame-\nters and the parameter \u03b3can be considered. The proposed\ntechnique in [6], [7] only requires the camera to observe a\nplanar pattern at a few (at least 3, if both the intrinsic\nand the extrinsic parameters are to be estimated uniquely)\ndi\ufb00erent orientations. Either the camera or the calibration\nobject can be moved by hand as long as they cause no sin-\ngularity problem and the motion of the calibration object\nor camera itself needs not to be known in advance.\nAfter estimation of camera parameters, a projection ma-\ntrixMcan directly link a point in the 3-D world reference\nframe to its projection (undistorted) in the image plane.\nThat is\n\u03bb\uf8ee\n\uf8f0u\nv\n1\uf8f9\n\uf8fb=M\uf8ee\n\uf8ef\uf8f0Xw\nYw\nZw\n1\uf8f9\n\uf8fa\uf8fb=A[R t]\uf8ee\n\uf8ef\uf8f0Xw\nYw\nZw\n1\uf8f9\n\uf8fa\uf8fb\n=\uf8ee\n\uf8f0\u03b1 \u03b3 u 0\n0\u03b2 v0\n0 0 1\uf8f9\n\uf8fb[R t]\uf8ee\n\uf8ef\uf8f0Xw\nYw\nZw\n1\uf8f9\n\uf8fa\uf8fb,(1)\nwhere\u03bbis an arbitrary scaling factor and the matrix A\nfully dependsonthe 5intrinsicparameterswith theirdetail\ndescriptions in Table I, where some other variables used\nthroughout this paper are also listed.\nThe calibration method used in this work is to \ufb01rst esti-\nmate the projection matrix and then use the absolute conic\nto estimate the intrinsic parameters[6], [7]. The detail pro-\ncedures are summarized below:\u2022Linear Parameter Estimation,\n\u2013Estimation of Intrinsic Parameters;\n\u2013Estimation of Extrinsic Parameters;\n\u2013Estimation of Distortion Coe\ufb03cients;\n\u2022Nonlinear Optimization.\nTABLE I\nList of Variables\nVariable Description\nPw= [Xw,Yw,Zw]T3-D point in world frame\nPc= [Xc,Yc,Zc]T3-D point in camera frame\nk= (k1, k2) Distortion coe\ufb03cients\n(ud, vd) Distorted image points\n(u, v) Undistorted image points\n(xd, yd)/bracketleftbiggxd\nyd\n1/bracketrightbigg\n=A\u22121/bracketleftbiggud\nvd\n1/bracketrightbigg\n(x, y)/bracketleftbiggx\ny\n1/bracketrightbigg\n=A\u22121/bracketleftbiggu\nv\n1/bracketrightbigg\nr r2=x2+y2\n\u03b1,\u03b2,\u03b3,u 0,v0 5 intrinsic parameters\nJ Objective function\nA=/bracketleftbigg\u03b1 \u03b3 u 0\n0\u03b2 v0\n0 0 1/bracketrightbigg\nCamera intrinsic matrix\nB=A\u2212TA\u22121Absolute conic\nM Projection matrix\nB. Radial Distortion\nRadial distortion causes an inward or outward displace-\nment of a given image point from its ideal location. The\nnegative radial displacement of the image points is referred\nto as the barrel distortion, while the positive radial dis-\nplacement is referred to as the pincushion distortion [8].\nThe radial distortion is governed by the following equation\n[6], [8]:\nF(r) =rf(r) =r(1+k1r2+k2r4+k3r6+\u00b7\u00b7\u00b7),(2)\nwherek1,k2,k3,...are the distortion coe\ufb03cients and r2=\nx2+y2with (x,y) the normalized undistorted projected\npoints in the camera frame. The distortion is usually dom-\ninated by the radial components, and especially dominated\nby the \ufb01rst term. It has also been found that too high an\norder in (2) may cause numerical instability [7], [9], [10].\nIn this paper, at most two terms of radial distortion are\nconsidered. When using two coe\ufb03cients, the relationship\nbetween the distorted and the undistorted image points\nbecomes [6]\nud\u2212u0= (u\u2212u0)(1+k1r2+k2r4)\nvd\u2212v0= (v\u2212v0)(1+k1r2+k2r4).(3)\nWhen using two distortion coe\ufb03cients to model radial dis-\ntortion asin [6], [11], the inverseofthe polynomial function\nin (3) isdi\ufb03cult toperformanalytically. In[11], the inversefunction isobtainednumericallyviaaniterativescheme. In\n[12], for practical purpose, only one distortion coe\ufb03cient\nk1is used. Besides the polynomial approximation method\nmentioned above,a techniquefor blindly removinglensdis-\ntortion in the absence of any calibration information in the\nfrequency domain is presented in [13]. However, the accu-\nracy reported in [13] is by no means comparable to that\nbased on calibration and this approach can be useful in ar-\neas where only qualitative results are required. The new\nradial distortion model proposed in this paper belongs to\nthe polynomial approximation category.\nThe rest of the paper is organized as follows. Sec. II\ndescribes the new radial distortion model and its inverse\nundistortion analytical formula. Experimental results and\ncomparison with existing models are presented in Sec. III.\nOne direct application of this new distortion model is dis-\ncussed in Sec. IV. Finally, some concluding remarks are\ngiven in Sec. V.\nII. Radial Distortion Models\nIn this paper, we focus on the distortion models while\nthe intrinsic parameters and the extrinsic parameters are\nachieved using the method presented in [6], [7]. According\nto the radial distortion model in (3), the radial distortion\ncan be resulted in one of the following two ways:\n\u2022Transform from the camera frame to the image plane,\nthen perform distortion in the image plane\n/bracketleftbigg\nx\ny/bracketrightbigg\n\u2192/bracketleftbigg\nu\nv/bracketrightbigg\n\u2192/bracketleftbigg\nud\nvd/bracketrightbigg\n;\n\u2022Perform distortion in the camera frame, then transform\nto the image plane\n/bracketleftbigg\nx\ny/bracketrightbigg\n\u2192/bracketleftbigg\nxd\nyd/bracketrightbigg\n\u2192/bracketleftbigg\nud\nvd/bracketrightbigg\n,\nwhere\nxd=xf(r), yd=yf(r). (4)\nSince\uf8ee\n\uf8f0u\nv\n1\uf8f9\n\uf8fb=A\uf8ee\n\uf8f0x\ny\n1\uf8f9\n\uf8fb=\uf8ee\n\uf8f0\u03b1 \u03b3 u 0\n0\u03b2 v0\n0 0 1\uf8f9\n\uf8fb\uf8ee\n\uf8f0x\ny\n1\uf8f9\n\uf8fb,\n(3) becomes\nud= (u\u2212u0)f(r)+u0\n=\u03b1xf(r)+\u03b3yf(r)+u0\n=\u03b1xd+\u03b3yd+u0,\nvd= (v\u2212v0)f(r)+v0\n=\u03b2yd+v0. (5)\nTherefore, it is also true that\n\uf8ee\n\uf8f0ud\nvd\n1\uf8f9\n\uf8fb=A\uf8ee\n\uf8f0xd\nyd\n1\uf8f9\n\uf8fb.\nThus, the distortion performed in the image plane can\nalso be understood as introducing distortion in the cam-\nera frame and then transform back to the image plane.A. The Existing Radial Distortion Models\nRadial undistortion is to extract ( u,v) from ( ud,vd),\nwhich can also be accomplished by extracting ( x,y) from\n(xd,yd). The following derivation shows the problem when\ntrying to extract ( x,y) from (xd,yd) using two distortion\ncoe\ufb03cients k1andk2in (3).\nFrom (ud,vd), we can calculate ( xd,yd) by\n\uf8ee\n\uf8f0xd\nyd\n1\uf8f9\n\uf8fb=A\u22121\uf8ee\n\uf8f0ud\nvd\n1\uf8f9\n\uf8fb=\uf8ee\n\uf8f01\n\u03b1\u2212\u03b3\n\u03b1\u03b2\u2212u0\n\u03b1+v0\u03b3\n\u03b1\u03b2\n01\n\u03b2\u2212v0\n\u03b2\n0 0 1\uf8f9\n\uf8fb\uf8ee\n\uf8f0ud\nvd\n1\uf8f9\n\uf8fb,(6)\nwhere the camera intrinsic matrix Ais invertible by na-\nture. Now, the problem becomes to extracting ( x,y) from\n(xd,yd). According to (4),\nxd=xf(r) =x[1+k1(x2+y2)+k2(x2+y2)2]\nyd=yf(r) =y[1+k1(x2+y2)+k2(x2+y2)2].(7)\nIt is obvious that xd= 0 i\ufb00x= 0. When xd/ne}ationslash= 0, by letting\nc=yd/xd=y/x, we have y=cxwherecis a constant.\nSubstituting y=cxinto the above equation gives\nxd=x[1+k1(x2+c2x2)+k2(x2+c2x2)2]\n=x+k1(1+c2)x3+k2(1+c2)2x5.(8)\nLetf(x) =x+k1(1+c2)x3+k2(1+c2)2x5. Thenf(\u2212x) =\n\u2212f(x) andf(x) is an odd function. The analytical solution\nof (8) is not a trivial task. This analytical problem is still\nopen (of course, we can use numerical method to solve it).\nBut if we set k2= 0, the analytical solution is available\nand the radial undistortion can be done easily. In [12], for\nthe same practical reason, only one distortion coe\ufb03cient\nk1is used to approximate the radial distortion, in which\ncase we would expect to see performance degradation. In\nSec. III, experimental results are presented to show the\nperformance comparison for the cases when k2= 0 and\nk2/ne}ationslash= 0 using the calibrated parameters of three di\ufb00erent\ncameras. Recallthattheinitialguessforradialdistortionis\ndoneafterhavingestimatedallotherparameters(including\nboth intrinsicand extrinsicparameters)and just before the\nnonlinearoptimizationstep. So, wecanreusetheestimated\nparameters and choose the initial guess for k2to be 0 and\ncompare the values of objective function after nonlinear\noptimization.\nThe objective function used for nonlinear optimization\nis [6]:\nJ=N/summationdisplay\ni=1n/summationdisplay\nj=1||mij\u2212\u02c6m(A,k1,k2,Ri,ti,Mj)||2,(9)\nwhere \u02c6m(A,k1,k2,Ri,ti,Mj) is the projection of point Mj\nin theithimage using the estimated parameters and Mjis\nthejth3D point in the world frame with Zw= 0. Here, n\nis the number of feature points in the coplanar calibration\nobject and Nis the number of imagestaken forcalibration.B. The New Radial Distortion Model\nOur new radial distortion model is proposed as:\nF(r) =rf(r) =r(1+k1r+k2r2),(10)\nwhich is also a function only related to radius r. The mo-\ntivation of choosing this radial distortion model is that the\nresultant approximation of xdis also an odd function of x,\nas can be seen next. For F(r) =rf(r) =r(1+k1r+k2r2),\nwe have\nxd=xf(r) =x(1+k1r+k2r2)\nyd=yf(r) =y(1+k1r+k2r2).(11)\nAgain, let c=yd/xd=y/x. We have y=cxwherecis\na constant. Substituting y=cxinto the above equation\ngives\nxd=x/bracketleftBig\n1+k1/radicalbig\nx2+c2x2+k2(x2+c2x2)/bracketrightBig\n=x/bracketleftBig\n1+k1/radicalbig\n1+c2sgn(x)x+k2(1+c2)x2/bracketrightBig\n=x+k1/radicalbig\n1+c2sgn(x)x2+k2(1+c2)x3,(12)\nwheresgn(x) gives the sign of x. Let\nf(x) =x+k1/radicalbig\n1+c2sgn(x)x2+k2(1+c2)x3.\nClearly,f(x) is also an odd function.\nTo perform the radial undistortion using the new distor-\ntion model in (10), that is to extract xfromxdin (12), the\nfollowing algorithm is applied:\n1)x= 0 i\ufb00xd= 0,\n2)Assuming that x >0, (12) becomes\nxd=x+k1/radicalbig\n1+c2x2+k2(1+c2)x3.\nUsingsolve, a Matlab Symbolic Toolbox function, we can\nget three possible solutions for the above equation denoted\nbyx1+,x2+, andx3+respectively. To make the equations\nsimple, let y=xd,p=k1\u221a\n1+c2andq=k2(1+c2). The\nthree possible solutions for y=x+px2+qx3are\nx1+=1\n6qE1+2\n3E2\u2212p\n3q,\nx2+=\u22121\n12qE1\u22121\n3E2\u2212p\n3q+\u221a\n3\n2(1\n6qE1\u22122\n3E2)j,(13)\nx3+=\u22121\n12qE1\u22121\n3E2\u2212p\n3q\u2212\u221a\n3\n2(1\n6qE1\u22122\n3E2)j,\nwhere\nE1={36pq+108yq2\u22128p3\n+12\u221a\n3q/radicalbig\n4q\u2212p2+18pqy+27y2q2\u22124yp3}1/3,(14)\nE2=p2\u22123q\nqE1,j=\u221a\n\u22121.\nFrom the above three possible solutions, we discard those\nwhose imaginary parts are not equal to zero. Then, from\nthe remaining, discard those solutions that con\ufb02ict with\nthe assumption that x >0. Finally, we get the candidatesolutionx+by choosing the one closest to xdif the number\nof remaining solutions is greater than 1.\n3)Assuming that x <0, there are also three possible so-\nlutions for\nxd=x\u2212k1/radicalbig\n1+c2x2+k2(1+c2)x3,(15)\nwhich can be written as\ny=x+(\u2212p)x2+qx3. (16)\nThe three solutions for (16) can thus be calculated from\n(13) and (14) by substituting p=\u2212p. With a similar\nprocedure as described in the case for x >0, we will have\nanother candidate solution x\u2212.\n4)Choose among x+andx\u2212for the \ufb01nal solution of xby\ntaking the one closest to xd.\nThe basic idea to extract xfromxdin (12) is to choose\nfrom several candidate solutions, whose analytical formula\nare known. The bene\ufb01ts of using this new radial distortion\nmodel are as follows:\n\u2022Low order \ufb01tting, better for \ufb01xed-point implementation;\n\u2022Explicit or analytical inverse function with no numerical\niterations;\n\u2022Better accuracy than using the radial distortion model\nf(r) = 1+k1r2.\nIII. Experimental Results and Comparisons\nNow, we want to compare the performance of three dif-\nferent radial distortion models based on the \ufb01nal value of\nobjectivefunction after nonlinearoptimizationby the Mat-\nlabfunction fminunc. Thethreedi\ufb00erentdistortionmodels\nfor comparison are:\ndistortionmodel 1:f(r) = 1+k1r2+k2r4,\ndistortionmodel 2:f(r) = 1+k1r2,\ndistortionmodel 3:f(r) = 1+k1r+k2r2.\nUsing the public domain test images [14], the desktop cam-\nera images [15] (a color camera in our CSOIS), and the\nODIS camera images [15] (the camera on ODIS robot built\nin our CSOIS, see Sec. IV-A and Fig. 1), the \ufb01nal ob-\njective function ( J), the 5 estimated intrinsic parameters\n(\u03b1,\u03b2,\u03b3,u 0,v0), and the estimated distortion coe\ufb03cients\n(k1,k2) areshowninTablesII, III, andIVrespectively[15].\nThe results show that the objective function of model 3is\nalways greater than that of model 1, but much smaller than\nthat ofmodel 2, which is consistent with our expectation.\nNote that, when doing nonlinear optimization with di\ufb00er-\nent distortion models, we always use the same exit thresh-\nolds.\nTo make the results in this paper repeatable\nby other researchers for further investigation, we\npresent the options we use for the nonlinear op-\ntimization: options = optimset(\u2018Display\u2019, \u2018iter\u2019,\n\u2018LargeScale\u2019, \u2018off\u2019, \u2018MaxFunEvals\u2019, 8000, \u2018TolX\u2019,\n10\u22125, \u2018TolFun\u2019, 10\u22125, \u2018MaxIter\u2019, 120) . The raw\ndata of the extracted feature locations in the image plane\nare also available upon request.A second look at the results reveals that for the cam-\nera used in [6], [7], [14], which has a small lens distortion,\nthe advantage of model 3overmodel 2is not so signi\ufb01cant.\nWhen the cameras are experiencing severe distortion, the\nradial distortion model 3gives a much better performance\novermodel 2, as can be seen from Tables III and IV.\nFig. 1\nThe mechanical and vetronics layout of ODIS\nTABLE II\nComparison of Distortion Models Using Images in [14]\u2217\nMicrosoft Images\nModel #1 #2 #3\nJ144.88 148.279 145.659\n\u03b1832.5010 830.7340 833.6623\n\u03b3 0.2046 0.2167 0.2074\nu0303.9584 303.9583 303.9771\n\u03b2832.5309 830.7898 833.6982\nv0206.5879 206.5692 206.5520\nk1-0.2286 -0.1984 -0.0215\nk2 0.1903 0-0.1565\n\u2217(k1,k2) formodel1andmodel2are de\ufb01ned in (2) and ( k1,k2) for\nmodel3is de\ufb01ned in (10).\nIV. Application: Non-iterative Yellow Line\nAlignment with a Calibrated Camera on\nODIS\nA. What is ODIS?\nThe Utah State University Omni-Directional Inspection\nSystem) (USU ODIS) is a small, man-portable mobile\nrobotic system that can be used for autonomous or semi-\nautonomous inspection under vehicles in a parking area\n[16], [17], [18]. The robot features (a) three \u201csmart wheels\u201d\n[19] in which both the speed and direction of the wheel\ncan be independently controlled through dedicated pro-\ncessors, (b) a vehicle electronic capability that includes\nmultiple processors, and (c) a sensor array with a laser,\nsonar and IR sensors, and a video camera. A unique fea-\nture in ODIS is the notion of the \u201csmart wheel\u201d developedTABLE III\nComparison of Distortion Models Using Desktop Images in [15 ]\nDesktop Images\nModel #1 #2 #3\nJ778.9768 904.68 803.307\n\u03b1277.1457 275.5959 282.5664\n\u03b3 -0.5730 -0.6665 -0.6201\nu0153.9923 158.2014 154.4891\n\u03b2270.5592 269.2307 275.9040\nv0119.8090 121.5254 120.0952\nk1 -0.3435 -0.2765 -0.1067\nk2 0.1232 0-0.1577\nTABLE IV\nComparison of Distortion Models Using ODIS Images in [15]\nODIS Images\nModel #1 #2 #3\nJ840.2650 933.098 851.262\n\u03b1260.7636 258.3206 266.0861\n\u03b3 -0.2739 -0.5166 -0.3677\nu0140.0564 137.2155 139.9177\n\u03b2255.1465 252.6869 260.3145\nv0113.1723 115.9295 113.2417\nk1 -0.3554 -0.2752 -0.1192\nk2 0.1633 0-0.1365\nby the Center for Self-Organizing and Intelligent Systems\n(CSOIS) at USU which has resulted in the so-called T-\nseries of omni-directional (ODV) robots [19]. With the\nODV technique, our robots including ODIS, can achieve\ncomplete control of the vehicle\u2019s orientation and motion in\na plane, thus making the robots almost holonomic - hence\n\u201comni-directional\u201d. ODIS employs a novel parameterized\ncommand language for intelligent behavior generation [17].\nA key feature of the ODIS control system is the use of an\nobject recognition system that \ufb01ts models to sensor data.\nThese models are then used as input parametersto the mo-\ntion and behavior control commands [16]. Fig. 1 shows the\nmechanical layout of the ODIS robot. The robot is 9.8 cm\ntall and weighs approximately 20 kgs.\nB. Motivation\nThe motivation to do camera calibration and radial\nundistortion is to better serve the wireless visual servoing\ntask for ODIS. Our goal is to align the robot to a park-\ning lot yellow line for localization. Instead of our previous\nyellow line alignment methods described in [18], [20], we\ncan align to the yellow line with a non-iterative way using\na calibrated camera. The detail procedure is discussed in\nthe next section.\nC. Localization Procedure\nLet us begin with a case when only ODIS\u2019s yaw and\nx,ypositions are unknown while ODIS camera\u2019s pan/tilt\nangles are unchanged since calibration. The task of yellowline alignment is described in detail as follows:\n\u2022Given:\n\u20133D locations of yellow line\u2019s two ending points\n\u2013Observedendingpointsofyellowlineintheimageplane\nusing ODIS camera\n\u2013ODIS camera\u2019s pan/tilt angles\n\u2013ODIS camera\u2019s intrinsic parameters\n\u2013Radial distortion model and coe\ufb03cients\n\u2022Find:ODIS\u2019s actual yaw and x,ypositions\nKnowing that a change in ODIS\u2019s yaw angle only results\nin a changeof angle sin theZYZEuler angles( a,b,s). So,\nwhen using ZYZEuler angles to identify ODIS camera\u2019s\norientation, the \ufb01rst two variables a,bare unchanged. In\nFig. 2, after some time of navigation, the robot thinks it is\nat Position 2, but actually at Position 1. Then it sees the\nyellow line, whose locations in 3D world reference frame\nare known from map (denoted by PAw\n1andPBw\n1). After\nextracting the corresponding points in the image plane of\nthe yellow line\u2019s two ending points, we can calculate the\nundistorted image points and thus recoverthe 3D locations\nof the two ending points (denoted by PAw\n2andPBw\n2), using\nODIS camera\u2019s 5 intrinsic parameters and radial distortion\ncoe\ufb03cients. From the di\ufb00erence between the yellow line\u2019s\nactual locations in map and the recovered locations, the\ndeviation in the robot\u2019s x,ypositions and yaw angle can\nbe calculated.\n \nFig. 2\nThe task of yellow line alignment\nLet (x,y) be the undistorted points in the camera frame\ncorresponding to the yellow line\u2019s two ending points in the\n3D world frame. Let R2andt2be the rotation matrix and\ntranslation vector at position 2 (where the vehicle thinks it\nis at), similarly R1andt1at position 1 (the true position\nand orientation), we can write R2= \u2206R\u00b7R1andt2=\nt1+\u2206t, where \u2206 Rand \u2206tare the deviation in orientation\nand translation. If the transform from the world reference\nframe to the camera frame is Pc=R\u22121(Pw\u2212t), \ufb01rst we\ncan calculate PAw\n2andPBw\n2.LetPAw\n2= [XAw\n2,YAw\n2,0], we have\n\uf8ee\n\uf8f0Xc\nYc\nZc\uf8f9\n\uf8fb=R\u22121\n2\uf8ee\n\uf8f0XAw\n2\u2212t21\nYAw\n2\u2212t22\n\u2212t23\uf8f9\n\uf8fb. (17)\nSince\nXc\nx=Yc\ny=Zc\n1, (18)\nwe have two equations containing two variables and PAw\n2\ncan be calculated out. By the same way, we can get PBw\n2.\nOncePAw\n2andPBw\n2are known, we have\n\u03bb\uf8ee\n\uf8f0x\ny\n1\uf8f9\n\uf8fb=R\u22121\n2\u2206R(PAw\n1\u2212t1) =R\u22121\n2(PAw\n2\u2212t2),(19)\nwhere\u03bbis a scaling factor. From (19), we get\nR\u22121\n2[\u2206R(PAw\n1\u2212t1)\u2212PAw\n2+t2] = 0. (20)\nSimilarly, we get\nR\u22121\n2[\u2206R(PBw\n1\u2212t1)\u2212PBw\n2+t2] = 0. (21)\nUsing the above two equations, we have ( PAw\n2\u2212PBw\n2) =\n\u2206R(PAw\n1\u2212PBw\n1), where \u2206 Ris of the form\n\u2206R=\uf8ee\n\uf8f0cos(\u2206\u03b8)\u2212sin(\u2206\u03b8) 0\nsin(\u2206\u03b8) cos(\u2206 \u03b8) 0\n0 0 1\uf8f9\n\uf8fb. (22)\nSo, \u2206\u03b8is just the rotation angle from vector PAw\n1\u2192PBw\n1\nto vector PAw\n2\u2192PBw\n2. When \u2206 Ris available, t1can be\ncalculated as t1=PAw\n1\u2212\u2206R\u22121(PAw\n2\u2212t2).\nV. Concluding Remarks\nThis paper proposes a new radial distortion model that\nbelongs to the polynomial approximation category. The\nappealing part of this distortion model is that it preserves\nhigh accuracy together with an easy analytical undistor-\ntion formula. Experiments results are presented showing\nthat this distortion model is quite accurate and e\ufb03cient\nespecially when the actual distortion is signi\ufb01cant. An ap-\nplication of the new radial distortion model is non-iterative\nyellow line alignment with a calibrated camera on ODIS."}
{"category": "abstract", "text": "\u2014Most algorithms in 3D computer vision rely on\nthe pinhole camera model because of its simplicity, whereas\nvirtually all imaging devices introduce certain amount of\nnonlinear distortion, where the radial distortion is the mo st\nsevere part. Common approach to radial distortion is by\nthe means of polynomial approximation, which introduces\ndistortion-speci\ufb01c parameters into the camera model and re -\nquires estimation of these distortion parameters. The task\nof estimating radial distortion is to \ufb01nd a radial distortio n\nmodel that allows easy undistortion as well as satisfactory\naccuracy. This paper presents a new radial distortion model\nwith an easy analytical undistortion formula, which also be -\nlongs to the polynomial approximation category. Experi-\nmental results are presented to show that with this radial\ndistortion model, satisfactory accuracy is achieved. An ap -\nplication of the new radial distortion model is non-iterati ve\nyellow line alignment with a calibrated camera on ODIS, a\nrobot built in our CSOIS (See Fig. 1).\nI. Introduction\nA. Related Work"}
{"category": "non-abstract", "text": "minimise D(P) subject to C(P)\u2264\u01eb. Where\u01ebis a function of\u03bb. Under large\nassumptions, minimising E\u03bb(P) is also equivalent to the dual problem: minimise C(P)\nsubjectto D(P)\u2264\u01eb\u2032,where\u01eb\u2032isalsoafunctionof \u03bb.Therefore\u03bbmaybeinterpretedas\nthe amount of freedom allowed to minimise D(D(P)\u2264\u01eb\u2032) while keeping Cas low as\npossible.Since\u01eb\u2032isagrowingfunctionof \u03bb,as\u03bbisgrowing,theconstrainton Dismore2 Jean-Hugues PRUVOT,Luc BRUN\nand more relaxedwhile the importanceof the term Cis getting more and more impor-\ntant. This parameter \u03bbmay thus be interpreted as a scale parameter which represents\ntherelativeweightingbetweenthetwo energyterms.\nIn many approaches the parameter \u03bbis \ufb01xed experimentally and a minimisation\nalgorithmdeterminesforavalueof \u03bbalocallyoptimalpartitionfromtheset Pofallthe\npossiblepartitionsonimage I.A sequenceof\u03bbmayalso bede\ufb01neda prioriinorderto\ncomputetheoptimalpartitiononeachsampledvalueof \u03bb[5].\nThescale set frameworkproposedbyGuigues[5]is basedon a d i\ufb00erentapproach.\nInsteadofperformingtheminimisationschemeonthewholes etPofpossiblepartitions\nofanimage I,Guiguesproposestorestrictthesearchonahierarchy H.Theadvantages\nofthisapproacharetwofold:\ufb01rstlyasshownbyGuiguestheg loballyoptimalpartition\nonHmay be found e\ufb03ciently while the search on the whole set Pof partitions only\nprovides local minima. Secondly, Guigues shown that if the e nergy satis\ufb01es some ba-\nsic properties, the whole set of solutions on Hwhen\u03bbdescribesR+corresponds to a\nsequenceof increasingcuts withinthe hierarchy Hherebyprovidinga contiguousrep-\nresentationofthe solutionsforthe parameter \u03bb. A methodto buildthe hierarchy Hhas\nbeenproposedbyGuigues.SincetheresearchspaceusedbyGu iguesisrestrictedtothe\ninitial hierarchy Hthe construction scheme of this hierarchy is of crucial impo rtance\nfortheoptimalpartitions within H builtinthe secondstep.\nThis paper explores di \ufb00erent heuristics to build the initial hierarchy.These heur is-\ntics represent di\ufb00erent compromisesbetween the energy of the \ufb01nal partitions and the\nexecution times. We \ufb01rst present in Section 2 the scale set fr amework. The di\ufb00erent\nheuristicsarethenpresentedinSection3.Theseheuristic sareevaluatedandcompared\ntothe methodofGuiguesinSection4.\n2 The Scale Setframework\nGiven an image Iand two partitions PandQonI, we will say that Pis\ufb01nerthanQ\n(orQis coarser then P) i\ufb00Qmay be deducedfrom Pby mergingoperations. This re-\nlationship is denotedby P/triangleleftequalQ. Let us now consider a theoreticsegmentationalgorithm\nP\u03bbparametrisedby\u03bb.We willsaythat Pisanunbiasedmulti-scalesegmentation algo-\nrithmi\ufb00foranycouple(\u03bb1,\u03bb2)suchthat\u03bb1\u2264\u03bb2,andanyimage I,P\u03bb1(I)/triangleleftequalP\u03bb2(I).IfP\u03bb\nis an unbiased multi-scale segmentation algorithm, P\u03bb(I) increases according to \u03bband\nthe setH=/uniontext\n\u03bb\u2208R+P\u03bb(I) de\ufb01nesa hierarchyas an union of nested partitions. Note th at\nthesetPofpartitionson Ibeing\ufb01nite, Hmustbealso \ufb01nite.\nUnbiased multi-scale segmentation algorithms follow a wel l known causal princi-\npal: increasing the scale of observation should not create n ew information. In other\nwords any phenomenon observed at one scale should be caused b y objects de\ufb01ned at\n\ufb01nerscales. Inourframework,increasingthescale shouldn otcreatenewcontours.\nThe family of energies considered by Guigues corresponds to the set of A\ufb03ne\nSeparable Energies (ASE) which can be written for any partit ionPofIinnregions\n{R1,...,Rn}as:\nE(P)=D(P)+\u03bbC(P)=n/summationdisplay\ni=1D(Ri)+\u03bbn/summationdisplay\ni=1C(Ri)=n/summationdisplay\ni=1D(Ri)+\u03bbC(Ri)ScaleSetrepresentation for image segmentation 3\nLetusconsidera hierarchy Handthesequence( C\u2217\n\u03bb(H))\u03bb\u2208R+ofoptimalcutswithin\nH.TheapproachofGuiguesisbasedonthefollowingresult:If E\u03bb(P)isanASEandif\nC\u03bb(P)isdecreasingwithin P:\n\u2200(P,Q)\u2208PP\u22b3Q\u21d2C(P)>C(Q)\nthen the sequence ( C\u2217\n\u03bb(H))\u03bb\u2208R+is an unbiased multi-scale segmentation. The union of\nall (C\u2217\n\u03bb(H))\u03bb\u2208R+de\ufb01nes thus a new hierarchy within H. The tree corresponding to the\nhierarchical structure of/uniontext\n\u03bb\u2208R+C\u2217\n\u03bb(H) may be deduced from Hby merging with their\nfathers all the nodes which do not belong to any optimal cuts. Note that an equivalent\nresult maybe obtainedif noconditionis imposedto CbutifDis increasingaccording\nto\u03bb.\nTherestrictionbyGuiguesoftheresearchspacetoahierarc hymaythusbejusti\ufb01ed\nbythefactthatthesetofpartitionsproducedbyanyunbiase dmulti-scalesegmentation\nalgorithm describes a hierarchy. Conversely, given a hiera rchyH, if the energy E\u03bbis\nan ASE with a decreasing term Cthe sequence of optimal cuts of Haccording to E\u03bb:\n(C\u2217\n\u03bb(H))\u03bb\u2208R+isan unbiasedmulti-scalesegmentationalgorithm.\nGiven a partition P\u2208P, the decrease of Cmay be equivalently expressed as a\nsub-additivityrelationship:\n\u2200(R,R\u2032)\u2208P|Risadjacentto R\u2032C(R\u222aR\u2032)<C(R)+C(R\u2032) (1)\nNotethat thesub-additivityoftheregularisingterm Cin commonismanyapplica-\ntions.Forexample,if Cisproportionaltosomequantitysummedupalongcontours, C\nis sub-additivedue to the removalof the commonboundariesb etween the two merged\nregions. Moreover, the term Cmay be interpreted within the Minimum Description\nLength framework [3] as the amount of information required t o encode a partition.\nTherefore,onecanexpect Ctodecreasewhenthepartitiongetscoarser.\nGiven a hierarchy H, the sequence of optimal cuts C\u2217\n\u03bb(H) withinHhas to be com-\nputed.Letusconsideroneregion Ratthesecondlevelofthehierarchy(computedfrom\nthebase)anditssetofsons S1,...,Sn.Letusadditionallyconsiderthetree H(R)rooted\natRwithinH(Fig. 1(a)). Since Ris a level 2 node, the hierarchy H(R) allows only\ntwo cuts:oneencodingthepartition P1madeofthesonsof Rwhoseenergyis equalto\nE\u03bb(P1)=/summationtextn\ni=1D(Si)+\u03bb/summationtextn\ni=1C(Si)andoneencodingthepartition P2reducedtothesin-\ngleregion R.Theenergyof P2isequalto E\u03bb(P2)=D(R)+\u03bbC(R).Duetothesubaddi-\ntivityofCwehave/summationtextn\ni=1C(Ri)>C(R).Therefore,usingthelinearexpressionof E\u03bb(P1)\nandE\u03bb(P2) in\u03bb, if/summationtextn\ni=1D(Si)<D(R) the line E\u03bb(P1)=/summationtextn\ni=1D(Si)+\u03bb/summationtextn\ni=1C(Ri) is\nbelowthe line E\u03bb(P2)=D(R)+\u03bbC(R) until a value\u03bb+(R)of\u03bbforwhich the two lines\ncross(Fig. 1(b)). If/summationtextn\ni=1D(Si)\u2265D(R),E\u03bb(P2) is always greater or equal to E\u03bb(P1) in\nwhich case we set \u03bb+(R) to 0. Therefore,in both cases the partition P1is associated to\na lower energy than P2for\u03bb=0 until\u03bb=\u03bb+(R). Above this value the partition P2is\nassociatedtothelowestenergy.Intermsofoptimalcuts, P1correspondstotheoptimal\ncut ofH(R)until\u03bb+(R)andP2isthe optimalcutabovethisvalue(Fig.1(c)).Thevalue\n\u03bb+(R)iscalledthe scale ofappearance ofthe region R.\nGuigues shown that the above process may be generalised to th e whole tree. Each\nnode ofHis then valuated by a scale of appearance. Some of the nodes of Hmay\nget a greater scale of appearance than their father. Such nod es do not belong to any4 Jean-Hugues PRUVOT,Luc BRUN\nS1...Sn\u261e\u261e\u276d\u276dR\n(a) H(R)\u2732\u273b\n\u2711\u2711\u2711\u2711\u2711\n\u270f\u270f\u270f\u270f\u270fE\u03bb(P1)\n/summationtextn\ni=1D(Si)D(R)E\u03bb(P2)=D(R)+\u03bbC(R)\n\u03bb+(R)\u03bbE\u03bb\n(b)E\u03bb(P1),E\u03bb(P2)\u2732\u273b\n\u2711\u2711\u2711\u270f\u270f\u270f\n\u03bb+(R)\u03bbE\u03bb\nE\u03bb(C\u2217\n\u03bb)\n(c)E\u03bb(C\u2217\n\u03bb(H(R)))\u2732\u273b\n\u0000\u0000\u271f\u271f\u2725\u2725\u2725\u2725\u2725\u2725\u2725\nAE\u03bb\n\u03bb\n(d)E\u03bb(C\u2217\n\u03bb(H))\nFig.1.(a) a node Rof the hierarchy whose sons {S1,...,Sn}correspond to initial regions. (b)\nthe energies of the partitions associated to Rand{S1,...,Sn}plotted as functions of \u03bb. (c) the\nenergy of the optimal cuts within H(R) (a). (d) an example of concave piecewise linear function\nencoding the energyof the optimal cuts withina global hiera rchyH.\noptimal cut and are removed from Hduring a cleaning step which merges them with\ntheir fathers. Each node Rof the resulting hierarchy belongs to an optimal cut from\n\u03bb=\u03bb+(R)untilthescale ofappearanceof itsfather \u03bb+(F(R)),whereF(R)denotesthe\nfatherofRinH.Thevalue\u03bb+(R)maybesetforeachnodeofthetreeusingabottom-up\nprocess. The optimal cut C\u2217\n\u03bb(H) for a given value of \u03bbmay then be determined using\na top-down process which selects in each branch of the tree th e \ufb01rst node with a scale\nof appearance lower than \u03bb. The set of selected nodes constitutes a cut of Hwhich\nis optimal by construction according to E\u03bb. The function E\u03bb(C\u2217\n\u03bb(H)) corresponds to a\nconcavepiecewiselinearfunctionwhoseeachlinearinterv alcorrespondstotheenergy\nofanoptimalcutwithin H(Fig.1(d)).\nGiven a hierarchy Hand the function E\u03bb(C\u2217\n\u03bb(H)) encoding the energy of the se-\nquence of optimal cuts, the optimality of Hmay be measured as the area under the\ncurveE\u03bb(C\u2217\n\u03bb(H)) for a given range of scales or as the area of the surface A(Fig. 1(d))\nbetweenE\u03bb(C\u2217\n\u03bb(H)) and the energy of the coarsest cut E\u03bb(Pmax). Where Pmaxdenote\nthe partition composed of a single region encoding the whole image. We propose in\nSection 4 an alternative measure of the quality of a hierarch y which allows to reduce\nthein\ufb02uenceoftheinitial image.\nGuiguesproposedtobuildahierarchy Hbyusinganinitialpartition P0andastrat-\negycalledthe scaleclimbing .Thisstrategymergesateachstepthetwoadjacentregions\nRandR\u2032suchthat:\n\u03bb+(R\u222aR\u2032)=D(R\u222aR\u2032)\u2212D(R)\u2212D(R\u2032)\nC(R)+C(R\u2032)\u2212C(R\u222aR\u2032)=min\n(R1,R2)\u2208P2,R1\u223cR2D(R1\u222aR2)\u2212D(R1)\u2212D(R2)\nC(R1)+C(R2)\u2212C(R1\u222aR2)\n(2)\nwherePdenotesthecurrentpartitionand R1\u223cR2indicatesthat R1andR2areadjacent\ninP.\nThis process merges thus at each step the two regions whose un ion would appear\nat the lowest scale. Such a construction scheme is coherent w ith the further processes\nappliedonthehierarchy.However,thereisnoevidencethat theresultinghierarchymay\nbeoptimalaccordingtoanyofthepreviouslymentionedcrit eria.Weindeedshowinthe\nnextsectionthat otherconstructionschemesofa hierarchy maylead tolowerenergies.ScaleSetrepresentation for image segmentation 5\n3 Construction ofthe initialhierarchy\nMany energies have been designed in order to encode di \ufb00erent types of homogeneity\ncriteria (piecewise constant [3,6], linear or Polynomial [ 3] variations,...). This paper\nbeing devotedto the constructionschemes of the hierarchy, we restrict our topic to the\npiecewise constant model described by Leclerc [3] and Mumfo rd and Shah [6]. The\nenergyofthismodelmaybewrittenas:\nE\u03bb(P)=D(P)+\u03bbC(P)=n/summationdisplay\ni=1SE(Ri)+\u03bb|\u03b4(Ri)| (3)\nwhereP={R1,...,Rn}representsthepartitionoftheimage, SE(Ri)=/summationtext\np\u2208R/bardblcp\u2212\u00b5R/bardbl2\nisthesquarederrorofregion Riand|\u03b4(Ri)|isthe totallengthofitsboundaries.\nWithin the Minimum Description Length framework, SE(Ri) may be understood\nas the amount of information required to encode the deviatio n of the data against the\nmodel, while|\u03b4(Ri)|is proportional to the amount of information required to enc ode\nthe shape of the model. Within the statistical framework, th e squared error may also\nbe understood as the log of the probability that the region sa tis\ufb01es the model (i.e. is\nconstant)usingaGaussian assumptionwhile |\u03b4(Ri)|isaregularisingterm.\nOurapproachfollowsthescaleclimbingstrategyproposedb yGuigues(equation2).\nGiven a set Wof regionswithin a partition Pwe thus considerthe scale of appearance\nof the region Rde\ufb01ned as the union of the regionsin W. The heuristics below use this\nbasicapproachbutdi \ufb00eronthesets Wwhichareconsideredandontheorderingofthe\nmergeoperations.\n3.1 SequentialMerging\nGiven a current partition P, let us consider for each region RofP, its setV(R) de\ufb01ned\nas{R}union its set of neighboursand the set P\u2217(V(R)) of all possible subsets of V(R)\nincluding R. Each subset W\u2208P\u2217(V(R)) encodes a possible merging of the region R\nwith at least one of its neighbour. Let us denote by RW=/uniontext\nR\u2032\u2208WR\u2032the region formed\nbytheunionoftheregionsin W. Notethat theregion RWis connectedsince Rbelongs\ntoWand all the regions of Ware adjacent to R. Let us additionally consider the two\npartitionsof RW:PRW={RW}andPW=W. The energiesassociated to these partitions\narerespectivelyequalto E\u03bb(PRW)=D(RW)+\u03bbC(RW)and:\nE\u03bb(PW)=D(W)+\u03bbC(W)=/summationdisplay\nR\u2032\u2208WD(R\u2032)+\u03bb/summationdisplay\nR\u2032\u2208WC(R\u2032)\nwhereD(W) andC(W) denote respectively the \ufb01t to data and the regularisingter ms of\nthepartition PW.\nSinceCissubadditive(equation1)wehave C(W)>C(RW).Theenergy E\u03bb(PW)is\nthuslowerthan E\u03bb(PRW)untilavalue\u03bb+(RW)calledthescaleofappearanceof RW(Sec-\ntion2).Usingthescaleclimbingprinciple,oursequential mergingalgorithmcomputes\nforeachregion Rofthe partitiontheminimalscaleofappearanceofa region RW:\n\u03bb+\nmin(R)=argminW\u2208P\u2217(V(R))D(RW)\u2212D(W)\nC(W)\u2212C(RW)6 Jean-Hugues PRUVOT,Luc BRUN\nthesetW\u2208P\u2217(V(R)) whichrealisestheminisdenoted Wmin(R).\nGiven the quantities \u03bb+\nmin(R) andWmin(R), our sequential algorithm iterates the fol-\nlowingsteps:\n1. LetPdenotesthecurrentpartitioninitialisedwithan initialp artitionP0,\n2. Foreachregion RofPcompute\u03bb+\nmin(R)andWmin(R)\n3. Compute Rmin=argminR\u2208P\u03bb+\nmin(R)andmergeall theregionsof Wmin(Rmin).\n4. Ifmorethanoneregionremainsgotostep 2,\n5. Outputthe\ufb01nalhierarchy Hencodingthesequenceofmergeoperations.\nThis algorithm performs thus one merge operation at each ste p of the algorithm.\nNotethat allthe regionsof Wmin(Rmin)areadjacentto Rmin. Therefore,withintheirreg-\nular pyramid framework,the merge operation may be encoded b y a contractionkernel\nofdepthonecomposedofasingletreewhoserootisequalto Rmin.Thecomputationof\n\u03bb+\nmin(R) for each region Rof the partition requiresto traverse P\u2217(V(R)) whose cardinal\nis equal to 2|V(R)|\u22121. Therefore, if the partition is encoded by a graph G=(V,E), the\ncomplexityof each step of ouralgorithmis boundedby O(|V|2k) where|V|denotesthe\nnumberof vertices(i.e. the numberofregions)and krepresentsthe maximalvertices\u2019s\ndegreeof G. The cardinal of Vis decreasedby|Wmin(Rmin)|\u22121 at each iteration.Since\n|Wmin(Rmin)|is at least equal to 2, the cardinal of Vdecreases by at least 1. The com-\nputation of\u03bb+\nmin(R) for each region Rof the partition may induce important execution\ntimes when the degree of the vertices of the graphis importan t.However,experiments\npresentedinSection4showthatthecardinalofthesubsets W\u2208P\u2217(R)maybebounded\nwithoutalteringsigni\ufb01cantlytheenergyoftheoptimalcut s.Letus\ufb01nallynotethatthis\nalgorithmincludesthescaleclimbingapproachproposedby Guigues.Indeed,themerge\noperationsstudiedbyGuigues(Section2)correspondtothe subsetsW\u2208P\u2217(V(R))with\n|W|=2 whichareconsideredbyouralgorithm.\n3.2 ParallelMerge algorithm\nOur parallel merge algorithm is based on the notion of maxima l matching. A set of\nedgesMofagraph G=(V,E)iscalledamaximalmatchingifeachvertexof Gisinci-\ndenttoatmostoneedgeof MandifMismaximalaccordingtothisproperty.Moreover,\nwewouldliketodesignamaximalmatching Msuchthatthescaleofappearanceofthe\nregions produced by the contraction of Mis as low as possible. Let us denote by \u03b9(e),\nthetwoverticesincidentto e.UsingthesameapproachasinSection3.1weassociateto\neachedge eofthegraphthescaleofappearance \u03bb+(\u03b9(e))(equation2)oftheregion R\u03b9(e)\nde\ufb01nedastheunionoftheregionsencodedbythetwovertices incidentto e.Following,\nthe same approach as Haxhimusa [7] we de\ufb01ne our maximal match ing as a Maximal\nIndependent Set on the set of edges of the graph. The iterativ e process which builds\nthe maximal independent set selects at each step edges whose scale of appearance is\nlocallyminimal.Thisprocessmaybeformulatedthankstotw obooleanvariables pand\nqattachedtoeachedgesuchthat:\n/braceleftBiggp1\ne=\u03bb+(e)=mine\u2032\u2208\u0393(e){\u03bb+(e\u2032)}\nq1\ne=/logicalandtext\ne\u2032\u2208\u0393(e)p1\ne\u2032and\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3pk+1\ne=pk\ne\u2228/parenleftBig\nqk\ne\u2227\u03bb+(e)=mine\u2032\u2208\u0393(e)|qk\ne\u2032{\u03bb+(e\u2032)}/parenrightBig\nqk+1\ne=/logicalandtext\ne\u2032\u2208\u0393(e)pk+1\ne\u2032\n(4)ScaleSetrepresentation for image segmentation 7\nwhere\u0393(e)denotestheneighbourhoodoftheedge eandisde\ufb01nedas\u0393(e)={e}\u222a{e\u2032\u2208\nE|\u03b9(e)\u2229\u03b9(e\u2032)/nequal\u2205}.\nThis iterative process stops when no change occurs between t wo iterations. If n\ndenotesthe\ufb01naliteration,thesetofedgessuchthat pn\neistruede\ufb01nesamaximalmatch-\ning[7]Mwhichencodesthesetofedgestobecontracted.Moreover,th esetofselected\nedgescorrespondstolocalminimaaccordingtothescaleofa ppearance\u03bb+(e).Roughly\nspeakingif\u03bb+(e)isunderstoodasamergescore,oneedgebetweentwovertice swillbe\nmarked(pk\ne=true)at iteration k,if amongall theremainingpossiblemergeoperations\ninvolving these two vertices, the one involving them is the o ne with the best merge\nscore. Note that the construction of a maximal matching is on ly the \ufb01rst step of the\nmethod of Haxhimusa which completes this maximal matching i n order to get a deci-\nmationratioof order2.The restrictionof ourmethodto a max imalmatchingallowsto\nrestrictthemergeoperationstoedgeswhichbecomelocally optimalatagiveniteration.\nWethusfavourtheenergycriterionagainstthereductionfa ctor.AsshownbyBield[8],\nthe reduction factor in terms of edges induced by the use of a m aximal matching is a\nleast equal to 2k\u22121\n2k\u22121wherekis the maximal vertex\u2019s degree of the graph. The edge\u2019s\ndecimation ratio may thus be very low for graphs with importa nt vertices\u2019s degrees.\nNevertheless, experimentsperformedon 100 natural images of the Berkeley database1\nshown that the mean vertex\u2019sdecimationratio betweenlevel s on this database is equal\nto1.73whichiscomparableto the2 .0decimationratioobtainedbyHaxhimusa.\nThelocal minimaselectedin equation4 are computedon decre asingsets alongthe\niterations in order to complete the maximal matching. We can thus consider that the\ndetected minima are less and less signi\ufb01cants as the iterati ons progress. We thus pro-\npose an alternative solution which consists in contracting at each step only the edges\nselected at the \ufb01rst iteration ( p1\ne=true). These edgescorrespondto minima computed\nonthewholeneighbourhoodofeachedge.Thismethodmaybeun derstoodasacombi-\nnationofthemethodproposedbyHaxhimusa[7]andthestocha sticdecimationprocess\nof Jolion [9] which consists in merging immediately vertice s corresponding to local\nminima.\n4 Experiments\nThe di\ufb00erent heuristics presented in this paper have been evaluate d on the Berkeley\ndatabase.Theevaluatedheuristicsincludeourparallelme rgeheuristicbasedonamax-\nimalmatching(MM)andthevariationofthismethod( MM1)whichmergesateachstep\ntheedgesselectedduringthe\ufb01rstiteration(Section3.2). Wealsoevaluatedoursequen-\ntialmethod(SM)andtwovariationsofthismethod:the\ufb01rstv ariation(SM2),considers\nfor each region Rof the partition the subsets of cardinal2 of V(R). This methodcorre-\nspondstotheheuristicproposedbyGuigues.Wealsoevaluat edanintermediatemethod\n(SM5)whichrestrictsthecardinalofthesubsetsof V(R)including Rtoanupperthresh-\nold\ufb01xedto\ufb01vein these experiments.All theexperimentshav eused aninitial partition\nobtainedbya Watershedalgorithm[10].\n1available at http://www.eecs.berkeley.edu /Research/Projects/CS/vision/bsds/8 Jean-Hugues PRUVOT,Luc BRUN\nMM\nMM1\nSM2\nSM\nSM5\n\u03bb=0.2\u03bb=0.4\u03bb=0.6\u03bb=0.8\u03bb=0.2\u03bb=0.4\u03bb=0.6\u03bb=0.8\nFig.2.Partitions of the mushroom and the \ufb01sherman images at di \ufb00erent scales. Each line of the\narraycorresponds toanheuristic whose acronym isindicate d onthe \ufb01rstcolumn.\nFig. 2 shows 5 optimal cuts obtained for increasing values of \u03bbon the Mushroom\nandFishermanimagesoftheBerkeleydatabase1.Theheuristicsusedtobuildthehier-\narchiesaredisplayedonthe\ufb01rstcolumnofFig.2.Theorigin alimagesaredisplayedin\nFig.4(a).\nFig.3(a)showsthein\ufb02uenceofthenumberofinitialregions ontheexecutiontime.\nThese curves have been obtained on the Mushroom image with di \ufb00erent initial parti-\ntionsobtainedbyvaryingthesmoothingparameterofthegra dientwithinourWatershed\nalgorithm.\nFig. 3(b)allowsto comparethe performanceof each heuristi con the whole Berke-\nley database. However, a direct comparison of the energies o btained by the di\ufb00erent\nheuristics on di\ufb00erent images would be meaningless since the shape of the func tion\nE\u03bb(C\u2217\n\u03bb(H)) dependsboth of the intrinsic performancesof the heurist ic used to build H\n1Color plates are available at the followingurl:http: //www.greyc.ensicaen.fr /\u223cjhpruvot/Cut/ScaleSetrepresentation for image segmentation 9\n(a) Executiontime\n (b)E\u03bb(C\u2217\n\u03bb(H))\u2732\u273b\n\u0000\u0000\u271f\u271f\u2725\u2725\n\u03bbmaxE\u03bbmax(Pmax)E\u03bb(Pmax)\nE0(P0)E\u03bb(C\u2217(H))\n\u03bb\n(c) Energy\u2019s Bounds\nFig.3.(a) execution times of the di \ufb00erent heuristics on the Mushroom image (Fig. 2) using an\ninitial partition witha varying number of regions. (b) mean energies of optimal cuts obtained by\nour heuristics on the Berkeleydatabase. (c) bounds of the op timal cut\u2019s energies.\nand of the image Ion which Hhas been built. We have thus to normalise the energies\nE\u03bb(C\u2217\n\u03bb(H))producedbythedi \ufb00erentheuristicsbeforeanycomparison.\nGiven a hierarchy H, sinceC\u2217\n\u03bb(H) is an unbiased multi-scale segmentation (Sec-\ntion 2), the hierarchy Hobtained by each of our methodsmay be associated to a value\n\u03bbH\nmaxabovewhich the optimal partition Pmaxis reducedto a single regionencodingthe\nwholeimage.Theenergyof Pmaxisde\ufb01nedas: E\u03bb(Pmax)=DI+\u03bbCIwhereDI=SE(I)\ndenotes the global image\u2019s squared error and CI=|\u03b4(I)|the perimeter of the image.\nSince the energy of the optimal cuts E\u03bb(C\u2217\n\u03bb(H)) of a hierarchy His a piecewise linear\nconcavefunctionof \u03bb,thefunction E\u03bb(C\u2217\n\u03bb(H))isbelowtheenergy E\u03bb(Pmax)associated\nto the coarser partition(Fig.3(c)). Moreover,if P0denotes the initial partition, the two\npoints (0,E0(P0)) and (\u03bbmax,E\u03bbmax(Pmax)) belong to the curve. Therefore, E\u03bb(C\u2217\n\u03bb(H))\nbeing concave, it should be above the line connecting these t wo points. Finally, the\nlineconnecting(0,0)to(\u03bbmax,E\u03bbmax(Pmax))beingbelowthelinejoining(0 ,E0(P0))and\n(\u03bbmax,E\u03bbmax(Pmax))we haveforanyhierarchy Handanyscale\u03bb(Fig.3(c)):\n\u03bb\n\u03bbmaxE\u03bbmax(Pmax)\u2264E\u03bb(C\u2217\n\u03bb(H))\u2264E\u03bb(Pmax)\nWe obtainfromthislast inequalityandaftersomecalculust hefollowingequation:\n\u2200\u03bb\u2208R+x\u03bb\u22641+x\u03bb\u22121\n1+x\u03bbEI\u2264E\u03bb(C\u2217\n\u03bb(H))\nE\u03bb(Pmax)\u22641withx\u03bb=\u03bb\n\u03bbmaxandEI=\u03bbmaxCI\nDI(5)\nTherefore, using the normalised energy,E\u03bb(C\u2217\n\u03bb(H))\nE\u03bb(Pmax)and the normalised scale x\u03bb=\u03bb\n\u03bbmax,\nanycurveE\u03bb(C\u2217\n\u03bb(H))\nE\u03bb(Pmax)liesintheupperleftpartoftheunitcube[0 ,1]2.Notethatthisresult\nisvalidforanyhierarchy Handthusanyheuristic.\nUsing our piecewise constant model (equation 3), the energy E\u03bb(Pmax) is roughly\nequal to the squared error of the image for small values of \u03bband may be interpreted\nas the global variation of the image. The normalised energy a llows thus to reduce the\nin\ufb02uence of the global variation of the images on the energy a nd to compare energies\ncomputed with a same heuristic but on di \ufb00erent images. Note however,that the use of\nthe normalisedscale x\u03bb=\u03bb\n\u03bbmaxdiscardsthe absolute valueof \u03bbmax. We thusdo nottake\ninto account the range of scales for which the optimal cut is n ot reduced to the trivial10 Jean-Hugues PRUVOT,Luc BRUN\n(a) OriginalImages\n (b)D(R)=SE(R)\n(c)D(R)=SE(R)(1+\nf(Int(R)\nExt(R))\nFig.4.(a) Original images. (b) and (c), partitions of the tower ima ge built with a same heuris-\ntic(SM) at a same normalised scale ( x\u03bb=.8) but with energies de\ufb01ned using two di \ufb00erent \ufb01t to\ndata terms. (b) is de\ufb01ned using the squared error D(R)=SE(R) while (c) is de\ufb01ned using the\nformula de\ufb01ned byequation 6.\npartitionPmax. However,theabsolutevalueof \u03bbmaxvariesaccordingtoeachimageand\neachheuristics.Thenormalisedscaleallowsthustoremove thein\ufb02uenceoftheimage.\nMoreover, our experiments shown thus that for each image, ou r di\ufb00erent heuristics\nobtainclose\u03bbmaxvalues.\nFig. 3(b) represents for each value of x\u03bband each heuristic, the mean value of\nthe normalised energyE\u03bb(C\u2217\n\u03bb(H))\nE\u03bb(Pmax)computed on the whole set of images of the Berckley\ndatabase.\nAsshowninFig 3(b)theenergyoftheoptimalcutsobtainedby theheuristic MM1\n(\u2212/trianglesolid\u2212) is lower than the one obtained by the maximal matching heuri stic (\u2212\u2022\u2212). This\nresult is con\ufb01rmed by Fig. 2 (lines MMandMM1) where the heuristic MMremoves\nmore details of the mushroom at a given scale. This result is c onnected to the greater\ndecimation ratio of the MMheuristic. The MMheuristic merges at each step regions\nwith important scale of appearance without considering reg ions which may appear at\nfurther steps. The algorithms MMandMM1induce equivalent execution times on a\nsequential machine. The execution times of the method MM1(\u2212/trianglesolid\u2212) are overlayed by\ntheonesofthemethod MM(\u2212\u2022\u2212)inFig. 3(a)duetotheverticalscaleofthis\ufb01gure.\nThe subjective quality of the partitions obtained by the heu risticsMM1andSM2\n(Fig.2)seemsroughlysimilar.We cannoticethattheheuris ticMM1seemstoproduce\nslightly coarser partitions at each scale. However, consid ering Fig. 3(b), the optimal\nenergyobtainedby the heuristic SM2(\u2212/square\u2212) are lower than the one obtainedby MM1\n(\u2212/trianglesolid\u2212).Notethattheheuristic MM1produceslowerexecutiontimesthan SM2evenon\na sequentialmachine(Fig.3(a)).\nAs shown by Fig. 3(b) the optimal energies producedby the heu risticSM(\u2212+\u2212)\nare always below the one produced by the heuristic SM2(\u2212/square\u2212). Note that, the curve\n(\u2212+\u2212) is close to the diagonal of the square [0 ,1]2. This last point indicates that on\nmostoftheimagesoftheBerkeleydatabasethehierarchiesp roducedbythe SMheuris-\nticprovideoptimalcutswhosenormalisedenergyisclosedf romthelowerboundofthe\noptimalcut\u2019senergies(equation5).Thisresultiscon\ufb01rme dbyFig.2wheretheheuris-\nticSMpreserves more details of the image at each scale. However, t he heuristic SM\nis the one which requiresthe more important executiontimes on a sequential machine\n(Fig.3(a)).ScaleSetrepresentation for image segmentation 11\nTheheuristic SM5maybeunderstoodasacompromisebetween SM2andSM. As\nshownbyFig.3(b)theoptimalenergiesobtainedbytheheuri sticSM5()arecloseto\ntheoneobtainby SM(\u2212+\u2212)andbelowtheoneobtainedby SM2(\u2212/square\u2212).Moreover,as\nshownbyFig. 3(a),the executiontimesrequiredby SM5are betweentheonerequired\nbytheheuristics SM2andSM. Finally,thepartitionsobtainedbythe SM5heuristicin\nFig.2 areclosedfromtheoneobtainedbytheheuristic SM.\nFig. 4 shows results obtained using an other \ufb01t to data criter ion based on the intu-\nitive notion of contrast. The basic idea of this criterion [1 1] states that a region should\nhaveahighercontrastwithitsneighbours(calledexternal contrast)thanwithinitseven-\ntualsubparts(calledinternalcontrast).Letusdenoteby Gethemeangradientcomputed\nalongthecontourassociatedtoanedge e.Theinternalandexternalcontrastsofaregion\nRare then respectivelyde\ufb01ned as Int(R)=maxe\u2208CC(R)GeandExt(R)=mine\u2208E|v\u2208\u03b9(e)Ge.\nWhereCC(R) denotes the set of edges which have been contracted to de\ufb01ne Rand\ne\u2208E|v\u2208\u03b9(e)denotestheset ofedgesincidentto v.Ournewenergycombinesthecon-\ntrast andthe squarederrorcriteriaasfollows:\nE\u03bb(P)=n/summationdisplay\ni=1SE(Ri)/parenleftBigg\n1+f/parenleftBiggInt(Ri)\nExt(Ri)/parenrightBigg/parenrightBigg\n+\u03bb|\u03b4(Ri)| (6)\nwheref()denotesasigmoidfunction.\nAcontrastedregionwillthushavealowratiobetweenitsint ernalandexternalcon-\ntrast. Conversely,a poorly contrasted region may have a \ufb01t t o data term close to twice\nits squared error. As shown by Fig. 3(b) and (c) this energy fa vours highly contrasted\nregions. For example, the cloud merged with the sky in Fig. 3( b) remains in Fig. 3(c).\nMoreover, experiments not reported here, shown us that the s ame type of discussion\nabout the advantages and drawbacks of the di \ufb00erent heuristics may be conducted on\nthisnewenergywiththesame conclusions.\n5 Conclusion\nTheScaleSetframeworkisbasedontwosteps:thedeterminat ionofahierarchyaccord-\ning to an energy criterion and the determination of optimal c uts within this hierarchy.\nWe have presentedin this article paralleland sequentialhe uristicsto build suchhierar-\nchies. The normalised energy of the optimal cuts, associate d with these hierarchy are\nboundedbellowbythediagonaloftheunitsquare[0 ,1]2.Ourexperimentalresultssug-\ngest that our sequential heuristic SMprovides hierarchies whose normalised energies\nare closed from this lower bound. This methods may however re quire important exe-\ncution times. We thus propose an alternative heuristic prov iding lower execution time\nat the price of generally slightly higher optimal cut\u2019s ener gies. Our parallel methods\nprovidegreater energiesthan the one producedby Guigues\u2019s heuristic. However,these\nmethodsrequirelessexecutiontimesevenonsequentialmac hine.\nHierarchies encoding a sequence of optimal cuts are usually composed of a lower\nnumber of levels and regions than the initial hierarchies bu ilt by our merge heuristics.\nIn the future, we would like to use these hierarchies of optim al cuts in order to match12 Jean-Hugues PRUVOT,Luc BRUN\ntwohierarchiesencodingthecontentoftwoimagessharinga signi\ufb01cantpartofasame\nscene."}
{"category": "abstract", "text": ". Segmentationalgorithmsbasedonanenergyminimisationfr amework\noften depend on a scale parameter which balances a \ufb01tto data a nd a regularising\nterm. Irregular pyramids are de\ufb01ned as a stack of graphs succ essively reduced.\nWithin this framework, the scale is often de\ufb01ned implicitly as the height in the\npyramid. However, each level of an irregular pyramid can not usually be readily\nassociated to the global optimum of an energy or a global crit erion on the base\nlevel graph. Thislastdrawbackis addressedbythe scaleset frameworkdesigned\nbyGuigues.Themethodsdesignedbythisauthorallowtobuil dahierarchyandto\ndesign cuts within this hierarchy which globally minimise a n energy. This paper\nstudies the in\ufb02uence of the construction scheme of the initi al hierarchy on the\nresulting optimal cuts. We propose one sequential and one pa rallel method with\ntwo variations within both. Our sequential methods provide partitions near an\nenergylowerboundde\ufb01nedinthispaper.Parallelmethods re quirelessexecution\ntimes thanthe sequential method of Guigues even on sequenti al machines.\n1 Introduction\nDespite much e\ufb00orts and signi\ufb01cant progresses in recent years, image segme ntation\nremains a notoriously challengingcomputer vision problem . It\u2019s usually a preliminary\nstep towardsimageinterpretationandplaysamajorroleinm anyapplications.\nThe use of an energy minimisation scheme within the region ba sed segmentation\nframework allows to de\ufb01ne criteria which should be globally optimised over a parti-\ntion. Several types of methods such as the Level set [1], the B ayesian [2], the min-\nimum description length [3] and the minimal cut [4] framewor ks are based on this\napproach. Within these frameworks the energy of a partition Pis usually de\ufb01ned as\nE\u03bb(P)=D(P)+\u03bbC(P) whereDandCdenote respectively the \ufb01t to data and the\nregularising term. The energy E\u03bb(P) corresponds to the Lagrangian of the constraint\nproblem"}
{"category": "non-abstract", "text": "Given a set of labelled data X(l), and a set of\nunlabelled data X(u)/defines{xj=U(\u03b7j)s, j= 1,...,m}that\ncorrespond to multiple transformed observations of s, the problem\nis to predict the correct class c\u2217of the original pattern s.\nOne may view Problem 1 as a special case of semi-supervised\nlearning [4], where the unlabelled data X(u)represent the multiple\nobservations with the extra constraint that all unlabelled data exam-\nples belong to the same (unknown) class. The problem then res ides in\nestimating the single unknown class, while generic semi-su pervised\nlearning problems attribute the test examples to different classes.\nIII. GRAPH-BASED CLASSIFICATION\nA. Label propagation\nWe propose in this section a novel method to solve Problem 1,\nwhich is inspired by label propagation [3]. The label propag ation\nalgorithm is based on a smoothness assumption , which states that\nifx1andx2are close by, then their corresponding labels y1and\ny2should be close as well. Denote by Mthe set of matrices with\nnonnegative entries, of size n\u00d7c. Notice that any matrix M\u2208 M\nprovides a labelling of the data set by applying the followin g rule:\nyi= max j=1,...,cMij. We denote the initial label matrix as Y\u2208MwhereYij= 1ifxibelongs to class jand 0 otherwise. The\nlabel propagation algorithm \ufb01rst forms the knearest neighbor ( k-\nNN) graph de\ufb01ned as\nG= (V,E),\nwhere the vertices Vcorrespond to the data samples X. An edge\neij\u2208 Eis drawn if and only if xjis among the knearest neighbors\nofxi.\nIt is common practice to assign weights on the edge set of G. One\ntypical choice is the Gaussian weights\nHij=(\nexp(\u2212/bardblxi\u2212xj/bardbl2\n2\u03c32)when(i,j)\u2208 E,\n0 otherwise .(1)\nThe similarity matrix S\u2208Rn\u00d7nis further de\ufb01ned as\nS=D\u22121/2HD\u22121/2, (2)\nwhereDis a diagonal matrix with entries Dii=Pn\nj=1Hij. See\nalso Fig. 2 for a schematic illustration of the k-NN graph and related\nnotation.\nNext, the algorithm computes a real valued M\u2217\u2208 Mbased\non which the \ufb01nal classi\ufb01cation is performed using the rule yi=\nmaxj=1,...,cM\u2217\nij. This is done via a regularization framework with\na cost function de\ufb01ned as\nU(M) =1\n2\u201cnX\ni,j=1Hij/bardbl1\u221a\nDiiMi\u22121p\nDjjMj/bardbl2+\n\u00b5nX\ni=1/bardblMi\u2212Yi/bardbl2\u201d\n, (3)\nwhereMidenotes the ith row of M. The computation of M\u2217\nis done by solving the quadratic optimization problem M\u2217=\nargmin M\u2208MU(M).\nIntuitively, we are seeking an M\u2217that is smooth along the edges of\nsimilar pairs (xi,xj)and at the same time close to Ywhen evaluated\non the labelled data X(l). The \ufb01rst term in (3) is the smoothness term\nand the second is the \ufb01tnessterm.\nNotice that when two examples xiandxjare similar (i.e., the\nweightHijis large) minimizing the smoothness term in (3) results\ninMbeing smooth across similar examples. Thus, similar data\nexamples will likely share the same class label. It can be sho wn\n[3] that the solution to problem (3) is given by\nM\u2217=\u03b2(I\u2212\u03b1S)\u22121\u00b5Y, (4)\nwhere\u03b1=1\n1+\u00b5and\u03b2=\u00b5\n1+\u00b5.\nFinally, several other variants of label propagation have b een\nproposed in the past few years. We mention for instance, the m ethod\nof [5] and the variant of label propagation that was inspired from the\nJacobi iteration algorithm [4, Ch. 11]. Finally, it is inter esting to note\nthat there have also been found connections to Markov random walks\n[6] and electric networks [7]. Note \ufb01nally that label propag ation is\nprobably the most representative algorithm among the graph -based\nmethods for semi-supervised learning.\nB. Label propagation with multiple observations\nWe propose now to build on graph-based algorithms to solve th e\nproblem of classi\ufb01cation of multiple observation sets. In g eneral,\nlabel propagation assumes that the unlabelled examples com e from\ndifferent classes. As Problem 1presents the speci\ufb01c constr aint that all\nunlabelled data belong to the same class, label propagation does not\n\ufb01t exactly the de\ufb01nition of the problem as it falls short of ex ploiting\nits special structure. Therefore, we propose in the sequel a novel\ngraph-based algorithm, which (i) uses the smoothness crite rion on3\nL\nU...c\nnlY\np\nFig. 3. Structure of the class-conditional label matrix Zp.\nthe manifold in order to predict the unknown class labels and (ii) at\nthe same time, it is able to exploit the speci\ufb01cities of Probl em 1.\nWe represent the data labels with a 1-of- cencoding, which permits\nto form a binary label matrix of size n\u00d7c, whoseith row encodes the\nclass label of the ith example. The class label is basically encoded\nin the position of the nonzero element.\nSuppose now that the correct class for the unlabelled data is thepth\none. In this case, we denote by Zp\u2208Rn\u00d7cthe corresponding label\nmatrix. Note that there are csuch label matrices; one for each class\nhypothesis. Each class-conditional label matrix Zphas the following\nform\nZp=2\n4Yl\u2208Rl\u00d7c\n1e\u22a4\np\u2208Rm\u00d7c3\n5\u2208Rn\u00d7c, (5)\nwhereep\u2208Rcis thepth canonical basis vector and 1\u2208Rmis the\nvector of ones. Fig. 3 shows schematically the structure of m atrix\nZp. The upper part corresponds to the labelled examples and the\nlower part to the unlabelled ones. Zpholds the labels of all data\nsamples, assuming that all unlabelled examples belong to th epth\nclass. Observe that the Zp\u2019s share the \ufb01rst part Yland differ only in\nthe second part.\nSince all unlabelled examples share the same label, the clas s labels\nhave a special structure that re\ufb02ects the special structure of Problem\n1, as outlined in our previous work [2]. We could then express the\nunknown label matrix Mas,\nM=cX\np=1\u03bbpZp, Zp\u2208Rn\u00d7c, (6)\nwhereZpis given in (5), \u03bbp\u2208 {0,1}and\ncX\np=1\u03bbp= 1. (7)\nIn the above, \u03bb= [\u03bb1,...,\u03bb c]is the vector of linear combination\nweights, which are discrete and sum to one. Ideally, \u03bbshould be\nsparse with only one nonzero entry pointing to the correct cl ass.\nThe classi\ufb01cation problem now resides in estimating the pro per\nvalue of\u03bb. We rely on the smoothness assumption and we propose\nthe following objective function\n\u02dcQ(M(\u03bb)) =1\n2\u201cnX\ni,j=1Hij/bardbl1\u221a\nDiiMi\u22121p\nDjjMj/bardbl2\u201d\n,(8)where the optimization variable now becomes the \u03bbvector. Notice\nthat the \ufb01tting term in Eq. (3) is not needed anymore due to\nthe structure of the Zmatrices. Furthermore, we observe that the\noptimization parameter \u03bbis implicitly represented in the above\nequation through M, de\ufb01ned in eq. (6).\nIn the above, Mi(resp.Mj) denotes the ith (resp. jth) row of\nM. In the case of normalized similarity matrix, the above crit erion\nbecomes\nQ(M(\u03bb)) =1\n2nX\ni,j=1Sij/bardblMi\u2212Mj/bardbl2, (9)\nwhereSis de\ufb01ned as in (2). It can be seen that the objective function\ndirectlyrelieson the smoothness assumption. Whentwo exam plesxi,\nxjare nearby (i.e., HijorSijis large), minimizing \u02dcQ(\u03bb)andQ(\u03bb)\nresults in class labels that are close too. The following pro position\nnow shows the explicit dependence of Qon\u03bb.\nProposition 1:Assume thedatasetissplitinto llabelledexamples\nX(l)andmunlabelled examples X(u), i.e.,X= [X(l),X(u)]. Then,\nthe objective function (9) can be written in the following fo rm,\nQ(\u03bb) =C+1\n2X\ni\u2264l,j>lSij/bardblYi\u2212\u03bb/bardbl2+1\n2X\ni>l,j\u2264lSij/bardblYj\u2212\u03bb/bardbl2(10)\nwhereC=P\ni\u2264l,j\u2264lSij/bardblYi\u2212Yj/bardbl2.\nProof:From equation (9) observe that\nQ(\u03bb) =1\n2nX\ni,j\u2264lSij/bardblMi\u2212Mj/bardbl2\n| {z }\nQ1+1\n2nX\ni,j>lSij/bardblMi\u2212Mj/bardbl2\n| {z }\nQ2\n+1\n2nX\ni\u2264l,j>lSij/bardblMi\u2212Mj/bardbl2\n| {z }\nQ3\n+1\n2nX\ni>l,j\u2264lSij/bardblMi\u2212Mj/bardbl2\n| {z }\nQ4.\nWe consider the following cases\n(i)i\u2264landj\u2264l: both data examples xiandxjare\nlabelled. Then, Mi= (Pc\np=1\u03bbp)Yi=Yi, due to the\nspecial structure of the Zmatrices (see (5)) and also due\nto the constraint from Eq. (7). Similarly, Mj=Yj. This\nresults in Q1=1\n2P\ni,j\u2264lSij/bardblYi\u2212Yj/bardbl2=C, which is a\nconstant term and does not depend on \u03bb.\n(ii)i > landj > l: both data samples xiandxjare unlabelled.\nIn this case, Mi=\u03bbandMj=\u03bb, again due to (5).\nTherefore the second term Q2is zero.\n(iii)i\u2264landj > l:xiis labelled and xjis unlabelled. In\nthis case, Mi=YiandMj=\u03bb. This results in Q3=\n1\n2P\ni\u2264l,j>lSij/bardblYi\u2212\u03bb/bardbl2.\n(iv)i > landj\u2264lis analogous to the case (iii) above,\nwhere the roles of xiandxjare switched. Thus, Q4=\n1\n2P\ni>l,j\u2264lSij/bardblYj\u2212\u03bb/bardbl2.\nPutting the above facts together yields Eq. (10).\nThe above proposition suggests that only the interface betw een\nlabelled and unlabelled examples matters in determining th e smooth-\nness value of a candidate label matrix M, or equivalently the solution\nvector\u03bb.Weusethisobservation inordertodesignanef\ufb01cientgraph -\nbased classi\ufb01cation algorithm that is described below.4\nAlgorithm 1 The MASC algorithm\n1:Input:\nX\u2208Rd\u00d7n: data examples.\nm: number of observations.\nl: number of labelled data.\n2:Output:\n\u02c6p: estimated unknown class.\n3:Initialization :\n4:Form the k-NN graph G= (V,E).\n5:Compute the weight matrix H\u2208Rn\u00d7nand the diagonal matrix\nD, whereDi,i=Pn\nj=1Hij.\n6:Compute S=D\u22121/2HD\u22121/2.\n7:forp= 1 :cdo\n8:M=\u00bbYl\n1e\u22a4\np\u2013\n9:q(p) =P\ni\u2264l,j>lSij/bardblMi\u2212Mj/bardbl2+P\ni>l,j\u2264lSij/bardblMi\u2212Mj/bardbl2.\n10:end for\n11:\u02c6p= argmin pq(p)\nC. The MASC algorithm\nWe propose in this section a simple, yet effective graph-bas ed\nalgorithmfortheclassi\ufb01cationofmultipleobservations f romthesame\nclass.Based onProposition1andignoring the constant term ,we need\nto solve the following optimization problem\nOptimization problem: OPT\nmin\u03bbP\ni\u2264l,j>lSij/bardblYi\u2212\u03bb/bardbl2+P\ni>l,j\u2264lSij/bardblYj\u2212\u03bb/bardbl2\nsubject to\n\u03bbp\u2208 {0,1},p= 1,...,c,Pc\np=1\u03bbp= 1.\nIntuitively, we seek the class that corresponds to the smoot hest\nlabel assignment between labelled and unlabelled data. Obs erve that\nthe above problem is a discrete optimization problem due to t he\nconstraints imposed on \u03bb, that can be collected in a set \u039b, where\n\u039b ={\u03bb\u2208Rc\u00d71:\u03bbp\u2208 {0,1},p= 1,...,c,cX\np=1\u03bbp= 1}.\nInterestingly, the search space \u039bis small. In particular, it consists of\nthe following cvectors:\n[1,0,...,0,...,0]\n[0,1,...,0,...,0]\n...\n[0,0,...,1,...,0]\n[0,0,...,0,...,1].\nThus,onemaysolve OPTbyenumeratingallabove possiblesol utions\nand pick the one \u03bb\u2217that minimizes Q(\u03bb). Then, the position of\nthe nonzero entry in \u03bb\u2217yields the estimated unknown class. We\ncall this algorithm MAnifold-based Smoothing under Constraints\n(MASC) and we show its main steps in Algorithm 1. The MASC\nalgorithm has a complexity that is linear with the number of c lasses,\nand quadratic with the number of samples.The construction o fk-\nNN graph (lines 4-6) scales as O( n2). Once the graph has been\nconstructed, the enumeration of all possible solutions sca les as O(c).\nWe conclude that the total computational cost is O( n2+c).\nIV. CLASSIFICATION OF MULTIPLE IMAGES SETS\nA. Handwritten digit classi\ufb01cation\nWe evaluate the performance of the proposed MASC algorithm\nwith respect to label propagation, in the context of handwri ttendigit classi\ufb01cation. Multiple transformed images of the sa me digit\nclass form a set of observations, which we want to assign in th e\ncorrect class. We use two different data sets for our experim ental\nevaluation; (i) a handwritten digit image collection1and (ii)the USPS\nhandwritten digit image collection. The \ufb01rst collection co ntains 20 \u00d7\n16bit binary images of \u201c0\u201d through \u201c9\u201d, where each class cont ains 39\nexamples. The USPS collection contains 16 \u00d716 grayscale images\nof digits and each class contains 1100 examples.\nRobustness to pattern transformations is a very important p roperty\nof the classi\ufb01cation of multiple observations. Transforma tion invari-\nance can be reinforced into classi\ufb01cation algorithms by aug menting\nthe labelled examples with the so-called virtual samples , denoted\nhereby as X(vs)(see [8] for a similar approach). The virtual samples\nare essentiallydata samples that are generated arti\ufb01ciall y,byapplying\ntransformations to the original data samples. They are give n the\nclass labels of the original examples that they have been gen erated\nfrom, and are treated as labelled data. By including the virt ual\nsamples in the data set, any classi\ufb01cation algorithm become s more\nrobust to transformations of the test examples. We therefor e adopt\nthis strategy in the proposed methods and we include nvsvirtual\nsamples X(vs)in our original data set that is \ufb01nally written as\nX={X(l),X(vs),X(u)}.\nWe compare the classi\ufb01cationperformance ofthe MASCalgori thm\nwith the label propagation (LP) method. In LP, the estimated class is\ncomputed by majority voting on the estimated class labels co mputed\nin Eq. (4). In our experiments, we use the same k-NN graph in\ncombination with the Gaussian weights from Eq. (1) in both LP and\nMASC methods. In order to determine the value of the paramete r\u03c3\nin Eq. (1) we adopt the following process; we pick randomly 10 00\nexamples, compute their pairwise distances and then set \u03c3equal to\nhalf of its median.\nWe \ufb01rst split the data sets into training and test sets by incl uding\n2 examples per class in the training set and the remaining are\nassigned to the test set. Each training sample is augmented b y 4\nvirtual examples generated by successive rotations of it, w here each\nrotation angle is sampled regularly in [\u221240\u25e6,40\u25e6]. This interval has\nbeen chosen to be suf\ufb01ciently small in order to avoid the conf usion\nof digits \u20196\u2019 and \u20199\u2019. Next, in order to build the unlabelled s et\nX(u)(i.e., multiple observations) of a certain class, we choose\nrandomly a sample from the test set of this class and then we ap ply\na random rotation on it by a random (uniformly sampled) angle\n\u03b8\u2208[\u221240\u25e6,40\u25e6].\nThe number of nearest neighbors was set to k= 5for both binary\ndigit collection and the USPS data set, in both methods. Thes e values\nofkhave been obtained by the best performance of LP on the\ntest set. We try different sizes of the unlabelled set (i.e., multiple\nobservations), namely m= [10 : 20 : 150] (in MATLAB notation).\nFor each value of m, we report the average classi\ufb01cation error rate\nacross 100 random realizations of X(u)generated from each one of\nthe 10 classes. Thus, each point in the plot is an average over 1000\nrandom experiments.\nFigures 4(a) and 4(b) show the results over the binary digits\nand the USPS digits image collections, respectively. Obser ve \ufb01rst\nthat increasing the number of observations gradually impro ves the\nclassi\ufb01cation error rate of both methods. This is expected s ince\nmore observations of a certain pattern give more evidence, w hich\nin turn results in higher con\ufb01dence in the estimated class la bel.\nFinally, observe that the proposed MASC algorithm unsurpri singly\noutperforms LP in both data sets, since it is designed to expl oit the\nparticular structure of Problem 1.\n1http://www.cs.toronto.edu/ \u223croweis/data.html5\n0 50 100 15010203040506070\nnumber of observationsclassification error rate (%)\n  \nLP\nMASC\nTSVM\n(a) Binary digits0 50 100 15001020304050607080\nnumber of observationsclassification error rate (%)\n  \nLP\nMASC\nTSVM\n(b) USPS digits\nFig. 4. Classi\ufb01cation results measured on two different dat a sets.\nB. Object recognition from multi-view image sets\nInthissectionwe evaluate ourgraph-based algorithm inthe context\nof object recognition from multi-view image sets. In this ca se, the\ndifferent views are considered as multiple observations of the same\nobject, and the problem is to recognize correctly this objec t.\nTheproposedMASCmethodimplements Gaussianweights(1)an d\nsetsk= 5intheconstructionofthe k-NNgraph.Wecompare MASC\nto well-known methods from the literature, which mostly gat her\nalgorithms based on either subspace analysis or density est imation\n(statistical methods):\n\u2022MSM. TheMutual Subspace Method [9],[10],whichisthe most\nwell known representative of the subspace analysis methods .\nIt represents each image set by a subspace spanned by the\nprincipal components, i.e.,eigenvectors ofthecovarianc e matrix.\nThe comparison of a test image set with a training one is then\nachieved by computing the principal angles [11] between the\ntwo subspaces. In our experiments, the number of principal\ncomponents has been set to nine, which has been found to\nprovide the best performance.\n\u2022KMSM. MSM has been extended to its nonlinear version called\nthe Kernel Mutual Subspace Method (KMSM) [12], in order\nto take into account the nonlinearity of typical image sets. The\nmain difference of KMSM from MSM is that the images are\n\ufb01rst nonlinearly mapped into a high dimensional feature spa ce,\nbefore modeling by linear subspaces takes place. In other wo rds,\nKMSM uses kernel PCA instead of PCA in order to capture the\nnonlinearities inthe data. In KMSM, we use the Gaussian kern el\nk(x,y) = exp(\u2212/bardblx\u2212y/bardbl2\n2\u03c32), where\u03c3is determined exactly in the\nsame way as in the Gaussian weights of our MASC method.\n\u2022KLD. The KL-divergence algorithm by Shakhnarovich et al [13 ]\nis the most popular representative of density-based statis tical\nmethods. It formulates the classi\ufb01cation from multiple ima ges\nas a statistical hypothesis testing problem. Under the i.i. d and\nthe Gaussian assumptions on the image sets, the classi\ufb01cati on\nproblem typically boils down to a computation of the KL\ndivergence between sets, which can be computed in closed for m\nin this case. The energy cut-off, which determines the numbe r of\nprincipalcomponents usedintheregularizationofthecova riance\nmatrices, has been set to 0.96.\nIn our evaluation, we use the ETH-80 image set [14], which\ncontains 80 object classes from 8 categories; apple, car, co w, cup,\ndog, horse, pear and tomato. Each category has 10 object clas sesMASC MSM KMSM KLD\n88.88 (1.71) 74.88 (5.02) 83.2500 (3.4) 52.5 (3.95)\nTABLE I\nOBJECT RECOGNITION RATE IN THE MEAN (STD)FORMAT,MEASURED ON\nTHEETH-80 DATABASE .\n(see Fig. 5(a)). Each object class then consists of 41 views o f the\nobject spaced evenly over the upper viewing hemisphere. Fig ure 5(b)\nshows the 41 views from a sample car object class. We use the\ncropped-close128 part of the database. All provided images\nare of size 128 \u00d7128 and they are cropped, so that they contain only\nthe object without any border area. We downsampled the image s to\nsize 32\u00d732 for computational ease. No further preprocessing is done .\nThe 41 views from each object class are split randomly into 21\ntraining and 20 test samples. In this case, the 20 different v iews\nin the test set correspond to the multiple observations of th e test\nobject. We perform 10 random experiments where the images ar e\nrandomly split into training and test sets. Table I shows the average\nobject recognition rate for each method. We also report the s tandard\ndeviation of each method in parentheses. Notice that the sub space\nmethods are superior to the KLD method which assumes Gaussia n\ndistribution of the data. Notice also that as one would expec t, KMSM\noutperforms MSM that falls short of capturing the nonlinear ities\nin the data. Finally, observe that our graph-based method cl early\noutperforms its competitors, as it is able to capture not onl y the\nnonlinearity but also the manifold structure of the data.\nV. VIDEO-BASED FACE RECOGNITION\nA. Experimental setup\nInthissectionweevaluate ourgraph-based algorithm inthe context\nof face recognition from video sequences. In this case, the d ifferent\nvideo frames are considered as multiple observations of the same\nperson, and the problem consists in the correct classi\ufb01cati on of\nthis person. We evaluate in this section the behavior of the M ASC\nalgorithm in realistic conditions, i.e., under variations in head pose,\nfacial expression and illumination. Note in passing that ou r algorithm\ndoes not assume any temporal order between the frames; hence , it\nis also applicable to the generic problem of face recognitio n from\nimage sets.\nWe use two publically available databases; the VidTIMIT [15 ] and\nthe \ufb01rst subset of the Honda/UCSD [16] database. The VidTIMI T6\n(a) ETH-80\n (b) 41 views of a sample car model\nFig. 5. Sample images from the ETH-80 database.\ndatabase2contains 43 individuals and there are three face sequences\nobtained from three different sessions per subject. The dat a set has\nbeen recorded in three sessions, with a mean delay of seven da ys\nbetween session one and two, and six days between session two and\nthree. In each video sequence each person performed a head ro tation\nsequence. In particular, the sequence consists of the perso n moving\nhis/her head to the left, right, back to the center, up, then d own and\n\ufb01nally return to center.\nThe Honda/UCSD database3contains 59 sequences of 20 subjects.\nIn contrast to the previous database, the individuals move t heir head\nfreely, in different speed and facial expressions. In each s equence,\nthe subjects perform free in-plane and out-of-plane head ro tations.\nEach person has between 2 and 5 video sequences and the number\nof sequences per subject is variable.\nFor preprocessing, in both databases, we used \ufb01rst P. Viola\u2019 s\nface detector [17] in order to automatically extract the fac ial region\nfrom each frame. Note that this typically results in misalig ned facial\nimages. Next, we downsampled the facial images to size 32 \u00d732 for\ncomputational ease. No further preprocessing has been perf ormed,\nwhich brings our experimental setup closer to real testing c onditions.\nB. Classi\ufb01cation results on VidTIMIT\nWe \ufb01rst study the performance of the MASC algorithm with the\nVidTIMIT database. Figure 6 shows a few representative imag es\nfrom a sample face manifold in the VidTIMIT database. Observ e\nthe presence of large head pose variations. Figure 7 shows th e\n3D projection of the manifold that is obtained using the ONPP\nmethod [18], which has been shown to be an effective tool for\ndata visualization. Notice the four clusters correspondin g to the four\ndifferent head poses i.e., looking left, right, up and down. This\nindicates that a graph-based method should be able to captur e the\ngeometry of the manifold and propagate class labels based on the\nmanifold structure.\nSince there are three sessions, we use the following metric f or\nevaluating the classi\ufb01cation performances\ne=1\n63X\ni=13X\nj=1,j/negationslash=ie(i,j), (11)\n2http://users.rsise.anu.edu.au/ \u223cconrad/vidtimit/\n3http://vision.ucsd.edu/ leekc/HondaUCSDVideoDatabase /HondaUCSD.html\n(a) pose 1\n (b) pose 2\n (c) pose 3\n (d) pose 4\n(e) pose 5\n (f) pose 6\n (g) pose 7\n (h) pose 8\nFig. 6. Head pose variations in the VidTIMIT database.\nwheree(i,j)is the classi\ufb01cation error rate when the ith session is\nused as training set and the jth session is used as test set. In other\nwords,eis the average classi\ufb01cation error rate calculated over the\nfollowing six experiments, namely (1,2), (2,1), (1,3), (3, 1), (2,3) and\n(3,2).\nRecognition rate (%) MASC MSM KMSM KLD\nr= 4 96.51 91.47 95.74 84.5\nr= 8 96.51 87.21 94.19 81.4\nr= 12 94.96 85.66 92.64 77.52\nr= 16 93.8 81.4 89.15 72.48\nTABLE II\nVIDEO FACE RECOGNITION RESULTS ON THE VIDTIMIT DATABASE .\nWe evaluate the video face recognition performance of all me thods\nfordiverse sizes ofthe trainingand testsets.The objectiv e is toassess\nthe robustness of the methods with respect to the size of the t raining\nand test set. For this reason, each image set is re-sampled as\nXi,r=Xi(:,1 :r:n), i= 1,...,c.\nInthe above, the imageset Xiisre-sampledwithstep r,i.e.,onlyone\nimage every rimages is kept. In our experiments, we use different\nvalues of rranging from 4 to 16 with step 4. For each value of r, we7\n\u2212500\n0\n500\u2212600\u2212400\u2212200 0200400600800050100150200250300350400\nFig. 7. A typical face manifold from the VidTIMIT database. O bserve the\nfour clusters corresponding to the four different head pose s (face looking left,\nright, up and down).\nFig. 8. Video face recognition results on the VidTIMIT datab ase.\nmeasure the average classi\ufb01cation error rate according to t he relation\n(11).\nTable II shows the recognition performance, for rranging from\n4 to 16 with step 4. Figure 8 shows graphically the same result s.\nObserve that the KLD method that relies on density estimatio n is\nsensitivetothenumber oftheavailabledata.Also,noticet hatMSMis\nsuperior toKLD,whichisexpectedsinceKLDreliesontheimp recise\nassumption that data follow a Gaussian distribution. Furth ermore,\nKMSM, the nonlinear variant of MSM, outperforms the latter t hat\nhas trouble in capturing the nonlinear structures in the dat a. Finally,\nwe observe that MASC clearly outperforms its competitors in the\nvast majority of cases. At the same time, it stays robust to si gni\ufb01cant\nre-sampling of the data, since its performance remains almo st the\nsame for each value of r.\nC. Classi\ufb01cation results on Honda/UCSD\nWe further study the video-based face recognition performa nce\non the Honda/UCSD database. Figure 9 shows a few representat ive\nimages from a sample face manifold in the Honda/UCSD databas e.\nObserve the presence of large head pose variations along wit h facial\nexpressions. The projection of the manifold on the 3D space u sing\nONPP shows again clearly the manifold structure of the data ( see\nFigure 10), which implies that a graph-based method is more s uitable\nfor such kind of data.\n(a) pose 1\n (b) pose 2\n (c) pose 3\n (d) pose 4\n(e) pose 5\n (f) pose 6\n (g) pose 7\n (h) pose 8\nFig. 9. Head pose variations in the Honda/UCSD database.\n\u22122000200400600\n\u22121500\u22121000\u22125000500\u2212400\u22122000200400600800\nFig. 10. A typical face manifold from the Honda/UCSD databas e.\nThe Honda/UCSD database comes with a default splitting into\ntraining and test sets, which contains 20 training and 39 tes t video\nsequences. We use this default setup and we report the classi \ufb01cation\nperformance of all methods, under different data re-sampli ng rates.\nSimilarly as above, both training and test image sets are re- sampled\nwith step r, i.e.,Xi,r=Xi(:,1 :r:n), i= 1,...,c. Table III\nFig. 11. Video face recognition results on the Honda/UCSD da tabase.8\nRecognition rate (%) MASC MSM KMSM KLD\nr= 4 10084.62 87.18 84.62\nr= 6 10084.62 87.18 79.49\nr= 8 97.44 84.62 84.62 61.54\nr= 10 97.44 87.18 84.62 66.67\nr= 12 97.44 76.92 82.05 61.54\nTABLE III\nVIDEO FACE RECOGNITION RESULTS ON THE HONDA/UCSD DATABASE .\nshows the recognition rates, when rvaries from 4 to 12 with step\n2. Figure 11 shows the same results graphically. Recall that larger\nvalues of rimply sparser image sets. Observe again that KLD is\nmostly affected by r, by suffering loss in performance. This is not\nsurprising since it is a density-based method and densities cannot be\naccurately estimated (in general) with a few samples. MSM se ems\nto be more robust, yielding better results than KLD, but as ex pected,\nit is inferior to KMSM in the majority of cases. Finally, MASC is\nagain the best performer and it exhibits very high robustnes s against\ndata re-sampling.\nRegarding the relative performance of MASC and KMSM, we\nshould \ufb01nally stress out that KMSM is a kernel technique that\nattempts to capture the nonlinear structure of the data by as suming\na linear model after applying a nonlinear mapping of the data into a\nhighdimensional space. Although thismethodology stays ge neric and\npresents certain advantages, it is still not clear whether i t is capable\nof capturing the individual (e.g., manifold) structure of d iverse data\nsets. On the other hand, the MASC method explicitly relies on a\ngraph model that may \ufb01t much better the manifold structure of\nthe data. Furthermore, it provides a way to cope with the curs e\nof dimensionality, since the intrinsic dimension of the man ifolds is\ntypically very small. We believe that graph methods have a gr eat\npotential in this \ufb01eld.\nD. Video-based face recognition overview\nFor the sake of completeness, we review brie\ufb02y in this last se ction\nthe state of the art in video-based face recognition. Typica lly, one\nmay distinguish between two main families of methods; those that\nare based on subspace analysis and those that are based on den sity\nestimation (statistical methods). The most representativ e methods for\nthese two families are respectively the MSM [9], [10] and KMS M\n[12] methods and the solution based on KLD [13], which have be en\nused in the experiments above.\nAmong the methods based on subspace analysis, we should men-\ntion the extension of principal angles from subspaces, to no nlinear\nmanifolds. In a recent article [19] it was proposed to repres ent\nthe facial manifold by a collection of linear patches, which are\nrecoveredbyanon-iterative algorithmthataugments thecu rrentpatch\nuntil the linearity criterion is violated. This manifold re presentation\nallows for de\ufb01ning the distance between manifolds as integr ation of\ndistances between linear patches. For comparing two linear patches,\nthe authors propose a distance measure that is a mixture betw een\n(i) the principal angles and (ii) exemplar-based distance. However,\nit is not clearly justi\ufb01ed why such a mixture is needed and wha t\nis the relative bene\ufb01t over the individual distances. Moreo ver, their\nproposed method requires the computation of both geodesic a nd\nEuclidean distances as well as setting four parameters. On t he\ncontrary, our MASC method needs only one parameter ( k) to be set\nand it requires the computation of the Euclidean distances o nly. Note\n\ufb01nally that their method achieves comparable results with M ASC on\nthe Honda/UCSD database, but at a higher computational cost and at\nthe price of tuning four parameters.Along the same lines, the authors in [20] propose a similarit y\nmeasure between manifolds that is a mixture of similarity be tween\nsubspaces andsimilaritybetweenlocallinearpatches. Eac hindividual\nsimilarity is based on a weighted combination of principal a ngles and\nthose weights are learnt by AdaBoost for improved discrimin ative\nperformance. In contrast to the previous paper [19], the lin ear patches\nare extracted here using mixtures of Probabilistic PCA (PPC A).\nPPCA mixture \ufb01tting is a highly non-trivial task, which requ ires an\nestimateofthelocal principalsubspace dimensionandital soinvolves\nmodel selection. This stepis quite computationally intens ive, as noted\nin [19].\nThe main limitation of the statistical methods such as KLD [1 3]\nis the inadequacy of the Gaussianity assumption of face imag es\nsets; face sequences rather have a manifold structure. The t est video\nframes are moreover not independent, so that the i.i.d assum ption is\nunrealistic as well. The authors in [21] therefore extend th e work\nof KL divergence by replacing the Gaussian densities by Gaus sian\nMixture Models (GMMs), which provides a more \ufb02exible method for\ndensity estimation. However, the KL divergence in this case cannot\nbe computed in a closed form, which makes the authors to resor t to\nMonte Carlo simulations that are quite computationally int ensive.\nFinally, therehave been afew other methods thatcannot be di rectly\ncategorized in the above families of methods. The authors in [22]\npropose ensemble similarity metrics that are based on proba bilistic\ndistance measures, evaluated in Reproducing Kernel Hilber t spaces.\nAll computations are performed under the Gaussianity assum ption,\nwhich is unfortunately not realistic for facial manifolds.\nIn [23], the authors provide a probabilistic framework for f ace\nrecognition from image sets. They model the identity as a dis crete or\ncontinuous random variable and they provide a statistical f ramework\nfor estimating the identity by marginalizing over face loca lization,\nillumination and head pose. Illumination-invariant basis vectors are\nlearnt for each (discretized) pose and the resulting subspa ce is used\nfor representing the low dimensional vector that encodes th e subject\nidentity. However, the statistical framework requires the computation\nof several integrals that are numerically approximated. Al so, the\nproposed method assumes that training images are available for every\nsubjectateachpossible pose andillumination,whichishar dtosatisfy\nin practice.\nX. Liu and T. Chen in [24] proposed a methodology based on\nadaptive hidden Markov models for video-based face recogni tion.\nThe temporal dynamics of each subject are learnt during trai ning and\nsubsequently used for recognition. However, the proposed a pproach\nassumes temporal order of the frames in the face sequence and\nunfortunately it is not applicable to the more generic probl em of\nrecognition from image sets. The study in [25] further inves tigates\nhow the performance of the above approach is affected by the f ace\nsequence length and the image quality.\nVI. CONCLUSIONS\nIn this paper we have addressed the problem of classi\ufb01cation of\nmultipleobservations ofthe same object. Wehave proposed t oexploit\nthe speci\ufb01c structure of this problem in a graph-based algor ithm\ninspired by label propagation. The graph-based algorithm r elies on\nthe smoothness assumption of the manifold in order to learn t he\nunknown label matrix, under the constraint that all observa tions\ncorrespond to the same class. We have formulated this proces s as\na discrete optimization problem that can be solved ef\ufb01cient ly by a\nlow complexity algorithm.\nWe provide experimental results that illustrate the perfor mance\nof the proposed solution for the classi\ufb01cation of handwritt en digits,\nfor object recognition and for video-based face recognitio n. In the\ntwo latter cases, the graph-based solution outperforms sta te-of-the-art9\nmethods on three publically available data sets. This clear ly outlines\nthe potential of the proposed graph-based solution that is a ble to\nadvantageously capture the structure of image manifolds.\nREFERENCES\n[1] C. Stauffer. Minimally-supervised classi\ufb01cation usin g multiple observa-\ntion sets. IEEE Int. Conf. on Computer Vision (ICCV) , 2003.\n[2] E. Kokiopoulou, S. Pirillos, and P. Frossard. Graph-bas ed classi\ufb01cation\nfor multiple observations of transformed patterns. IEEE Int. Conf.\nPattern Recognition (ICPR) , December 2008.\n[3] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, and B. Schol kopf.\nLearning with local and global consistency. Advances in Neural\nInformation Processing Systems (NIPS) , 2003.\n[4] O. Chapelle, B. Scholkopf, and A. Zien. Semi-Supervised learning . MIT\nPress, 2006.\n[5] X. Zhu and Z. Ghahramani. Learning from labeled and unlab eled data\nwith label propagation. Technical report CMU-CALD-02-107 , 2002.\nCarnegie Mellon University, Pittsburgh.\n[6] M. Szummer and T. Jaakkola. Partially labeled classi\ufb01ca tion with\nmarkov random walks. Advances in Neural Information Processing\nSystems (NIPS) , 2002.\n[7] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using\ngaussian \ufb01elds and harmonic functions. 20th Int. Conf. on Machine\nLearning (ICML) , 2003.\n[8] A. Pozdnoukhov and S. Bengio. Graph-based transformati on manifolds\nfor invariant pattern recognition with kernel methods. IEEE Int. Conf.\non Pattern Recognition (ICPR) , 2006.\n[9] K. Fukui and O. Yamaguchi. Face recognition using multi- viewpoint\npatterns for robot vision. Int. Symp. on Robotics Research , 15:192\u2013201,\n2005.\n[10] O.Yamaguchi, K.Fukui, and K.Maeda. Face recognition u sing temporal\nimage sequence. IEEE Int. Conf. on Automatic Face and Gesture\nRecognition , pages 318\u2013323, 1998.\n[11] G. H. Golub and C. Van Loan. Matrix Computations, 3rd edn . The John\nHopkins University Press, Baltimore, 1996.\n[12] H. Sakano and N. Mukawa. Kernel mutual subspace method f or robust\nfacial image recognition. 4th Int. Conf. on Knowledge-based Intelligent\nEngineering Systems and Allied Technologies (KES 2000) , 2000.\n[13] G. Shakhnarovich, J. W. Fisher, and T. Darrel. Face reco gnition from\nlong-term observations. European Conference on Computer Vision\n(ECCV), 3:851\u2013868, 2002.\n[14] B. Leibe and B. Schiele. Analyzing appearance and conto ur based\nmethods for object categorization. Int. Conf. on Computer Vision and\nPattern Recognition (CVPR\u201903) , 2003.\n[15] C. Sanderson. Biometric Person Recognition: Face, Speech and Fusion .\nVDM-Verlag, 2008.\n[16] K. C. Lee, J. Ho, M. H. Yang, and D. Kriegman. Video-based face\nrecognition using probabilistic appearance manifolds. IEEE Interna-\ntional Conference on Computer Vision and Pattern Recogniti on (CVPR) ,\npages 313\u2013320, 2003.\n[17] P. Viola and M. Jones. Robust real-time face detection. International\nJournal of Computer Vision , 57(2):137\u2013154, 2004.\n[18] E. Kokiopoulou and Y. Saad. Orthogonal neighborhood pr eserving\nprojections: A projection-based dimensionality reductio n technique.\nIEEE Transactions on Pattern Analysis and Machine Intellig ence,\n29(12):2143\u20132156, December 2007.\n[19] R. Wang, S. Shan, X. Chen, and W. Gao. Manifold-manifold distance\nwith application to face recognition based on image set. IEEE Interna-\ntional Conference on Computer Vision and Pattern Recogniti on (CVPR) ,\n2008.\n[20] T-K. Kim, O. Arandjelovic, and R. Cipolla. Boosted mani fold principal\nangles for image set-based recognition. Pattern Recognition , 40:2475\u2013\n2484, 2007.\n[21] O. Arandjelovi\u00b4 c, G. Shakhnarovich, J. Fisher, R. Cipo lla, and T. Darrell.\nFace recognition with image sets using manifold density div ergence.\nIEEE Int. Conf. on Computer Vision and Pattern Recognition ( CVPR),\n1:581\u2013588, 2005.\n[22] S. Zhou and R. Chellappa. From sample similarity to ense mble\nsimilarity: Probabilistic distance measures in reproduci ng kernel hilbert\nspace.IEEE Transactions on Pattern Analysis and Machine Intellig ence,\n28(6):917\u2013929, June 2006.\n[23] S. K. Zhou and R. Chellappa. Probabilistic identity cha racterization\nfor face recognition. IEEE Int. Conf. on Computer Vision and Pattern\nRecognition (CVPR) , 2:805\u2013812, 2004.[24] X. Liu and T. Chen. Video-based face recognition using a daptive hidden\nmarkov models. IEEE Int. Conference on Computer Vision and Pattern\nRecognition (CVPR) , 1:I\u2013340 \u2013 I\u2013345, 2003.\n[25] A. Hadid and M. Pietikainen. From still image to video-b ased face\nrecognition: an experimental analysis. Sixth IEEE International Con-\nference on Automatic Face and Gesture Recognition , pages 813 \u2013 818,\n2004."}
{"category": "abstract", "text": "\u2014We consider the problem of classi\ufb01cation of an object given\nmultiple observations that possibly include different tra nsformations. The\npossible transformations of the object generally span a low -dimensional\nmanifold in theoriginal signal space.Weproposeto takeadv antage of this\nmanifold structure for the effective classi\ufb01cation of the o bject represented\nby the observation set. In particular, we design a low comple xity solution\nthat is able to exploit the properties of the data manifolds w ith a\ngraph-based algorithm. Hence, we formulate the computatio n of the\nunknown label matrix as a smoothing process on the manifold u nder the\nconstraint that all observations represent an object of one single class. It\nresults into a discrete optimization problem, which can be s olved by an\nef\ufb01cient and low complexity algorithm. We demonstrate the p erformance\nof the proposed graph-based algorithm in the classi\ufb01cation of sets of\nmultiple images. Moreover, we show its high potential in vid eo-based\nface recognition, where it outperforms state-of-the-art s olutions that fall\nshort of exploiting the manifold structure of the face image data sets.\nIndex Terms \u2014Graph-based classi\ufb01cation, multiple observations sets,\nvideo face recognition, multi-view object recognition.\nI. INTRODUCTION\nRecent years have witnessed a dramatic growth of the amount o f\ndigital data that is produced by sensors or computers of all s orts. That\ncreates the need for ef\ufb01cient processing and analysis algor ithms in\norder to extract the relevant information contained in thes e datasets.\nIn particular, it commonly happens that multiple observati ons of\nan object are captured at different time instants or under di fferent\ngeometric transformations. For instance, a moving object m ay be\nobserved over a time interval by a surveillance camera (see F ig. 1(a))\nor under different viewing angles by a network of vision sens ors (see\nFig. 1(b)). This typically produces a large volume of multim edia\ncontent that lends itself as a valuable source of informatio n for\neffective knowledge discovery and content analysis. In thi s context,\nclassi\ufb01cation methods should be able to exploit the diversi ty of\nthe multiple observations in order to provide increased cla ssi\ufb01cation\naccuracy [1].\nWebuildonour previous work[2]andwe focus here onthe patte rn\nclassi\ufb01cation problem with multiple observations. We furt her assume\nthat observations are produced from the same object under di fferent\ntransformations, so that they all lie on the same low-dimens ional\nmanifold. We propose a novel graph-based algorithm built on label\npropagation [3]. Label propagation methods typically assu me that the\ndata lie on a low dimensional manifold living in a high dimens ional\nspace. They rely upon the smoothness assumption , which states that\nif two data samples x1andx2are close, then their labels y1and\ny2should be close as well. The main idea of these methods is to\nbuild a graph that captures the geometry of this manifold as w ell as\nthe proximity of the data samples. The labels of the test exam ples\nare derived by \u201cpropagating\u201d the labels of the labelled data along the\nmanifold, while making use of the smoothness property. We ex ploit\nthe speci\ufb01cities of our particular classi\ufb01cation problem a nd constrain\nThis work has been mostly performed while the \ufb01rst author was with the\nSignal Processing Laboratory (LTS4) of EPFL. It has been par tly supported\nby the Swiss National Science Foundation, under grant NCCR I M2.object s x1\nx2\nxm\n(a) Video frames of a moving object\nobject s\nx1\nxm-1xm\n(b) Network of vision sensors\nFig. 1. Typical scenarios of producing multiple observatio ns of an object.\ntheunknown labels tocorrespond toone singleclass.Thisle ads tothe\nformulation of a discrete optimization problem that can be o ptimally\nsolved by a simple and low complexity algorithm.\nWe apply the proposed algorithm to the classi\ufb01cation of sets\nof multiple images in handwritten digit recognition, multi -view\nobject recognition or video-based face recognition. In par ticular, we\nshow the high potential of our graph-based method for ef\ufb01cie nt\nclassi\ufb01cation of images that belong to the same data manifol d. For\nexample, the proposed solution outperforms state-of-the- art subspace\nor statistical classi\ufb01cation methods in video-based face r ecognition\nand object recognition from multiple image sets. Hence, thi s paper\nestablishes new connections between graph-based algorith ms and the\nproblems of classi\ufb01cation of multiple image sets or video-b ased\nface recognition, where the proposed solutions are certain ly very\npromising.\nThe paper is organized as follows. We \ufb01rst formulate the prob lem\nof classi\ufb01cation of multiple observation sets in Section II . We\nintroduce our graph-based algorithm inspired by label prop agation\nin Section III. Then we demonstrate the performance of the pr o-\nposed classi\ufb01cation method for handwritten digit recognit ion, object\nrecognition and video-based face recognition in Sections I V-A, IV-B\nand V, respectively.\nII. PROBLEM DEFINITION\nWe address the problem of the classi\ufb01cation of multiple obse r-\nvations of the same object, possibly with some transformati ons. In2\nlabelled example\nunlabelled  example\nFig. 2. Typical structure of the k-NN graph. Nirepresents the neighborhood\nof the sample xi.\nparticular, the problem is to assign multiple observations of the test\npattern/object sto a single class of objects. We assume that we have\nmtransformed observations of sof the following form\nxi=U(\u03b7i)s, i= 1,...,m,\nwhereU(\u03b7)denotes a (geometric) transformation operator with\nparameters \u03b7,whichisappliedon s.Forinstance, inthecase of visual\nobjects,U(\u03b7)may correspond to a rotation, scaling, translation, or\nperspective projection of the object. We assume that each ob servation\nxiisobtained byapplying atransformation \u03b7ions,whichis different\nfrom its peers (i.e., \u03b7i/ne}ationslash=\u03b7j, fori/ne}ationslash=j). The problem is to classify\nsin one of the cclasses under consideration, using the multiple\nobservations xi, i= 1,...,m.\nAssume further that the data set is organized in two parts\nX={X(l),X(u)}, whereX(l)={x1,x2,...,x l} \u2282Rdand\nX(u)={xl+1,...,x n} \u2282Rd, where n=l+m. Let also\nL={1,...,c}denote the label set. The lexamples in X(l)are\nlabelled{y1,y2,...,y l}, yi\u2208 L, and the mexamples in X(u)are\nunlabelled. The classi\ufb01cation problem can be formally de\ufb01n ed as\nfollows.\nProblem 1"}
{"category": "non-abstract", "text": "segmentation, handwriting, text lines, Historical documents, survey \n  2   \n1. Introduction  \nText line extraction is generally seen as a pr eprocessing step for tasks such as document \nstructure extraction, printed ch aracter or handwriting recognition. Many techniques have been \ndeveloped for page segmentation of printed doc uments (newspapers, scientific journals, \nmagazines, business letters) produced with modern  editing tools [57] [38]  [14] [39] [2]. The \nsegmentation of handwritten documents has also  been addressed with the segmentation of \naddress blocks on envelopes and mail pieces [9 ] [10] [15][48], and for authentication or \nrecognition purposes [53] [60]. More recently, th e development of handwritten text databases \n(IAM database, [34]) provides new material  for handwritten page segmentation.  \n Ancient and historical documents, printed or handwritten, strongly differ from the documents \nmentioned above because layout formatting requirements were looser. Their physical structure is thus harder to extract. In additi on, historical documents ar e of low quality, due to \naging or faint typing. They incl ude various disturbing elements such as holes, spots, writing \nfrom the verso appearing on th e recto, ornamentation, or seal s. Handwritten pages include \nnarrow spaced lines with overlapping and touc hing components. Characters and words have \nunusual and varying shapes, depending on the wr iter, the period and the place concerned. The \nvocabulary is also large and may include unusual names and words. Full text recognition is in \nmost cases not yet available, except for prin ted documents for which dedicated OCR can be \ndeveloped.   However, invaluable collections of historical documents are already digitized and indexed for \nconsulting, exchange and distant access pur poses which protect them from direct \nmanipulation. In some cases, highly structured editions have been established by scholars. But \na huge amount of documents are still to be ex ploited electronically. To  produce an electronic \nsearchable form, a document has to be indexe d. The simplest way of indexing a document \nconsists in attaching its main characteristics such as date, place and author (the so called \n\u2018metadata\u2019). Indexing can be enhanced when the document structure and content are exploited. When a transcription ( published version, diplomatic tran scription) is available, it \ncan be attached to the digitized document: this  allows users to retrieve documents from \ntextual queries.   Since text based representations  do not reflect the graphical features of such documents, a \nbetter representation is obtained  by linking the transcription to the document image. A direct \ncorrespondence can then be established betw een the document image and its content by \ntext/image alignment techniques [55]. This allows the creation of indexes where the position of each word can be recorded, and of links be tween both representations. Clicking on a word \non the transcription or in the index through a GUI allows user s to visualize the corresponding \nimage area and vice versa. To make such queries possible for handwritten sources of literary works, several projects have been carried out  under EU and National Programs: for instance \nthe so-called \u2018philological workstation\u2019 Bambi  [6][8] and within the Philectre  reading and \nediting environment [47]. The document analysis  embedded in such syst ems provides tools to \nsearch for blocks, lines and words, and ma y include a dedicated handwriting recognition \nsystem. Interactive tools are generally offe red for segmentation and recognition correction \npurposes. Several projects also  concern printed material: Debora  [5] and Memorial  [3]. Partial \nor complete logical structure can also be ex tracted by document analysis and corrected with  3  GUI as in the Viadocs  project [11][18]. However, document structure can also be used when \nno transcription is available. Word spotting techniques [22] [5 5] [46] can retrieve similar \nwords in the image document through an image query. When words of the image document are extracted by top down segmentation, which is generally the case, text  lines are extracted \nfirst.  \n                                                           \n     \n  \n \nFig. 1.  Examples of historical documents a) Provencal medieval manuscript.  b) one page from De Gaulle\u2019s \ndiaries c) an ancient Arabic do cument from Tunisian Archives. \n \nThe authentication of manuscripts in the paleographic sense can also make use of document analysis and text line extraction. Authenticatio n consists in retrievi ng writer characteristics \nindependently from document content. It genera lly consists in dating documents, localizing \nthe place where the document was produced, id entifying the writer by using characteristics \nand features extracted from blank spaces, line orie ntations and fluctuations, word or character \nshapes [43] [27] [4]. Page segmentation into text lines is performe d in most tasks mentioned above and overall \nperformance strongly relies on th e quality of this process. The purpose of this article is to \nsurvey the efforts made for historical documen ts on the text line segmentation task. Section 2 \ndescribes the characteristics of te xt line structures in histori cal documents and the different \nways of defining a text line. Pr eprocessing of document images (g ray level, color or black and \nwhite) is often necessary before text line extracting to prune superfluous information (non textual elements, textual elements from the verso)  or to correctly binarize the image.  This \nproblem is addressed in Section 3.1. In Sections 3.2-3.7 we survey  the different approaches to \nsegment the clean image into text lines. A taxono my is proposed, listed as projection profiles, \nsmearing, grouping, Hough-based, repul sive-attractive network a nd stochastic methods. The \nmajority of these techniques have been develo ped for the projects on historical documents \nmentioned above. In Section 3.8, we addres s the specific problem  of overlapping and \ntouching components. Concluding re marks are given in Section 4.  \n       \n 4   \n     \n \n \n \n \n \n \nFig. 2.  Reference lines and interfering lines with overlapping and touching components. \n \n2. Characteristics and representation of text lines  \nTo have a good idea of the physical structure of  a document image, one only needs to look at \nit from a certain distance: the lines and the blocks are immedi ately visible. These blocks \nconsist of columns, annotations in margins, stanzas, etc... As bloc ks generally have no \nrectangular shape in historical documents, th e text line structure becomes the dominant \nphysical structure. We first give some definiti ons about text line components and text line \nsegmentation. Then we describe the factors which make this text line segmentation hard. \nFinally, we describe how a text  line can be represented.  \n2.1 Definitions \nbaseline : fictitious line which follows and joins the lo wer part of the character bodies in a text \nline (Fig. 2)  median line : fictitious line which follows and joins the upper part of the character bodies in a \ntext line.  upper line : fictitious line which joins the top of ascenders. \n lower line : fictitious line which joins the bottom of descenders. \n overlapping components: overlapping components are descende rs and ascenders located in \nthe region of an adjacent line (Fig. 2).    touching components : touching components are ascenders  and descenders belonging to \nconsecutive lines which are thus connected . These components are large but hard to \ndiscriminate before te xt lines are known.  \n \n 5  text line segmentation : text line segmentation is a labeling process which consists in assigning \nthe same label to spatially aligned units (such as pixels , connected components or \ncharacteristic points). There are two categor ies of text line segmentation approaches: \nsearching for (fictitious) separating lines or paths, or sear ching for aligned physical units. The \nchoice of a segmentation technique depends on th e complexity of the text line structure of the \ndocument. \n2.2 Influence of author style \nbaseline fluctuation : the baseline may vary due to writer movement. It may be straight, \nstraight by segments, or curved.  \n \nline orientations : there may be different line orientations , especially on authorial works where \nthere are corrections and annotations.   \n \nline spacing : lines that are rather widely spaced lines are easy to find. The process of \nextracting text lines grows more difficult as interlines are narrowing; the lower baseline of the first line is becoming closer to the upper base line of the second line; also, descenders and \nascenders start to fill the blank space left for separating two adjacent text lines (Fig. 3).  \n \ninsertions : words or short text lines may appear betw een the principal text lines, or in the \nmargins.  \n2.3 Influence of poor image quality \nimperfect preprocessing : smudges, variable background intens ity and the presence of seeping \nink from the other side of the document make  image preprocessing particularly difficult and \nproduce binarization errors.  \n \nstroke fragmentation and merging:  punctuation, dots and broken strokes due to low-quality \nimages and/or binarization may produce many connected components;  conversely, words, \ncharacters and strokes may be split into several connected components. The broken \ncomponents are no longer linked to the median baseline of the writing and become ambiguous \nand hard to segment into th e correct text line (Fig. 3). \n2.4 Text line representation \nseparating paths and delimited strip: separating lines (or paths) ar e continuous fic titious lines \nwhich can be uniformly straight, made of straight  segments, or of curving joined strokes. The \ndelimited strip between two consecutive separating lines receives the same text line label. So \nthe text line can be represen ted by a strip with its couple of separating lines (Fig. 4). \n \nclusters: clusters are a general set-base d way of defining text lines. A label is associated with \neach cluster. Units within the same cluster belo ng to the same text line. They may be pixels,  6  connected components, or blocks enclosing pi eces of writing. A text line can be represented \nby a list of units with the same label.  \n \nFig. 3.  The three main axes of document complexity for text line segmentation. \n \nFig. 4. Various text line representations: paths, strings and baselines. \n \nstrings: strings  are lists of spatially aligned and ordered units. Each  string represents one text \nline.   baselines : baselines follow line fluctuations but partia lly define a text line. Units connected to \na baseline are assumed to belong to it. Comple mentary processing has to be done to cluster \nnon-connected units and touching components. \nline    proximityline fluctuationwriting fragmentation\nline    proximityline fluctuationwriting fragmentation\n 7   \n3. Text line segmentation  \n \nPrinted historical documents be long to a large period from 16th to 20th centuries (reports, \nancient books, registers, card archives). Th eir printing may be faint, producing writing \nfragmentation artifacts. However, text lines are still enclosed in rectangular areas. After the \ntext part has been extracted and restored, top-down and smearing te chniques are generally \napplied for text line segmentation. A large propor tion of historical documents are handwritten: \nscrolls, registers, private and official letter s, authorial drafts. The type of writing differs \nconsiderably from one document to another. It  can be calligraphed or cursive; various styles \ncan be observed (Fig. 1). In the context of cu rsive handwriting, statistic al information about \nline spacing and line orientation is hard to capture. Several techniques, which take into account handwriting and layout irregularities, as we ll as local and global characteristics of the \ntext lines, have been developed \n3.1 Preprocessing \nText line extraction would id eally process document images  without background noise and \nwithout non-textual elements; the writing w ould be well contrasted  with as little \nfragmentation as possible. In reality, preprocessing is often necessary. Although \npreprocessing has to be accurately adapted to each document and to its characteristics, we \nshortly describe here some pr eprocessing techniques that can be  performed before text line \nextraction.   Non-textual elements around the text such as book bindings, book sides, parts of fingers \n(thumb marks from someone holding the book open  f.i.) should be removed upon criteria such \nas position and intensity level. On the document itself, holes, stains, may be removed by high-pass filtering [12]. Other non-te xtual elements (stamps, seals) but also ornamentation, \ndecorated initials, can be removed using knowledg e about the shape, the color or the position \nof these elements [17]. Extracti ng text from figures (text segmen tation) can also be performed \non texture grounds [20][36] or by morphological f ilters [16][37]. Linear  graphical elements \nsuch as big crosses (called \u201cSt Andre\u2019s crosses\u201d ) appear in some of Flaubert\u2019s manuscripts. \nRemoving these elements is performed thr ough GUI by Kalman filtering in [31].   \n Textual but unwanted elements such as the wr iting on the verso (bleed through text) may be \nremoved by filtering and wavelet techniques [24][54][32] and by combining the verso image (the reverse side image) with th e recto one (front side image).  \n Binarization, if necessary, can be perfor med by global or local thresholding. Global \nthresholding algorithms are not generally ap plicable to historic al documents, due to \ninhomogeneous background. Thus, global thresholdi ng results in severe deterioration in the \nquality of the segmented document image. Seve ral local thres holding techniques have already \nbeen proposed to partially overcome such difficu lties [21]. These local methods determine the \nthreshold values based on the local properties of an image, e.g. pixel-by-pixel or region-by-\nregion, and yield relatively better binariza tion results when compared with global \nthresholding methods. Writing may be faint so  that over-segmentation or under-segmentation  8  may occur. The integral ratio technique [52] is  a two-stage segmentation technique adapted to \nthis problem. Background normalization [51] can be  performed before binarization in order to \nfind a global threshold more easily.  \n3.2 Projection\u2013based methods  \nProjection-profiles are commonly used for pr inted document segmentation. This technique \ncan also be adapted to handwritten documents with little overlap. The vertical projection-\nprofile is obtained by su mming pixel values along the horizont al axis for each y value. From \nthe vertical profile, the gaps between the text lines in the vertical direction can be observed \n(Fig. 5). \n\u2211\n\u2264\u2264=\nMxyxf y profile\n1),( )(  \nThe vertical profile is not sens itive to writing fragmentation. Variants for obtaining a profile \ncurve may consist in projecting black/white trans itions such as in Marti and Bunke [35] or the \nnumber of connected components, rather than pixels. The profile curve can be smoothed, e.g. \nby a Gaussian or median filter to eliminate lo cal maxima [33]. The profile curve is then \nanalysed to find its maxima and minima. Ther e are two drawbacks: sh ort lines will provide \nlow peaks, and very narrow lin es, as well as those includi ng many overlapping components \nwill not produce significant peaks. In case of skew or moderate fluctuations of the text lines, \nthe image may be divided into vertical strips and profiles sought inside each strip (Zahour et \nal. [58]). These piecewise projections are thus a means of adap ting to local fluctuations within \na more global scheme.   In Shapiro et al. [49], the global orientation (skew angl e) of a handwritten page is first \nsearched by applying a Hough transform on the entire image. Once this skew angle is \nobtained, projections are achieved along this angle. The number of maxima of the profile give \nthe number of lines. Low maxima are discarded on their value, which is compared to the \nhighest maxima. Lines are delimited by strips, se arching for the minima of projection profiles \naround each maxima. This techni que has been tested  on a set of 200 pa ges within a word \nsegmentation task.   In the work of Antonacopoulos and Karatzas [3], each minimum of the profile curve is a \npotential segmentation point.  Po tential points are then scored according to their distance to \nadjacent segmentation points. The reference distance is obtained from the histogram of \ndistances between adjacent potential segmenta tion points. The highest scored segmentation \npoint is used as an anchor to derive the re maining ones. The method is applied to printed \nrecords of the second World War which have regu larly spaced text lines. The logical structure \nis used to derive the text regions wh ere the names of interest can be found.  9   \nFig. 5.  Vertical projection-profile extracted on an autograph of Jean-Paul Sartre. \n \nThe RXY cuts method applied in He and Downt on [18], uses alternatin g projections along the \nX and the Y axis. This results in a hierarchic al tree structure. Cuts are found within white \nspaces. Thresholds are necessary to derive inte r-line or inter-block distances. This method can \nbe applied to printed documents (which are assu med to have these regular distances) or well \nseparated handwritten lines. \n3.3 Smearing methods \nFor printed and binarized documents, smearing methods such as the Run-Length Smoothing \nAlgorithm (Wong et al.  [57]) can be applied. Consecutive black pixels along the horizontal \ndirection are smeared: i.e. the white space between  them is filled with black pixels if their \ndistance is within a predefin ed threshold.  The bounding boxes of the connected components \nin the smeared image enclose text lines.   A variant of this method adapted to gray leve l images and applied to printed books from the \nsixteenth century consists in accumulating the image gradient along th e horizontal direction \n(LeBourgeois [25]). This method has been ad apted to old printed documents within the \nDebora project [26]. For this purpose, numer ous adjustments in the method concern the \ntolerance for character alignm ent and line justification.  \n Text line patterns are found in the work of Sh i and Govindaraju [50] by building a fuzzy run \nlength matrix. At each pixel, the fuzzy run-leng th is the maximal extent of the background \nalong the horizontal direction. Some foreground pixels may be skipped if their number does \nnot exceed a predefined value. This matrix is threshold to make pieces of text lines appear \nwithout ascenders and des cenders (Fig. 6).  Parameters have  to be accurately and dynamically \ntuned.  \n3.4 Grouping methods  \nThese methods consist in building alignments  by aggregating units in a bottom-up strategy. \nThe units may be pixels or of higher level, su ch as connected components, blocks or other \nfeatures such as salient points. Units are then joined together to form alignments. The joining scheme relies on both local and gl obal criteria, which are used  for checking local and global \nconsistency respectively.    \n 10   \n \nFig. 6 Text line patterns extracted from a letter of Georges Washington (reprinted from Shi and Govindaraju \n[50], \u00a9 [2004] IEEE). Foreground pixels have been smeared along the horizontal direction. \n \nContrary to printed documents, a simple neares t-neighbor joining scheme  would often fail to \ngroup complex handwritten units, as the nearest neighbor often belongs to another line. The \njoining criteria used in the methods described below are adapted to the type of the units and \nthe characteristics of the documents under study.  But every method has to face the following: \n- initiating alignments: one or several seeds for each alignment. - defining a unit\u2019s neighborhood for reaching the next  unit. It is generally a rectangular or \nangular area (Fig. 7).  - solving conflicts. As one unit may belong to several alignments  under constructi on, a choice \nhas to be made: discard one alignment or keep bo th of them, cutting the un it into several parts. \n Hence, these methods include one or several quali ty measures which ensure that the text line \nunder construction is of good qu ality. When comparing the quality measures of two \nalignments in conflict, the alignment of lower quality can be discarded (Fig. 7).  Also, during \nthe grouping process, it is possible to choose between th e different units that can be \naggregated within the same neighborhood by eval uating the quality of each of the so-formed \nalignments.  \n 11   \nFig. 7. Angular and rectangular neighborhoods from point and rectangular units (left). Neighborhood defined by \na cluster of units (upright).  Two alignments A and B in conflict: a quality measure will choose A and discard B \n(down right). \n \nQuality measures generally include the strength of the alignment, i.e. the number of units \nincluded. Other quality elements may concer n component size, component spacing, or a \nmeasure of the alignment\u2019s straightness.   \nFig. 8. Text lines extracted on Church Registers (reprinted from Feldbach [12] with permission from the author). \n  \nLikforman-Sulem and Faure have developed in [28] an iterat ive method based on perceptual \ngrouping for forming alignments, which has been a pplied to handwritten pa ges, author drafts \nand historical documents [29][47]. Anchors ar e detected by selectin g connected components \nelongated in specific directions (0\u00b0, 45\u00b0, 90\u00b0, 12 5\u00b0). Each of these anchors becomes the seed \nof an alignment. First, each anchor, then each alignment, is extended to the left and to the \nright. This extension uses three Gestalt criter ia for grouping components: proximity, similarity \nand direction continuity. The threshold is iteratively incremented in order to group \ncomponents within a broader neighborhood until no change occurs. Between each iteration, \nalignment quality is checked by a quality measure which gives higher rates to long alignments including anchors of the same direction. A pe nalty is given when th e alignment includes \nanchors of different directions. Two alignments  may cross each other, or overlap. A set of \nABcd\n\u03b1\nbd\ncABcd\n\u03b1\nbd\ncAB\nABcd\n\u03b1cd\n\u03b1\nbd\ncbd\nc 12  rules is applied to solve these conflicts taking into account the quality of each alignment and \nneighboring components of higher order (Fig. 14). \n \nIn the work of Feldbach and T\u00f6nnies [12 ][13], body baselines are searched in Church \nRegisters images. These documents include lots of fluctuating and overlapping lines. \nBaselines units are the minima points of the writ ing  (obtained here from the skeleton). First \nbasic line segments (BLS) are co nstructed, joining each minima point to its neighbors.  This \nneighborhood is defined by an angular region (+-20\u00b0) for the first unit grouped, then by a \nrectangular region enclosing the points already joined for the remaining ones. Unwanted basic \nsegments are found from minima points det ected in descenders and ascenders. These \nsegments may be isolated or in conflict w ith others. Various heuristics are defined to \neliminate alignments on their size, or the lo cal inter-line distance a nd on a quality measure \nwhich favours alignments whose units are in the same direction rather than nearer units but \npositioned lower or higher than the current direction. Conflicting alignments can be \nreconstructed depending on the topology of th e conflicting alignments. The median line is \nsearched from the baseline and from maxima points (Fig. 8). Pixels lying within a given \nbaseline and median line are clustered in th e corresponding text line,  while ascenders and \ndescenders are not segmented. Correct segmen tation rates are reported between 90% and 97 \n% with adequate parameter adjustment. The seven documents tested range from the 17th to the \n19th century. \n3.5 Methods based on the Hough transform \nThe Hough transform is a very popular technique [19] for finding straight  lines in images. In \nLikforman-Sulem et al.  [30], a method has been deve loped on a hypothe sis-validation \nscheme. Potential alignments are hypothesized in the Hough domain and validated in the \nImage domain. Thus, no assumption is made about  text line directions  (several may exist \nwithin the same page). The centroids of the connected components are the units for the Hough \ntransform. A set of aligne d units in the image along a line with parameters ( \u03c1, \u03b8) is included \nin the corresponding cell ( \u03c1, \u03b8) of the Hough domain. Alignmen ts including a lot of units \ncorrespond to high peaked cells of the Hough domai n.  To take into acc ount fluctuations of \nhandwritten text lines, i.e. the fact that units within a text line are not perfectly aligned, two \nhypotheses are considered for each alignment and an alignment is formed from units of the \ncell structure  of a primary cell .   13   \nFig. 9.  Hypothesized cells ( \u03c10, \u03b80) and (\u03c11, \u03b81) in Hough space. Each peak corresp onds to perfectly aligned units. \nAn alignment is composed of units belonging to a cluster of cells (the  cell structure ) around a primary cell. \n \nA cell structure of a cell ( \u03c1, \u03b8) includes all the cells lying in a cluster centered around ( \u03c1, \u03b8). \nConsider the cell ( \u03c10, \u03b80) having the greatest count of units. A second hypothesis ( \u03c11, \u03b81) is \nsearched in the cell structure of ( \u03c10, \u03b80).  The alignment chosen between these two hypotheses \nis the strongest one, i.e. the one which includ es the highest number of units in its cell \nstructure. And the corresponding cell ( \u03c10, \u03b80) or (\u03c11, \u03b81) is the primary cell (Fig. 9).  \nHowever, actual text lines rare ly correspond to alignments with the highest number of units as \ncrossing alignments (from top to bottom for wr iting in horizontal direction) must contain \nmore units than actual text lines. A potential  alignment is validated (or invalidated) using \ncontextual information, i.e. considering its internal  and external  neighbors. An internal \nneighbor of a unit j is a within-Hough alignmen t neighbor. An external neighbor is a out of \nHough alignment neighbor which lies  within a circle of radius \u03b4j from unit j. Distance \u03b4j is the \naverage distance of the internal  neighbor distances from unit j.  To be validated, a potential \nalignment may contain fewer external units than internal ones. This enables the rejection of alignments which have no perceptual relevance.  This method can extract oriented text lines \nand sloped annotations under the assumption that such lines are almost straight (Fig. 10).  \u03c1\n\u03b8(\u03c10, \u03b80)\n(\u03c11, \u03b81)#units\n\u03c1\n\u03b8(\u03c10, \u03b80)\n(\u03c11, \u03b81)#units 14   \nFig. 10. Text lines extracted on an autograph of Miguel Angel Asturias. The orientations of traced lines \ncorrespond to those of the prim ary cells found in Hough space. \n \nThe Hough transform can also be ap plied to fluctuating lines of handwritten drafts such as in \nPu and Shi [45]. The Hough transform is first a pplied to minima points (units) in a vertical \nstrip on the left of the image. The alignments in the Hough domain are searched starting from \na main direction, by grouping cells  in an exhaustive search in 6 directions. Then a moving \nwindow, associated with a clustering scheme in  the image domain, assigns the remaining units \nto alignments.  The clustering scheme ( Natural Learning Algorithm ) allows the creation of \nnew lines starting in the middle of the page.  \n3.6 Repulsive-Attractive network method \nAn approach based on attractive-repulsive forces  is presented in Oztop et al.  [40]. It works \ndirectly on grey-level images  and consists in iteratively adapting the y-position of a \npredefined number of baseline units. Baselines are constructed one by one from the top of the \nimage to bottom.  Pixels of the image act as  attractive forces for baselines and already \nextracted baselines act as repulsive forces.  The ba seline to extract is initialized just under the \npreviously examined one, in order to be repe lled by it and attracted by the pixels of the line \nbelow (the first one is initialized in the bla nk space at top of the document). The lines must \nhave similar lengths. The result is a set of pseudo-baselines, each one passing through word \nbodies (Fig. 11). The method is applied to an cient Ottoman document archives and Latin \ntexts.  \n 15  Fig. 11. Pseudo baselines extracted by a Repulsive-Attractive network on an Ancient Ottoman text (reprinted \nfrom Oztop et al.  [40] Copyright (1999) with permission from Elsevier). \n3.7 Stochastic method \nWe present here a method based on a probabilistic Viterbi  algorithm (Tseng and Lee \n[56]), which derives non-linear  paths between overlapping te xt lines. Although this method \nhas been applied to modern Chinese handwritten  documents, this princi ple could be enlarged \nto historical documents which often include overlapping lines. Line s are extracted through \nhidden Markov modeling. The image is first divi ded into little cells (depending on stroke \nwidth), each one corresponding to  a state of the H MM (Hidden Markov Model). The best \nsegmentation paths are searched from left to  right; they correspond to paths which do not \ncross lots of black points and whic h are as straight as possible.  However, the displacement in \nthe graph is limited to immediately superior or in ferior grids. All best paths ending at each y \nlocation of the image are considered first. Elim ination of some of these paths uses a quality \nthreshold T: a path whose probabi lity is less than T is discar ded. Shifted paths are easily \neliminated (and close paths are removed on qua lity criteria). The method succeeds when the \nground truth path between text lines is slightly changing along the y-direc tion (Fig. 12). In the \ncase of touching components, the path of hi ghest probability will cross the touching \ncomponent at points with as less black pixels  as possible. But the method may fail if the \ncontact point contains a lot of black pixels.  \nFig. 12. Segmentation paths obtained by a stochastic method (reprinted from Tseng and Lee [56], Copyright \n(1999) with permission from Elsevier). \n \n \n 16  3.8  Processing of overlappi ng and touching components \nOverlapping and touching components are the main  challenges for text line extractions since \nno white space is left between lines. Some of  the methods surveyed above do not need to \ndetect such components because they extract only baselines (3.4, 3.6), or because in the \nmethod itself some criteria make paths avoid cr ossing black pixels (c.f . Section 3.7). This \nsection only deals with methods  where ambiguous components ( overlapping or touching) are \nactually detected before, during or  after text line segmentation \n Such criteria as component size, the fact that the component belongs to several alignments, or \non the contrary to no alignment, can be used for detecting ambiguous components. Once the component is detected as ambiguous, it must be classified into three categories: the \ncomponent is an overlapping component which be longs to the upper (res p. lower) alignment, \nthe component is a touching component which has to be decomposed into several parts (two \nor more parts, as components may belong to three or more alignm ents in historical \ndocuments).  The separation along the vertical di rection is a hard problem which can be done \nroughly (horizontal cut), or more accurately by  analysing stroke contours and referring to \ntypical configurations (Fig. 13). \n \n \nFig. 13. Set of typical overlapping configurations (adapted from Piquin et al.  [44]). \n \nThe grouping technique presented in Section 3.4 detects an ambiguous component during the \ngrouping process when a conflict occurs between tw o alignments [28] [29] . A set of rules is \napplied to label the component as overlap ping or touching. The ambiguous component \nextends in each alignment region. The rules use as  features the density of  black pixels of the \ncomponent in each alignment region, alignment proximity and contextual information (positions of both alignments around the component). An overlapping component will be assigned to only one alignment. And the se paration of a touching component is roughly \nperformed by drawing a horizontal frontier segmen t. The frontier segment position is decided \nby analysing the vertical projection profile of the component. If the projection profile includes \ntwo peaks, the cut will be done middle way from  them, as in Figure 14. Else the component \nwill be cut into two equal parts.  17   \nFig. 14. Touching component separated  in a \u2018Lettre de Remission\u2019. \n \nIn Likforman-Sulem et al.  [30], touching and overlapping co mponents are detected after the \ntext line extraction process de scribed in Section 3.5. These co mponents are those which are \nintersected by at least two different lines ( \u03c1,\u03b8) corresponding to primar y cells of validated \nalignments.  \n In Zahour \net al.  [58][59], the document page is first cut into eight equal columns. A \nprojection-profile is performed on each colum n. In each histogram, tw o consecutive minima \ndelimit a text block. In order to detect t ouching and overlapping components, a k-means \nclustering scheme is used to classify the text  blocks so extracted in to three classes: big, \naverage, small. Overlapping components necess arily belong to big phy sical blocks. All the \noverlapping cases are found in the big text blocks  class. All the \u201cone line\u201d blocks are grouped \nin the average block text class. A second k-mean s clustering scheme finds the actual inter-line \nblocks; put together with the \u201cone line\u201d bloc k size, this determines the number of pieces a \nlarge text block must be cut into (cf. Fig. 16).   A similar method such as the one presented a bove is applied to Bangla handwriting Indian \ndocuments in Pal and Datta [41]. The document is  divided into vertical strips. Profile cuts \nwithin each strip are computed to obtain anch or points of segmentation (PSLs) which do not \ncross any black pixels. These poi nts are grouped through strips by neighboring cr iteria. If no \nsegmentation point is present in the adjacent strip, the baseline is extended near the first black \npixel encountered which belongs to an overlap ping or touching compon ent. This component \nis classified as overlapping or touching by analys ing its vertical extens ion (upper, lower) from \neach side of the intersection point. An empirical rule classifies the component. In the touching \ncase, the component is hor izontally cut at the inte rsection point (Fig. 15). \n \nFig.15. Overlapping components separated (circle) and touching component separated into two parts (rectangle) \nin Bangla writing (from Pal and Datta [41], \u00a9 [2003] IEEE). \n \n 18  Some solutions for separation of units belonging to several text lines can be found also in the \ncase of mail pieces and handwritten databases wh ere efforts have been  made for recognition \npurposes [44] [7]. In the work of Piquin et al.  [44], separation is made  from the skeleton of \ntouching characters and the use of a dictionary  of possible touching co nfigurations (Fig. 13). \nIn Bruzzone and Coffetti [7], the contact poin t between ambiguous strokes is detected and \nprocessed from their external border. An accurate  analysis of the contour near the contact \npoint is performed in order to separate the st rokes according to two registered configurations: \na loop in contact with a stroke, or two loops in contact. In simple cases of handwritten pages \n(Marti  and Bunke [35]), the center of gravity of the connect ed component is used either to \nassociate the component to the current line or to the following line, or to cut the component \ninto two parts. This works well if the compon ent is a single character. It may fail if the \ncomponent is a word, or part of a word, or even several words.  \n3.9 Non Latin documents \nThe inter-line space in Latin documents is fill ed with single dots, ascenders and descenders. \nThe Arabic script is connected and cursive. La rge loops are present in the inter-line space and \nancient Arabic documents include diacritical points [1]. In the Hebrew squared writing, the baseline is situated on top of characters. Do cuments such as decorated Bibles, prayer books \nand scientific treatises include diacritical po ints which represent vowels. Ancient Hebrew \ndocuments may include decorated words but no d ecorated initials as there is no upper/lower \ncase character concept in this script. In the al phabets of some Indian scripts (like Devnagari, \nBangla and Gurumukhi), many basic characters have an horizontal line (the head line) in the \nupper part [42]. In Bangla and Telugu text, t ouching and overlapping occur frequently [23].To \ndate, the published studies on historical documents concern Ar abic and Hebrew. Work about \nChinese and Bangla Indian writings on good quality  documents have been already mentioned \nin Sections 3.7 and 3.8: they should be also su itable to ancient documen ts as they include \nlocal processing.  \n3.9.1 Ancient Arabic documents \nFigure 1 is a handwritten page extracted from  a book of the Tunisian National Library. The \nwriting is dense and inter-line space is faint. Se veral consecutive lines are often connected by \none character at least, and the overlapping situations are obvious. Baseline waving produces \nvarious text or ientations.  \n The method developed in Zahour \net al.  [59] begins with the de tection of overlapping and \ntouching components presented in \u00a73.8, using a tw o-stage clustering process which separates \nbig blocks including several lines into severa l parts.  Blocks are then linked by neighborhood \nusing the y coordinates. Figure 16 shows line separators using the clustering technique \nrecursively, as descri bed in Section  3.8. \n  19   \nFig.  16.  Text line segmentation of the an cient Arabic handwritten document in Fig. 1. \n3.9.1  Ancient Hebrew documents \nThe manuscripts studied in Likforman-Sulem et al.  [27], are written in Hebrew, in a so-called \nsquared writing as most characters are made of  horizontal and vertical  strokes. They are \nreproducing the biblical text of the Pentateu ch. Characters are calligraphed by skilled scribes \nwith a quill or a calamus. The Scrolls, intende d to be used in the synagogue, do not include \ndiacritics. Characters and words are written pr operly separated but digitization make some \ncharacters touch. Cases of overlapping components  occur as characters such as Lamed, Kaf, \nand final letters include ascende rs and descenders. Since the majority of characters are \ncomposed of one connected component, it is  more convenient to perform text line \n 20  segmentation from connected components un its. Fig. 17 shows the resulting segmentation \nwith the Hough-based method presented in Section 3.5.  \n \n \nFig. 17. Text line segmentation of a Hebrew document (Scroll). \n 21   Table 1. Text line segmentation methods suitable for historical documents \n \nAuthors Description Line \nDescription Writing \nType Units Suitable for Project/ \nDocuments \n[Antonacopoulos and Karatzas, \n2004] projection \nprofiles linear paths Latin \nprinted pixels separated \nlines Memorial/person\nal records(World \nWar II) \n[Calabretto and \nBozzi, 1998] projection \nprofiles (gray level image) linear paths cursive \nhandwriting pixels separated \nlines Bambi/italian \nmanuscripts (16\nth \ncentury) \n[Feldbach and T\u00f6nnies, 2001] grouping  \nmethod baselines  cursive \nhandwritingminima \npoints fluctuating \nlines Church registers \n(18\nth, 19th \ncentury) \n[He and Downton, \n2003] projections \n(RXY cuts) linear paths Latin \nprinted and \nhandwritingpixels separated \nlines Viadocs/ Natural \nHistory Cards \n[Lebourgeois et al. , \n2001] smearing \n(accumulated \ngradients) clusters Latin \nprinted  pixels separated \nlines Debora/books \n(16th century) \n[Likforman-Sulem and Faure, 1994] grouping  strings Latin \nhandwriting connected \ncomponentsfluctuating \nlines  Philectre/ \nauthorial manuscripts \n[Likforman-Sulem \net al. , 1995] Hough \ntransform, \n(hypothesis-validation \nscheme) strings Latin \nhandwriting connected \ncomponentsdifferent \nstraight line \ndirections Philectre/ \nauthorial \nmanuscripts, manuscripts of \nthe 16\nth century \n[Oztop et al. , 1997] repulsive -\nattractive network baselines Arabic and \nLatin  handwritingpixels (gray \nlevels) fluctuating \nlines (same size) ancient Ottoman \ndocuments \n[Pal and Datta, \n2003] piecewise \nprojections piecewise \nlinear paths Bangla    \nhandwriting  \n segmentatio\nn points overlapping/ \ntouching lines Indian \nhandwritten \ndocuments \n[Pu and Shi, 1998] Hough \ntransform \n(moving window)  clusters Latin \nhandwritingminima \npoints fluctuating \nlines handwritten \ndocuments \n[Shapiro et al. , \n1993] projection \nprofiles linear paths Latin \nhandwritingpixels skewed \nseparated \nlines handwritten \ndocuments \n[Shi and \nGovindaraju, 2004] smearing \n(fuzzy run \nlength) cluster Latin \nhandwritingpixels straight \ntouching lines Newton, Galileo \nmanuscripts \n[Tseng and Lee, 1999 ] stochastic \n(probabilistic Viterbi \nalgorithm) non linear \npaths Chinese \nhandwritingpixels overlapping \nlines handwritten \ndocuments \n[Zahour et al. , \n2004] piecewise \nprojection and k-means \nclustering piecewise \nlinear paths Arabic \nhandwritingtext blocks overlapping/ \ntouching lines. ancient Arabic \ndocuments        \n \n \n    22   \n4. Discussion and concluding remarks \nAn overview of text line segmentation methods  developed within di fferent projects is \npresented in Table 1. The achieved taxonomy consists  in six major categories. They are listed \nas: projection-based, smeari ng, grouping, Hough-based, repulsive -attractive network and \nstochastic methods. Most of these methods ar e able to face some im age degradations and \nwriting irregularities specific to historical documents, as shown in the last column of Table 1.   Projection, smearing and Hough-ba sed methods, classically adapted to straight lines and \neasier to implement, had to be completed and enriched by local considerations (piecewise \nprojections, clustering in Hough space, use of a moving window, ascender and descender \nskipping), so as to solve some problems in cluding: line proximity, overlapping or even \ntouching strokes, fluctuating cl ose lines, shape fragmentation occurrences.  The stochastic \nmethod (achieved by the Viterbi decision algorith m) is conceptually more robust, but its \nimplementation requires great care, particularly the initialization phase. As a matter of fact, \ntext-line images are initially divided into mxn grids (each cell being a node), where the values \nof the critical parameters m and n are to be determined according to the estimated average \nstroke width in the images. Re presenting a text line by one or  more baselines (RA method, \nminima point grouping) must be completed by labeling those pixels not connected to, or \nbetween the extracted baselines.  The recurrent nature  of the repulsive-a ttractive method may \ninduce cascading detecting errors followi ng a unique false or bad line extraction.  \n Projection and Hough-based methods are suitable for clearly separated lines. Projection-based \nmethods can cope with few overlapping or to uching components, as long text lines smooth \nboth noise and overlapping effects. Even in more critical cases, classifying the set of blocks \ninto \u201cone line width\u201d blocks and \u201cseveral line s width\u201d blocks allows the segmentation process \nto get statistical measures so as to segment more  surely the \u201cseveral lines  width\u201d  blocks. As a \nresult, the linear separator path may cross overlapping components. However, more accurate segmentation of the overlappi ng components can be performed after getting the global or \npiecewise straight separator, by looking closely at the so crossed strokes. The stochastic \nmethod naturally avoids crossing  overlapping components  (if they are not too close): the \nresulting non linear paths turn around obstacles. When lines are very close, grouping methods \nencounter a lot of conflicting configurations . A wrong decision in an early stage of the \ngrouping results in errors or incomplete alignm ents.  In case of touching components, making \nan accurate segmentation requires additional know ledge (compiled in a dictionary of possible \nconfigurations or represented by logical or fuzzy rules).    \n \n    Concerning text line fluctuations, baselin e-based representations seem to fit naturally. \nMethods using straight line-based representations  must be modifi ed as previously to give non \nlinear results (by piecewise projections or neighboring considerations in Hough space). The \nmore fluctuating the text line, the more refi ned local criteria must be. Accurate locally \noriented processing and careful grouping ru les make smearing and grouping methods \nconvenient. The stochastic methods also se em suited, for they can  generate non linear \nsegmentation paths to separate overlapping char acters, and even more to derive non linear \ncutting paths from touching character s by identifying the shortest paths. \n  23  Pixel based methods are naturally robust at dealing with writing fragmentation. But, as a \nconsequence of writing fragmentation, when un its become fragmented, sub-units may be \nlocated far from the baseline. Spurious charac teristic points are then  generated, disturbing \nalignment and implying a loss of accuracy, or more, a wrong final representation.  \n Quantitative assessment of performance is not generally yielded by the authors of the methods; when it is given, this is on a reduced  set of documents. As for all segmentation \nmethods, ground truth data are harder to obtain than for classification methods. For instance \nthe ground truth for the real baseline may be hard  to assess. Text line segmentation is often a \nstep in the recognition algorithm  and the segmentation task is not evaluated in isolation. To \ndate, no general study has been carried out to compare the different methods. Text line \nrepresentations differ and methods are gene rally tuned to a class of documents.  \n Analysis of historical document images is a relatively new domain. Text line segmentation \nmethods have been developed within several projects which perfor m transcript mapping, \nauthentication, word mapping or word recogniti on. As the need for recognition and mapping \nof handwritten material increases, text lin e segmentation will be used more and more. \nContrary to printed modern documents, a hist orical document has unique characteristics due \nto style, artistic effect and writer skills. Th ere is no universal segmentation method which can \nfit all these documents. The techniques presen ted here have been proposed to segment \nparticular sets of documents. They can howev er be generalized to other documents with \nsimilar characteristics, with parameter tuning th at depends on script size, stroke width and \naverage spacing.   \nThe major difficulty consists in obtaining a precise text line, with all descenders and \nascenders segmented for accessing isolated words. As segmentation and recognition are dependent tasks, the exact segmentation of touching pieces of writing may need some \nrecognition, or knowledge about th e possible touching configurati ons. Text line segmentation \nalgorithms will benefit from automatic extraction of document characteristics leading to an easier adaptation to the document under study."}
{"category": "abstract", "text": "There is a huge amount of historical documents in libraries and in various National Archives that have not been \nexploited electronically. Although auto matic reading of complete pages re mains, in most cases, a long-term \nobjective, tasks such as word spotting, text/image ali gnment, authentication and extraction of specific fields are \nin use today. For all these tasks, a major step is doc ument segmentation into text  lines. Because of the low \nquality and the complexity of these documents (backgro und noise, artifacts due to aging, interfering lines), \nautomatic text line segmentation remains an open research  field. The objective of this paper is to present a \nsurvey of existing methods, developed during the last decade, and dedicated to documents of historical interest.  \n \nKeywords"}
{"category": "non-abstract", "text": "A region based \nsegmentation is applied on a 10 meter resolution \nmulti-spectral image. The result is used as marker in a \n\u201cmarker-controlled watershed method using edges\u201d \non a 2.5 meter resolution panc hromatic image. Very \npromising results have been obtained even on images \nwhere the limits of the target objects are not appar ent.\n1. Introduction\nWith the spread of very high-resolution satellite \nimages, more sophisticated image processing systems \nare required for the automatic extraction of \ninformation. In the frame of a research project of \nCNES1 to develop tools and algorithms to exploit \nimages acquired by the new generation Pleiades \nsatellites, a database of \u201ccartographic object images\u201d \nhas been prepared. This database will be used to detect \nthe target objects in a global scene, on a very large \nsatellite image. For this task, low-level pixel based \nclassification methods are not very successful, mainly \nbecause they disregard the structural features of the \ntarget objects.  Man-made  geographical objects have \nwell-defined (but quite variable), mostly geometrical \n1 French National Space Agencystructures. In low-resolution, the objects disappear and \nbecome part of the texture. However, in high \nresolution, the  shape features and the spatial relations \nbetween the objects  are  perceivable and exploitable.\nAs a first module, a detection method that uses very \nbasic structural and spatial features of the objects has \nbeen developed [1].  This method generates an object \nmodel from the given examples. As input, the system \nuses sample images of objects manually segmented by \nan expert. The images are transformed to Attributed \nRelational Graphs (ARGs). A model representing the \ncommon features of the objects is constructed applying \ngraph-matching algorithms [2]. \nIn order to generate the model from the initial \nsatellite images, the preliminary task is the extraction \nof the objects from the sample images (as done  by the \nmanual segmentation). The satellite images of the same \nobject in two different spectral bands and resolutions \nare provided (Fig 1): A panchromatic image (Ip) with \nresolution of 2.5 meters, and a multispectral (Im) image \nwith resolution of 10 meters. \nFig 1. Ip and Im images of a bridge and a round- about\nThis paper describes a hybrid segmentation \napproach that uses the two images of different \nresolutions, and that combines region-based and edge-\nbased methods by a \u201cmarker-controlled watershed \nmethod using edges\u201d. The originality of this method \nlies in the fusion of complementary information from \ndifferent sources and methods  to obtain a good \nsegmentation. \nThe ultimate objective is to combine the two \nmodules and to integrate them in an interactive object \ndetection software based on query by example. The \nobject model learned from the examples will be used as a semantic filter (together with a radiometric filter) in \nthe detection of the candidate regions.  \n2. M odel Generation\nThe segmented images are first split into primitives \neither using simple geometric shapes (rectangles and \ncircles) or using the skeleton of the objects. They are \nthen transformed into ARGs. The features of primitives \nare stored in the vertices of the graph and the features \nof connections in the edges. The vertex attributes are \nthe type of the object and the edge attributes are the \ntype and direction of the connection.  \nFig 2. Model generation\nFrom the ARGs belonging to a class of object, the \nprototypes, the most common representations of each \nclass, are detected using an exact matching algorithm. \nThe model is obtained by finding the Maximal \nCommon Subgraph (MaxCSg) and the Minimal \nCommon Supergraph  (MinCSg) from the prototypes of \nan object. The main steps of the algorithm are given in \nfigure 2.\nTo evaluate the quality of a model, the metric \npropos ed by Bunke [3] to calculate the edit distance \nbetween two graphs is adapted to the distance between \nan ARG and the model.\nFigure 3 presents the models generated for the bridge \nand round-about objects. The models are simple and \nquite similar to manually generated models. They \nrepresent the geometrical and spatial characteristics of \nthe target objects.\nFig 3. Bridge and round- about  models: using \nrectangles and circles, using  line segments and circles \n.  \nThe manually segmented images are not always \navailable. In order to use the method on a real world \nsystem, the target objects should be first extracted from the background. Consequently, our work is \nconcentrated on the extraction of target objects from \nthe satellite images.  \n3. Ext raction of Target Objects\nExtraction of objects on Ip images using naive \nmethods does not give a satisfactory result due to the \nimprecision of the boundaries separating cartographic \nobjects from the background. On the other hand, while \nthe objects can be well differentiated from the \nbackground on Im images, their shape is too coarse. The \nuse of both images makes it possible to overrun these \nlimitations.\nThe steps of the method are given in the following \nsections. \n3.1. Segmentation of the Im Image.\nThe first step is to clip the central part of the Im \nimage and to magnify it by a factor of 4, so that its \nresolution becomes the same as the resolution of the Is \nimage. A linear interpolation is applied to magnify the \nimage. \nThe Im image consists of three channels I1, I2, I3 with \nspectral ranges 0.50- 0.59 \u03bcm, 0.61- 0.69 \u03bcm and 0.78-\n0.89 \u03bcm. The target objects are consisting of pieces of \npossibly different types of roads in a specific spatial \narrangement. We generated the linear combination of \nthe three channels, that discriminates better the roads \nfrom other objects, fixing the coefficients empirically. \nA deeper analysis in parameter space combined with \nprior knowledge on reflectance values of spectral bands \nmay result in a much better discrimination. The \nresulting image If is obtained by: \nIf = (I1 + I2) * 0.3 - I3\nOn If a hysteresis threshold is applied to extract the \nregion RM containing the target object. The threshold \nvalue is calculated by finding the most frequent gray \nvalue in a 5x5 central window on all Im images. The \nmethod is very successful in detecting the true positives \n(Fig. 4). There are some false positives due to the \nincorrect threshold value for some of the images, but \nthese are acceptable considering that the result will be \nrefined in the next steps.\n3.2. Matching the Mask.\nIt is necessary to search for the exact location of the \nmask on the Is image. There exist many sophisticated \nimage registration techniques [4]. In our case, we don't deal with the transformations and deformations that the \nimage has been subject to, and the difference between \ncenters is limited to at most 10 pixels in horizontal and \nin vertical in both directions. A matching method \nmoving the center of the mask in a 20x20  window \nlocated on the center of the Is image would be sufficient \nto visit all \u201ccandidate\u201d points. \nFor each candidate point pi a matching score should \nbe calculated. The mask, in the correct place, should \ncontain the whole target object. The most pertinent \ninformation that indicates the location of the object is \ngiven by the edges. A correctly placed mask that \ncontains an object should also contain its edges. \nBesides that, the edges in the exterior part of an object \nwould remain out of the mask if the mask is not well \nplaced. Consequently, the score is calculated using the \nnumber of edge points contained in a region.  \nFig. 4. Mask on Im image and on Ip image after \nmatching\nSub-pixel precision edges are obtained using Canny \nedge detection algorithm [5]. In order to eliminate \nnoisy short edge segments, the edges are smoothed and \nedges with closer extremities are merged. Finally short \nedges are eliminated. Before calculating the score, a \ndilation is  applied on the mask so that it would be \nslightly bigger than the object. In case of equal score, \nthe location that gives the smallest variance score for \nthe masked region is selected. The mask center is \nshifted to the point with the highest score (Fig. 4). \n3.3. Extraction of the object.\nThe region covered by the mask is an approximation \nof the object shape obtained from a lower resolution \nimage. The mask should be refined in order to extract \nthe object.\nRegion-based methods fail to locate the object \nboundaries well. It is also difficult to select the regions \nbelonging to the object. Edge-based methods give us a \ngood localization of the separation lines between the \nobject and the background. However, the extracted \nedges are not continuous, and consequently they don't \ngive us closed regions.\nIt is a very common approach to integrate region \ngrowing and edge detection methods, as Pavlidis et al. \n[6] that uses the edge intensities to eliminate the \nboundaries in an over- segmented image, or Le Moigne et al. [7] that uses the edge features to determine the \nstopping criterion of the region growing algorithm. \nIn his multi-scale segmentation algorithm, [8] \npropos ed to increase the intensities of the straight edge \nsegments obtained from a different source,  on the \ngradient image before applying the watershed. In that \nway these segments are preserved in the final \nwatersheds.\nThe method propos ed in this paper is a marker-\ncontrolled watershed segmentation that uses the edge \ninformation as in [8].  \nWatershed method produc e an important over-\nsegmentation. A solution is to apply a fusion algorithm. \nSince we look for a unique object, a very appropriate \nalternative is to use markers to limit the number of \nresultant regions. The main difficulty is to detect the \nmarkers.  Usually, this is done  through user interaction. \nIn our case, a mask covering the object is already \nobtained. It can be assumed that the skeleton of the \nmask should match with the skeleton of the object, \nconsequently it is a good marker for the object. \nSimilarly the external boundaries of the mask can be \nused to mark the background.\nThe steps of the final algorithm are as follows:\n1.Detect:\nSm = skeleton of the mask M.\nBm = the external boun dary of M after dilation.\nIg = gr adient of Is\nEs = edges of Is (after previously mentioned \npost processing)\n2.Set Ig(Es) to max(Ig) (Pixels belonging to the \nedges will have the highest gradient values)\n3.Modify Ig so that it has minima only at Sm and \nBm.\n4.Apply the watershed.   \nThe algorithm gives a unique region as the object \naround the skeleton bou nded by the detected edges.   \n4.3. Experimental Results \nWe tested our system on 20 bridge and 20 round-\nabout objects. \nStepObjectResults\nCorrectAcceptableIncorrect\nSegm.Bridge1532\nRound-about1055\nMatch.Bridge1415\nRound-about1037\nExtract.Bridge866\nRound-about659\nTable 1. The cumulative resultsWe evaluated the results by manual inspection in \nthe end of each step and by comparing them with \nimages segmented by an expert at the end of the whole \nprocess. Table 1 gives a brief overview of the results at \nthe end of each step. \nThe results of the first step are very successful for \nthe bridges. In almost all of the images the object is \npartially or totally detected. For the round-abouts there \nare some bad segmentations mainly due to two reasons: \nthe small size of the object in some images makes the \nsegmentation very difficult in the low resolution image \nand buildings very near to the round-abouts are \nconfused with the roads. In the second step, the rate of \nexact match is very high. The incorrect matches occur \nmainly when the mask size is large. After eliminating \nthe errors from the previous steps, the results of the \nextraction phase are very successful.\nFigure 5. The extracted objects\nA qualitative analysis seems us more appropriate \nfor the final step. The segmentation follows exactly the \nedge lines and connects them according to the \nmaximum values of the gradient values in disconnected \nregions. This behavior is conform to what we aimed \nusing a hybrid approach.. However, the result is highly \ndependent on the skeleton obtained from the mask, and \nwhen there are multiple edges it may follow the \nincorrect edges (Fig. 5). Defining a weight on the \nlength and the position of the edge line may help to \nsolve this problem.\n4. Conclusions\nThis study is a first approach to a difficult problem, \nand we propos ed a system that can be refined with \nseveral improvements. The extraction module gives \npromising results. However, the results should be \nimproved in order to apply the model generation \ndirectly on them.     \nDetection of circles and line segments may help \ncorrecting the irregularities of the final segmentation. \nFor the round-about images, the early detection of the \ncentral circle using Hough transformation may be used \nto improve the results of the first two steps. Another \nalternative is to implement deformable models .\nWe can propose several refinements and \nameliorations for the model generation. Considering \nthe numerical attributes, using a fuzzy modelisation of the symbolic concepts and integrating methods issued \nfrom qualitative spatial reasoning as in [9] may \nenhance the results of our system. \nAnother important perspective is to iterate the \nsegmentation process using the generated model so that \nthe high-level knowledge would be used in low-level \nprocessing. \n5. Acknowledgements\nWe would like to thank French Space Agency - \nCNES, Toulouse, and especially Gilbert Pauc and Jordi \nInglada for their support. We would also like to thank \nDr. Z. Hamrouni and Cultural Service of the French \nEmbassy in Ankara.\n10. R eferences\n [1] Erus G., Lom\u00e9nie N., \u201cAutomatic Learning  of Structural \nMode ls of Cartogr aphic  Obje cts\u201d, IAPR - Workshop on \nGraph-based Representations in Pattern Recogniti on, 2005, \nPoitiers.\n[2] Cordella L. P., Foggia P., Sansone C., M. Vento: \nLearning structural shape descriptions from examples. \nPattern Recognition Letters. 23(2002) 1427\u20141 437\n[3] Bunke H.,Shearer K.: A Graph distance metric \nbased on the Maximal Common Subgraph. Pattern \nRecognition Letters 19 (1998 ) 255\u201425 9\n[4] Brown,L:A Survey of Image Registration \nTechniques, ACM Computing Surveys. 25-376, 1992\n[5] Canny, J.: A Computational Approach to Edge \nDetection, IEEE Transactions on Pattern Analysis and \nMachine Intelligence, Vol 8, No. 6, Nov 1986.\n[6] Pavlidis T. Liow Y.: Integrating region growing and \nedge detection. IEEE Transactions on Pattern Analysis \nand M achine Intelligence, 12(3):225-233 Mar 1990\n[7] Le Moigne J., Tilton J.C.: Refining image \nsegmentation by integration of edge and region data. \nIEEE Transactions on Geoscience and Remote \nSensing, 33-3 May 1995.\n[8] Bloch, I., Colliot, O., Camara, O., G\u00e9raud, T., \nFusion of spatial relationships for guiding recognition, \nexample of brain structure recognition in 3D MRI, \nPRL(26), No. 4, March 2005 , pp. 449- 457. \n[9] Guigues  L. \u201cMod\u00e8les multi-\u00e9chelles pour  la \nsegmentation d'images\u201d Ph. D. Thesis, 2003"}
{"category": "abstract", "text": "The aim of this study is to detect man-made \ncartographic objects in high-resolution satellite \nimages. New generation satellites offer a sub-metric \nspatial resolution, in which it is possible (and \nnecessary) to develop methods  at object level rather \nthan at pixel level, and to exploit structural features of \nobjects. With this aim, a method to generate structural \nobject models from manual ly segmented images has \nbeen developed. To generate the model from non-\nsegmented images, extraction of the objects from the \nsample images is required. A hybrid method of \nextraction (both in terms of input  sources and \nsegmentation algorithms) is propos ed"}
{"category": "non-abstract", "text": "Non-linear wavelet compression, rectangular wavelet tran sform,\nhyperbolic wavelets, anisotropic Besov space, sparse grid .\n1 Introduction\nWavelet analysis has become a powerful tool for image and signal pr ocessing.\nInitially developed for approximations and analysis of functions of a s ingle\nreal variable, wavelet techniques was almost immediately generalized for the\ncase of two and many variables.\nWe will start our discussion with the most recent generalization, non-separable\nwavelets. In this construction, lattice of integers in one dimensional case is\nEmail address: vyacheslavz@semiconductor.com (Vyacheslav Zavadsky).\nPreprint submitted to Elsevier Science 29 October 2018replaced by the quincunx or a general lattice in the multi\u2013dimensional case,\nand a wavelet analysis is derived directly for this lattice (see [ ?,?]).\nAlthoughthenon-separableapproachnowattractsthemajority ofresearchers,\nwe will focus this paper on comparison of the squareand therectangular sepa-\nrable wavelets for the two-dimensional case. We will show that the r ectangular\nwavelet transform in some applications is more suitable than classical square\none. The \u201crectangularization\u201d technique can also be applicable to so me non-\nseparable wavelet constructions.\nSeparable approaches involve building a basis in L2(Rd) using elements of a\nwavelet basis for L2(R). DeVore et al.in [?] and numerous other researchers\nstudied the following basis in L2(R2):\n/braceleftBig\n\u03c6(2lx\u2212i)\u03c6(2ly\u2212j),\u03c8(2kx\u2212i)\u03c8(2ky\u2212j),\n\u03c6(2kx\u2212i)\u03c8(2ky\u2212j),\u03c8(2kx\u2212i)\u03c6(2ky\u2212j)/bracerightBig\n,(1)\nwhere\u03c6is ascaling function ,\u03c8is the corresponding wavelet, l\u2208Z,k\u2265l,\nk\u2208Z, andi,j\u2208Z.\nBecause the supports of the basis functions are localized in the squ ares with\nsizesO(2\u2212k\u00d72\u2212k), the decomposition on this basis is called the square wavelet\ntransform . It is well known that if a function has bounded (in a certain space)\nderivatives of the total order Mand\u02dc\u03c8hasMvanishing moments then the\nrate of approximation by Nelements of this basis is O(N\u2212M/2).\n(a)\n (b)\nFig. 1. The Lena images decomposed by square and rectangular wavelet transforms\nbased on the Haar basis.\nRectangular wavelet transform is inspired by approximation theory technique\nknown as \u201csparse grid\u201d, or \u201chyperbolic cross\u201d (see, for example, [?]). The\nsquare wavelet transform (and it\u2019s higher dimensional analogs) su\ufb00 ers from so\n2called \u201ccurse of dimensionality\u201d. In order to get the same precision o f approx-\nimation as by Ncoe\ufb03cients in the one dimensional case, one needs to utilize\nO(Nd) coe\ufb03cients for the function of dvariables. The \u201csparse grid\u201d technique\nallows to build a basis for functions of many variables that yields the sa me\nrate of convergence (up to a logarithmic factor) as in the case of o ne variable.\nIn application to the construction of a wavelet basis, it yields a basis\n/braceleftBig\n\u03c8(2k1x\u2212i)\u03c8(2k2y\u2212j),\u03c6(2lx\u2212i)\u03c8(2k2y\u2212j),\n\u03c8(2k1(x\u2212i)\u03c6(2ly\u2212j,\u03c6(x\u2212i)\u03c6(y\u2212i))/bracerightBig\n,(2)\nwherek1,k2\u2265l,k1,k2\u2208Z,i,j\u2208Zandlis a \ufb01xed integer number. Because\nelements of the basis are supported on rectangles of size O(2\u2212k1\u00d72\u2212k2), and\nk1/negationslash=k2, the corresponding transform can be called rectangular .\nThe application of this basis to the problem of multivariate surface de noising\nwas studied in [ ?] and [?]. The approximation properties of the rectangular\nwavelet transform in Besov spaces was studied DeVore et al.in [?], and by\nZavadsky in [ ?].\nThis paper aims at providing a self contained study of the approximat ion\nproperties of the rectangular wavelet transform from the persp ective of image\nprocessing and benchmarking of the rectangular transforms with respect to\nthe square one for image compression.\nWe show that if the approximated function has the mixed derivative o f total\norder 2Mand\u02dc\u03c8hasMvanishing moments, then the rate of approximation\nbyNelements of this basis is O(N\u2212MlogCN).\nThe rest of the paper is organized as follows. In section 2, we recall some basic\nproperties of one dimensional wavelet bases. In section 3, we cons ider our\nmotivations for migration from square wavelet transform to recta ngular one.\nInsection 4, we consider approximation ratesofthenon-linear app roximations\nbytherectangularwavelet basisin Lp(R2),1\u2264p<\u221e.Insection5,weprovide\nnumerical experiments results on standard test images.\n2 One-dimensional wavelet transform\nBiorthogonal bases of compactly supported wavelets were build in [ ?]. Follow-\ning this paper, we assume that we have four compactly supported f unctions\n\u03c6,\u02dc\u03c6,\u03c8,\u02dc\u03c8\u2208L\u221e(R) and coe\ufb03cients vectors h,\u02dch,g,\u02dcgthat satisfy the following\n3conditions:\n\u03c6(x) =/summationdisplay\ni\u2208Zhi\u03c6(2x\u2212i), (3)\n\u02dc\u03c6(x) =/summationdisplay\ni\u2208Z\u02dchi\u02dc\u03c6(2x\u2212i), (4)\n\u03c8(x) =/summationdisplay\ni\u2208Zgi\u03c6(2x\u2212i), (5)\n\u02dc\u03c8(x) =/summationdisplay\ni\u2208Z\u02dcgi\u02dc\u03c6(2x\u2212i), (6)\n<\u03c6(x),\u02dc\u03c6(x\u2212i)>=\uf8f1\n\uf8f2\n\uf8f31 ifi= 0,\n0 otherwise,i\u2208Z, (7)\n<\u03c8,\u02dc\u03c8(2kx\u2212i)>=\uf8f1\n\uf8f2\n\uf8f31 ifi= 0 andk= 0,\n0 otherwise,i,k\u2208Z, (8)\n<\u03c6,\u02dc\u03c8(2kx\u2212i)>=\uf8f1\n\uf8f2\n\uf8f31 ifi= 0 andk= 0,\n0 otherwise,i\u2208Z,k\u2208Z+,(9)\n<\u03c8,\u02dc\u03c6(2kx\u2212i)>=\uf8f1\n\uf8f2\n\uf8f31 ifi= 0 andk= 0,\n0 otherwise,i\u2208Z,k\u2208Z\u2212,(10)\nwhere< f1(x),f2(x)>=/integraltextf1(x)f2(x)dx. We further assume that the basis\nhasMdual vanishing moments :\n/integraldisplay\nxk\u02dc\u03c8(x)dx= 0, k= 0,...,M\u22121. (11)\nIt can be shown that for any scale l\u2208Zany function f\u2208L2(R) can be\nrepresented as\nf(x) =/summationdisplay\ni\u2208Z2l\u03c6(2lx\u2212i)\u03b1l,i+\u221e/summationdisplay\nk=l2k\u03c8(2kx\u2212i)\u03b2k,i,(12)\nwhere\u03b1k,i=<f,\u02dc\u03c6(2kx\u2212i)>, (13)\n\u03b2k,i=<f,\u02dc\u03c8(2kx\u2212i)>. (14)\nThefast wavelet transform allows to quickly go in representation (12) from\nscaleltol\u22121 and back ( inverse fast wavelet transform ) by observing that\n\u03b1l\u22121,i=<f,\u02dc\u03c6(2l\u22121x\u2212i)>=/summationdisplay\nj\u2208Z\u02dchj<f,\u02dc\u03c6(2lx\u22122i\u2212j)>=/summationdisplay\nj\u2208Z\u02dchj\u03b1l,2i+j,\n(15)\n\u03b2l\u22121,i=<f,\u02dc\u03c8(2l\u22121x\u2212i)>=/summationdisplay\nj\u2208Z\u02dcgj<f,\u02dc\u03c6(2lx\u22122i\u2212j)>=/summationdisplay\nj\u2208Z\u02dcgj\u03b1l,2i+j,\n(16)\n4\u03b1l,i=<f,\u02dc\u03c6(2lx\u2212i)>= 2l\u22121/summationdisplay\nj\u2208Z\u03b1l\u22121,j<\u03c6(2l\u22121x\u2212j),\u02dc\u03c6(2lx\u2212i)>+\n+2l\u22121/summationdisplay\nj\u2208Z\u03b2l\u22121,j<\u03c8(2l\u22121x\u2212j),\u02dc\u03c6(2lx\u2212i)>+\n+\u221e/summationdisplay\nk=l2k\u22121/summationdisplay\nj\u2208Z\u03b2k\u22121,j<\u03c8(2kx\u2212j),\u02dc\u03c6(2lx\u2212i)>.(17)\nAccording to (10) the last term is 0. Expanding \u03c6(2l\u22121x\u2212j) and\u03c8(2l\u22121x\u2212j)\nby (3) and (5) and then using (7), one can show that\n\u03b1l,i=1\n2\uf8ee\n\uf8f0/summationdisplay\nj\u2208Zh2j\u2212i\u03b1l\u22121,j+/summationdisplay\nj\u2208Zg2j\u2212i\u03b2l\u22121,j\uf8f9\n\uf8fb (18)\nIn practice, if we process a discrete signal sampled with step 2\u2212l, it is typically\nassumed that in expansion (12) \u03b2ki= 0 and\u03b1l,iare the discrete values of\nobserved signal.\nThe wavelet transformis typically used to compress or denoise signa ls by mov-\ning to representation (12) with smaller lby iterative applications of (15),(16);\nsetting certain \u03b2k,ito0 (thatwill reduce datasize and\ufb01lter outrandomnoise);\nand moving back to the original lby iterative application of (18).\n3 Motivation\nLet us start from considering square wavelet transform build from the D4\nDaubechies orthogonal wavelet with 2 vanishing moments, see Fig. 2 . As we\nsee, the\u03c6(x)\u03c8(y)termcharacterizes horizontaledges, \u03c8(x)\u03c6(y)\u2013verticaledges.\nAlthough some authors suggested that \u03c8(x)\u03c8(y) can be used to characterize\nedges in the diagonal directions, we can see that this term have di\ufb00e rent\nstructure from the previous two and we can expect that the corr esponding\ncoe\ufb03cients will decline with di\ufb00erent speed.\nNow let us have a look at Figs. 1(a), 3(a), and 3(c). We can see that the\ndiagonal squares, that are shows coe\ufb03cients of \u03c8(2kx\u2212i)\u03c8(2ky\u2212j) are less\nbright than others.\nNow let us consider how the value of coe\ufb03cients with terms \u03c8(x)\u03c8(y) and\n\u03c6(x)\u03c8(y),\u03c8(x)\u03c6(y) can be compared from the theoretical standpoint. The\nproof of the following lemma is given in Appendix:\nLemma 1 For any wavelet basis with Mdual vanishing moments and for any\np\u22651there exist constants C,Lsuch that for any \u03c31,\u03c32>0and\u03b81,\u03b82\u2208R\n5(a) Scaling function \u03c6=\u02dc\u03c6\n(b) Wavelet function \u03c8=\u02dc\u03c8\n(c)\u03c6(x)\u03c8(y)\n (d)\u03c8(x)\u03c6(y)\n(e)\u03c8(x)\u03c8(y)\nFig. 2. D4: the Daubechies orthogonal wavelet with 2 vanishi ng moments and ele-\nments of square wavelet basis built from it.\nand for any function f(x,y)with the appropriate derivative in Lloc\np:\n/vextendsingle/vextendsingle/vextendsingle<f(x,y),\u02dc\u03c8(\u03c31x\u2212\u03b81)\u02dc\u03c6(\u03c32y\u2212\u03b82)>/vextendsingle/vextendsingle/vextendsingle\u2264\n\u2264C1\n\u03c3M+1\u22121/p\n1/bracketleftBigg/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32/vextendsingle/vextendsingle/vextendsingledM\ndxMf(x,y)/vextendsingle/vextendsingle/vextendsinglepdydx/bracketrightBigg1/p\n,(19)\n6(a) D4 decomposition\n (b) D4 energy distribution\n(c) CRF(13,7) decomposi-\ntion\n(d) CRF(13,7) energy distribution\nFig. 3. The Lena image decomposed by square wavelet transfor m built from the\nDaubechies orthogonal D4 \ufb01lter and the JPEG2000 CRF(13,7) \ufb01 lter. The energy\ndistribution plots in (b), (d) show the distribution of L2norms of\u03b2\u03c6(x)\u03c8(y),\n\u03b2\u03c8(x)\u03c6(y) (edge detection terms ) and\u03b2\u03c8(x)\u03c8(y) (cross terms ).\n/vextendsingle/vextendsingle/vextendsingle<f(x,y),\u02dc\u03c6(\u03c31x\u2212\u03b81)\u02dc\u03c8(\u03c32y\u2212\u03b82)>/vextendsingle/vextendsingle/vextendsingle\u2264\n\u2264C1\n\u03c3M+1\u22121/p\n2/bracketleftBigg/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32/vextendsingle/vextendsingle/vextendsingledM\ndyMf(x,y)/vextendsingle/vextendsingle/vextendsinglepdydx/bracketrightBigg1/p\n,(20)\n/vextendsingle/vextendsingle/vextendsingle<f(x,y),\u02dc\u03c8(\u03c31x\u2212\u03b81)\u02dc\u03c8(\u03c32y\u2212\u03b82)>/vextendsingle/vextendsingle/vextendsingle\u2264\n\u2264C1\n(\u03c31\u03c32)M+1\u22121/p/bracketleftBigg/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32/vextendsingle/vextendsingle/vextendsingled2M\ndxMdyMf(x,y)/vextendsingle/vextendsingle/vextendsinglepdydx/bracketrightBigg1/p\n.(21)\n74 Non-linear approximation\nTheorem 2 For any wavelet basis with Mdual vanishing moments and for\nanyp\u2265max(1,1/M);q\u22651there exists a constant Csuch that any function\nf(x,y)with supportlocalizedin [0,1]2andthe mixedderivatived2M\ndxMdyMf(x,y)\u2208\nLpfor anyN >0can be approximated by a function fNsuch that\n/bardblf\u2212fN/bardblLq\u2264CN\u2212M/vextenddouble/vextenddouble/vextenddoubled2M\ndxMdyMf(x,y)/vextenddouble/vextenddouble/vextenddouble\nLplogCN, (22)\nwherefNhas non more than CNnon zero coe\ufb03cients in the expansion on the\nrectangular wavelet basis.\nPROOF. Denote:\nI=Z2, (23)\nL=Z2\n+, (24)\n\u03c6i=\u03c6(x\u2212i1)\u03c6(x\u2212i2),(i1,i2)\u2208 I, (25)\n\u02dc\u03c6i=\u02dc\u03c6(x\u2212i1)\u02dc\u03c6(x\u2212i2), (26)\n\u03c8l,i=\u03c8(2l1x\u2212i1)\u03c8(2l2y\u2212i2),(l1,l2)\u2208 L,(i1,i2)\u2208 I,(27)\n\u02dc\u03c8l,i=\u02dc\u03c8(2l1x\u2212i1)\u02dc\u03c8(2l2y\u2212i2), (28)\n\u03b1i=<f,\u02dc\u03c6i>, (29)\n\u03b2l,i=<f,\u02dc\u03c8l,i>, (30)\n/bardbll/bardbl=l1+l2,(l1,l2)\u2208 L, (31)\nD=/vextenddouble/vextenddouble/vextenddoubled2M\ndxMdyMf(x,y)/vextenddouble/vextenddouble/vextenddouble\nLp. (32)\nBecause the set/braceleftBig\n\u03c6i,\u03c8l,i/vextendsingle/vextendsingle/vextendsinglei\u2208 I,l\u2208 L/bracerightBig\nis the tensor product of the wavelet\nbasis inLq(R), it forms a basis in Lq(R2) and\nf(x,y) =/summationdisplay\ni\u2208I\u03b1i\u03c6i+/summationdisplay\nl\u2208L2/bardbll/bardbl/summationdisplay\ni\u2208I\u03b2l,i\u03c8l,i. (33)\nBecausef(x,y) is compactly supported, there exists a constant Csuch that\nfor anyl\u2208 L\nN(/bardbll/bardbl) =/vextendsingle/vextendsingle/vextendsingle/braceleftBig\ni|i\u2208 I, \u03b2l,i/negationslash= 0/bracerightBig/vextendsingle/vextendsingle/vextendsingle\u2264C2/bardbll/bardbl+C, (34)\nwhere here and below Cdenotes the universal constant that is maximum of\nall appropriate constants in the proof and depends only on the wav elet basis,\np, andq.\n8Denote byl0the maximum lsuch thatl2N(l)\u2264N.\nDe\ufb01ne\n\u01ebl=D/2l+M+1/p\n2l0+M\u22121/p\n2l. (35)\nLet\nfN=/summationdisplay\ni\u2208I\u03b1i\u03c6i+/summationdisplay\nl\u2208L2/bardbll/bardbl/summationdisplay\ni\u2208I\u02dc\u03b2l,i\u03c8l,i, (36)\nwhere\n\u02dc\u03b2l,i=\uf8f1\n\uf8f2\n\uf8f30,if/bardbll/bardbl>l0and\u03b2l,i\u2264\u01eb/bardbll/bardbl,\n\u03b2l,iotherwise.(37)\nNow let us calculate the number of non-zero coe\ufb03cients \u02dcNin\u02dc\u03b2l,i. From (21)\nit follows that for any \ufb01xed l\n/bardbl\u03b2l,\u00b7/bardbllp\u2264CD/2/bardbll/bardbl(M+1\u22121/p). (38)\nLet us note that if /bardbl\u03b2/bardbllp<Athen the number of coe\ufb03cients bigger than \u01eb>0\nin\u03b2can not exceed Ap/\u01ebp. Taking into account (34), we have\n\u02dcN\u2264Cl0/summationdisplay\nl=0l(2l+C)+\u221e/summationdisplay\nl=l0+1lDp\n2lp(M+1\u22121/p)\u01ebp\nl\u2264CN. (39)\nBecause\u03c8\u2208L\u221eand is compactly supported,\n/bardblf\u2212fN/bardblLq\u2264C\u221e/summationdisplay\nl=l0+1l2l\u01ebl=\n=CD\u221e/summationdisplay\nl=l0+1l/2M+1/p\n2l0+M\u22121/p\n2l\u2264CN\u2212M/vextenddouble/vextenddouble/vextenddoubled2M\ndxMdyMf(x,y)/vextenddouble/vextenddouble/vextenddouble\nLplogCN(40)\n\u2737\n5 Experiments\nTo numerically compare results of square and rectangular wavelet t ransform\nwe limited ourselves to the problem of image compression. We took the stan-\ndard 512 \u00d7512 \u201cMandrill\u201d, \u201cHouse\u201d, \u201cLena\u201d, and \u201cBarbara\u201d greyscale images\nand compress them to reduce the number of non-zero coe\ufb03cients by factors\nof 80 and 1601. The results are shown in Figs 4, 5, 6, and 7.\n1Some papers on image compression use 256 \u00d7256 images. For the same quality,\nthey report 4 times smaller compression numbers.\n9(a) The original\n\u201dMandrill\u201d 512 \u00d7512\nimage.\n(b) D4 orthogonal\nwavelet, compression\n1:83, rectangular\ntransform.\n(c) D4 orthogonal\nwavelet, compres-\nsion 1:83, square\ntransform.\n(d) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:87,\nrectangular trans-\nform.\n(e) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:81,\nsquare transform.\n(f) D4 orthogonal\nwavelet, compression\n1:172, rectangular\ntransform.\n(g) D4 orthogonal\nwavelet, compression\n1:167, square trans-\nform.\n(h) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:163,\nrectangular trans-\nform.\n(i) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:166,\nsquare transform.\nFig. 4. TheMandrill image compressed by the rectangular and squarewavelet trans-\nforms.\n10(a) The original\n\u201dHouse\u201d 512 \u00d7512\nimage.\n(b) D4 orthogonal\nwavelet, compression\n1:89, rectangular\ntransform.\n(c) D4 orthogonal\nwavelet, compres-\nsion 1:86, square\ntransform.\n(d) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:82,\nrectangular trans-\nform.\n(e) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:79,\nsquare transform.\n(f) D4 orthogonal\nwavelet, compression\n1:163, rectangular\ntransform.\n(g) D4 orthogonal\nwavelet, compression\n1:166, square trans-\nform.\n(h) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:163,\nrectangular trans-\nform.\n(i) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:162,\nsquare transform.\nFig. 5. The \u201dHouse\u201d image compressed by the rectangular and s quare wavelet trans-\nforms.\n11(a) The original\n\u201dLena\u201d 512 \u00d7512\nimage.\n(b) D4 orthogonal\nwavelet, compression\n1:87, rectangular\ntransform.\n(c) D4 orthogonal\nwavelet, compres-\nsion 1:86, square\ntransform.\n(d) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:85,\nrectangular trans-\nform.\n(e) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:84,\nsquare transform.\n(f) D4 orthogonal\nwavelet, compression\n1:160, rectangular\ntransform.\n(g) D4 orthogonal\nwavelet, compression\n1:166, square trans-\nform.\n(h) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:162,\nrectangular trans-\nform.\n(i) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:163,\nsquare transform.\nFig. 6. The \u201dLena\u201d image compressed by the rectangular and sq uare wavelet trans-\nforms.\n12(a) The original\n\u201dBarbara\u201d 512 \u00d7512\nimage.\n(b) D4 orthogonal\nwavelet, compression\n1:80, rectangular\ntransform.\n(c) D4 orthogonal\nwavelet, compres-\nsion 1:92, square\ntransform.\n(d) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:88,\nrectangular trans-\nform.\n(e) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:83,\nsquare transform.\n(f) D4 orthogonal\nwavelet, compression\n1:166, rectangular\ntransform.\n(g) D4 orthogonal\nwavelet, compression\n1:169, square trans-\nform.\n(h) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:166,\nrectangular trans-\nform.\n(i) CRF(13,7)\nbiorthogonal wavelet,\ncompression 1:164,\nsquare transform.\nFig. 7. The \u201dBarbara\u201d image compressed by the rectangular an d square wavelet\ntransforms.\n13We believe that results clearly show that the rectangular wavelet tr ansform\nvisually outperforms the square one for all test images. Although t he e\ufb00ect is\nmore visible on the Mandrill and Barbara images and less visible on the Le na\nand House images, in all cases, the results of rectangular compres sion at the\n1:160 rate are visually close to the results of square compression at the 1:80\nrate.\n6 Conclusions\nAs we shown, the rectangular wavelet transform has substantially better con-\nvergence rate if the approximated function has the mixed derivativ e of appro-\npriate order. In this case, it allows to solve the \u201dcurse of dimensiona lity\u201d issue\nand gives the same degree of approximation (up to a logarithmic fact or) as\nthe wavelet basis in the one dimensional case.\nFromtheresultsofnumerical experiments wecanseethatthemixe dderivative\nassumption is reasonable for the standard test images and rectan gular wavelet\ntransform allows to improve the compression rate (measured in the number of\nnon-zero coe\ufb03cient) by factor of 2.\nIt would be worthwhile to investigate what actual bit rate compress ion an in-\ndustrial grade image compression algorithm (such as JPEG2000) will produce\nif the square wavelet transform is replaced by rectangular one.\nAlthough further experiments in this direction are needed, we believ e that it\u2019s\nworthwhile to investigate the applicabity of rectangular wavelet tra nsform to\nvideo compression. Because the actual dimension of data in this cas e is 3, the\nsolution to the \u201dcurse of dimensionality\u201d can be particularly bene\ufb01cia l in this\ncase. Althoughcertaintechnical work toorganizecodecto ensur e intermediate\nframe recovery is needed, the results can be quite interesting not only from\nthe actual bit-rate standpoint, but also because expensive move ment search\noperation is replaced by a straightforward fast wavelet decompos ition.\nThe rectangular wavelet transform is especially e\ufb03cient to represe nt details\nthat are either in the horizontal or vertical directions. Certain se cond gener-\nation schemes (such as wavelets for the quincunx lattice) can be mo di\ufb01ed in\nto ensure the same rate of convergence for the functions with bo unded mixed\nderivativeasinthecaseoftherectangularwavelet transform.Th ismayfurther\nimprove the results.\n147 Appendix: proof of Lemma 1\nBecause we assumed that \u02dc\u03c8is compactly supported, there exists L>0 such\nthat|x| \u2265L\u21d2\u02dc\u03c8(x) = 0. Let us de\ufb01ne functions \u02dc\u03a8mform\u2208Nas follows:\n\u02dc\u03a8m(x) :=/integraldisplayx\n\u2212L\u03c4m\u22121\u02dc\u03c8(\u03c4)d\u03c4. (41)\nBecause \u02dc\u03c8\u2208L\u221e,\u02dc\u03a8m(x)\u2208Lloc\n\u221e. Moreover, because \u02dc\u03c8hasMvanishing\nmoments \u02dc\u03a8mis supported on [ \u2212L,L] form\u2264M.\nLemma 1 is a combination of the following results:\nLemma 3 For any function f(x)withM-th derivative\n<f(x),\u02dc\u03c8(\u03c3x\u2212\u03b8)>=1\n\u03c3M/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c3f(M)(x)\u02dc\u03a8M(\u03c3x\u2212\u03b8)dx. (42)\nPROOF.\n<f(x),\u02dc\u03c8(\u03c3x\u2212\u03b8)>=/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c3f(x)\u02dc\u03c8(\u03c3x\u2212\u03b8)dx=\n=/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c3f\u2032(x)/integraldisplayx\n\u03b8\u2212L\n\u03c3\u02dc\u03c8(\u03c3\u03c4\u2212\u03b8)d\u03c4dx=\u00b7\u00b7\u00b7=\n=/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c3f(M)(x)/integraldisplayx\n\u03b8\u2212L\n\u03c3/bracketleftBig\n\u03c4\u2212\u03b8\u2212L\n\u03c3/bracketrightBigM\u22121\u02dc\u03c8(\u03c3\u03c4\u2212\u03b8)d\u03c4dx.(43)\nLemma 4 For any wavelet basis with Mdual vanishing moments and for any\np\u22651there exist constants C,Lsuch that for any \u03c3 >0and\u03b8\u2208Rand for\nany function fwithM-th derivative in Lloc\np\n/vextendsingle/vextendsingle/vextendsingle<f(x),\u02dc\u03c8(\u03c3x\u2212\u03b8)>/vextendsingle/vextendsingle/vextendsingle\u2264C1\n\u03c3M+1\u22121/p/bracketleftBigg/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c3|f(M)(x)|pdx/bracketrightBigg1/p\n.(44)\nMoreover, if the basis does not has M+1dual vanishing moments, then there\nexists a function fsuch that\n/vextendsingle/vextendsingle/vextendsingle<f(x),\u02dc\u03c8(\u03c3x\u2212\u03b8)>/vextendsingle/vextendsingle/vextendsingle\u22651\nC1\n\u03c3M+1\u22121/p/bracketleftBigg/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c3|f(M)(x)|pdx/bracketrightBigg1/p\n.(45)\nPROOF. DenoteNq:=/bardbl\u02dc\u03a8M/bardblLq. Let us note that Nq<\u221eforq >0.\n15By applying the H\u00a8 older inequality to (42), we get\n/vextendsingle/vextendsingle/vextendsingle<f(x),\u02dc\u03c8(\u03c3x\u2212\u03b8)>/vextendsingle/vextendsingle/vextendsingle\u2264N1/(1\u22121/p\n\u03c3M+1\u22121/p/bracketleftBigg/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c3|f(M)(x)|pdx/bracketrightBigg1/p\n.(46)\nTo proof (45), consider f(x) =xM/M!. After substituting it to (42):\n<xM\nM!,\u02dc\u03c8(\u03c3x\u2212\u03b8)>=1\n\u03c3M+1\u02dc\u03a8M+1(L). (47)\nNote that since \u02dc\u03c8does not has M+ 1 vanishing moments, \u02dc\u03a8M+1(L)/negationslash= 0.\nTaking into the account that\n/bracketleftBigg/integraldisplay\u03b8+L\n\u03c3\n\u03b8\u2212L\n\u03c31dx/bracketrightBigg1/p\n= (2L/\u03c3)1/p, (48)\nand putting C= max((2L)1/p/|\u02dc\u03a8M+1(L)|,N1/(1\u22121/p), we conclude the proof.\nCorollary 5 For any wavelet basis with Mdual vanishing moments and for\nanyp\u22651there exist constants C,Lsuch that for any \u03c31,\u03c32>0and\u03b81,\u03b82\u2208R\nand for any function f(x,y)with the appropriate derivative in Lloc\np:\n/vextendsingle/vextendsingle/vextendsingle<f(x,y),\u02dc\u03c8(\u03c31x\u2212\u03b81)\u02dc\u03c6(\u03c32y\u2212\u03b82)>/vextendsingle/vextendsingle/vextendsingle\u2264\n\u2264C1\n\u03c3M+1\u22121/p\n1/bracketleftBigg/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32|dM\ndxMf(x,y)|pdydx/bracketrightBigg1/p\n,(49)\n/vextendsingle/vextendsingle/vextendsingle<f(x,y),\u02dc\u03c6(\u03c31x\u2212\u03b81)\u02dc\u03c8(\u03c32y\u2212\u03b82)>/vextendsingle/vextendsingle/vextendsingle\u2264\n\u2264C1\n\u03c3M+1\u22121/p\n2/bracketleftBigg/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32|dM\ndyMf(x,y)|pdydx/bracketrightBigg1/p\n.(50)\nLemma 6 For any wavelet basis with Mdual vanishing moments and for any\np\u22651there exist constants C,Lsuch that for any \u03c31,\u03c32>0and\u03b81,\u03b82\u2208R\nand for any functiond2M\ndxMdyMf(x,y)\u2208Lloc\np:\n/vextendsingle/vextendsingle/vextendsingle<f(x,y),\u02dc\u03c8(\u03c31x\u2212\u03b81)\u02dc\u03c8(\u03c32y\u2212\u03b82)>/vextendsingle/vextendsingle/vextendsingle\u2264\n\u2264C1\n(\u03c31\u03c32)M+1\u22121/p/bracketleftBigg/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32|d2M\ndxMdyMf(x,y)|pdydx/bracketrightBigg1/p\n.(51)\n16PROOF. Let us consider\ng(y) =/integraldisplay\nf(x,y)\u02dc\u03c8(\u03c31x\u2212\u03b8)dx=\n=1\n\u03c3M\n1/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31dM\ndxMf(x,y)\u02dc\u03a8M(\u03c31x\u2212\u03b81)dx.(52)\nThen\n<f(x,y),\u02dc\u03c8(\u03c31x\u2212\u03b81)\u02dc\u03c8(\u03c32y\u2212\u03b82)>=<g(y),\u02dc\u03c8(\u03c32y\u2212\u03b82)>=\n=1\n\u03c3M\n1\u03c3M\n2/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32\u02dc\u03a8M(\u03c32y\u2212\u03b82)dM\ndyM/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31dM\ndxMf(x,y)\u02dc\u03a8M(\u03c31x\u2212\u03b81)dxdy\n(53)\nBecaused2M\ndxMdyMf(x,y) and\u02dc\u03a8Mare absolutely integrable, we can di\ufb00erentiate\nunder the integral sign:\n<f(x,y),\u02dc\u03c8(\u03c31x\u2212\u03b81)\u02dc\u03c8(\u03c32y\u2212\u03b82)>=\n=1\n\u03c3M\n1\u03c3M\n2/integraldisplay\u03b81+L\n\u03c31\n\u03b81\u2212L\n\u03c31/integraldisplay\u03b82+L\n\u03c32\n\u03b82\u2212L\n\u03c32\u02dc\u03a8M(\u03c31x\u2212\u03b81)\u02dc\u03a8M(\u03c32y\u2212\u03b82)d2M\ndxMdyMf(x,y)dydx\n(54)\nBy applying the H\u00a8 older inequality we conclude the proof.\n17"}
{"category": "abstract", "text": "We study image compression by a separable wavelet basis/braceleftbig\n\u03c8(2k1x\u2212i)\u03c8(2k2y\u2212\nj), \u03c6(x\u2212i)\u03c8(2k2y\u2212j), \u03c8(2k1(x\u2212i)\u03c6(y\u2212j), \u03c6(x\u2212i)\u03c6(y\u2212i)/bracerightbig\n,wherek1,k2\u2208\nZ+;i,j\u2208Z; and\u03c6,\u03c8are elements of a standard biorthogonal wavelet basis in\nL2(R). Because k1/negationslash=k2, the supports of the basis elements are rectangles, and\nthe corresponding transform is known as the rectangular wavelet transform . We\nprove that if one-dimensional wavelet basis has Mdual vanishing moments then\nthe rate of approximation by Ncoe\ufb03cients of rectangular wavelet transform is\nO(N\u2212MlogCN) for functions with mixed derivative of order Min each direction.\nThe square wavelet transform yields the approximation rate isO(N\u2212M/2) for\nfunctions with all derivatives of the total order M. Thus, the rectangular wavelet\ntransform can outperform the square one if an image has a mixe d derivative. We\nprovide experimental comparison of image compression whic h shows that rectangu-\nlar wavelet transform outperform the square one.\nKey words"}
{"category": "non-abstract", "text": "an IFS can be exactly transformed into Culik\u2019s image code. Using this\ntransformation, we will prove that in a self-similar (part of an) image any zero wavelet coefficient\nis the root of a zerotree, or its branch.\nThe paper discusses the zerotree coding of (wavelet/projection) coefficients as a common\npredictor/corrector, applied vertically through different layers of a multiresolutional\ndecomposition, rather than within the same view. This interpretation leads to an insight into the\nevolution of image compression techniques: from a causal single-layer prediction, to non-causal\nsame-view predictions (wavelet decomposition among others) and to a causal cross-layer\nprediction (zero-trees, Culik\u2019s method). A non-causal cross-level prediction appears to be the next\nstep. Will someone take it?\nI. Introduction\nThe present paper deals with analysis, generalizations and unifications of\nthe latest group of powerful image compression techniques: fractal image\ncompression with Iterated Function Systems (IFS) [BARN93], Culik\u2019s\ncompression with finite automata [CULI95] and Shapiro\u2019s embedded coding of\n* This work was supported in part by US Navy SPAWAR Grant N00039-94-C-0013 \u201cCompression of Geophysical\nData\u201d and by US Army Research Office TN 93-461 administered by Battelle Research Office.2wavelet coefficients using zerotrees [SHAP94]. All three techniques achieve\npremium results by exploiting properties of self-similarity of typical images. In\nmore precise terms, they all rely on the fact that parts of image representations at\ndifferent resolutions may in some sense be similar. Therefore, a higher-\nresolution representation may be rather accurately predicted from a low-\nresolution one. This leads to compression due to compactness of the low-\nresolution view and smallness of the prediction errors (corrections).\nAlthough this conceptual unity is fairly obvious, details of the precise\nrelationship among these methods are a bit obscure. This is partly because of\nspecialized non-intersecting terminology domains used to describe these\ntechniques: iterated transforms, finite automata, wavelet image transform. In the\npresent paper, we will show that all three methods can be formulated in plain\nterms of a common language, which makes the kinship of these techniques\nmanifest. Furthermore, it turns out that these methods are not only conceptually\nrelated, they are algorithmically reducible as well. The paper demonstrates an\nalgorithm by which an image coding with one technique can be exactly\ntransformed into another method\u2019s image code, with both codes yielding identical\nreconstruction results. Specifically, we will show how an IFS can be rendered in\nterms of projection matrices of Culik\u2019s method. Although these techniques appear\nto function in opposite ways (an IFS iteration shrinks the image iterated upon,\nwhile a Culik\u2019s iteration expands it), the reduction of one to the other is indeed\npossible, with both iterations producing identical results at all steps up to the final\nreconstructed image. This transformation also allows us to demonstrate in exact,\nprecise terms how self-similarity of a part of an image gives rise to a zerotree of\ncorresponding wavelet coefficients. In other words, if an image can be adequately\nrepresented by an IFS, every zero/insignificant wavelet coefficient in its\ndecomposition is a root of a zerotree branch.\nThe three methods above can be considered the latest step in evolution of\nimage compression techniques. Since every compressor is based on modelling\n(prediction) of a source and compact representation (or disregarding) of the\nprediction errors, what sets different algorithms apart is whether prediction is\ncausal, and what quantity is predicted. For example, CCITT Group III, JPEG\nlossless, etc., use a causal prediction of a pixel from its same-resolution\nneighborhood. A Laplacian pyramid decomposition, perfected by a Wavelet\nimage transform, is an example of a non-causal prediction, as first noted by Burt\n[BURT83]. There, the neighborhood surrounds the pixel in question on all flanks.\nThis usually leads to a more accurate prediction (and, therefore, smaller\ncorrection). Then came a zerotree coding, a causal prediction of a\ncoefficient/pixel based upon its resolutional neighborhood. This cross-resolutional\nprediction is indeed causal: if a parent tree node is zero (insignificant), all kid\nnodes are anticipated to be zeros as well. Non-causal cross-resolution predictor\nawaits: wavelet decomposition of layers of wavelet decomposition?3II. Culik\u2019s method revealed: fat pixels and exposing projectors\nCulik\u2019s method is based on an alternative exact representation of an image\nas a single \u201cfat\u201d pixel, which gets stretched and smeared during repeated\nexpansion operations, until it covers the whole area of the original picture.\nUnlike a regular, \u201cthin\u201d, pixel (which holds a single value: brightness of the\ncorresponding picture element), the fat pixel is a vector. The brightness of the\ncorresponding picture element is computed as a linear combination of the fat\npixel vector elements. In the simplest case, one can consider the first element of a\nfat pixel vector to be a \u201cvisible\u201d brightness, with the rest of the vector values\nbeing \u201chidden\u201d. The hidden values show up during projection by four matrices,\nwhich arrange fat pixel(s) into four quadrants of a larger picture. This\nrepresentation of an image by a single fat pixel is always possible, and the\noriginal image can be reconstructed in its entirety. As an example, the picture\nbelow shows a representation of a 4 \u00d74 image by a single 16-vector (fat pixel).\nDifferent pixels are numbered 1 through 16: these are merely pixel labels rather\nthan actual pixel values.\n 1 2 5 6..\n 3 4 7 8..\n 9 10 13 14..\n 11 12 15 161\n 6  8  14  16 ...  5  7  13  15 ... 1  3  9  11 ...  2  4  10  12 ...\n16151413121110987654321C3\nC2C1\nC0\nC0\nFig. 1. Example of fat pixel revelations to precisely reconstruct an image. Far left: the original\nimage. Far right: a single fat pixel with 16 components. Center: a partial revelation of hidden\ncomponents (grayed) upon application of the four transformation matrices.\nThere are only four projection matrices, C0-C3, which are applied over and over\nagain to produce an image at a finer resolution. For example, applying transform\nC0 to the original fat pixel (Fig. 1, far right) makes a lower-left fat pixel of a 2 \u00d72\nsquare, at the center of Fig. 1. Applying C0 again, to the entire 2 \u00d72 square, gives\nthe lower-left quadrant of the image on Fig. 1, far left. The projection matrices\nin the example above are trivial:\n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n.........0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n.........0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n.........0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n.........\nC1 C3 C0 C2\nFig. 2. Projection matrices for Fig. 1.4They obviously are permutation matrices: matrix C1 picks up every forth element\nof a vector, matrix C3 picks the next ones, etc.\nThe image representation above does not yet give any compression;\nmoreover, we need an additional space to store coefficients of the projection\nmatrices. However, it might turn out that the original image or its close\napproximation can be reconstructed with less fat pixels. For example, consider a\nSierpinski gasket:\nFig. 3. Making of the Sierpinski gasket: two steps of expanding a thin pixel.\nAs the figure shows, one needs only a single \u201cthin\u201d pixel and four 1 \u00d71 matrices\nC0=C1=C2=1, C3=0 to make the gasket at any resolution. Another example is a\ndiagonal grayscale ramp (this example is almost identical to the one given in\nCulik\u2019s paper [CULI95]). The numbers in squares in the figure below are the\npixel values themselves, on a 1-256 scale.\n25612864\n256\n192\n256256128\n256128\n21432\n128\n192\n192160\n160160 128\n128128\n969696\n6464\nFig. 4a. Original fat pixel (a\nhidden component is grayed)Fig. 4b. One step of\ntransformationFig. 4c. Two steps of\ntransformation (all hidden\ncomponents have the value of\n256, and not shown)\nThe projection matrices are as follows:\nC0=1212\n0 1\uf8eb\n\uf8ed\uf8ec\uf8f6\n\uf8f8\uf8f7,C1=C2=1214\n0 1\uf8eb\n\uf8ed\uf8ec\uf8f6\n\uf8f8\uf8f7,C3=120\n0 1\uf8eb\n\uf8ed\uf8ec\uf8f6\n\uf8f8\uf8f7 (1)\nOne can iterate further to obtain a bigger image, with smoother gray scale\ngradations. In any case, one needs only a single 2-vector (fat pixel) and four 2 \u00d72\nmatrices, 18 short integers total, to represent even 256 \u00d7256 and bigger images.\nAs Culik\u2019s presentations at DCC conferences have demonstrated, even\nrealistic pictures (of lenna, among others) can be represented quite compactly\nwith only 300 or so coefficients total (as compared to 1/4M pixels in case of a\n512\u00d7512 grayscale picture).5III. Culik\u2019s Compression and Iterated Function Systems\nExamples of Culik\u2019s iterations shown above strongly suggest that Culik\u2019s\nmethod must be very closely related to iterated function systems. It is indeed: in\nthis section, we will show how one can convert an IFS into Culik\u2019s transform/fat\npixel.\nIterated Function System (IFS) is a finite collection of contraction\nmappings [BARN93]. In practice [BARN93, KOMI95], these mappings are\nusually specified as transformations between two partitionings of the same image\ninto blocks. One, a finer scale partitioning into range blocks, is usually a regular\ntiling of the image into non-overlapping, usually 4 \u00d74 blocks. Another\npartitioning uses bigger blocks, called domain blocks, which can overlap and do\nnot have to cover the whole picture. Usually domain blocks are twice as big as the\nrange blocks. An IFS is made of separate transformations from a domain block to\na range block. A single transformation squeezes the domain block and linearly\nadjusts its brightness. For example, the figure below depicts a very simple IFS\nwith a single domain block and four smaller range blocks:\nD\n\u03b10D+\u03b20\u03b13D+\u03b23\n\u03b11D+\u03b21\n\u03b12D+\u03b22\nFig. 5. Sample IFS with a single domain block\nNote that the exact sizes of the blocks are irrelevant in this example. The only\nthing that matters is that the range blocks and the domain block both partition the\nsame image, and that range blocks are half as big in each dimension as compared\nto a domain block. A linear transform of block\u2019s brightness \u03b1D+\u03b2 applies to all\npixels of the domain block D. For example, starting with a square image with a\nuniform brightness (grayscale) value y, and applying the transformations above\nonce, and then again, one obtains:6\u03b11y+\u03b21\n\u03b12y+\u03b22 \u03b10y+\u03b20\u03b13y+\u03b23\n\u03b10\u03b11y+\n\u03b10\u03b21+\u03b20\u03b11\u03b13y+\n\u03b13\u03b21+\u03b23\n\u03b11\u03b12y+\n\u03b11\u03b22+\u03b21\n\u03b10\u03b13y+\n\u03b10\u03b23+\u03b20\u03b121y+\n\u03b11\u03b21+\u03b21\u03b11\u03b13y+\n\u03b11\u03b23+\u03b21\n\u03b10\u03b11y+\n\u03b11\u03b20+\u03b21\n\u03b120y+\n\u03b10\u03b20+\u03b20\u03b10\u03b12y+\n\u03b10\u03b22+\u03b20\u03b123y+\n\u03b13\u03b23+\u03b23\n\u03b122y+\n\u03b12\u03b22+\u03b22\u03b10\u03b12y+\n\u03b12\u03b20+\u03b22\u03b11\u03b12y+\n\u03b12\u03b21+\u03b22\u03b12\u03b13y+\n\u03b12\u03b23+\u03b22\u03b10\u03b13y+\n\u03b13\u03b20+\u03b23\u03b12\u03b13y+\n\u03b13\u03b22+\u03b23\nFig. 6a. Application of IFS, Fig. 5, to a square\nimage of uniform brightness y considered as a\nsingle domain blockFig. 6b. Application of IFS, Fig. 5, to a an\nimage Fig. 6a considered as a single domain\nblock\nThe result of one iteration is an image of the same size but with four times as\nmany details. Once the size of a detail diminishes down to one pixel, one may stop\niterating: for all practical purposes, \u201cconvergence\u201d is achieved. It is obvious that\nthe content of the starting image becomes less and less important, as it shrinks\ntwice at each iteration. Moreover, providing \u03b1i<1, all the series on Fig. 6b\nconverge, to a limit not depending on the initial value y.\nPrecisely the same result can be obtained with Culik\u2019s transforms, with the\noriginal fat pixel i and projection matrices as follows:\ni=y\n1\uf8eb\n\uf8ed\uf8ec\uf8f6\n\uf8f8\uf8f7,Ck=\u03b1k\u03b2k\n0 1\uf8eb\n\uf8ed\uf8ec\uf8f6\n\uf8f8\uf8f7,k=0,1,2,3 (2)\nIndeed, applying the Culik\u2019s projection once to the fat pixel i gives a picture\nexactly like Fig. 6a. The only difference is that each quadrant is now a pixel\n(rather than a square \u2018subimage\u2019), and it is a fat pixel with a hidden value of 1.\nApplying the projection once again results in Fig. 6b, with the identical\ninterpretation. In general, it is obvious that an IFS launched from a square image\nof size 2m, and the Culik\u2019s transform give identical (and identically sized) results\nafter m iterations each.\nNote Fig. 4 above is a particular case of this example, Fig. 6 and eq. (2),\nwith\n\u03b1k=12,\u03b20=128,\u03b21=\u03b22=64,\u03b23=0,y=128 (3)\nLet us consider now a more complex IFS, with several domain blocks.\nFirst, we will try an example with a single transform, a mapping between a\ndomain and a range block:7D\n\u03b1D+\u03b2\nFig. 7. Sample IFS with a single domain-to-range block mapping\nIterating upon a square with a uniform brightness y yields, in turn:\n\u03b1y+\u03b20000\n00 0\n000\n0000\n0\u03b12y+\n\u03b1\u03b2+\u03b200\n000\n00\n0000\n0\u03b2\u03b2\u03b2\u03b13y+\u03b12\u03b2\n\u03b1\u03b2+\u03b2\n0 00\n0000\n0\u03b2\u03b2\u03b2\n0 00\n0000\n0\n0 00\n0000\n00 00\n0000\n00 00\n0000\n00 00\n0000\n0\n\u03b2\n\u03b2\u03b2\u03b2\u03b2\n\u03b2\u03b2\u03b2\u03b2\u03b1\u03b2+\u03b2 \u03b1\u03b2+\u03b2\u03b1\u03b2+\u03b2\nFig. 8a. Application of IFS,\nFig. 7, to a square image of\nuniform brightness y\nconsidered covered by the 4\ndomain blocksFig. 8b. Application of IFS,\nFig. 7, to the image Fig. 8a\nconsidered covered by the 4\ndomain blocks. Only the lower\nright quadrant is shown.Fig. 8c. Application of IFS,\nFig. 7, to the image Fig. 8b\nconsidered covered by the 4\ndomain blocks. Only the lower\nright quadrant is shown.\nThe corresponding Culik\u2019s transformation is:\ni=0\ny\n1\n1\uf8eb\n\uf8ed\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7,C0=0 1 0 0\n0 0 0 0\n0 0 1 0\n0 0 0 1\uf8eb\n\uf8ed\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7,C1=C2=0 0 0 0\n0 0 0 0\n0 0 1 0\n0 0 0 1\uf8eb\n\uf8ed\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7,C3=0 0 0 0\n0\u03b10\u03b2\n0 0 1 0\n0 0 0 1\uf8eb\n\uf8ed\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7(4)\nIt is evident that the first iteration of projecting the starting fat pixel i leads to a\npicture on the right-hand side of Fig. 7; the second iteration results in Fig. 8a.\nIterating once more gives, in turn, Fig. 8b and Fig. 8c, etc.\nA more complex example involves two domain blocks and two \u201cmutually\ndependent\u201d transforms:\nD0\u03b3D+\u03b4 D1\n\u03b1D+\u03b2\nFig. 9. Sample IFS with a mutually-dependent domain-to-range block mapping\nwhich converges to something like8Fig. 10. Third iteration of IFS Fig. 7\nThe fat pixel i and the projection matrices corresponding to the example are as\nfollows:\ni=0\nx\ny\n1\n1\n1\uf8eb\n\uf8ed\uf8ec\n\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7\n\uf8f7,C0=0 1 0 0 0 0\n0 0 0 0 0 0\n0\u03b10 0\u03b20\n0I\uf8eb\n\uf8ed\uf8ec\uf8ec\uf8f6\n\uf8f8\uf8f7\uf8f7,C1=C2=0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 00I\uf8eb\n\uf8ed\uf8ec\uf8ec\uf8f6\n\uf8f8\uf8f7\uf8f7,C3=0 0 1 0 0 0\n0 0\u03b30 0\u03b4\n0 0 0 0 0 00I\uf8eb\n\uf8ed\uf8ec\uf8ec\uf8f6\n\uf8f8\uf8f7\uf8f7(5)\nwhere I is a 3\u00d73 unit matrix and 0 is a 3\u00d73 zero matrix. As one can easily verify\nby applying these projection matrices to the fat pixel i over and over again, the\nresult at each iteration is identical to that for the IFS Fig. 9.\nThus the IFS and Culik\u2019s image compression methods are indeed very\ntightly related, despite outward differences: an IFS iteration shrinks and\nreshuffles input image tiles, while a Culik\u2019s iteration merely rearranges its input,\nalways in the same regular way. The Culik\u2019s method starts with a single domain\nblock (that is, the entire image, or a \u201cfat\u201d pixel), and uses \u201crange\u201d blocks of the\nsame size as the domain block itself. However, the Culik\u2019s method makes up for\nthe lost \u201ctranslational\u201d degrees of freedom by using \u201cfat\u201d pixels and a more\ncomplex luminance transform: although linear, but vector rather than scalar.\nSimilar to IFS, Culik\u2019s matrices are required not to amplify pixels\u2019 luminance\n[CULI95]; i.e., the matrices should be contractive, or at least, not expanding.\nFinally, as examples above show, an IFS can indeed be algorithmically reduced to\na Culik\u2019s transform. The general algorithm of this reduction and its inverse are\ndiscussed in more detail in the paper.\nIt is obvious from the examples above that an IFS with k transforms\nrequires a 2(k+1)-element fat pixel and four 2(k+1)\u00d72(k+1) projection matrices.\nNote that the bottom half of these matrices is just a unit matrix, which does not\nhave to be stored at all. The upper halves are also very sparse, which ought to be\ntaken advantage of. For example, one can regard a projection matrix as a distance\nmatrix for a directed weighted graph. Since the matrix is sparse, the\ncorresponding graph would have rather few edges, which can be more efficiently\nstored as a list. Thus we come to exactly the same weighted graphs Culik\noriginally used to represent his finite automata [CULI95]. Hence, the automata\ndescribed by Culik are nothing but a neat trick to efficiently store and use sparse\nprojection matrices.9IV. An IFS image has a zerotree of wavelet coefficients\n The title of the section is actually a formulation of a theorem the paper\npresents and proves. In precise terms, within an image or a part of it with a\nproperty of self-similarity, i.e., which can be adequately described/reproduced by\nan IFS, a zero wavelet coefficient is always a root of a zerotree branch. In other\nwords, if a wavelet coefficient at some particular resolution turns out to be zero\n(exactly or within some tolerance), all the child coefficients, at finer resolutions,\nwill be zero as well (exactly or within the same tolerance). Thus, as long as an\nimage has enough self-similarity to allow efficient compression by an IFS (or,\nwhich is the same, by Culik\u2019s method), a zerotree coding of wavelet coefficients\nwould be beneficial. This is the unifying idea mentioned above; the theorem gives\nit a more precise meaning.\nBecause of space constraints, we will show the proof of the theorem on a\nsmall but characteristic example. We will analyze a case of a simple Haar wavelet\ntransform, which has very short wavelet filters, spanning, in 2D, over a cluster\nof four \u201cpixels\u201d. Consider a set of zoomed-out views of a self-similar (part of a)\npicture, and assume that the top view is made of the 4-pixel cluster. Following the\npremise of self-similarity, all these views are well described by an IFS, or (which\nis the same as we saw above) by a Culik\u2019s transform. Let corresponding \u201cfat\u201d\npixels of the cluster be F0, F1, F2, and F3, and the projection matrices C0-C3.\nLet us arrange the four-pixel neighborhood in a block-vector F=(F0 F1 F2 F3)'.\nNote that the fat pixels are vectors themselves, that is why we call F a block-\nvector. Finer resolution views of the cluster can be obtained by projecting it with\n(block) matrices Ci. For example, the lower-left quadrant of the cluster at a\nhigher resolution can be computed as\nC00 0 0\n0C00 0\n0 0C00\n0 0 0 C0\uf8eb\n\uf8ed\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7F0\nF1\nF2\nF3\uf8eb\n\uf8ed\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7\u2261C0F (6)\nwhere C0 is a block-projection matrix. The pixels F0-F3 can be combined to\nyield a (fat) wavelet coefficient W, by using a 2D (high-pass) filter with\ncoefficients [h 0,h1,h2,h3]:\nW=HF\u2261h0I h1I h2I h3I\nh0I h1I h2I h3I\nh0I h1I h2I h3I\nh0I h1I h2I h3I\uf8eb\n\uf8ed\uf8ec\n\uf8ec\uf8f6\n\uf8f8\uf8f7\n\uf8f7F\u2261I\nI\nI\nI\uf8eb\n\uf8ed\uf8ec\uf8f6\n\uf8f8\uf8f7h0I h1I h2I h3I ( ) F (7)\nwhere I is a unit matrix of the size that of Fi. The pivoting point of the proof is\nthe fact that matrix H is commutative with Ci. This is easy to see by directly\ncomputing CiH and HCi, which in both cases gives:10CiH\u2261HCi\u2261I\nI\nI\nI\uf8eb\n\uf8ed\uf8ec\uf8f6\n\uf8f8\uf8f7h0Cih1Cih2Cih3Ci ( ) (8)\nA child wavelet coefficient at any finer resolution can be computed then as\n  Wkid=HCi0Ci1LCikF=Ci0Ci1LCikHF (9)\nNote that the first element of a fat pixel Fi, Fi0, is the visible pixel. The hidden\nelements are either 1, or can be set to 1, because it does not matter in the case of\ncontracting matrices Ci, as we saw above. Since the wavelet filter H is high-pass,\n(h0 h1 h2 h3)(1 1 1 1)' is exactly zero. Therefore, if the wavelet-filtering of\nvisible pixels F i0 gives zero as well, the entire fat wavelet coefficient W=HF is a\nzero matrix. It follows then from eq. (9) that all the children wavelet coefficients\nare zeros as well.\nOne can easily accommodate other wavelet filters by considering larger\nneighborhood of pixels. Block-vector F and block-matrices Ci would have more\nblock-rows/columns, but the derivations remain the same. It is also easy to\ngeneralize the result to a case when a wavelet coefficient is not exactly zero, but\nsmall. As long as matrices Ci are not expanding (that is, convergence is\nguaranteed), all kid wavelet coefficients would be just as small as their parent."}
{"category": "abstract", "text": "Fractal image compression, Culik\u2019s image compression and zerotree prediction coding of\nwavelet image decomposition coefficients succeed only because typical images being compressed\npossess a significant degree of self-similarity. This is a unifying, common concept of these\nseemingly dissimilar compression techniques, which may not be apparent due to particular\nterminologies each of the methods uses. Besides the common concept, these methods turn out to\nbe even more tightly related, to the point of algorithmical reducibility of one technique to another.\nThe goal of the present paper is to demonstrate these relations.\nThe paper offers a plain-term interpretation of Culik\u2019s image compression, a very capable\nyet undeservingly underrepresented method giving spectacular results. The Culik\u2019s method will be\nexplained in regular image processing terms, without resorting to finite state machines and similar\nlofty language. The interpretation is shown to be algorithmically  related to an IFS fractal image\ncompression method"}
{"category": "non-abstract", "text": "+81-75-705-1694, e-mail: aogaki@cc.kyoto-su.ac.jp).\nI. Moritani is with the Department of Information and Communication\nSciences, (tel.: +81-75-705-1694, e-mail: i654168@cc.kyoto-su.ac.jp).\nT. Sugai is with the Department of Information and Communication\nSciences, (tel.: +81-75-705-1694, e-mail: sugai@cc.kyoto-su.ac.jp).\nF. Takeutchi is with the Department of Computer Sciences, (tel.:\n+81-75-705-1694, e-mail: takeut@ksuvx0.kyoto-su.ac.jp).\nF.M. Toyama is with the Department of Information and Communication\nSciences, (tel.: +81-75-705-1898, e-mail: toyama@cc.kyoto-su.ac.jp).test image. Section IV is for the summary.\nII. C ONDITIONAL EXPRESSIONS FOR BLURS\nWe consider a situation where any noise is absent. In the\nsituation a given (observed) image g(x,y)can be modeled as\nthe convolution of a true image f(x,y)and a blur image\nh(x,y),i.e.,\ng(x,y)=f(x,y)*h(x,y), (1)\nwhere the blur h(x,y)is assumed to be caused by a linear\nshift system. Throughout this paper the sizes of the given\nimage and the blur image are denoted by M\u0001Nandm\u0001n,\nrespectively. The z-transform H(u,v)of a m\u0001nblur\nh(x,y)is written as\nH(u,v)=1\nmnh(x,y)uxvy\ny=0n\u00011\n\u0002\nx=0m\u00011\n\u0002 , (2)\nwhere uand vare complex variables. In he following we\nconsider zero-values \u0002i(u)(i=1, 2,\u0001,n':n'\u0003n\u00011)which\nare the solutions of H(u,v)=0for a given u.\nThe CE is derived as follows. From (2) we obtain the\nfollowing simultaneous equations for h(x,y)with the \u0001iat\nmndifferent points of u\u0002(\u0002=0, 1,\u0001,mn\u00011),\nH(u0,\u0002i(u0))=0,\nH(u1,\u0002i(u1))=0,\n\u0001\nH(umn\u00011,\u0002i(umn\u00011))=0.\u0001\u0001\u0001\u0001\u0001\u0001\u0001 (3)\nFrom (3), the mn\u0001mn matrix Dcomposed of the\ncoefficients of the blur elements is given as\nwhere q=mn\u00011and we dropped the constant factor 1/mn.\nThe CE is given as E\nm\u0003nu(\u0002i(u0),\u0001,\u0002i(umn\u00011))\u0004D=0.T h i s\nis actually the condition for obtaining non-trivial solutions for\nh(x,y).The E\nm\u0003nu(\u0002i(u0),\u0001,\u0002i(umn\u00011))implicitly includes the\nCEs for any blurs of the sizes smaller than m\u0001n,except for\nthe size r\u00011(r=1,\u0001,m).T h i sc a nb ep r o v e nb yb a s i c\nmanipulations for the determinant. Further, the CE can detectS. Aogaki, I. Moritani, T. Sugai, F. Takeutchi, and F.M. Toyama, Kyoto Sangyo University,\nKyoto-603, JapanConditional Expressions for Blind\nDeconvolution: Multi-point form\nL\n(4)D=1\u0002i(u0)\u0001\u0002i(u0)n\u00011u0u0\u0002i(u0)\u0001u0\u0002i(u0)n\u00011\u0001\u0001\u0001 u0m\u00011u0m\u00011\u0002i(u0)\u0001u0m\u00011\u0002i(u0)n\u00011\n1\u0002i(u1)\u0001\u0002i(u1)n\u00011u1u1\u0002i(u1)\u0001 u1\u0002i(u1)n\u00011\u0001\u0001\u0001 u1m\u00011u1m\u00011\u0002i(u1)\u0001 u1m\u00011\u0002i(u1)n\u00011\n\u0002\u0002 \u0002 \u0002 \u0002 \u0002 \u0002 \u0002 \u0002\n1\u0002i(uq)\u0001\u0002i(uq)n\u00011uquq\u0002i(uq)\u0001uq\u0002i(uq)n\u00011\u0001\u0001\u0001 uqm\u00011uqm\u00011\u0002i(uq)\u0001uqm\u00011\u0002i(uq)n\u00011\u0003\n\u0005\u0004\n\u0004\u0004\n\u0004\u0006\n\b\u0007\n\u0007\u0007\n\u0007\n,2the zero-values of blurs of the size 1\u0001k(k=n+1,\u0001,N).\nThis can be seen as follows. In such a one dimensional blur\nthe zero-values are all constant, i.e., do not depend on u.\nHence when we substitute the zero-values of 1\u0001kblurs into\nD ,more than two row vectors become proportional to each\nother. This is why the CE constructed for the m\u0001nblurs\ndetects the zero-values of 1\u0001kblurs.\nT h i sv e r s i o n2s e e m st ob es i m p l ef o r m a l l yc o m p a r e dw i t h\nthe version 1 in the sense that we do not have to evaluatederivatives of the zero-values. However, there is a nervoussituation in this version 2. In order to evaluate the CE, we\nhave to solve\n\u0001iatmn different points of\nu\u0002(\u0002=0,\u0001,mn\u00011)numerically. Further, the mnsolutions\nmust be taken to be of the same \u0001i. Further, when we\nreconstruct an original image by the inverse Fourier\ntransform after removing the zero-values \u0001iof the blurs from\nthose of the given image, we have to find \u0001iof the blurs at\nevery step uj(j=0,\u0001,M\u00011). Hence, in each uj,w eh a v e\nto solve mnsolutions of \u0001i.T od ot h i sw et a k e mndifferent\npoints u\u0002(\u0002=0, 2,\u0001,mn\u00011)in the vicinity of each uj, i.e.,\nasuj,\u0001=uj+\u0001\u0001u.This requires an optimization for the\nparameter \u0001u.N a m e l y ,t oh o l dt h es a m e \u0001iwe have to take\n\u0001uconsiderably small. On the other hand, if we take \u0001utoo\nsmall,\u0001isolved for mndifferent points uj,\u0001become close to\neach other. This may cause more than two row vectors in the\ndeterminant Dto be nearly proportional to each other. Thus,\nundesired D\u00010may be caused. Therefore, we have to\ndetermine an optimal \u0001uto accomplish the detection process\nsuccessfully.\nIn order to complete the image restoration we need the CE\nalso for the variable u.The CE for ui sg i v e ni nt h es i m i l a r\nform to (4). The CE for uis denoted as\nE\nm\u0003nv(\u0002i(v0),\u0001,\u0002i(vmn\u00011))=0,where\u0001iare the zero-values\nofufor the blurs.\nIII. I LLUSTRATION\nIn the preceding section we have given the version 2 of the\nCEs. In this section, we illustrate the multiple blur detectionby using the same image as that used for the illustration of theversion 1 [2].\nFig.1 shows the test images that are the same as those used\nin [2]. Fig. 1(a) shows a\n40\u000140model image that we regard\nas a true image [3]. Figs. 1(b), 1(c), 1(d), and 1(e) representblur images of the sizes\n1\u00012, 2\u00011,2\u00012,and 2\u00013,\nrespectively. We convolved these four blurs into the true\nimage of Fig. 1(a). Fig. 1(f) shows the convolved image, ofwhich size is\n43\u000144.\nFig. 1. (a): True image of 40\u000140size that we took from [3]. (b): Blur\nimage of 1\u00012size. (c): Blur image of 2\u00011size. (d): Blur image of 2\u00012\nsize. (e): Blur image of 2\u00013size. (f): The image that was obtained by\nconvolving the four blurs of (b)-(e) into the true image of (a). The size of the\nconvolved image is 43\u000144.\nIn Fig. 2 we show the results of the numerical evaluations of\nthe CEs E\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))and E\n2\u00023v(\u0001i(u0),\u0001,\u0001i(u5)),\nw h e r ew ep l o t t e d log[| E2\u00023u(\u0001i(u0),\u0001,\u0001i(u5)) |\u00021050+1]and\nlog[| E2\u00023v(\u0001i(v0),\u0001,\u0001i(v5)) |\u00021050+1].We optimized \u0001u\u0001\nand\u0001vas\u0002u=\u0002v=\u0004e\u0001i\u0002\u0003with\u0001\u0002=\u0003/ 2150 and took as\n\u0001=1.Note that we took \u0001\u0002very small to hold the same\n\u0001iin sampling mnpoints. As we mentioned earlier, this\ncauses not true E\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))\u00030.To distinguish the\ntrue E\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))=0from the not true\nE\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))\u00030we need a high precision. This is\nwhy we multiplied the factor 1050in evaluating the CE. As\nwe stressed in the preceding section, both\nE\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))and E\n2\u00023v(\u0001i(u0),\u0001,\u0001i(u5))include\nCEs for blurs of smaller sizes 1\u00012,2\u00011,2\u00012,and\n2\u00013implicitly. Therefore, E\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))must\ndetect totally four (=1+1+2)zero-values \u0001i.When there\nexists a degenerate zero-value in the 2\u00013blur, it may be3three (=1+1+1).On the other hand, the number of\nzero-values \u0001ithat should be detected by\nE\n2\u00023v(\u0001i(u0),\u0001,\u0001i(u5))is just three (=1+1+1).As seen in\nFig. 2(a), for \u0001u=0,four zero-values \u00011,\u000113,\u000120,and\u000121\nare detected by E\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5)).A l s oa to t h e r \u0001ufour\nzero-values are well detected by E\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5)).\nHere, note that the detected solution numbers of the\nzero-values are different at each \u0001u.In this illustration we\nnumerically solved the zero-values by Mathematica ,\nseparately at each \u0001u.Accordingly, the order of the\nnumerical solutions (zero-values) is at random at each \u0001u.As\nseen in Fig. 2(b), for \u0001v=0,three zero-values \u00011,\u00013,and\n\u000112are detected by E\n2\u00023v(\u0001i(u0),\u0001,\u0001i(u5)). Also at other\npoints\u0001vthree zero-values are well detected by\nE\n2\u00023v(\u0001i(u0),\u0001,\u0001i(u5)). Note that the detected zero-values by\nE\n2\u00023u(\u0001i(u0),\u0001,\u0001i(u5)) and E\n2\u00023v(\u0001i(u0),\u0001,\u0001i(u5))are\nperfectly the same as those \u0001detected by the CEs of the\nversion 1 [2]. This is quite natural because we used the same\nimage for the two illustrations. This implies that the detectionprocesses have been done successfully for both the twoversions of the CEs.\nFig. 3 shows the restored image by removing the blurs.\nThe restored image is perfectly the same as the original imageof Fig. 1(a).\nThe version 2 of the CE is formally simpler than the\nversion 1 of [2] because the version 2 of the CE can beevaluated with only the zero-values. However, we have to\nspend a time to optimize the sampling parameters\n\u0001uand\n\u0001v.\n4\nFig. 2. Results of the evaluations of the version 2 of the CEs\nE2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))and E2\u00023v(\u0001i(v0),\u0001,\u0001i(v5)).(a) and (b) show the\nresults of the tests done for \u0001iand\u0001i,respectively. We took \u0001uand\u0001vas\n\u0001u=\u0001v=1.\nFig. 3. Restored image by removing the four and three zero-values that were\nrespectively detected by E2\u00023u(\u0001i(u0),\u0001,\u0001i(u5))and E2\u00023v(\u0001i(v0),\u0001,\u0001i(v5))..IV. S UMMARY\nWe have derived the version 2 of the CEs for finding\nmultiple blur convolved in a given image, in a situation whereany noise is absent. The CE is given in terms of the\nzero-values of the given image evaluated at multi-point. By\nusing the CE we can avoid a tough analysis of the zero-sheetsof the given image and reduce the processing time in the LB\u2019sdeconvolution. The CE constructed for\nm\u0001nblurs can\nactually detect any blurs of any sizes smaller than m\u0001nall\nat once. Therefore, when we apply the CE for an actual image,it is desirable to construct the CE for blurs of as large size as\npossible.\nAs we mentioned above the CE detects multiple blur all at\nonce. Instead the sizes of the detected blurs cannot be\ndetermined, except for a single blur. In our third paper [4] we\npresent a simple method to find a single blur of only aspecified size\nm\u0001n.\nACKNOWLEDGEMENT\nThis work was supported in part by the Science Research Promotion from\nthe Promotion and Mutual Aid Corporation for private Schools of Japan, and\na grant from Institute for Comprehensive Research, Kyoto SangyoUniversity.\nREFERENCES\n[1] R.G. Lane and R.H. Bates, \"Automatic multidimensional\ndeconvolution ,\"J. Opt. Soc. Am , vol. A4, 1987, pp. 180-188.\n[2] S. Aoagaki, I. Moritani, T. Sugai, F. Takeutchi, and F.M. Toyama,\n\"Conditional expressions for blind deconvolution: Derivative form\", \u0001\nICITA 2007, submitted.\n[3] http://www.imageprocessingplace.com/; Rafael C. Gonzalez, Richard\nE. Woods, Digital image processing, (R.C. Gonzalez, R.E. Woods),\n329-341, Second Edition, 2002.\n[4] S. Aogaki, I. Moritani, T. Sugai, F. Takeutchi, and F.M. Toyama, \u0001\n\"Simple method to eliminate blur based on Lane and Bates algorithm \",\nICITA2007, submitted."}
{"category": "abstract", "text": "\u2014We present conditional expression (CE) for finding\nblurs convolved in given images. The CE is given in terms of the\nzero-values of the blurs evaluated at multi-point. The CE candetect multiple blur all at once. We illustrate the multipleblur-detection by using a test image.\nIndex Terms Deconvolution, Image restoration \u0002\n\u0002\nI. I NTRODUCTION\nane and Bates\u2019 (LB) blind deconvolution [1] is based on\nthe zero-sheets of the z-transform of given images. This\nmethod enables us to uniquely eliminate blurs convolved inthe given images. However, due to its advanced analytical\nmethod, the computational complexity for the imageprocessing is enormous.\nIn our first paper [2] we presented the version 1 of\nconditional expressions (CEs) for LB\u2019s blind deconvolution.The CEs can detect the zero-values of the blurs automatically.\nThe CEs of the version 1 are given in terms of the derivatives\nof the zero-values of the blurs. Hence, they can be evaluatedat a single point. This is a great advantage of the version 1.On the other hand, for blurs of large sizes the CEs of the\nversion 1 become very complex.\nIn this paper, we present a simple version of the CEs,\nwhich we call the version 2 throughout this paper. The CE of\nthe version 2 is given in terms of only zero-values of blurs\nevaluated at multi-point. The version 2 reduces thecomputational complexity considerably compared with those\nof the version 1. Instead we need to optimize a sampling\nparameter for the multi-point. This version 2 can also detectblurs of any sizes all at once.\nIn Sec. II we present the CE of the version 2. In Sec. III we\ngive an illustration of the multiple blur detection by using a\nS. Aogaki is with the Department of Information and Communication\nSciences, (tel."}
{"category": "non-abstract", "text": "aliasing, upsampling, frequency \ufb01lter, edge detection\n1 Introduction\nRaster images often have distortions connected with their r aster structure. These\ndistortions can for example be an undersampling, distorted intensity response\ncurves or processing like sharpening or unsharp mask. Upsam pling the distorted\nimages, using for example the bicubic interpolation [5, 9], might in e\ufb00ect sub-\nstantially yield the raster structure of the original image , what is known in image\nprocessing as aliasing [9]. Additionally, upsampling meth ods that attempt to pro-\nduce sharp images, might have an intrinsic trait of introduc ing the aliasing [9].\nThe presented method attempts to remove the aliasing artifa cts using frequency\n\ufb01lters based on the discrete fast Fourier transform, and app lied directionally in\ncertain regions placed along the edges detected in the image . The selective di-\nrectional applying of these \ufb01lters serves the purpose of est imating the presence of\nthe aliasing in the places where it is likely to occur, and whe re it is at the same\ntime unlikely that the objects in the image will be confused w ith the aliasing.\nThe special feature of the method is that it aims to selective ly reduce the\naliasing, trying at the same time to preserve the sharpness o f image details. It\nmakes it di\ufb00erent from typically used interpolations like th e bilinear or bicubic\nones [5, 9], that produce images that are blurry or aliased, o r various anisotropic\nsmoothing methods like these described in [13, 12], that aim to generally smoothen\nobjects in the image, what might lead, as it will be illustrat ed in tests, to very\nunnatural looking images. On of the more widely used complex image restoration\nmethods \u2013 NEDI [6], also makes some textures look unnatural a nd still produces\nsubstantial aliasing in some images.\n1The following sections discuss, in order, aliasing, a custo m sub\u2013pixel precision\nedge detection method used to direct the \ufb01ltering, and the fr equency \ufb01ltering.\nFinally, some tests are presented.\n2 Aliasing\nThe discussed aliasing in the upsampled images is connected with the raster of the\nsource image, and not of the upsampled image. In Fig. 1, a sche matic example\nof an object upsampled four times in each direction is shown. Bold lines show\nborders of the original pixels, smallest rectangles show bo rders of the pixels in\nthe upsampled image. The image shows a dark object on a white b ackground.\nFigure 1: A schematic example of upsampling.\nThe object boundary in the original image consisted of pixel s whose brightness\nchanged approximately periodically, with the period conne cted to the period of\npassing of the horizontal line between the pixels in the orig inal raster. It can\nbe seen in the upsampled image \u2013 the brighter boundary pixels in the original\nimage have corresponding 4 \u00d74 pixel blocks in the upsampled image that consist\nof mostly white pixels, and conversely, the darker pixels in the original image\nhave corresponding blocks of mostly dark pixels. Similarly , of course, if boundary\nwould be more close to a vertical one, the period of passing of the vertical raster\nlines would be important in turn. As can be seen in the example in Fig. 2, various\ndistortions of the image may cause \u2018waving\u2019 of location, col or or sharpness of the\nupsampled boundaries, depending on the particular distort ion and the upscaling\nmethod. What is important here, though, is that the period l0of the \u2018waving\u2019 for\na straight boundary is the same as the period of the brightnes s variability of the\npixels in the original image, which in turn, as it was discuss ed and also can be\nseen in Fig. 2, is approximately equal to the length of the obj ect border between\ntwo either horizontal or vertical subsequent lines of the or iginal raster, depending\non if the border is either more close to, respectively, the ho rizontal or the vertical\ndirection. For a straight border, l0is thus as follows:\nl0=\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3U/vextendsingle/vextendsingle/vextendsingle/vextendsinglexl\u2212x0\nyl\u2212y0/vextendsingle/vextendsingle/vextendsingle/vextendsingleif|xl\u2212x0| \u2265 |yl\u2212y0|\nU/vextendsingle/vextendsingle/vextendsingle/vextendsingleyl\u2212y0\nxl\u2212x0/vextendsingle/vextendsingle/vextendsingle/vextendsingleif|xl\u2212x0|<|yl\u2212y0|(1)\n2Figure 2: An example of aliasing. The \ufb01rst column contains original 64 \u00d764 images,\nthat are, in subsequent rows, with only small distortions, distorte d gray response curves\nand sharpened. The second column contains corresponding 128 \u00d7128 upsampled images,\nusing the bicubic interpolation.\nwhereUis the scale of the upsampling, ( x0,y0) is the \ufb01rst pixel of a straight\nfragment of a boundary and ( xl,yl) is the last pixel of the fragment, using the co-\nordinates of the upsampled image. If the fragment is only app roximately straight,\nthe equation gives an approximate common l0, while local periods can vary along\nthe fragment. An example of such an approximately straight f ragment is illus-\ntrated in Fig. 7.\nThus, estimation the period on basis of the orientation of a b order might be\na good way of detecting the corresponding artifacts, what in turn might be the\n\ufb01rst stage of reducing these detected artifacts. This is the basic presume of the\npresented method.\n3 Sub\u2013pixel precision edge detection\nEdge detection [7, 2, 4, 14] in raster images is one of the basi c methods of fea-\nture extraction from images. This paper employs a simple low \u2013level de\ufb01nition of\nan edge described in [8]: an abrupt change in some low\u2013level i mage feature as\nbrightness or color, as opposed to a boundary, described in t he cited paper as\na higher\u2013level feature. The presented edge detection metho d is designed to give\nedges with sub\u2013pixel precision, and to detect even small dis continuities in the im-\nage. This is because the aim is, as opposed to typical edge det ection methods, not\nto extract the more prominent edges, but to get a precise edge map for frequency\n\ufb01ltering. Additionally, the edge detector employed must ha ve a high resistance to\nthe image distortions discussed, like undersampling. This is why a custom edge\ndetector was designed.\n33.1 Finding edges\nIn the \ufb01rst step of the edge detection, a Sobel operator [11] i s applied to the up-\nsampled image. If the image has multiple bands, each one is pr ocessed separately\nand then the resulting images are averaged into one single\u2013b and image. Then, the\nroof edges [10, 1] are searched for in that resulting image. A s the Sobel operator\nproduces a gradient map\u2013like image, the roof edges obtained are e\ufb00ectively the\ndiscontinuity edges as discussed in [8].\nTo detect the roof edges, an operator called peakiness detec tion is used. The\nedge detection basically works by \ufb01nding \u2018bumps\u2019 in the grad ient image, at various\nangles. The computational complexity is kept low by using th e following approach.\nFor each of the angles ai= (i+ 0.5)1\n2\u03c0/N,i= 0, ... N\u22121, scan the image along\nlines that are at the angle aito the horizontal axis, so that:\n\u2022ifa1\u2264\u03c0/4, let the consecutive lines be one vertical pixel apart;\n\u2022ifa1> \u03c0/ 4, let the consecutive lines be one horizontal pixel apart;\n0\n1\n2\n3\n...\nFigure 3: A series of scan lines for a given angle.\nand let the lines cover such a range, that, together, they cov er the entire area of\nthe image. An example case for a1\u2264\u03c0/4 is illustrated in Fig. 3. As it can be\nseen, such a way of aligning subsequent lines provides that a ll pixels are covered,\nin scans for each a1. Yet, there is not a separate searching for \u2018bumps\u2019 around\neach pixel at an angle ai\u2013 sequential searching for \u2018bumps\u2019 on a single line at an\nangleaicovers searching for \u2018bumps\u2019 for each pixel on that line, wha t decreases\nthe mentioned computational complexity. The value of N= 7 was chosen, as a\nprecise enough and making the scanning reasonably fast at th e same time.\nThe \u2018bump\u2019 criterion is as follows. Let p0, p1, ... pM\u22121be intensities of sub-\nsequent pixels on a given scanned line of Mpixels. The searching for \u2018bumps\u2019\nwithin a single line works as follows: for the pixel nth, if its intensity is larger\nbydthan both the intensity of the pixel ( n\u2212r)th and the intensity of the pixel\n(n+r)th, then increase the \u2018peakiness\u2019 of the pixel by 1. The coe\ufb03 cientsdand\nrshould be large enough to reduce single\u2013pixel level noise, a nd small enough to\nmaintain good edge location. To improve the detection of edg es at various scales,\npmax= 3 passes of the edge detection are performed, each modifyin g common\n4\u2018peakiness\u2019 of a pixel, with three di\ufb00erent sets of values for dandr:\np= 1,2,...pmax\nrp=p+ 2\ndp= 0.015 + 0.005p(2)\nwhere the index pdenotes a respective pass. If the image processed is very blu rry,\nrmight require an appropriate increase.\nBecause pmaxN > 1 scanning lines pass through each pixel, one for each angle,\nthe \u2018peakiness\u2019 is an averaged value of several tests for the \u2018bumps\u2019, what may\nobviously reduce single\u2013pixel level noise.\nOnly pixels whose accumulated peakiness value is equal or la rger than a given\nthreshold eminare regarded as the edge ones, to reduce the detection of what is\nan image noise, and not a real edge. The value of emin= 6 was adjusted in tests.\nIt can be decreased for images with low noise and weak edges, a nd increased for\nimages with high level of noise.\nThe roof edges obtained using this method can be thick, while the needed\nedges must be one\u2013pixel wide. To correct that, centers of the roof edges are\nextracted using a simple thinning method, for example that d escribed in [3], with\nthe 8\u2013neighborhood criterion.\n3.2 Correction of the edges\nThe discussed distortions, the edge detection method itsel f, or image noise may\ndecrease the quality of the obtained edges. Therefore, clea ning of the edges from\nsmall branches and protruding pixels, and the reduction of \u2018 waving\u2019 of the edges,\nis used.\n3.2.1 Cleaning the edges\nBoth the method of the waving reduction, and the \ufb01nding of app roximately\nstraight fragments discussed later in Sec. 4, are sensitive to two kinds of \u2018noise\u2019\nof the edges \u2013 small branches and single protruding pixels. E xample of such dis-\ntortions is shown in Fig. 4. The work\u2013around is straightforw ard \u2013 edges below\na given length are deleted, where each pixel connecting thre e or more branches\nis considered a boundary between the edges. In the \ufb01rst itera tion, edges of the\nlength of 1 are deleted, then edges of the length 2, and so on, t ill some value\nLmin\u22121, with re\u2013measuring of the edge lengths after each iteratio n. If, instead,\nwe\u2019d immediately begin with deleting all edges of length les s or equal than Lmin,\nthen the edges like the grayed one in Fig. 4 would be deleted, i nstead of only the\ntwo small branches visible in the image.\nThe small protruding single pixels, like that seen in Fig. 4, are moved back to\nthe edge, using a trivial method.\n5protruding single pixelsbranches\nFigure 4: Example edge \u2018noise\u2019 to be cleaned.\nFigure 5: An example of the reduction of waving for an image with two d i\ufb00erent gray\nresponse curves.\n3.2.2 Reduction of waving\nAliasing in the upsampled image may produce variously \u2018wavi ng\u2019 edges, An exam-\nple of \u2018waving\u2019, and its correction, is shown in Fig. 5. The ed ge detector should be\nresistant to the aliasing artifacts, thus, the reduction of the waving is performed.\nThe procedure to reduce waving has the following steps:\n1. Find junctions, that is corners of pixels that have two nei ghboring edge\npixels.\n2. For each of these two pixels, \ufb01nd the length of rectangular sequences Sthat\nbegin at the subsequent edge pixel. A rectangular sequence i s a sequence of\n4\u2013neighboring pixels that is either vertical or horizontal .\n3. If one of these cases occurs: both rectangular sequences Sare horizontal,\nor both are vertical, one consists of a single pixel and the ot her has more\nthan one pixel, then such a junction can be classi\ufb01ed as, acco rdingly, either\nhorizontal or vertical. In such a case, then, it is assumed th at the junction\nis a part of some edge Ethat can, respectively, be classi\ufb01ed as being locally\neither closer to some horizontal or vertical direction.\n4. ifEcould be classi\ufb01ed as locally closer to horizontal or vertic al direction,\nthe junction is marked as a movable one along that closer dire ction, that is,\nable to modify E, by shortening the longer Sand extending the shorter S,\nas it is shown in the example in Fig. 6.\n5. For each movable junction, set the maximum length lmaxthe junction is\nallowed to move. This constraint exists to prevent the edges from too large\nmoves. Let the two rectangular sequences Sneighboring to a junction have\n6VH\nVH\n(a) (b)\nFigure 6: (a) Examples of junctions. The edge pixels are marked with rectangles. All\njunctions are marked with crosses. Only two junctions, for clarity , have their sequences\nSmarked with gray color. Possible moving direction of these two junct ions are marked\nwith arrows. The junction His the horizontal one, and the junction Vis the vertical one.\n(b) The same edge, the junctions HandVwere moved by the length of one pixel.\ntheir respective lengths s1ands2. Then,\nlmax= min [ max ( s1,s2), l1min (s1,s2) +l2] (3)\nThe basic limitation in (3) is that lmax\u2264max (s1,s2), thus, an approxi-\nmately straight edge can not be moved aside by more than about one pixel.\nThe coe\ufb03cients l1andl2precisely regulate lmax. It was found in tests, that\nl1= 3 and l2= 1 gives a good trade\u2013o\ufb00 between e\ufb00ective waving reduction\nand a none to moderate displacement of the edges.\n6. After lmaxwas determined for each movable junction, the following sub \u2013\nprocedure Wis repeatedly performed, each time for the whole image, unti l\neither the number of the repetitions of performing Wreaches a given number\nNw= 50, or the stability is reached, that is a given run of Wdoes not change\nanything in the image. The limitation by using Nwis only to prevent the\nwave reduction from taking too long time.\nWis as follows. For each movable junction, if |s1\u2212s2|>1, and the junction\ndid not reach its lmaxvalue, move the junction so that to shorten its larger\nsequence Sby one pixel and extend its shorter sequence Sby one pixel. The\ncondition |s1\u2212s2|>1 exists to cause the rectangular sequences Sto have\nmore similar length, with the prevention of a junction to be m oved back and\nforth in subsequent executions of the procedure W.\n4 Frequency \ufb01ltering\nThe frequency \ufb01ltering has two stages: in the \ufb01rst stage, \ufb01nd approximately\nstraight fragments of edges, and in the second stage, do freq uency \ufb01ltering directed\nalong each such a fragment. Because the fragments are approx imately straight, a\ncommon approximate base period of aliasing artifacts can be determined for each\nfragment. Such a base period l0is used then in \ufb01ltering the frequency spectrums.\n74.1 Finding approximately straight fragments\nAn approximately straight fragment Fis an edge or a part of the edge. A fragment\nFcan not include the branch pixels, that is those that have mor e than two\nneighbors being edge pixels. The criterion of a fragment to b e approximately\nstraight is very simple: all pixels of the fragment must not b e further from the\nstraight line between two endings of Fby more than d=sdU.Uis the scale\nof upsampling, and it occurs in the formula because image fea tures are linearly\nproportional to U, andsdis a coe\ufb03cient regulating how approximately straight\nFshould be. It was determined in tests that the value of sd= 0.4 is a good\ntrade\u2013o\ufb00 between many short fragments for small sdand bad approximation of\ncommon l0for the whole Ffor large sd. There is yet another condition, Q, forF:\nits subsequent pixels must all either have always increasin g or always decreasing\nxcoordinates or ycoordinates. If the condition applies to the xcoordinates, the\nfragment Fis called a horizontal one, and otherwise it is called the ver tical one.\nAn example of a horizontal Fis shown in Fig.7. The need for the condition Q\nwill be explained in Sec. 4.2.\nThe fragments Fare searched for as follows: \ufb01nd an edge pixel Pthat is a\npart of an edge whose pixels are not assigned to any fragment Fyet. Trace the\nunassigned edge pixels from the pixel P, using the 8\u2013neighborhood criterion, the\nsame that was used during thinning of the edges. Do that until the end of the\nunassigned edge pixels is found, or a branch pixel is found. T hen trace the pixels\nback to search for the other end of these unassigned pixels, u ntil the other end is\nreached or the criterion of approximate straight edge stops to be ful\ufb01lled, and this\nway \ufb01nd a new F. Then set the pixels of the new Fas ones assigned to F, and\ncontinue searching for fragments Funtil all pixels, excluding the branch pixels,\nare assigned to some F.\n4.2 Fragment-directed frequency \ufb01ltering\nIt is important for the frequency \ufb01ltering to be applied poss ibly along object\nboundaries. Applying it, for example, across fence pales, m ay alter important\nimage matter, as the periodicity of occurring of the pales mi ght be confused with\naliasing. This is why the edge detector is employed, and then the fragments F\nare extracted.\nWith each F, the \ufb01ltering strength Sfis estimated. Sfis directly related\nto the size of the region along Fthat is \ufb01ltered, as illustrated in Fig. 7 \u2013 the\nfragment is moved Sftimes up and Sftimes down for horizontal F, orStimes\nleft and Stimes right for vertical F. For each of the resulting placements, the\nbrightness of subsequent pixels in the upsampled image, cov ered by the moved\nF, is determining the brightness functions Bb\ni(x),x= 0, ... N B\u22121, where NBis\nthe number of pixels in Fandi=\u2212Sf,\u2212Sf+ 1, ... Sfis assigned for each move\n8ofFas shown in the example in Fig. 7, The index b= 0,1, ... c\u22121 determines\none of the cbands of the upsampled image. Each of these functions is subj ect to\nfrequency \ufb01ltering as described in the next section.\n(x  , y  )l       l(x  , y  )0      0s\ni=0i=1i=2i=3\ni=\u22121\ni=\u22122\ni=\u22123\nFigure 7: An example of a region \ufb01ltered along a fragment. The fragm ent is marked\nblack.\nThe pixels across di\ufb00erent ido not overlap, that is the band within each pixel\nis frequency \ufb01ltered once, thanks to the condition Qdescribed in Sec. 4.1.\nThe variability of Sfcomes from the presume that an aliasing artifact is better\ndetectable if it has the size of at least several lengths of l0. It is because the artifact\nmight be otherwise too easily mistaken with something that i s not such an artifact.\nFor example a region being a normal image matter without any a rtifacts might\nlikely have the brightness that is approximately given by a f ragment of a single\nlobe of the sine function that has the period of l0, yet it might be much less likely\nthat the brightness of that region is approximately given by as much as several\nlobes of such a sine function.\nThe formula for computing Sfon basis on the number of pixels NBinFis as\nfollows:\nSf=/braceleftBigg\n0 if NB< sll0\nsuNBifNB\u2265sll0(4)\nAs it can be seen, Sf= 0 if the fragment is too short, to decrease the probability\nof confusing an artifact with image matter, as discussed ear lier in this section.\nOtherwise, Sfgradually increases with suNB. The coe\ufb03cients slandsuwere\ntuned in a series of tests. Small slmeans a greater probability of an undesired\ndistortion caused by the frequency \ufb01ltering of image object s that are not artifacts.\nConversely, large slmeans that more artifacts might be left uncorrected. The\ncoe\ufb03cient suregulates the strength of S, which in turn is connected with the\nrange along Fthat is \ufb01ltered. Thus, small sumeans that an artifact might be\ncorrected only in its small part closer to F, and large sumeans that some regions\nlying further to Fmight be undesirably distorted by the \ufb01ltering. It was found\nexperimentally, that sl= 2 and su= 0.25 give relatively good results.\n94.3 Filtering of the brightness function\nThe FFT requires the transformed function to have the number of pairs to be the\npower of 2, what is untrue in general for Bb\ni(x). To prevent spurious high frequency\ncomponents and to ful\ufb01ll the requirement for the number of pa irs,Bb\ni(x) is padded\nwith additional elements to create Cb\ni(x). Let the number of pairs in Cb\ni(x) be\nsuch a smallest possible value NCthat it is the power of 2 and the number of pad\npairsNC\u2212NBto add to Bb\ni(x) to create Cb\ni(x) is greater than or equal to \u230aNB/2\u230b,\nLet the mean value of Bb\ni(x) betb\ni. The function Cb\ni(x) is de\ufb01ned as follows:\nx= 0...NC\u22121\nmC=\u230a(NC\u2212NB)/2\u230b\neC=\u230amC+NB\u22121\u230b\nwl=x/(mC\u22121)\nwr= (NC\u22121\u2212x)/(NC\u22122\u2212eC)\nCb\ni(x) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3wlBb\ni(mC\u2212x) + (1\u2212wl)tb\niforx < m C\nBb\ni(x\u2212mC) for x\u2265mC\u2227\nx\u2264eC\nwrBb\ni(2eC\u2212x) + (1\u2212wr)tb\niforx > eC(5)\nThus, there are two mirror margins added with their widths of at least\u230aNB/4\u230b\neach, that converge to the mean value of Bb\ni(x) at the lowest and the highest\narguments of Cb\ni(x). The requirement for minimum width of the margins and the\ncommon convergence value minimize spurious high frequency components in the\nspectrum of Cb\ni(x). There is an example of the function Cb\ni(x) in Fig. 8.\nC\u2032b\ni(x)Cb\ni(x)xbrightness\n18 44230225220215210205200195190185180\nFigure 8: An example of the functions Cb\ni(x) andC\u2032b\ni(x). Their fragments from index 18\nto index 44 are exact repetitions of, respectively, Bb\ni(x\u221218) andB\u2032b\ni(x\u221218).\nLet the brightness function Cb\ni(x) after transforming it with the FFT be Fb\ni(f),\nf= 0,... N C\u22121. Because Cb\ni(x) is real, it holds true that Fb\ni(f) =Fb\ni(NC\u22121\u2212f)\nfor the whole domain of Fb\ni(f). Further, because of that symmetry, each operation\n10toFb\ni(f) will also implicitly be applied to Fb\ni(NC\u22121\u2212f), and charts will show\nFb\ni(f) only for f= 0,... N C/2\u22121. Letf0=NC/l0be the frequency corresponding\ntol0. IfBb\ni(x) contains aliasing, peaks are expected near to f0and its harmonics\ninFb\ni(f). Because altering only the peak at f0appeared to be very e\ufb00ective, the\npeaks at harmonics of f0are ignored. Simply setting the values of Fb\ni(f) at or near\nf0to 0 might produce the valley that would distort fragments th at do not have any\nartifacts, because a valley might appear in the frequency sp ectrum where there\nwas not even any peak resulting from the aliasing. The soluti on is to compute\nthe mean maround the expected peak at f0, and if the modulo of peak values\nexceedm, \ufb02atten the peak down to the mean m. The mean mis weighted using\nthe weight function W(f) such that it has its maximum values at approximately\n1/2 and 3/2 off0, and thus, these regions of maximum values are placed away\nfrom both f0and the harmonics of f0, which in turn could contain peaks resulting\nfrom the aliasing, and thus skew the value of m. The corresponding formulas for\ncomputing mare as follows:\nW(f) = 1/[1 +ws(f\u22121\n2f0)2]+\n1/[1 +ws(f\u22123\n2f0)2]\nS=f<NC/summationdisplay\nf=0W(f)\nm=/summationtextf<NC\nf=0W(f)Fb\ni(f)\nS(6)\nThe coe\ufb03cient wsdetermines the width of each of the two peaks and was tuned\nto 3 using test images. The value Sis computed to normalize the weight function\nW(f). An example diagram of W(f) is shown in Fig. 9.\nThe reduction of the peak is in detail performed as follows. F irstly, let the\npeak be located by a function M(f). The value of the function is interpreted\nas follows: 1 for no altering of the spectrum at f, 0 for a maximum altering of\nthe frequency spectrum at f, that is, lowering its modulo values to mif greater,\nValues of M(f) in between 0 and 1 determine respective partial alteration . The\nfunction M(f) is computed as follows:\nM(f) =/braceleftBigg1 if f= 0\ntanh/bracketleftBig\nms(NC/f\u2212l0)2/bracketrightBig\niff >0(7)\nThe function is constructed so that M(f) creates a valley at and near the peak\nwith the lowest value close to 0, is equal to 1 at the constant c omponent f=\n0, almost equal to 1 for frequencies substantially lower or h igher than f. The\ncoe\ufb03cient ms= 0.03 was tuned to regulate the width and slopes of the valley.\nThe limited steepness of the slopes of M(f) reduces the the possible distortions\n11F\u2032b\ni(f)M(f), multiplied by 100W(f), multiplied by 100Fb\ni(f)f30 25 20 15 10 5 0200\n150\n100\n50\n0\nFigure 9: An example of \ufb01ltering of a function Bb\ni(x) from Fig. 8. The functions W(f)\nandM(f) are so distorted because of the low value of NC.\nin the space domain caused by the frequency \ufb01ltering. The lef t slope is so steep\nto decrease the reduction of low frequencies. Reducing them , because of their\nusually large values, appeared to produce strong discontin uity e\ufb00ects between the\n\ufb01ltered region and the rest of the image.\nThe discussed alteration of Fb\ni(f) so that it creates F\u2032b\ni(f) has the following\nequation:\n\u2200f|F\u2032b\ni(f)|=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3M(f)|Fb\ni(f)|+\n+[1\u2212M(f)]mif|Fb\ni(f)|> m\n|Fb\ni(f)| if|Fb\ni(f)| \u2264m(8)\nAn example of \ufb01ltering of Fb\ni(f) is shown in Fig. 9. The function F\u2032b\ni(f) is\ntransformed using reverse FFT into C\u2032b\ni(x), from which is extracted B\u2032b\ni(x) =\nC\u2032b\ni(x\u2212mC),x= 0... NB\u22121, to remove the padding introduced in (5). B\u2032b\ni(x)\nis thus a frequency\u2013\ufb01ltered Bb\ni(x), and is written back to the upsampled image,\nto the exact pixels from which Bb\ni(x) was constructed.\n5 Tests\nAn example image processed with the presented method is show n in Fig. 10(d).\nThe image was upsampled four times using bicubic interpolat ion that employed\nCatmull\u2013Rom spline [9]. The edge map of the image is shown in F ig. 10(b). As it\ncan be seen, the image with reduced aliasing is visually radi cally improved over\nthe image obtained using plain upscaling without the freque ncy \ufb01ltering, shown in\nFig. 10(c). The aliasing in Fig. 10(d) is almost reduced, wit hout any substantial\nblur, loss of small details or other distortions visible. It di\ufb00ers the presented\nmethod from that of an anisotropic smoothing [12] shown in Fi g. 11, which, while\nreducing the aliasing, distorts the image so that it looks ve ry unnatural and\n12blurred. For example, most of the details in the center of the petal in Fig. 11 are\nalmost lost.\n(a) (b)\n(c) (d)\nFigure 10: An example of \ufb01ltering a photograph: (a) original image, ( b) subpixel precision\nedges found, thickened in the illustration to make them better visible , (c) upsampled\nimage without the frequency \ufb01ltering, (d) upsampled image with the f requency \ufb01ltering.\nIt can be seen in the image, that the introduced method works w ell for vari-\nous non\u2013straight curves, even that it splits them into the ap proximately straight\nfragments before the frequency \ufb01ltering.\n6 Conclusion\nThe presented method can be applied to images upsampled usin g di\ufb00erent inter-\npolation method, and can radically reduce aliasing, with a v ery good preservation\nof the rest of the \ufb01ltered image. The method has the side e\ufb00ect o f producing a sub-\npixel precision edge map, that can be used in various edge pro cessing algorithms,\n13Figure 11: The image from Fig. 10(a) upsampled using a GREYC anisotr opic smoothing.\nlike the sharpening of edges in the upsampled image, for furt her improvement of\nits quality."}
{"category": "abstract", "text": ". Raster images can have a range of various distortions connected t o\ntheir raster structure. Upsampling them might in e\ufb00ect substantia lly yield the raster\nstructure of the original image, known as aliasing. The upsampling its elf may introduce\naliasing into the upsampled image as well. The presented method attem pts to remove the\naliasing using frequency \ufb01lters based on the discrete fast Fourier t ransform, and applied\ndirectionally in certain regions placed along the edges in the image.\nAs opposed to some anisotropic smoothing methods, the presente d algorithm aims to\nselectively reduce only the aliasing, preserving the sharpness of ima ge details.\nThe method can be used as a post\u2013processing \ufb01lter along with variou s upsampling\nalgorithms. It was experimentally shown that the method can improv e the visual quality\nof the upsampled images.\nkeywords"}
{"category": "non-abstract", "text": "Sparse representations, dimensionality reduction, spherical\nrepresentations, 3D face recognition.\n1. Introduction\nAutomatic recognition of human faces is an actively researched are a, which\n\ufb01nds numerous applications such as surveillance, automated scree ning, authen-\n\u2729This work has been partly supported by the Swiss National Sci ence Foundation, under\ngrants NCCR IM2 and 200020-120063.\nPreprint submitted to Elsevier October 25, 2018PREPROCESSING DIMENSIONALITY\nREDUCTION \n3D point cloud RECOGNITION \n( ) \u03d5 \u03b8,is icIdentity Spherical signal Feature vector \n[ ]. . . \nFigure 1: Block diagram of the 3D face recognition system.\nticationorhuman-computerinteraction. The faceisaneasilycollect ible, univer-\nsal and non-intrusive biometric [1], which makes it ideal for application s where\nother biometrics such as \ufb01ngerprints or iris scanning are not possib le.\nThere has been a considerable progress in the area of two-dimensio nal face\nrecognitionwhereintensity/colorimagesofhumanfacesareemploy ed. However,\nthese systems are sensitive to illumination, pose variations, occlusio ns, facial\nexpressions and make-up. On the other hand, recognition system s based on\n3D face information have the potential for greater recognition ac curacy and are\ncapable of overcoming part of the limitations of 2D face recognition s ystems\n[2, 3]. The 3D shape of a face, usually given as a 3D point cloud, depend s on\nits anatomical structure and it is independent of its pose, which can be further\ncorrected by rigid rotations in the 3D space [4].\nWe consider in this paper the problem of 3D face recognition and we de sign\na fully automatic algorithm based on simultaneous sparse expansions on the\nsphere. We \ufb01rst propose a preprocessing step that automatically registers the\n3Dpointcloudspriortodimensionalityreduction. Itselectsthefacia lregionand\nregisters all the faces by an accurate automatic two-step algorit hm based on an\nAverage Face Model (AFM) and on the Iterative Closest Point (ICP ) algorithm\n[4]. Contrarily to most of the existing algorithms, the proposed regis tration\nprocess does not require any manual intervention. Registered po int clouds are\nthen mapped on the 2D sphere where the spherical face functions are created\nby nearest neighbor interpolation. The spherical representation enables the\nuse of spherical signal processing techniques, which consider the face signals as\ncombinations of basis functions with diverse shape, position and orie ntation on\n2the sphere.\nThe spherical face signals then undergo a dimensionality reduction s tep that\nrepresents each face with a reduced set of discriminant features . We build a dic-\ntionary of functions on the sphere and we select the discriminant ba sis functions\nby simultaneous sparse approximations. The face signals are \ufb01nally p rojected\nonto the resulting reduced subspace, in order to generate featu re vectors. We\n\ufb01nally implement a recognition step where Linear Discriminant Analysis ( LDA)\nis performed on the subspace representation of the faces. The r ecognition sys-\ntem is illustrated on Fig. 1, where si(\u03b8,\u03c6) denotes the spherical signal sias a\nfunction of position ( \u03b8,\u03c6) on the 2D sphere, and ciis a feature vector.\nTheperformanceofthe3Dfacerecognitionsystemisevaluatedon theFRGC\nv.1.0 data set. The proposed algorithm outperforms state-of-th e-art solutions\nbasedonPrincipalComponentAnalysis(PCA,[5])orLinearDiscriminan tAnal-\nysis (LDA) on depth images. Our fully automatic system provides e\ufb00e ctive\nclassi\ufb01cation performance that shows that 3D face recognition wit h spherical\nrepresentations certainly represents a promising solution for per son identi\ufb01ca-\ntion.\nThe paper is organized as follows. We provide an overview of the relat ed\nworkin3DfacerecognitioninSectionII.SectionIIIdescribesthea utomaticface\nregistration process that permits to align the 3D points clouds befo re analysis.\nThe dimensionality reduction step with simultaneous sparse approxim ations on\nthe sphere is presented in Section IV and experimental results are \ufb01nally pro-\nvided in Section V.\n2. Related work\n3D face recognition has attracted a lot of research e\ufb00orts in the p ast few\ndecades due to the advent of new sensing technologies and the high potential\nof 3D methods for building robust systems with invariance to head po se and\nillumination variations. We review in this section the most relevant work in 3D\nface recognition, which can be categorized in methods using point clo ud rep-\n3resentations, depth images, facial surface features or spheric al representations\nrespectively. Surveys of the state-of-the-art in 3D face recog nition are further\nprovided in [2, 3].\nThe recognition methods that work directly on 3D point clouds consid er the\ndata in their original representation based on spatial and depth inf ormation. A\nprioriregistrationofthe point clouds is commonly performed by ICP algorithms\n[4, 6]. The classi\ufb01cation is generally based on the Hausdor\ufb00 distance t hat per-\nmits to measure the similarity between di\ufb00erent point clouds [7]. Alter natively,\nrecognition could be performed with \u201c3D eigenfaces\u201d that are cons tructed di-\nrectly from the 3D point clouds [8]. The main drawback of the recognit ion\nmethods based on 3D point clouds however resides in their high compu tational\ncomplexity that is driven by the large size of the data.\nMany recognition systems use depth or range images that permit to for-\nmulate the 3D face recognition as a problem of dimensionality reductio n for\nplanar images, where each pixel value represents the distance fro m the sensor\nto the facial surface. Principal Component Analysis (PCA) and \u201cEig enfaces\u201d\ncan be used for dimensionality reduction [9], where the basis vectors are how-\never typically holistic and of global support. PCA can be combined with Linear\nDiscriminant Analysis (LDA) to form \u201cFisherfaces\u201dwith enhanced cla ss separa-\nbility properties [10]. Alternatively, dimensionality reduction can be pe rformed\nvia variants of non-negative matrix factorization (NMF) algorithms [11, 12, 13]\nthat produce part-based decompositions of the depth images. Pa rt-based de-\ncompositionsbasedon non-negativesparsecoding [14] haverecen tlybeen shown\nto provide improved recognition performance than NMF methods in f ace recog-\nnition [15]. Recent methods have proposed to concentrate dimensio nality re-\nduction around facial landmarks like the nose tip [16] or in multiple car efully\nchosen regions [17] or to compute geodesic distances among the se lected \ufb01ducial\npoints [18]. They however require a selection of the \ufb01ducial points or areas of\ninterest that is often performed manually and prevents the impleme ntation of\nfully automatic systems.\nFacial surface features have also been proposed for 3D face rec ognition. The\n4idea of recognizing 3D faces using curvature descriptors has been originally in-\ntroduced in [19], where features are chosen to represent both cu rvature and\nmetric size properties of faces. More recently, level sets of the d epth function\non range image have been used to de\ufb01ne sets of facial curves [20]. They are fur-\nther embedded in an appropriately de\ufb01ned shape manifold and compa red based\non geodesic distances. Facial curve representations provide glob al information\nabout the whole facial surface, which unfortunately does not per mit to take\nadvantage of discriminative local features.\nFinally, spherical representations have been used recently for mo delling il-\nlumination variations [21, 22] or both illumination and pose variations in f ace\nimages [23]. Spherical representations permit to e\ufb03ciently represe nt facial sur-\nfaces and overcome the limitations of other methods towards occlu sions and\npartial views [24]. To the best of our knowledge, the representatio n of 3D face\npoint clouds as spherical signals for face recognitionhas howevern ot been inves-\ntigated yet. We therefore propose to take bene\ufb01t of the robust ness of spherical\nrepresentations and of spherical signal processing tools to build a n e\ufb00ective and\nautomatic 3D face recognition system. We perform dimensionality re duction\ndirectly on the sphere, so that the geometry of 3D faces is preser ved. The re-\nduced feature space is extracted by sparse approximations with a dictionary\nof localized geometric features on the sphere that e\ufb00ectively capt ure spatially\nlocalized and salient 3D face features that are advantageous in the recognition\nprocess.\n3. Automatic preprocessing of 3D face data\n3.1. Automatic face extraction\nWeproposeinthissectionafullyautomaticpreprocessingmethodfo rprepar-\ning and aligning 3D face point clouds before feature extraction and r ecognition.\nUnlike most of the algorithms in the literature, the preprocessing st ep does not\nrequire any manual intervention, which is an enormous advantage f or the de-\nsign of fully automated face recognition systems. The preprocess ing scheme is\n5(a) Binary matrix A(b) After lateral\nthresholding\n(c) Pro\ufb01le view (d) After depth\nthresholding (pro\ufb01le\nview)\n(e) After depth\nthresholding(f) After morpho-\nlogical processing\nFigure 2: Main steps in facial region extraction\nbased on two main tasks, respectively the extraction of the facial region, and\nthe registration of the 3D face. We present these tasks in more de tails in the\nrest of the section.\nThe main purpose of the face extraction step is to remove irrelevan t infor-\nmation from the 3D point clouds, such as data that correspond to s houlder, or\nhair for example. The output of a facial scan typically forms a 3D poin t cloud\n{X,Y,Z}, whereXandYform a uniform Euclidean grid and Zprovides the\ncorresponding depth values. The point cloud is also accompanied by a binary\nmatrixAof valid points, which has the same resolution as the grid implied by\nX\u00d7Y. The nonzero pattern of such a sample binary matrix is shown in Fig.\n2(a). There is however no guarantee that the points exclusively co rrespond to\n6face depth information, and face extraction is therefore necess ary to ensure that\nthe feature extraction concentrates on capturing discriminative facial informa-\ntion.\nThe \ufb01rst step in face extraction consists in removing data points on the\nsubject\u2019s shoulders. We estimate a vertical projection curve fro m the point\ncloud by computing the column sum of the matrix A. Then, we de\ufb01ne two\nlateral thresholds on the left and right in\ufb02exion points of the proje ction curve,\nand we remove all data points beyond these thresholds, as illustrat ed in Fig.\n2(b). We further remove the data points corresponding to the su bject\u2019s chest by\nthresholding of the histogram of depth values. It removes the dat a points with\nlarge depth values that are typically situated behind the data corre sponding\nto frontal face information, as shown in Figs 2(c) and 2(d). We \ufb01na lly have\nto remove outlier points that remain in regions disconnected from th e main\nfacial area, as shown in Fig. 2(e). We therefore perform morpholo gical image\nprocessingonthe correspondingbinarymatrix A, wherewekeeponlythelargest\nregion that typically correspond to the facial region, as presente d in 2(f).\n3.2. Automatic face registration\nAfter extracting the main facial region from the 3D scans, the fac e signals\nhave to be registered in order to ensure that all have the same pos e before the\nrecognition step. The registration typically applies rigid transforma tions on the\n3D faces in order to align them. We propose a two-step approach fo r automatic\nregistration, where an Average Face Model (AFM) is computed and then used\nfor accurate registration.\nFirst, we randomly pick a training face, and we align all the faces appr oxi-\nmately to the sample face using the Iterative Closest Point (ICP) alg orithm [4].\nGiven a model and a query point cloud, ICP computes a rigid transfor mation,\nconsisting of rotations and translations, by minimizing the sum of squ are errors\nbetween the closest model points and query points. After coarse registration\nwith ICP, the face signals are re-sampled on a uniform 2D grid using ne arest\nneighbor interpolation. It permits to construct an AFM, by comput ing at each\n7(a) Depth map\u22120.6\u22120.4\u22120.200.20.40.60.8\n\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.8\u22120.100.10.20.30.40.50.6\n(b) Point cloud\nFigure 3: Average Face Model given as a depth map or a 3D point c loud.\n(a) Before (depth\nmap)\u22120.5\u22120.4\u22120.3\u22120.2\u22120.100.10.20.30.40.5\n\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.80.050.10.150.20.250.30.350.40.45\n(b) Before (point\ncloud)\n(c) After (depth map)\u22120.5\u22120.4\u22120.3\u22120.2\u22120.100.10.20.30.40.5\n\u22120.8\u22120.6\u22120.4\u22120.200.20.40.600.10.20.30.40.5\n(d) After (point\ncloud)\nFigure 4: Illustration of ellipse cropping on depth maps and equivalent 3D point clouds.\ngrid point the average depth value among all training faces (see Figu re 3) . The\nAFM is subsequently used as reference in order to de\ufb01ne an ellipse th at contains\nthe main facial region. Since, the faces are already registered, th is ellipse can\nbe used to crop closely all faces in the training set. The ellipse croppin g step\nremoves all the irrelevant information that may be left over from th e previous\npreprocessing steps, as shown in Figure 4.\nA \ufb01ne alignment of the faces can now be performed on the signals tha t\nhave been cleaned from outliers. The accurate alignment is \ufb01nally obt ained by\nrunning ICP one more time. The AFM is now used as a reference face m odel,\nand all faces signals are registered with respect to the AFM.\n84. Recognition with sparse spherical representations\n4.1. Simultaneous sparse approximations\nE\ufb03cient face recognition algorithms usually include a dimensionality red uc-\ntion step, where high dimensional data are represented in a reduce d subspace.\nWe propose to use sparse signal representation methods for dime nsionality re-\nduction. Such methods have demonstrated good performance in 2 D face recog-\nnition [25]. They present the advantage of capturing the main signal charac-\nteristics in a very small set of meaningful features, which are more over de\ufb01ned\na priori in a dictionary of functions. This presents an interesting ad vantage\ncompared to classical methods such as PCA, whose feature vecto rs are data-\ndependent. In addition, a proper choice of the dictionary permits t o build\nfeatures that capture the geometrical information in the face sig nal. We give\nbelow a brief overview of sparse approximations, and we show later h ow we use\nthem for dimensionality reduction on the sphere.\nLet denote by si,i= 1,...,N, a set of functions in the Hilbert space H. Let\nfurther denote by D={g\u03b3,\u03b3\u2208\u0393}an overcomplete dictionary of unit L2norm\nfunctions indexed by \u03b3, which spans the space H. A function sihas a sparse\nrepresentation in Dif it can be represented in terms of a linear superposition of\nsmall set of basis functions {g\u03b3} \u2208 D. In other words, it can be expressed as\nsi= \u03a6Iici, where \u03a6 Iidenotes a matrix whose columns are atoms in DIi\u2282 D\nthat forms the sparse support of the signal si. The vector cirepresents the\ncoe\ufb03cients of the linear approximation of siwith atoms in DIi.\nFinding the sparsest representation of a signal in a redundant dict ionaryD\nis in general an NP-hard problem. Greedy algorithms like Matching Pur suit\n[26] have however shown to provide suboptimal yet e\ufb03cient solution s with a\nlimited computational complexity. It selects iteratively the function s from the\ndictionary that best matches the signals si. We have however to ensure that the\natoms that form the support of the di\ufb00erent signals si\u2019s are identical, in order to\npermit to classify them in the feature space. Dimensionality reductio n can thus\nbe performed by simultaneous decomposition of all the signals si,i= 1,...,N.\n9Finding the sparse support DIthat is common to all the signals {si}can be\nachieved by the Simultaneous MP (SMP) [27] algorithm, which only induc es a\nsmall increase of complexity compared to MP on a single signal [25]. In short,\nSMP greedily selects DIsuch that all the Nfunctionssiare simultaneously\napproximated in the same basis. It results in the extraction of Katoms such\nthat all signals are simultaneously represented by linear combination s of them.\nEach signal can be re-written as si= \u03a6Ici, where \u03a6 Idenotes the matrix whose\ncolumns are the atoms in the common sparse support DI\u2282 D. Finally, a\nfew iterations are typically su\ufb03cient to capture most of the energy of the face\nsignals to be approximated. It has been shown that residual error of the SMP\napproximationdecaysexponentiallyforcorrelatedsignalswiththe s amesupport\nand additive white noise [27].\n4.2. Spherical subspace selection with SMP\nWe propose to perform the classi\ufb01cation of 3D face by dimensionality re-\nduction on the sphere. We therefore project the 3D point cloud on to the unit\nsphereS2, and then we select a subspace that spans functions on S2. Since\nfaces are typically star-shaped objects, spherical projection p reserves the face\ngeometry information, while reducing the classi\ufb01cation complexity by map-\nping a 3D signal to a 2D spherical signal. Each face, given by a 3D point -\ncloud{pn}={(xn,yn,zn)}is, therefore, represented as a spherical function\nr=s(\u03b8,\u03d5) sampled at points {(rn,\u03b8n,\u03d5n)}, which are obtained by transform-\ning Euclidean coordinates from the point cloud to spherical coordina tes given\nby (\u03b8,\u03d5) that represent the elevation and azimuth angles.\nSince we represent 3D faces as square-integrable functions on S2, denoted as\nL2(S2), we can use the SMP to select a subspace of spherical basis funct ions as\na dimensionality reduction step. We use a spherical dictionary propo sed in [28],\nwhere the atoms are created by applying local geometric transfor ms to a gener-\nation function g(\u03b8,\u03d5) de\ufb01ned on the sphere. Local transforms include atom mo-\ntion (\u03c4,\u03bd) (position on the sphere with respect to ( \u03b8,\u03d5), respectively), rotation\n\u03c8, and anisotropic scaling by two scales ( \u03b1,\u03b2) in orthogonal directions. Motion\n10Figure 5: Gaussian atoms.\nand rotation are realized using a rotation in SO(3), which is the rotation group\ninR3. Five transform parameters form the atom index \u03b3= (\u03c4,\u03bd,\u03c8,\u03b1,\u03b2 )\u2208\u0393,\nand the redundant dictionary is \ufb01nally constructed by applying a larg e set of\ndi\ufb00erent\u03b3\u2019s tog. A detailed explanation of the dictionary construction is given\nin [28]. An example of the generating function is a 2-D Gaussian functio n in\nL2(S2), given by:\ng(\u03b8,\u03d5) = exp(\u2212tan2\u03b8\n2). (1)\nFunction in Eq.(1) represents an isotropic gaussian function, cent ered at the\nNorth Pole. In Figure 5 weshowa few sampleGaussianatoms that are obtained\nby applying di\ufb00erent local transforms to the generating function in Eq.(1).\nEquipped with the spherical dictionary, we can directly apply SMP to \ufb01 nd\nthe common support of the spherical faces, where the inner prod uct between\ntwo spherical functions f=f(\u03b8,\u03d5) andg=g(\u03b8,\u03d5) is however given by:\n/an}bracketle{tf,g/an}bracketri}ht=/integraldisplay\n\u03b8/integraldisplay\n\u03d5f(\u03b8,\u03d5)g(\u03b8,\u03d5)sin\u03b8d\u03b8d\u03d5. (2)\nIn the following, we refer to this special case of SMP for spherical s ignals\nusing the dictionary de\ufb01ned on the sphere, as simultaneous spherical matching\npursuit(SSMP).\n4.3. Recognition on the sphere\nThe algorithm for recognition of 3D faces on the sphere is \ufb01nally illustr ated\nin Figure 6. The \ufb01rst step performs dimensionality reduction, by pro jecting the\nspherical signals on the subspace spanned by the selected atoms i.e ., span{DI},\nas described above. If we denote the set of face signals by S= [s1,...,s n],\nthe SSMP performs the dimensionality reduction step by greedily sele cting a\n11DIM REDUCTION \nSSMP LDA MATCHING \n\u03a6( ) \u03d5\u03b8,isTest face \noptional \n\u03a6 , IC( ) \u03d5\u03b8,ts\nTraining faces Class label \nC~\nFigure 6: Block diagram of the recognition process.\nset ofKbasis vectors DI={g\u03b31,...,g \u03b3K}from the dictionary D, such that all\nspherical faces are simultaneously approximated as,\nS\u2248\u03a6I\u00b7C. (3)\nThe matrix C\u2208RK\u00d7nholds the coe\ufb03cient vectors (in its columns) and \u03a6 I=\n[g\u03b31,...,g \u03b3K].\nThe coe\ufb03cient vector conveys quite discriminative information abou t the\nfaces signals. However, the class separability of the coe\ufb03cient vec tors in the\nreduced space could yet be improved by performing an optional Line ar Dis-\ncriminant Analysis (LDA) step before matching. LDA exploits the clas s labels\ninformation of the training samples in order to enhance the discrimina nt prop-\nerties of the coe\ufb03cient vectors. It introduces supervision in the r ecognition\nprocess and permits to build a new set of coe\ufb03cient vectors \u02dcC=CWwhere\nthe weights Ware chosen to optimize the ratio of between-class variance and\nwithin-class variance for training data [10].\nFinally, the matching is performed by comparing the coe\ufb03cient vecto rsC,\nwhich represent the lower dimensional data samples. The recognitio n is per-\nformed by nearest neighbor classi\ufb01cation. We iteratively compute t he coe\ufb03-\ncientsctof the test face signal ston the sub-dictionary DI. The classi\ufb01cation\nis then performed by computing the L1distance between ctand any coe\ufb03cient\nvectorcicorresponding to the training signals\nd(ct,ci) =K/summationdisplay\nj=1|ct(j)\u2212ci(j)|. (4)\nThe class of the test signal is \ufb01nally given by the class of the signal sithat leads\n12Test iNumber of Training Test\ncon\ufb01guration subjects set set\nT1 1200 200 673\nT2 2166 332 474\nT3 3121 363 308\nT4 486 344 187\nTable 1: Test con\ufb01gurations and their characteristics.\nto the smallest distance d(ct,ci) between the coe\ufb03cients vectors. The same\nclassi\ufb01cation method is used for coe\ufb03cients \u02dcCmodi\ufb01ed by LDA. The choice of\ntheL1distance metric is mostly empiric as it leads to superior classi\ufb01cation\nperformance compared to other metrics.\n5. Experimental results\n5.1. Experimental setup\nIn this section, we evaluate the performance of the proposed algo rithms in\nboth recognition and veri\ufb01cation scenarios. We compare our algorit hms with\nPCA and LDA on depth images that have undergone the same prepro cessing\nstep as the data used in the SSMP algorithm. PCA and LDA are well kno wn\nmethods that represent state-of-the-art technologies for 3D recognition.\nFor our evaluation, we use the UND (University of Notre Dame) Biome tric\ndatabase [29, 30], also known as FRGC v.1.0 database. It contains 95 3 facial\nimages of 277 subjects, where each subject has between one and eight scans.\nEach facial scan is provided in the form of a 3D point-cloud, along with a\ncorresponding binary matrix of valid points. The number of vertices in a point-\ncloud typically varies between 30.000 and 40.000.\nWe de\ufb01ned several test con\ufb01gurations for our experimental eva luation. Each\ncon\ufb01guration is characterized by the number of samples per subje ct that form\nthe training set. For each con\ufb01guration Ti, we keep only the subjects from the\n13database that have at least i+ 1 samples, and we use itraining samples per\nclass (randomly chosen), while assigning the rest to the test set. T he subjects\nthat have only one facial scan can not be used in the recognition tes ts. Table 1\nsummarizes the test con\ufb01gurations and their main characteristics .\nSSMP implementation. For the dictionary construction in SSMP-based meth-\nods, we have used the 2D Gaussian on the sphere (1) as the genera ting function.\nThe atom indexes \u03b3that de\ufb01ne the dictionary, have to take discrete values in\npractice. We use here a discretization of the dictionary as in [28], mos tly built\non empirical choices for atom parameter values. The position param eters,\u03c4and\n\u03bdare uniformly distributed on the interval [0 ,\u03c0], and [\u2212\u03c0,\u03c0), respectively, with\nequal resolution of 128 points. The rotation parameter \u03c8is uniformly sampled\non the interval [ \u2212\u03c0,\u03c0), with the same resolution as \u03c4and\u03bd. This choice is\nmostly due to the use of fast computation of correlation on SO(3) f or the full\natom search within the SSMP algorithm. In particular, we used the Spharmon-\nicKitlibrary1, which is part of the YAW toolbox2. Finally, scaling parameters\nare distributed in a logarithmic manner, from 1 to half of the resolutio n of\u03c4\nand\u03bd, with a granularity of one third of octave. The largest atom covers half\nof the sphere.\nThe use of fast computation of correlation on the SO(3) group req uires the\nspherical data to be sampled on an equiangular ( \u03b8,\u03d5) grid, de\ufb01ned as:\nG={(\u03b8i,\u03d5j),\u03b8i=(2i+1)\u03c0\n2N\u03b8,and\u03d5j=j2\u03c0\nN\u03d5}. (5)\nwhere:i= 0,...,N\u03b8\u22121 andj= 0,...N\u03d5\u22121. Since 3D face point clouds are\nprojected as scattered data on the sphere, an interpolation ste p is necessary.\nFor its simplicity we use k-nearest neighbor interpolation, where the value on\neach spherical grid point ( \u03b8i,\u03d5j) is computed as an average of its knearest\nneighbors. We have used k= 4 and a resolution of N\u03b8= 128,N\u03d5= 128.\nNote \ufb01nally that, for the sake of computational ease, dimensionalit y reduction\n1http://www.cs.dartmouth.edu/ ~geelong/sphere/\n2http://fyma.fyma.ucl.ac.be/projects/yawtb/\n14with SSMP is performed o\ufb00-line, using only one training face per subje ct. The\nresulting subspace is then used for projecting both training and te st samples.\nVirtual faces. The size of the training set is important in determining the clas-\nsi\ufb01cation performance. We propose to enrich the training set with virtual faces\n(see e.g., [31] and references therein). These are faces that are arti\ufb01cially gen-\nerated by slight variations of the original training faces. They are g iven the\ncorresponding class labels of the training face they originate from, and they\nare treated as training samples. The use of virtual faces is motivat ed by two\nmain reasons: (i) they compensate for small registration errors ( recall that our\nregistration process is fully automatic and it is expected to contain a few reg-\nistration errors) and (ii) by augmenting the training set, they may c ontribute\nto the performance of sample-based methods (e.g., LDA) that can bene\ufb01t from\nlarge sample sets. Note that the virtual faces do not introduce an y new infor-\nmation to the training set, since they are synthetically generated b y the original\ntraining faces. For computational convenience, we construct th em by one or\ntwo pixel translations in the spherical domain. Note \ufb01nally that virtu al faces\nare used only in the SSMP+LDA method.\n5.2. Recognition results\nWe present recognition results of our methods and we compare the m with\nPCA and LDA on depth images. For the sake of completeness, we also report\nthe classi\ufb01cation performances of the Euclidean distance (EUC) be tween depth\nimages, and Mean Square Error (MSE) between spherical function s. For the\ntwo latter methods, each test face is recognized as the closest ne ighbor in the\ntraining set. In SMMP+LDA (resp. PCA+LDA), the number of dimens ions\nused in LDA is set to the minimum between the number of features in SS MP\n(resp. PCA) and c\u22121, wherecis the number of classes (subjects). Virtual\nfaces are used in the SSMP+LDA method in con\ufb01gurations T1,T2andT3only,\nsince they correspond to small training sets. In these cases, eac h training face\nis used to generate 8 virtual faces.\n150 50 100 150 2000102030405060708090100\nNumber of basis vectorsClassification Error Rate (%)\n  \nEUC\nPCA\nMSE\nSSMP\nSSMP + LDA\n(a) Test Con\ufb01guration T10 50 100 150 2000102030405060708090100\nNumber of basis vectorsClassification Error Rate (%)\n  \nEUC\nPCA\nLDA\nMSE\nSSMP\nSSMP + LDA\n(b) Test Con\ufb01guration T2\n0 50 100 150 2000102030405060708090100\nNumber of basis vectorsClassification Error Rate (%)\n  \nEUC\nPCA\nLDA\nMSE\nSSMP\nSSMP + LDA\n(c) Test Con\ufb01guration T30 50 100 150 2000102030405060708090100\nNumber of basis vectorsClassification Error Rate (%)\n  \nEUC\nPCA\nLDA\nMSE\nSSMP\nSSMP + LDA\n(d) Test Con\ufb01guration T4\nFigure 7: Rank-1 recognition results: average classi\ufb01cati on error rate versus the dimension of\nthe subspace.\nWe start with rank-1 recognition, which refers to the scenario whe re a class\nprediction is considered to be a hit when the label of the closest neigh bor is\nthe correct one. Then, we will discuss the generic rank- kscenario, where the\nprediction is a hit when the correct label is included in the labels of the c losest\nkneighbors.\nRank-1 recognition. All tests are performed 10 times, by splitting randomly the\nsamples into the training and the test sets. Figure 7 shows the class i\ufb01cation\nerror rate for all con\ufb01gurations, averaged over the 10 random e xperiments. No-\ntice the remarkable improvement introduced by the employment of s pherical\nfunctions for facial representation. This is evident from the fact that the recog-\nnition performance of nearest neighbor classi\ufb01cation with Mean Squ are Error\n16T1T2T3T4\nPCA 45,1760,9774,3582,89\nPCA + LDA -74,8980,5293,58\nSSMP 62,8577,2287,0194,12\nSSMP + LDA 67,6194,7398,70 100\nTable 2: Best rank-1 recognition rates (%) reached by each me thod in experiment 5.2.\n0 20 40 60 80 100405060708090100\nRankRecognition rate (%)\n  \nPCA\nSSMP\nSSMP + LDA\n(a) Test Con\ufb01guration T10 20 40 60 80 100405060708090100\nRankRecognition rate (%)\n  \nPCA\nLDA\nSSMP\nSSMP + LDA\n(b) Test Con\ufb01guration T2\nFigure 8: Rank- krecognition results in terms of CMC curves.\n(MSE) between spherical signals, outperforms that of Euclidean d istances be-\ntween depth images (EUC). This provides also the main motivation for working\non the sphere. Based on this observation, it seems reasonabletha t our SSMP al-\ngorithmoutperformsPCAin allcon\ufb01gurations. Notice\ufb01nally thatSS MP+LDA\nis the best performer. In T2, SSMP reaches recognition performa nce of 77,22%,\nwhile SSMP+LDA reaches 94 ,73%. The latter goes to the maximum 100% in\nT4, even in the absence of virtual faces. Table 2 shows the highest recognition\nrates achieved by each method in all con\ufb01gurations.\nRank-krecognition. We report rank- krecognition performances in terms of cu-\nmulative match characteristic (CMC) curves. A CMC curve simply illust rates\nthe \ufb02uctuation of the recognition rate versus the rank k. Figure 8 shows the\nobtained CMC curves for T1andT2that represent the most interesting cases,\n17since T3 and T4 correspond to very good performances for all met hods. The\nCMC curves in this \ufb01gure are averages over 10 random tests, wher e the best\nnumber of dimensions for each algorithm is used (obtained from the p revious\nrank-1 recognition experiments). As expected, notice again that SSMP is su-\nperior to PCA, and LDA introduces in both methods a signi\ufb01cant perf ormance\nboost.\n5.3. Veri\ufb01cation results\n0 0.2 0.4 0.6 0.8 100.20.40.60.81\nFalse Positive RateTrue Positive Rate\n  \nSSMP\nSSMP+LDA\nPCA\n(a) Test Con\ufb01guration T10 0.2 0.4 0.6 0.8 100.20.40.60.81\nFalse Positive RateTrue Positive Rate\n  \nSSMP + LDA\nSSMP\nPCA\nLDA\n(b) Test Con\ufb01guration T2\n0 0.2 0.4 0.6 0.8 100.20.40.60.81\nFalse Positive RateTrue Positive Rate\n  \nSSMP\nSSMP+LDA\nPCA\nLDA\n(c) Test Con\ufb01guration T30 0.2 0.4 0.6 0.8 100.20.40.60.81\nFalse Positive RateTrue Positive Rate\n  \nSSMP\nSSMP+LDA\nPCA\nLDA\n(d) Test Con\ufb01guration T4\nFigure 9: Veri\ufb01cation performance in terms of ROC curves.\nWecomparenowalltheabovemethodsintheveri\ufb01cationscenario,w herethe\ntest subject claims an identity and the system has to either accept or reject this\nclaim. If the identity is the correct one, then the test subject is ca lled aclient;\notherwise, it is called an impostor . In systems that output a con\ufb01dence score\n18about the test subject, a hard decision (i.e., accept or reject) is t ypically reached\naccordingto a thresholdvalue. We reportthe veri\ufb01cationperform ancesin terms\nof receiver operating characteristic (ROC) curves, which show th e \ufb02uctuation\nof the true positive rate (TPR) versus the false positive rate (FPR ) across all\nvalues of the threshold. For the computation of the ROC curve we c onsider\nevery possible pair of subject and claimed identity.\nIn our experimental setup, we use the dimensions that yields the be st perfor-\nmance, which corresponds to 200 atoms in SSMP and 100 dimensions in PCA.\nThe number of LDA dimensions in both SSMP+LDA and PCA+LDA is set\nwith the same rule as in the recognition experiments (i.e., using the minim um\nbetween the number of PCA/SSMP features and c\u22121). Also, in SSMP+LDA\nwe use virtual faces only for con\ufb01gurations T1 and T2. Figure 9 sho ws the\naverage ROC curves over 10 random experiments for all con\ufb01gura tions. Similar\nconclusionscanbe drawnhere aswell. Unsurprisingly, observeagain that SSMP\nconsistently outperforms PCA in all con\ufb01gurations and SSMP+LDA is the best\nperformer.\n5.4. Discussion\nIt is worth noting that supervised versions of SSMP could be also use d\n[25]. The idea would be then to select the atoms from the dictionary ac cording\nto discriminative criteria. However, in the proposed scheme the sup ervision\ninformation is already taken into account in the LDA postprocessing step, and\nprior experience has shown that this su\ufb03ces, when prede\ufb01ned dict ionaries are\nused.\nNote also that the importance of each region of the face in terms of recogni-\ntion performance is certainly not uniform [17]. Although the selection of such\nregions is typically performed manually and it maybe sensitive to the te sting\nconditions, one possible approachto take advantage of this obser vation could be\nto group the features selected by SSMP into regions by clustering o n the sphere,\ndo a classi\ufb01cation per region and then fuse the results (e.g., by majo rity voting).\nSuch an approach however requires a su\ufb03cient number of atoms in e ach area,\n19and the performance of such a region-based classi\ufb01er has not bee n convincing.\nNote \ufb01nally that the proposed dimensionality reduction scheme is gen eric\nand simple extensions could be proposed to make the classi\ufb01cation mo re sen-\nsitive to some speci\ufb01c areas. For example, the SSMP scheme can eas ily be\nadapted to give priorities to regions of high interest such as the nos e or the\neyes. Such a prioritization can be achieved by giving proper weights t o atoms\nlocated in di\ufb00erent areas, in order to force the dimensionality reduc tion step\nto select features in areas that are expected to be more discrimina tive. This\nhowever goes along the lines of supervised versions of SSMP mention ed above\nwith the main di\ufb00erence that discriminative capability in this case is most ly\nde\ufb01ned in a region-based way.\n6. Conclusions\nWe have proposed a methodology for 3D face recognition based on s pherical\nsparse representations. First, we introduced a fully automatic pr ocess for ex-\ntraction, preprocessing and registrationof facial information in 3 D point clouds.\nNext, weproposedtoconvertfacesfrompoint cloudstospherica lsignals. Sparse\nspherical representation of faces allows for e\ufb00ective dimensionalit y reduction\nthroughsimultaneoussparseapproximations. Thedimensionalityre ductionstep\npreserves the geometry information, which in turn leads to high per formance\nmatching in the reduced space. We provide ample experimental evide nce that\nindicates the advantagesofthe proposedapproachoverstate- of-the-artmethods\nworking on depth images.\n7. Acknowledgements\nThe authors would like to thank Prof. Patrick Flynn for sharing with u s the\nUND Biometrics database.\n20"}
{"category": "abstract", "text": "This paper addresses the problem of 3D face recognition using simult aneous\nsparse approximations on the sphere. The 3D face point clouds are \ufb01rst aligned\nwith a novel and fully automated registration process. They are th en repre-\nsented as signals on the 2D sphere in order to preserve depth and g eometry\ninformation. Next, we implement a dimensionality reduction process w ith si-\nmultaneous sparse approximations and subspace projection. It p ermits to rep-\nresent each 3D face by only a few spherical functions that are able to capture\nthe salient facial characteristics, and hence to preserve the disc riminant facial\ninformation. We eventually perform recognition by e\ufb00ective matchin g in the\nreduced space, where Linear Discriminant Analysis can be further a ctivated for\nimproved recognition performance. The 3D face recognition algorit hm is eval-\nuated on the FRGC v.1.0 data set, where it is shown to outperform cla ssical\nstate-of-the-art solutions that work with depth images.\nKey words"}
{"category": "non-abstract", "text": "Given a 3D object in 3-\ndimensional Euclidean space R3, determine homology\ngroups of the object in the most effective way by only\nanalyzingthedigitizationoftheobject.\nThe properties of homology groups have applica-\ntions in many areas of bioinformatics and image pro-\ncessing [9]. We particularly look at a set of points in\n3D digital space, and our purpose is to \ufb01nd homology\ngroupsofthedataset.\nMany researchers have made signi\ufb01cant contribu-\ntions in this area. For R3, based on simplicial decom-\nposition, Dey and Guha have developed an algorithm\nfor computing the homology group with generators in\nO(n2\u00b7g),wheregisthemaximumgenusamongalldis-\nconnected boundary surfaces. This algorithm has been\nimprovedby Damiand et al [4]. They used the bound-\nary information to simplify the process. However, the\noveralltime complexityremainedthe same.\nIn 2D, both R2and the cubical complex of linear\nalgorithms(whichissimilartothatofdigitalspaces)are\nfound to calculate Betti numbers, which are essentially\nthe sameasthegenus[5]and[8].\nOther algorithms for homology groups in cubical\nspaces are studies in 2D, these algorithms are either\nO(nlog2n)in[13] or O(nlog3n)in [8]. Ingeneral,the\nhomology group can only be computed in O(n3)time\nfor the cubical complex in [8]. More about computa-\ntionalhomologyisdiscussedin [9].\nInthispaper,wediscussthegeometricandalgebraic\npropertiesofmanifoldsin3Ddigitalspacesandtheop-\ntimal algorithms for calculating these properties. We\nconsiderdigital manifolds as de\ufb01ned in [3]. More in-\nformation related to digital geometry and topology can\nbe foundin [11]and [12].\nInthispaper,weintroduceanoptimalalgorithmwith\ntimecomplexity O(n)tocomputegenusandhomology\ngroups in 3D digital space, where nis the size of the\ninput data. In Section 2, we review some properties of\ndigital surfaces and manifolds [3]. Based on the clas-\nsical Gauss-Bonnet Theorem, we calculate the genus\nof a digital closed surface in 3D. Section 3 covers thenecessary background in 3-manifold topology. Using\nAlexander duality, we relate homology groups of a 3D\nobjecttoits2-dimensionalboundaries. InSection4,we\npresentouralgorithmforhomologygroups.\n2 Gauss-Bonnet Theorem and Closed\nDigitalSurfaces\nCubical space with direct adjacency, or (6,26)-\nconnectivityspace,hasthesimplesttopologyin3Ddig-\nital spaces. It is also believed to be suf\ufb01cient for the\ntopologicalpropertyextractionof digitalobjectsin 3D.\nTwopointsaresaidtobeadjacentin(6,26)-connectivity\nspace if theEuclideandistanceofthese two pointsis1,\ni.e.,directadjacency.\nLetMbe a closed (orientable)digital surface in the\n3D grid space in direct adjacency. We know that there\nareexactly6-typesofdigitalsurfacepoints[3][2].\nFigure 1. Six types of digital surfaces\npointsin 3D\nAssume that Mi(M3,M4,M5,M6) is the set of\ndigital points with ineighbors. We have the following\nresultforasimplyconnected M[3]:\n|M3|= 8+|M5|+2|M6|. (1)\nM4andM6hastwo differenttypes,respectively.\nThe Gauss-Bonnet theorem states that if Mis a\nclosedmanifold,then\n/integraldisplay\nMKGdA= 2\u03c0\u03c7(M) (2)\nwheredAisanelementofareaand KGistheGaussian\ncurvature.\nItsdiscreteformis\n\u03a3{pisapointin M}K(p) = 2\u03c0\u00b7(2\u22122g)(3)\nwheregisthegenusof M.Assume that Kiis the curvature of elements in Mi,\ni=3,4,5,6. We have\nLemma 2.1 (a)K3=\u03c0/2, (b)K4= 0, forbothtypes\nofdigitalsurfacepoints,(c) K5=\u2212\u03c0/2,and(d)K6=\n\u2212\u03c0, forbothtypesof digitalsurfacepoints.\nProof. We can see that K4is always 0 since\nKG=K1\u00d7K2and one of the principal curva-\ntures must be 0. We know that there exists a simply\nclosed surface that contains only 8 points of M3\nand several points within M4. Based on (1) and (3),\n8\u00b7K3= 2\u03c0\u00b7(2\u22120). Therefore, K3=\u03c0/2. There\nis a simply closed surface that only contains M3,M4,\nandM5. So,K3=\u2212K5. Samefor K6=\u22122\u00b7K3. So,\nK6=\u22122\u03c0/2 =\u2212\u03c0.\nLemma 2.1 can also be calculated by the discrete\nGaussian Curvature Theorem [16]. The curvature of\nthe centerpointofthepolyhedraisdeterminedby\n/integraldisplay\nMKGdA= 2\u03c0\u2212\u03a3i\u03b8i. (4)\nGauss-BonnetTheoremwill holdbasedon(4).\nTherefore,wecanobtainthesameresultsasLemma\n2.1. For example, in 3D digital space, the angle of one\nface is\u03c0/2. So,K5= 2\u03c0\u22125\u00b7\u03c0/2 =\u2212\u03c0/2. (The\nproof of Lemma 2.1 is necessary if we are admitting\nthe Gauss-Bonnet Theorem\ufb01rst and then obtaining the\ncurvature for each surface point. We include a proof\nhere since the reference [16] does not contain such a\nproof.)\nGiven a closed 2D manifold, we can calculate the\ngenusgby counting the number of points in M3,M5,\nandM6. Accordingto(3),we have\n\u03a36\ni=3Ki\u00b7|Mi|= 2\u03c0\u00b7(2\u22122g),\n\u03c0/2\u00b7|M3|\u2212\u03c0/2\u00b7|M5|\u2212\u03c0\u00b7|M6|= 2\u03c0\u00b7(2\u22122g),\n|M3|\u2212|M5|\u22122|M6|= 4\u03c0\u00b7(2\u22122g).\nTherefore,\ng= 1+(|M5|+2\u00b7|M6|\u2212|M3|)/8.(5)\nLemma 2.2 There is an algorithm that can calculate\nthe genusof Minlineartime.\nProof.Scan through all points (vertices) in Mand\ncount the neighbors of each point. We can see that a\npoint inMhas 4 neighbors indicating that it is in M4\nas areM5andM6. Put points to each category of Mi.\nThenuseformula(5)to calculatethegenus g.The two following examples show that the formula\n(5) is correct. The \ufb01rst example shown in Fig. 2, is\nthe easiest case. In Fig.2 (a), there are 8 points in M3\nand no points in M5orM6. (To avoid the con\ufb02ict be-\ntween a closest digital surface and a 3-cell [3], we can\ninsertsome M4pointsonthesurfacebutnotatthecen-\nter point.) According to (5), g= 0. Extend Fig. 2 (a)\nto a genus1 surface as shown in Fig. 2 (b) where there\nare still 8 M3points but 8 M5points. Thus, Fig. 2 (b)\nsatis\ufb01es equation (5). We can extend it to Fig. 2 (c), it\nhas16M5pointsand8 M3points. So g= 2. Usingthe\nsamemethod,onecaninsertmorehandles.\nThe second example came from the Alexander\nhorned sphere. See Fig. 3. First we show a \u201cU\u201d shape\nbase in Fig. 3(a). It is easy to see that there are 12\nM3points and 4 M5points. So g= 0according to\nEquation (5). Then we attach a handle to Fig. 3(a)\nshown in Fig. 3(b). We have added 4 M3points and\n12M5points.g= 1+(|M5|+2\u00b7|M6|\u2212|M3|)/8 =\n1+(4+12 \u221212\u22124)= 1. Finally,weaddanotherhan-\ndle to the other side of the \u201cU\u201d shape in Fig. 3(a), the\ngenusnumberincreasesbyonesince we still add4 M3\npoints and 12 M5points shown in Fig. 3(c). g= 2for\n(c). For morecomplexcaseslike the Alexanderhorned\nsphere, one just needs to insert two smaller handles to\nan existing handle, so the genus will increase accord-\ningly.\nFigure 2. Simple examples of closed sur-\nfaceswith g= 0,1,2\nThe above idea can be extended to simplicial cells\n(triangulation)or even general CW k-cells. This is be-\ncause, for a closed discrete surface, we can calculate\nGaussian curvature at each vertex point using formula\n(4). (The key is to calculate all angles separated by 1-\ncells at a vertex) Then use (3) to obtain the genus g.\nSince each line-cell (1-cell) is involved in exactly two\n2-cells,itisonlyassociatedwithfourangles. Therefore\nFigure 3. An example came from Alexan-\nderhorned spherein digitalspace\nthe total complexity will be O(|E|)whereEis the set\nof1-cells(edges). Thus,\nLemma 2.3 There is an algorithm that can calculate\nthegenusofaclosedsimplicialsurfacein O(|E|)where\nEthe set of1-cells(edges).\nThereareexamplesthat |E|isnotlineartothenum-\nberofvertices |V|.\n3 Homology Groups of Manifolds in 3D\nDigitalSpace\nHomologygroupsareotherinvariantsintopological\nclassi\ufb01cation. For a k-manifold, Homologygroup Hi,\ni= 0,...,kindicates the number of holes in each i-\nskeleton of the manifold. Once we obtain the genus of\na closed surface, we can then calculate the homology\ngroupscorrespondingtoits 3-dimensionalmanifold.\nConsider a compact 3-dimensional manifold in R3\nwhose boundary is represented by a surface. We show\nits homology groups can be expressed in terms of its\nboundary surface (Theorem 3.4). This result follows\nfrom standard results in algebraic topology. Since it\ndoes not seem to be explicitly stated or proved in any\nstandard reference, we include a self-contained proof\nhere[7]. Thisresultfollowsfromstandardresultsinal-\ngebraic topology. It also appears in [6] in a somewhat\ndifferent form. For the convenience of readers, we in-\ncludea shortself-containedproofhere.\nFirst,werecallsomestandardconceptsandresultsin\ntopology. Given a topological space M, its homologygroups,Hi(M), are certain measures of i-dimensional\n\u201dholes\u201d in M. For example, if Mis the solid torus,\nits \ufb01rst homology group H1(M)\u223c=Z, generated by\nitslongitudewhichgoesaroundtheobvioushole. Fora\nprecisede\ufb01nition,see,e.g. [7]. Let bi=rankHi(M,Z)\nbe theith Betti number of M. The Euler characteristic\nofMisde\ufb01nedby\n\u03c7(M) =/summationdisplay\ni\u22650(\u22121)ibi\nIfMis a 3-dimensional manifold, Hi(M) = 0for\nalli >3essentiallybecausethereareno i-dimensional\nholes. Therefore, \u03c7(M) =bo\u2212b1+b2\u2212b3. Further-\nmore, ifMis inR3, it must have nonempty boundary.\nThisimpliesthat b3= 0.\nThefollowinglemmaiswellknownfor3-manifolds.\nIt holds, with the same proof, for any odd dimensional\nmanifolds.\nLemma 3.1 LetMbeacompactorientable3-manifold\n(whichmay ormaynotbein R3).\n(a)IfMis closed(i.e. \u2202M=\u2205),then\u03c7(M) = 0.\n(b)Ingeneral, \u03c7(M) =1\n2\u03c7(\u2202M).\nProof.(a) If\u2202M=\u2205,the result follows from the\nPoincare duality which says that Hi(M)\u223c=H3\u2212i(M),\nand the Universal Coef\ufb01cient Theoremwhich says that\nthe freepartof H3\u2212i(M)is isomorphicto the freepart\nofH3\u2212i(M). Together, they imply that bi=b3\u2212i.\nHence\u03c7(M) =b0\u2212b1+b2\u2212b3= 0.\n(b) In general, we consider the doubleofM\ndenoted by DM, which is obtained by gluing two\ncopies of Malong\u2202Mvia the identity map. By\nwhat we just proved, \u03c7(DM) = 0. On the other\nhand, the Euler characteristic satis\ufb01es a nice additive\nproperty: \u03c7(DM) =\u03c7(M) +\u03c7(M)\u2212\u03c7(\u2202M) =\n2\u03c7(M)\u2212\u03c7(\u2202M). Thisimplies \u03c7(M) =1\n2\u03c7(\u2202M).\nNext,we recalltheAlexanderduality.\nProposition3.2 LetX\u2282Snbe a compact, lo-\ncally contractible subspace of SnwhereSnis then-\ndimensionalsphere. Then\n\u02dcHi(Sn\u2212X)\u223c=\u02dcHn\u2212i\u22121(X)for alliwhere\u02dcHis the\nreducedhomology.\nWe remarkthat Snisthe onepointcompacti\ufb01cation\nofRn. Therefore, a submanifold in Rnis automati-\ncally considered as a submanifold in Snin a natural\nway. Conversely, a submanifold MinSnis automati-\ncallya submanifoldin SnunlessM=Sn.\nBefore we prove Theorem 3.4, we \ufb01rst prove a spe-\ncial casewhen \u2202Misconnected.Lemma 3.3 LetSbeaclosedconnectedsurfacein S3.\n(a)Itscomplement, S3\u2212S,musthaveexactlytwocon-\nnectedcomponents. We denotethemby MandM\u2032.\n(b)H1(M)\u223c=H1(M\u2032)\u223c=Z1\n2b1(S),H2(M)\u223c=\nH2(M\u2032) = 0.\nProof.(a) By the Alexander duality, \u02dcH0(S3\u2212S)\u223c=\nH2(S)\u223c=Z. Therefore S3\u2212Smust have exactly two\ncomponents.\n(b) Again, by the Alexander duality,\nH2(S3\u2212S)\u223c=\u02dcH0(S) = 0 , and this im-\npliesH2(M)\u223c=H2(M\u2032) = 0. We also have\nH1(M)\u2295H1(M\u2032)\u223c=H1(M\u2294M\u2032) =H1(S3\u2212S)\u223c=\nH1(S)\u223c=Zb1(S). In particular, there is no torsion in\nH1(M)orH1(M\u2032). ByLemma3.1, 1\u2212rankH1(M) =\n1\u2212rankH1(M\u2032) =1\n2\u03c7(S) =1\n2(2\u2212b1(S)). This\nimpliesH1(M)\u223c=H1(M)\u223c=Z1\n2b1(S)\nNow we consider a general compact connected 3-\nmanifold MinS3. Its boundary, \u2202M, is a closed ori-\nentable 2-dimensional manifold possibly with several\ncomponents.\nTheorem 3.4 LetMbe a compact connected 3-\nmanifoldin S3. Then\n(a)H0(M)\u223c=Z.\n(b)H1(M)\u223c=Z1\n2b1(\u2202M), i.e.H1(M)is torsion-free\nwithrankbeinghalfof rank H1(\u2202M).\n(c)H2(M)\u223c=Zn\u22121wherenisthenumberofcompo-\nnentsof\u2202M.\n(d)H3(M) = 0unlessM=S3.\nProof.Statements (a) and (d) are obvious: (a) follows\nfromconnectednessof M,(d)isduetothefactthat \u2202M\nis non-empty. Next,we prove(c)and(d).\nLetS1,\u00b7\u00b7\u00b7,Snbetheconnectedcomponentsof \u2202M.\nByLemma3.3,each Siseparates S3intotwoconnected\ncomponents, MiandM\u2032\ni. SinceMisconnected,itmust\nbe entirely contained in either MiorM\u2032\ni. LetM\u2032\nibe\nthe one containing M. It follows that S3\u2212 \u222aiSi=\nM\u2294\u2294iMi.\nBy the Alexander duality, H2(M\u2294 \u2294iMi) =\nH2(S3\u2212\u222aiSi)\u223c=\u02dcH0(\u222aiSi)\u223c=Zn\u22121. ButH2(Mi) =\n0for each i(Lemma 3.3). Therefore H2(M)\u223c=\nH2(M\u2294\u2294iMi)\u223c=Zn\u22121.\nNext, also by the Alexander duality, H1(M\u2294\n\u2294iMi) =H1(S3\u2212\u222aiSi)\u223c=H1(\u222aiSi) =H1(\u2202M)\u223c=\nZb1(\u2202M). But LHS = H1(M)\u2295 \u2295iH1(Mi) =\nH1(M)\u2295 \u2295iZ1\n2b1(Si)\u223c=H1(M)\u2295Z1\n2b1(\u2202M). It\nfollowsthat H1(M)\u223c=Z1\n2b1(\u2202M).4 A Linear Algorithm of \ufb01nding Homol-\nogy Groups in3D\nBased on the results we presented in Sections 2 and\n3,wenowdescribealinearalgorithmforcomputingthe\nhomologygroupof3Dobjectsin3D digitalspace.\nAssuming we only have a set of points in 3D. We\ncan digitize this set into 3D digital spaces. There are\ntwo ways of doing so: (1) by treating each point as a\ncube-unit that is called the raster space, (2) by treating\neach point as a grid point. It is also called the point\nspace. These two are dual spaces. Using the algorithm\ndescribed in [3], we can determine whether the dig-\nitized set forms a 3D manifold in 3D space in direct\nadjacency for connectivity. The algorithm is in linear\ntime.\nThe more detailed considerations of recognition\nalgorithms related to 3D manifolds can be found\nin [1] where Brimkov and Klette made extensive\ninvestigationsinboundarytracking. The discussionsof\n3Dobjectsin rasterspacecanbefoundin [14].\nAlgorithm4.1 Let usassume thatwe havea connected\nMthat isa3D digitalmanifoldin3D.\nStep 1.Track the boundary of M,\u2202M, which is a\nunion of several closed surfaces. This algorithm\nonly needs to scan though all the points in Mto\nsee if the point is linked to a point outside of M.\nThatpointwill beonboundary.\nStep 2.Calculate the genus of each closed surface in\n\u2202Musing the method described in Section 2. We\njust need to count the number of neighbors on a\nsurface. andputthemin Mi,usingtheformula(5)\ntoobtain g.\nStep 3.Using the Theorem 3.4, we can get H0,H1,\nH2, andH3.H0isZ. ForH1, we need to get\nb1(\u2202M)that is just the summation of the genus\nin all connected componentsin \u2202M. (See [7] and\n[6].)H2is the numberof componentsin \u2202M.H3\nistrivial.\nLemma 4.1 Algorithm4.1isa lineartimealgorithm.\nProof.Step 1 uses linear time. We can \ufb01rst track\nall points in the object using breadth-\ufb01rst-search. We\nassume that the points in the object are marked as \u201c1\u201d\nand the others are marked as \u201c0.\u201d Then, we test if a\npoint in the object is adjacent to both \u201c0\u201d and \u201c1\u201d by\nusing 26-adjacency for linking to \u201c0.\u201d Such a point is\ncalled a boundary point. It takes linear time because\nthe total numberof adjacent pointsis only 26. Anotheralgorithmistotest ifeachlinecellontheboundaryhas\nexactly two parallel moves on the boundary [3]. This\nprocedureonlytakeslineartimeforthetotalnumberof\nboundarypointsinmostcases.\nStep2 isalsoin lineartimebyLemma2.2.\nStep 3 is just a simple math calculation. For H0,\nH2, andH3, they can be computed in constant time.\nForH1,thecountingprocessisat mostlinear.\nTherefore, we can use linear time algorithms to cal-\nculategand all homology groupsfor digital manifolds\nin 3DbasedonLemma2.2andLemma4.1.\nTheorem 4.2 There is a linear time algorithm to cal-\nculate all homology groups for each type of manifolds\nin 3D.\nTo some extent, researchers are also interested in\nspace complexity that is regarded to running space\nneeded beyond the input data. Our algorithms do not\nneed to store the past information, the algorithms pre-\nsented inthis paperare always O(logn). Here,lognis\nthe bitsneededto representanumber n.\nAcknowledgement. The authors would like to thank\nProfessor Allen Hatcher for getting the authors con-\nnected which led to the result of this paper. The sec-\nond author is partially supported by NSF grant DMS-\n051391."}
{"category": "abstract", "text": "In this paper, we design linear time algorithms to\nrecognizeanddeterminetopologicalinvariantssuchas\nthegenusandhomologygroupsin3D.Theseproperties\ncan be used to identify patterns in 3D image recogni-\ntion. This has many and is expected to have more ap-\nplications in 3D medical image analysis. Our method\nis based on cubical images with direct adjacency, also\ncalled (6,26)-connectivityimages in discrete geometry.\nAccordingto the fact thatthere are onlysix typesof lo-\ncal surface points in 3D and a discrete version of the\nwell-known Gauss-Bonnett Theorem in differential ge-\nometry, we \ufb01rst determine the genus of a closed 2D-\nconnected component (a closed digital surface). Then,\nweuseAlexanderdualitytoobtainthehomologygroups\nofa3Dobjectin3Dspace. Thisideacanbeextendedto\ngeneral simplicial decomposed manifolds or cell com-\nplexesin3D.\n1. Introduction\nIn recent years, there have been a great deal of new\ndevelopments in applying topological tools to image\nanalysis. Inparticular,computingtopologicalinvariant s\nhasbeenofgreatimportanceinunderstandingtheshape\nof an arbitrary 2-dimensionall (2D) or 3-dimensional\n(3D) object [8]. The most powerful invariant of these\nobjects is the fundamental group [7]. Unfortunately,\nfundamental groups are highly non-commutative and\ntherefore dif\ufb01cult to work with. In fact, the general\nproblem in determining whether two given groups are\nisomorphicis undecidable(meaningthat there is no al-\ngorithm can solve the problem) [15]. For fundamen-\ntal groups of 3D objects, this problem is decidable but\nno practical algorithm has been found yet. As a result,\nhomology groups have received the most attention be-\ncausetheircomputationsaremorefeasibleandtheystill\nprovide signi\ufb01cant information about the shape of the\nobject [6] [9] [4]. This leads to the motivating prob-lem addressed in this paper"}
{"category": "non-abstract", "text": "image information content discovery and image information \ncontent interpretation. Despite of its widespread use, the notion of \u201cimage \ninformation content\u201d is still ill defined, intuitive, and ambiguous. Most often, it is used in the Shannon\u2019s sense, which means information content assessment \naveraged over the whole signal ensemble. Humans, however, rarely resort to \nsuch estimates. They are very effective in decomposing images into their meaningful constituents and focusing attention to the perceptually relevant image \nparts. We posit that following the latest findings in human attention vision \nstudies and the concepts of Kolmogorov\u2019s complexity theory an unorthodox segmentation approach can be proposed that provides effective image \ndecomposition to information preserving image fragments well suited for \nsubsequent image interpretation. We  provide some illustrative examples, \ndemonstrating effectiveness of this approach. \n1   Introduction \nMeaningful image segmentation is an issue of paramount importance for image \nanalysis and processing tasks. Natural and effortless for human beings, it is still an unattainable challenge for computer vision designers. Usually, it is approached as an interaction of two inversely directed subt asks. One is an unsupervised, bottom-up \nevolving process of initial image information discovery and localization.  The other is a supervised, top-down propagating process, which conveys the rules and the knowledge that guide the linking and grouping of the preliminary information features into more large aggregations and sets. It is generally believed that at some higher level of the processing hierarchy this interplay culminates with the required scene decomposition (segmentation) into its meaningful constituents (objects), which then can be used for further scene analysis and interpretation (recognition) purposes. \nIt is also generally believed that this way of processing mimics biological vision \npeculiarities, especially the characteristics of the Human Visual System (HVS). Treisman\u2019s Feature Integrating Theory [1], Biederman\u2019s Recognition-by-components theory [2], and Marr\u2019s theory of early visual information processing [3] are well known 18 E. Diamant \n milestones of biological vision studies that for years influenced and shaped computer \nvision development. Although biological studies since then have seriously improved and purified their understanding of HVS properties [4], these novelties still have not find their way to modern computer vision developments. \n2   Inherited Initial Misconceptions \nThe input front-end of a visual system has always been acknowledged as the most critical system\u2019s part. That is true for th e biological systems and for the artificial \nsystems as well. However, to cope with input information inundation the systems have developed very different strategies. Biological systems, in course of their natural evolution, have embraced the mechanism of Selective Attention Vision, which allows sequential part-by-part scene information gathering. Constantly moving the gaze from one scene location to another, the brain drives the eye\u2019s fovea (the eye\u2019s high-resolution sensory part) to capture the necessary information. As a result, a clear and explicit scene representation is built up and is kept in the observer\u2019s mind. (Every one, relying on his personal experience, will readily confirm this self-evident truth.) \nHuman-made visual systems, unfortunately, have never had such ability. Attempts \n(in robotic vision) to design sensors with log-polar placing of sensory elements (imitating fovea) that are attached to a steerable camera-head (imitating attentional \nfocusing) have permanently failed. The only reasonable solution, which has survived and became the mainstream standard, was to place the photosensitive elements uniformly over the sensor\u2019s surface, covering the largest possible field of view of an imaging device. Although initially overlooked, the consequences of this move were dramatic: The bottom-up principle of input information gathering, which prescribes that every pixel in the input image must be visited and processed (normally referencing its nearest neighbors) at the very beginning, imposes an enormous system computational burden. To cope with it, unique image-processing-dedicated Digital \nSignal Processors (DSPs) were designed and put in duty. The latest advertised prodigy \u2013 the Analog Devices TigerSHARC \u2013 is able to provide 3,6 GFLOPS of computing power. However, despite of that, to meet real-life requirements, the use of a PCI Mezzanine Card (a BittWare product) featuring four TigerSHARCs on a single board is urgently advised. As well, applications where up to four such cards, performing simultaneously, are envisioned and afford ed (delivering approximately 57 GFLOPS \nper cluster.) \nIt is worth to be mentioned here that the necessity of the DSPs usage was perfectly \nclear even when the \u201cstandard\u201d image size has not exceeded 262K pixels (512x512 sensor-array). Today, as 3 \u20135 Megapixel arrays have became the de facto commercial standard, 16 Megapixel arrays are mastered by professionals, and 30 (up to 80) Megapixel arrays are common in military, space, medical and other quality-demanding applications, what DSP clusters arrangement  would meet their bottom-up processing \nrequirements?   Paving the Way for Image Understanding 19 \n The search for an answer always returns to  biological vision mysteries. Indeed, the \nattentional vision studies have never been so widespread and extensive as in the last 5-10 years. The dynamics of eye saccadic movement is quite well understood now. As well, the rules of attention focus guidance. At the same time, various types of perceptual blindness have been unveiled and investigated. The latest research reports \nconvincingly evidence: The hypothesis that our brain image is entire, explicit and clear \u2013 (the principal justification for the bottom-up processing) \u2013 is simply not true. It is just an illusion [5]. \nIt will be interesting to note that despite of these impressive findings, contemporary \ncomputational models of attentional vision  (and their computer vision counterparts) keep on to follow the bottom-up information gathering principle [6]. Once upon a time, as well as we are considered, someone had tr ied to warn about the trap of such an \napproach [7], but who was ready to hear? Today, a revision of the established canon is inevitable. \n3   The Revised Se gmentation Approach \nConsidering the results of the latest selective attention vision studies and juxtaposing \nthem with the insights of Kolmogorov Complexity theory, which we adopt to explain the empirical biological findings, we have recently proposed a new paradigm of introductory image processing [8]. For the clarity of our discussion, we will briefly repeat some of its key points. \nTaking into account the definitions of Kolmogorov\u2019s Complexity, we formulate the \nproblem of image information content discovery and extraction as follows: \n\u2022 Image information content is a set of descriptions of the observable image data \nstructures. \n\u2022 These descriptions are executable, that is, following them the meaningful part \nof image content can be faithfully reconstructed. \n\u2022 These descriptions are hierarchical an d recursive, that is, starting with a \ngeneralized and simplified description of image structure they proceed in a \ntop-down fashion to more and more fine information details resolved at the lower description levels. \n\u2022 Although the lower bound of description details is unattainable, that does not \npose a problem because information content comprehension is generally fine details devoid. \nAn image processing strategy that can be drawn from these rules is depicted in Fig.1. \nAs one can see, the proposed schema is comprised of three main processing paths: the bottom-up processing path, the top-down processing path and a stack where the discovered information content (the generated descriptions of it) are actually accumulated. \nAs it follows from the schema, the input image is initially squeezed to a small size of \napproximately 100 pixels. The rules of this shrinking operation are very simple and fast: four non-overlapping neighbour pixels in an image at level L are averaged and the \nresult is assigned to a pixel in a higher ( L+1)-level image. This is known as \u201cfour 20 E. Diamant \n children to one parent relationship\u201d. Then, at the top of the shrinking pyramid, the \nimage is segmented, and each segmented region is labeled. Since the image size at the top is significantly reduced and since in the course of the bottom-up image squeezing a severe data averaging is attained, the im age segmentation/classification procedure does \nnot demand special computational resources. Any well-known segmentation methodology will suffice. We use our own proprietary technique that is based on a low-level (local) information content evaluation, but this is not obligatory. \nLast (top) levelBottom-up path Top-down path Object list\nSegmentation\nClassificationObject shapes\nLabeled objects Top level object descriptors4 to 1 comprsd\nimage\n4 to 1 compressed\nimage1 to 4 expanded\nobject  mapsLevel  n-1\nLevel  1\nLevel 0Level  n-1 objects\nLevl 1 obj.4 to 1 compressed\nimage1 to 4 expanded\nobject  maps\n1 to 4 expanded\nobject mapsOriginal imageL 0.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .       \n \nFig. 1.  The Schema of the proposed approach \nFrom this point on, the top-down processing path is commenced. At each level, the \ntwo previously defined maps (average region intensity map and the associated label map) are expanded to the size of an image at the nearest lower level. Since the regions at different hierarchical levels do not exhib it significant changes in their characteristic \nintensity, the majority of newly assigned pixels are determined in a sufficiently correct manner. Only pixels at region borders and seeds of newly emerging regions may significantly deviate from the assigned values. Taking the corresponding current-level image as a reference (the left-side unsegmented image), these pixels can be easily detected and subjected to a refinement cycle. In such a manner, the process is subsequently repeated at all descending le vels until the segmentation/classification of \nthe original input image is successfully accomplished. \nAt every processing level, every image object-region (just recovered or an inherited \none) is registered in the objects\u2019 appearance list, which is the third constituting part of the proposed scheme. The registered object parameters are the available simplified object\u2019s attributes, such as size, center-of-mass position, average object intensity and hierarchical and topological relationship within and between the objects (\u201csub-part of\u2026\u201d, \u201cat the left of\u2026\u201d, etc.). They are sparse, general, and yet specific enough to capture the object\u2019s characteristic features in a variety of descriptive forms.  Paving the Way for Image Understanding 21 \n 4   Illustrative Example \nTo illustrate the qualities of the proposed approach we have chosen a scene from the \nPhoto-Gallery of the Natural Resources Conservation Service, USA Department of Agriculture, [9]. \n \nFig. 2.  Original image, size 1052x750 pixels \n \nFig. 4.  Level 4 segmnt., 25 object-regions \n \nFig. 6.  Level 2 segmnt., 132 object-regions\nFig. 3.  Level 5 segmnt., 14 object-regions \nFig. 5.  Level 3 segmnt., 44 object-regions \nFig. 7. Level 1 segmnt., 234 object-regions  22 E. Diamant \n Figure 2 represents the original image, Figures 3 \u2013 7 illustrate segmentation results at \nvarious levels of the processing hierarchy. Level 5 (Fig. 3) is the topmost nearest level (For this size of the image the algorithm creat es a 6-level hierarchy). Level 1 (Fig. 7) is \nthe lower-end closest level. For space saving, we do not provide all exemplars of the segmentation succession, but for readers\u2019 convenience all presented examples are expanded to the size of the original image. \nExtracted from the object list, numbers of distinguished (segmented) at each \ncorresponding level regions (objects) are given in each figure capture. \nBecause real object decomposition is not known in advance, only the generalized \nintensity maps are presented here. But it is clear that even such simplified representations are sufficient to grasp the imag e concept. It is easy (for the user) now to \ndefine what region combination depicts the target object most faithfully. \n5   Paving the Way for Image Understanding  \nIt is clear that the proposed segmentation scheme does not produce a meaningful human-like segmentation. But it does produce the most reasonable decomposition of visually distinguishable image components, which now can be used as building blocks for an appropriate component grouping and binding procedure. Actually, image understanding arises from the correct arrangement and the right mutual coupling of the elementary information pieces gathered in the initial processing phase. The rules and the knowledge needed to execute this procedur e are definitely not a part of an image. \nThey are not an image property. They are always external, and they exist only in the head of the user, in the head of a human observer. Therefore, widespread attempts to learn them from the image stuff (automatically, unsupervised, or by supervised training) is simply a dominant misunderstanding. Nevertheless, numerous learning techniques have been devised and put in duty, including the most sophisticated biology-inspired Neural Networks. However, the trap of low-level information gathering had once again defeated the people\u2019s genuine attempts. By definition, neural network tenets assume unsupervised statistical learning, while human learning is predominantly supervised and declarative, that means, essentially natural language based and natural language supported. \nWhat is, then, the right way to introduce to system\u2019s disposal the necessary human\u2019s \nknowledge and human\u2019s reasoning rules? Such a question immediately involves a subsequent challenge: how this knowledge can be or must be expressed and represented?  We think that the answer is only one: as well as human\u2019s understanding relies on his world ontology, a task-specific and task-constrained ontology must be provided to system\u2019s disposal to facilitate meaningful image processing [10]. It must be human-created and domain-constrained. Th at means manually created by a human \nexpert and bearing only task-specific and task-relevant knowledge about image parts \nconcepts, their relations and interactions. \nIt must be specifically mentioned that these vexed questions are not only the fortune \nof those who are interested in image understanding issues. A huge research and development enterprise is going on now in the domain of the Semantic Web  Paving the Way for Image Understanding 23 \n development [11]. And the unfolding of our own ideas is directly inspired by what is \ngoing on in the Semantic Web race.   \nOur proposed segmentation technique pretty well delineates visually discernable \nimage parts. Essentially, the output hierarchy of segment descriptions by itself can be perceived as a form of a particular ontology, implemented in a specific description language. Therefore, to successfully accomplish the goal of knowledge incorporation, the system designer must also provide the mapping rules between these two ontologies (the mapping also has to be manually created). Because we do not intend to solve the general problem of knowledge transfer to a thinking machine, because we are always aimed on a specific and definite task, it seems that the burden of manual ontology design and its subsequent mapping can be easily carried out. If it is needed, a set of multiple ontologies can be created and cross-mapped, reflecting real life multiplicity of world to a task interaction. At least, such we hope, the things would evolve, when we shall turn to a practical realization of this idea. \n6   Conclusions \nIn this paper, we have presented a new technique for unsupervised top-down-directed image segmentation, which is suitable for image understanding and content recognition applications. Contrary to traditional approaches, which rely on a bottom-up (resource exhaustive) processing and on a top-down mediating (which requires early external knowledge incorporation), our approach exploits a top-down-only processing strategy (via a hierarchy of simplified image representations). That means, considerable computational load shrinking can be attained. Especially important is its indifference to \nany user or task-related assumptions, its unsupervised fashion. The level of segmentation details is determined only by structures discernable in the original image data (the information content of an image, nothing other). \nIt must be mentioned explicitly: information content description standards like \nMPEG-4 and MPEG-7, which are fully relying on the concept of a recovered object, left the topic of object segmentation without the scope of the standards (for the reason of irresolvable problem\u2019s complexity). As far as we are concerned, that is the first time when a technique is proposed that autonomously yields a reasonable image decomposition (to its constituent objects), accompanied by concise object descriptions that are sufficient for reverse object reconstruction with different levels of details. Moreover, at the final image interpretation stage the system can handle entire objects, and not (as usually) pixels, from which they (obviously) are composed."}
{"category": "abstract", "text": ".  In this paper we present an unconventional image segmentation \napproach which is devised to meet the requirements of image understanding and \npattern recognition tasks. Generally image understanding assumes interplay of two sub-processes"}
{"category": "non-abstract", "text": "a differential essential matrix is determined from the optical \ufb02ow, leading to\na unique camera velocity estimation. Another well-known ap proach is based on motion\nparallax, notably developped by Tomasi and Shi in [12], Lawn and Cipolla in [13] and\nIrani et al. in [15]. Tomasi et al. propose in [14] a compariso n of algorithms which only\nuseoptical\ufb02owforestimatingcameramotion.\nFinally, direct methods use directly the content of a couple of images. They are gen-\nerallybasedontheconstraintofconstantillumination(al so calledoptical\ufb02owconstraint),\nthat is minimized by a least square approach, on the paramete rs of a given motion model.\nDifferentassumptionsareusedtoavoidestimatingdepthso nallpoints;forexample,Horn\nandWeldonin[16]andBergenetal.,in[17],assumethatthed epthmapislocallyconstant.\nIn[18],NegahdaripourandHornconsiderthatit isplanaror quadratic.\nLet us notice that featurescorrespondences-basedtechniq ueswork best with well sep-\narated views, when the displacement (especially the transl ation or the so-called baseline)\nbetweenframesissuf\ufb01cientlylarge. Onthecontrary,optic al\ufb02owmethodsanddirectmeth-\nods,basedonin\ufb01nitesimalapproximations,are well-adapt edtoverysmall motions.\nOur method deals with adjacent frames of a sequence, so with n arrow baselines and\nrestrictedcamerarotations. Itisadirectmethod,veryfas t androbust,basedonaquadratic\napproximationofimagedeformation.\nThe outline of the paper is as follows. In Section 2, we descri be our framework. We\nrecall the image deformation generated by camera motion. Th en, we show that we can\nassume in the deformation formula that depth of projected po ints is constant (in camera\ncoordinatesystem) underfollowingcondition: the product of the norm of translation with\nthe maximal variation of inverse depth has to be suf\ufb01ciently small. Thus, two consecutive\nimages are linked by a planar transformation. In this contex t, we introduce in Section 3\nthe registration group, used for modeling image deformatio n generated by a camera dis-\nplacement. We also propose a new camera motion decompositio n, that separates image\ndeformation in a \u201cpurely\u201d projective deformation, due to ch ange of optical axis direction,\nand a similarity. As camera displacement is restricted, we o btain a quadratic approxima-\ntion of optical \ufb02ow between two adjacent frames. This approx imation is used in Section\n4 to de\ufb01ne an algorithmofmotionestimation; we show estimat ion resultson syntheticse-\nquencesand use motion estimations on real video sequencesf or mosaicingand simpli\ufb01ed\naugmentedreality. ConcludingremarksaregiveninSection 5.\n2 Framework\n2.1 Pinholecamera model\nAcameraprojectsapointin3Dspaceona2Dimage. Thistransf ormationcanbedescribed\nusing the well-known pinhole camera model [7] presented in \ufb01 gure 1. The camera is lo-\ncated onC, the optical center, and directed by k, the optical axis. The camera projects a\npointMof the 3D space on the plane R:{Z=fc}. The plane Ris called the retinal\nplaneandfcthefocallength. Theprojection mofMisthentheintersectionoftheoptical\nray(CM)withR.\nLetcbe the intersection of the optical axis with R. If(X,Y,Z)are the coordinates\nofMin the camera coordinate system (C,i,j,k)and(x,y)the coordinates of min the\northogonalbasis (c,i,j), therelationshipbetween (x,y)and(X,Y,Z)isfollowing\n\uf8f1\n\uf8f2\n\uf8f3x=fcX\nZ\ny=fcY\nZ.\nAsfcjust acts as a scaling factor on the image, we choose in this pa per, without loss of\n2generality, to set the focal length to one. Then, fcwill be the unit of camera and image\ncoordinatesystems.\nPSfragreplacements\nCc\nijkfcm(x,y)M(X,Y,Z)\nR\nFigure1: Pinholecameramodel.\n2.2 Camera motion\nLetDbe a displacement of the camera or in an equivalent way a displ acement of the\nplaneR. The movement Dmay be written in a unique way as D= (R,t), whereRis a\nrotation with axis containing Candta translation. The set of displacements D= (R,t)\nformstheLiegroupofrigidtransformationsin R3calledSE(3),whichdenotesthespecial\nEuclidian group. The displacement D= (R,t)transforms a point Mbelonging to R3\ninM\u2032=RM+t. Thus, the camera is identi\ufb01ed before the displacement by (C,i,j,k)\nand after the displacementby (C\u2032,R(i),R(j),R(k)), withCC\u2032=t. In the following,we\ndenote\nR=\uf8eb\n\uf8eda1b1c1\na2b2c2\na3b3c3\uf8f6\n\uf8f8andt=\uf8eb\n\uf8edt1\nt2\nt3\uf8f6\n\uf8f8.\nLetnowfandgbe twoadjacentimagesina sequencede\ufb01nedonrectangulardo mains\nKofRandK\u2032ofR\u2032(withfc= 1). LetMbeapointin R3suchthatitsprojections mand\nm\u2032onRandR\u2032belongtoKandK\u2032. We denote m= (x,y)in(c,i,j)andm\u2032= (x\u2032,y\u2032)\nin(c\u2032,R(i),R(j)). Thus,if wemaketheassumptionofconstantillumination,w ehave\nf(x,y) =g(x\u2032,y\u2032),\nandthetwopointsarelinkedby\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032=a1x+a2y+a3\u2212/an}bracketle{tt\nZ(x,y),R(i)/an}bracketri}ht\nc1x+c2y+c3\u2212/an}bracketle{tt\nZ(x,y),R(k)/an}bracketri}ht\ny\u2032=b1x+b2y+b3\u2212/an}bracketle{tt\nZ(x,y),R(j)/an}bracketri}ht\nc1x+c2y+c3\u2212/an}bracketle{tt\nZ(x,y),R(k)/an}bracketri}ht(1)\nand \uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x=a1x\u2032+b1y\u2032+c1+t1\nZ\u2032(x\u2032,y\u2032)\na3x\u2032+b3y\u2032+c3+t3\nZ\u2032(x\u2032,y\u2032)\ny=a2x\u2032+b2y\u2032+c2+t2\nZ\u2032(x\u2032,y\u2032)\na3x\u2032+b3y\u2032+c3+t3\nZ\u2032(x\u2032,y\u2032),(2)\nwhereZ(x,y)andZ\u2032(x\u2032,y\u2032)arethedepthsof Mrespectivelyin (C,i,j,k)and(C\u2032,R(i),\nR(j),R(k)).\n32.3 Depths approximationby a constant\nWe nowwish to approximatethe depthsbya constantin the two f ormulas(1) and(2). Let\nZ0belongto R\u2217\n+. By a Taylorexpansionofequation(1)on1\nZ(x,y)about1\nZ0,we obtain\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032=a1x+a2y+a3\u2212/an}bracketle{tt\nZ0,R(i)/an}bracketri}ht\nc1x+c2y+c3\u2212/an}bracketle{tt\nZ0,R(k)/an}bracketri}ht+\n/parenleftBig\n1\nZ(x,y)\u22121\nZ0/parenrightBig/parenleftBigg\n\u2212/an}bracketle{tt,R(i)/an}bracketri}ht+/an}bracketle{tt,R(k)/an}bracketri}hta1x+a2y+a3\u201c\nc1x+c2y+c3\u2212/angbracketleftt\nZ0,R(k)/angbracketright\u201d2/parenrightBigg\n+o/parenleftBig\n1\nZ(x,y)\u22121\nZ0/parenrightBig\ny\u2032=b1x+b2y+b3\u2212/an}bracketle{tt\nZ0,R(j)/an}bracketri}ht\nc1x+c2y+c3\u2212/an}bracketle{tt\nZ0,R(k)/an}bracketri}ht+\n/parenleftBig\n1\nZ(x,y)\u22121\nZ0/parenrightBig/parenleftBigg\n\u2212/an}bracketle{tt,R(j)/an}bracketri}ht+/an}bracketle{tt,R(k)/an}bracketri}htb1x+b2y+b3\u201c\nc1x+c2y+c3\u2212/angbracketleftt\nZ0,R(k)/angbracketright\u201d2/parenrightBigg\n+o/parenleftBig\n1\nZ(x,y)\u22121\nZ0/parenrightBig\n.\nThus, if for all (x,y)\u2208K,/parenleftBig\n1\nZ(x,y)\u22121\nZ0/parenrightBig\n/bardblt/bardblis small enough with respect to the image\ncoordinates,we cansubstitute Z0inplaceofZ(x,y).\nWe now make some numerical and technical assumptions that ar e little restrictive and\nso arelikelyveri\ufb01edbyacoupleofconsecutiveimages.\nHypothesis 1 \u2013LetD= (R,t)\u2208SE(3)andKbe the rectangular domain where fis\nde\ufb01ned. Let Zbe thedepthfunctionofprojectedpoints,de\ufb01nedon K. We assumethat\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1\nc1x+c2y+c3\u2212/an}bracketle{tt\nZ(x,y),R(k)/an}bracketri}ht/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\u22644\n3.\nHypothesis 2 \u2013LetD= (R,t)\u2208SE(3)andKbe the rectangular domain where f\nis de\ufb01ned, having maximal dimension L. LetZbe the depth function of projected points,\nde\ufb01nedonK. Fortwomatchingpoints (x,y)and(x\u2032,y\u2032)(inthesenseofformulas(1)and\n(2)),we supposethat\nmax{|x\u2032\u2212x|,|y\u2032\u2212y|} \u2264L\n2.\nThe \ufb01rst hypothesis comes from the fact that the variation of optical axis direction\nand its translation along the axis k, between two consecutiveacquisitions, have to be very\nsmall so that images were workable. The second one formulate s the limitation of points\ndisplacements between two images; we assume that the two com ponents of optical \ufb02ow\ncannotbelargerthanthehalfofimagelargerdimension.\nWiththesetwo assumptions,weshowin AppendixA thefollowi ngtheorem.\nTheorem 1 \u2013LetD= (R,t)\u2208SE(3)andKbe the rectangular domain where fis\nde\ufb01ned,andhavingmaximaldimension L. LetZbethedepthfunctionofprojectedpoints,\nde\ufb01nedonK,boundedby Zinf>0andZsup. We assume that ZandDverifyhypothesis\n1and2. If/parenleftbigg1\nZinf\u22121\nZsup/parenrightbigg\n/bardblt/bardbl2(L+1)\n3\u2264\u03b5 (3)\nthenthere exists Z0>0so thatwe canreplace Z(x,y)byZ0inthe equations(1) with an\nerrorboundedby \u03b5.\n4Thevalueof Z0that minimizes \u03b5is\n/hatwideZ0=argmin\nZ0max\n(x,y)\u2208K/vextendsingle/vextendsingle/vextendsingle1\nZ(x,y)\u22121\nZ0/vextendsingle/vextendsingle/vextendsingle=2ZsupZinf\nZsup+Zinf.\nWecanalsoshowthatwecansubstitutethesame Z0inplaceofZ\u2032(x\u2032,y\u2032)inequations(2)\nwithanerrorboundedby \u03b5+\u03b5\u2032if\n4\n9Zinf/bardblt/bardbl(L+1)<\u03b5\u2032. (4)\nForsmallvaluesof \u03b5and\u03b5\u2032, conditions(3)and(4)canbeveri\ufb01edinthefollowingcases :\n\u2022ifthereisnotranslation,depthsdonotappearinformulas( 1) and(2),\n\u2022ift/ne}ationslash= 0, the scene must be far enough from the camera for verifying co ndition (4).\nThevariationsofamplitudeof 1/Zmustalsobesmallenoughforverifyingcondition\n(3): the further the scene takes place from the camera, the bi ggerare the authorized\nvariationsofdepth.\nWiththisframework,relations(1)and(2)between fandgbecome\nf(x,y) =g/parenleftBigg\na1x+a2y+a3\u2212/an}bracketle{t/tildewidet,R(i)/an}bracketri}ht\nc1x+c2y+c3\u2212/an}bracketle{t/tildewidet,R(k)/an}bracketri}ht,b1x+b2y+b3\u2212/an}bracketle{t/tildewidet,R(j)/an}bracketri}ht\nc1x+c2y+c3\u2212/an}bracketle{t/tildewidet,R(k)/an}bracketri}ht/parenrightBigg\n=g\u25e6\u03c8(x,y)\nand\ng(x\u2032,y\u2032) =f/parenleftBigg\na1x\u2032+b1y\u2032+c1+/tildewidet1\na3x\u2032+b3y\u2032+c3+/tildewidet3,a2x\u2032+b2y\u2032+c2+/tildewidet2\na3x\u2032+b3y\u2032+c3+/tildewidet3/parenrightBigg\n=f\u25e6\u03d5(x\u2032,y\u2032),\nwhere/tildewidet=t\nZ0. In the sequel of the paper, we will assume that conditions (3 ) and (4) are\nveri\ufb01ed: we will use applications \u03d5and\u03c8as the relations between fandg. As we will\nconsidertwoconsecutiveimagesina sequence,thetranslat iontisverysmall.\n3 Modelisation\nWe nowconsidertwo consecutiveimages fandgin asequence,obtainedbeforeandafter\na cameramotion D= (R,t).\n3.1 Registrationgroup\nTheapplications \u03d5and\u03c8areprojectiveapplications,eachde\ufb01nedbysixparameters ,three\nfor the rotation and three for the translation. Projective a pplications are classically repre-\nsented in the projective group in R2. This group is isomorphic to the special linear group\nSL(R3)ofinvertiblematrices. Thus,theapplications \u03d5and\u03c8areassociatedtothefollow-\ninginvertiblematrices M\u03d5andM\u03c8\nM\u03d5=\uf8eb\n\uf8eda1b1c1+/tildewidet1\na2b2c2+/tildewidet2\na3b3c3+/tildewidet3\uf8f6\n\uf8f8=R\uf8eb\n\uf8ed1 0 /an}bracketle{t/tildewidet,R(i)/an}bracketri}ht\n0 1 /an}bracketle{t/tildewidet,R(j)/an}bracketri}ht\n0 0 1+ /an}bracketle{t/tildewidet,R(k)/an}bracketri}ht\uf8f6\n\uf8f8=RH (5)\nand\nM\u03c8=\uf8eb\n\uf8eda1a2a3\u2212/an}bracketle{t/tildewidet,R(i)/an}bracketri}ht\nb1b2b3\u2212/an}bracketle{t/tildewidet,R(j)/an}bracketri}ht\nc1c2c3\u2212/an}bracketle{t/tildewidet,R(k)/an}bracketri}ht\uf8f6\n\uf8f8=R\u22121\uf8eb\n\uf8ed1 0\u2212/tildewidet1\n0 1\u2212/tildewidet2\n0 0 1\u2212/tildewidet3\uf8f6\n\uf8f8=R\u22121/tildewideH.\n5Our aim is to estimate cameramotion throughimage deformati on,each de\ufb01nedby six\nparameters. But the projective group is an eight parameters group and the matrix decom-\nposition shows that M\u22121\n\u03d5/ne}ationslash=M\u03c8inSL(R3). Thus we are going to model the projective\ntransformationinanothergroup,well-adapted: theregist rationgroup,introducedbyDibos\nin[19].\nDe\ufb01nition1 \u2013LetAbethesubset ofprojectiveapplications\nA=/braceleftBig\n\u03c6:R2\u2192R2sothat\u2200(x,y)\u2208R2,\n\u03c6(x,y) =/parenleftbigga1x+b1y+c1+\u03b1\na3x+b3y+c3+\u03b3,a2x+b2y+c2+\u03b2\na3x+b3y+c3+\u03b3/parenrightbigg\n,\nwhereR=\uf8eb\n\uf8eda1b1c1\na2b2c2\na3b3c3\uf8f6\n\uf8f8\u2208SO(3)and(\u03b1,\u03b2,\u03b3)\u2208R3/bracerightBig\n.\nThe registration groupis (A,\u22c6), where the compositionlaw \u22c6is deducedfrom the compo-\nsitionlaw \u25e6ofSE(3)throughtheisomorphism\nI:A \u2212\u2192SE(3)\n\u2200\u03c6\u2208 A I(\u03c6) = (R,t)\nwhereRistherotationde\ufb01nedaboveand t= (\u03b1,\u03b2,\u03b3)isthetranslation.\nMoreprecisely,let \u03c61and\u03c62belongto A,theycorrespondtothedisplacements D1=\n(R1,t1)andD2= (R2,t2), respectively. Then, \u03c61\u22c6\u03c62=\u03c6where\u03c6is the projective\napplicationassociatedtothedisplacement D=D1\u25e6D2= (R,t)wheretisthetranslation\nwith vectort=t1+R1t2andR=R1R2. The notation D1\u25e6D2means that the camera\n\ufb01rst performs the displacement D1and second D2. Moreover, if \u03c6belongs to Aand is\nassociatedto D= (R,t), then\u03c6\u22121isassociatedto D\u22121= (R\u22121,\u2212R\u22121t).\nThe applications \u03d5and\u03c8belong to A; we haveg(x,y) =f(\u03d5(x,y))andf(x,y) =\ng(\u03c8(x,y))with\u03c8=\u03d5\u22121intheregistrationgroup(butnotin theprojectivegroup).\nBymodelingthecameradisplacementintheregistrationgro up,wereducetheproblem\nto the determination of six parameters of a planar applicati on, asRandtare respectively\nde\ufb01nedbythreeparameters.\n3.2 Camera motiondecomposition\nWeproposeheretodecomposeacameramotioninordertosepar atetheimagedeformation\nin two components: a similarity part and a \u201cpurely\u201d projecti ve part. Indeed, any camera\nmotioncanbedecomposedintothreebasictypesofmotion:\n\u2022a translation, which producesan homothetytranslationon t he imagefbelongingto\ntheplane R,\n\u2022arotationwith axis k,whichproducesaplanarrotationon f,\n\u2022arotationwith axisinthe plane (C,i,j)whichdistorts f.\n3.2.1 Decompositionofrotation\nLet us consider a camera rotation Rwith axis containing C. We decompose Rin two\nparticular rotations R2R1. The \ufb01rst one R1, with axis \u2206belonging to the plane (C,i,j)\ntransforms the direction of the optical axis kinR(k); this rotation induces a projective\ndeformationoftheimage f. Thesecondone R2isarotationwithaxis R(k):R2inducesa\nplanarrotationoftheimage R1(f). Anycamerarotationcanbewritteninsucha way.\n6This decomposition is interesting because of the induced de formations of the image.\nR1producesa\u201cpurely\u201dprojectivedeformationoftheimage fwhereasR2createsaplanar\nrotationoftheimage R1(f).\nLetusexpresstherotation R1withtwoparameters: \u03b8forthelocationof \u2206intheplane\n(C,i,j)and\u03b1fortheangleoftherotation. Ifwedenote Rl\natherotationmatrixwithaxis l\nandanglea, theexpressionof R1in(C,i,j,k)is\nR1=Rk\n\u03b8Ri\n\u03b1Rk\n\u2212\u03b8\nwhich we denote in the following R\u03b8,\u03b1. Now,let\u03b2be the angle of the rotation R2around\nthenewopticalaxis R(k). We canthenwritethe rotation R2in(C,i,j,k)\nR2=Rk\n\u03b8Ri\n\u03b1Rk\n\u03b2Ri\n\u2212\u03b1Rk\n\u2212\u03b8.\nFinally,the expressionoftheglobalrotation Ris\nR=R2R1=Rk\n\u03b8Ri\n\u03b1Rk\n\u03b2Rk\n\u2212\u03b8=R\u03b8,\u03b1Rk\n\u03b2.\nThus, the rotation Rmay also be decomposedin a rotation around the axis kfollowed by\ntherotationR\u03b8,\u03b1.\nPSfragreplacements\nCCC\nc\nC\u2032\nc\u2032\nii\njj\nkkR\nR(i)R(i)\nR(j)R(j)\nR(k)R(k)R(k)\nR1(i)R1(j)R1 R2\n\u03b1\n\u03b8\u03b2\n\u2206\nFigure2: Decompositionofa camerarotation Rintwo rotations R2R1.\n3.2.2 Decompositionofacompletemotion\nAcompletecameramotion D= (R,t)inducesaprojectivedeformation \u03d5oftheimage f.\nThematrixassociatedto \u03d5isRH,accordingtoformula(5),whichcannowbe writtenas\nRH=R\u03b8,\u03b1Rk\n\u03b2H.\nIfwedenote r\u03b8,\u03b1the\u201cpurely\u201dprojectivedeformationassociatedtotherota tionR\u03b8,\u03b1ands\nthesimilarityassociatedto Rk\n\u03b2Hthenwehave\ng(x,y) =f(\u03d5(x,y)) =f(r\u03b8,\u03b1\u25e6s(x,y)) =f\u25e6r\u03b8,\u03b1\u25e6s(x,y).\n7We obtain therefore six parameters de\ufb01ning the camera motio n, two for the rotation\nR\u03b8,\u03b1and four for the translation tand rotation Rk\n\u03b2. We express now camera motion with\nthefollowingparameters (\u03b8,\u03b1,\u03b2,A,B,C )where(\u2212A,\u2212B,\u2212C)arethecoordinatesof t\nin the basis (R(i),R(j),R(k)). These new notations allow to obtain an easier writting of\nthe projectiveapplication \u03c8(the inverseof \u03d5in the registrationgroup),which we will use\nlater\n\u03c8(x,y) =/parenleftbigga1x+a2y+a3+A\nc1x+c2y+c3+C,b1x+b2y+b3+B\nc1x+c2y+c3+C/parenrightbigg\n. (6)\nRemark that the six parameters (\u03b8,\u03b1,\u03b2,A,B,C )allow to access explicitly the camera\ndisplacement D= (R,t). Indeed,\n\uf8f1\n\uf8f2\n\uf8f3/tildewidet=\u2212AR(i)\u2212BR(j)\u2212CR(k)\nR=R\u03b8,\u03b1Rk\n\u03b2.\n3.3 Parameter values\nAs we consider two successive images of a video sequence with a high frame rate (classi-\ncally 24imagespersecond),thecameramotionbetweentwo im agesisverysmall and the\nparameter values are restricted, except for the angle \u03b8which belongs to ]\u2212\u03c0,\u03c0]. Let us\nremark that the dimensionsof KandK\u2032verify a practical constraint: the view angle of a\ncamera is usually not larger than 150\u25e6. This means that L, the maximal dimension of K,\nmust verify L\u22648fc, as the relation between the view angle a,fcandL, illustrated on\n\ufb01gure3,is\ntana\n2=L\n2fc.\nAsfc= 1,we haveL\u22648.\nPSfragreplacements\na\nL\n2-L\n2\nCfc\nFigure 3: Relation between the view angle aof the camera, the focal length fcand the\nmaximaldimension Lofimages.\nTable 1 gives orders of magnitude of parameter values that we have obtained by ex-\nperiment, when we take a unit focal length. These experiment s consist in taking images\nand applying the six parameters projective application. As the images have not to be too\ndeformed,wededucetheordersofmagnitudeofparameters.\nParameter Values\n\u03b8(radian) ]\u2212\u03c0,\u03c0]\n\u03b1(radian) [0,0.03]\n\u03b2(radian) [\u22120.05,0.05]\nA,B[\u22120.09,0.09]\nC[\u22120.03,0.03]\nTable1:Parametervalues( A,BandCareexpressedin unitsoffocallength).\n83.4 Optical \ufb02ow approximation\nTheorem 2 \u2013Let us considera scene orthogonalto the axis k. LetD= (R,t)belongto\nSE(3), also denoted D= (\u03b8,\u03b1,\u03b2,A,B,C ). LetKandK\u2032be the domainswhere fand\ngarede\ufb01ned,withmaximaldimension L,and(x,y)and(x\u2032,y\u2032)twomatchingpointsof K\nandK\u2032. We assume that hypothesis 1 is veri\ufb01ed, |\u03b1|<1and|\u03b2|<1. Then, the optical\n\ufb02owat(x,y)veri\ufb01es\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032\u2212x=\u2212Cx+A+\u03b2y+\u03b1x(ycos\u03b8\u2212xsin\u03b8)\u2212\u03b1sin\u03b8+o(C)+o(\u03b1)+o(\u03b2)\n+o(/radicalbig\n|\u03b1A|)+o(/radicalbig\n|\u03b1C|)+o(/radicalbig\n|AC|)+o(/radicalbig\n|C\u03b2|)+o(/radicalbig\n|\u03b1\u03b2|)\ny\u2032\u2212y=\u2212Cy+B\u2212\u03b2x+\u03b1y(ycos\u03b8\u2212xsin\u03b8)+\u03b1cos\u03b8+o(C)+o(\u03b1)+o(\u03b2)\n+o(/radicalbig\n|\u03b1B|)+o(/radicalbig\n|\u03b1C|)+o(/radicalbig\n|BC|)+o(/radicalbig\n|C\u03b2|)+o(/radicalbig\n|\u03b1\u03b2|)\nand\n\uf8f1\n\uf8f2\n\uf8f3/vextendsingle/vextendsinglex\u2032\u2212x\u2212(\u2212Cx+A+\u03b2y+\u03b1x(ycos\u03b8\u2212xsin\u03b8)\u2212\u03b1sin\u03b8)/vextendsingle/vextendsingle\u2264T(L,\u03b1,\u03b2,A,C )\n/vextendsingle/vextendsingley\u2032\u2212y\u2212(\u2212Cy+B\u2212\u03b2x+\u03b1y(ycos\u03b8\u2212xsin\u03b8)+\u03b1cos\u03b8)/vextendsingle/vextendsingle\u2264T(L,\u03b1,\u03b2,B,C )\nwith\nT(L,\u03b1,\u03b2,A,C ) =/bracketleftBig\nL32\u03b12\n3+L2/parenleftBig\n4|C\u03b1|\n3+2|\u03b2\u03b1|\n3+4|\u03b1|3\n9/parenrightBig\n+L/parenleftBig\n\u03b12/parenleftBig\n2+|\u03b2|+|C\u22121|\n3/parenrightBig\n+4|A\u03b1|\n3+2|\u03b2C|\n3+\u03b22\n3+2C2\n3+|\u03b2|3\n9/parenrightBig\n+|\u03b1|/parenleftBig\n2\u03b22\n3+4|\u03b2|\n3+4|C|\n3+2|\u03b1A|\n3+8\u03b12\n9/parenrightBig\n+4|AC|\n3/bracketrightBig\n.\nThe proof of this theorem is given in Appendix B. Thanks to the parameter values\ngiven in table 1, the optical \ufb02ow can be approximated by a quad ratic formula in (x,y).\nIndeed, these parameter values allow to make the bound Tsmall in comparison to the\nvalueofeachcomponentofoptical\ufb02ow. Forexample,intheca seofapuretranslationwith\nA=B= 0.09andC= 0.03, the boundTis equal to 4.2 10\u22123forL= 1and8.4 10\u22123\nforL= 8, whereasthe componentsof optical\ufb02ow haveanorderofmagni tudeof10\u22122or\n10\u22121. Forapurelyprojectiverotationwith \u03b1= 0.01,theoptical\ufb02owhasanorderof 10\u22122\nandtheboundisequalto 3 10\u22124forL= 1and5.2 10\u22123forL= 4. ForL= 8,theoptical\n\ufb02owhasanorderof 10\u22121andtheboundis 3.610\u22122.\nIfL,\u03b1,\u03b2,A,B,C are suf\ufb01ciently small, the optical \ufb02ow can be approximated b y the\nsum of three independent terms; the component (\u2212Cx+A,\u2212Cy+B)is due to the\ntranslation of the camera, (\u03b2y,\u2212\u03b2x)to the rotation Rk\n\u03b2and(\u03b1x(\u2212xsin\u03b8+ycos\u03b8)\u2212\n\u03b1sin\u03b8,\u03b1y(\u2212xsin\u03b8+ycos\u03b8)+\u03b1cos\u03b8)to the rotation R\u03b8,\u03b1. These three terms are ap-\nproximations of optical \ufb02ows, respectively produced by the translation, the rotations Rk\n\u03b2\nandR\u03b8,\u03b1.\nRemarks\n\u2022Letusremarkthatattheimagecenter,when xandyhave10\u22121order(foraunitfocal\nlength), the quadratic term is negligiblein comparisonto t he other terms. Thus, the\ndeformationofthecenteroftheimageismainlyaf\ufb01ne.\n\u2022At the beginningof this paper,we did assume that the transla tiontand the depth of\nthescenehavetoverify\n/parenleftbigg1\nZinf\u22121\nZsup/parenrightbigg\n/bardblt/bardbl2(L+1)\n3\u2264\u03b5\n9\u2192\n\u2193 \u2193\nFigure 4: Decomposition of deformation. On left, a checkerboard defo rmed by a cam-\nera motion. On right, the deformation can be decomposed in, \ufb01 rst, a \u201cpurely\u201d projective\ndeformation,generatedbytherotation R\u03b8,\u03b1(attop)followedbyasimilarity (bottom).\nforsubstitutingdepthsbyaconstantinformulas(1). Asthe approximationofoptical\n\ufb02owhasanorderof 10\u22122, wemustchooseanapproximationerror \u03b5atleast inferior\nto10\u22122.\n3.5 Modelisationassets\nInthissection,wehave\ufb01rstproposedtoworkintheregistra tiongroup,well-adaptedtothe\nprojective applications \u03d5and\u03c8that link two consecutive images fandg. The advantage\nof this group is the isomorphism with the Lie group SE(3), which allows to compose\nprojectivedeformationsthroughthe compositionofcamera motions.\nSecond, we have described a new camera motion decomposition to emphasize two\ncomponentsofimagedeformation: asimilarityanda\u201cpurely \u201dprojectivedeformation,due\nto the changeof optical axis direction. This decomposition is interesting because it corre-\nspondstoaphysicalperceptionofcameramotioneffectsonc onsecutiveimages. Asshown\non \ufb01gure4, we easily perceivethe two deformations: the \u201cpur ely\u201dprojectivedeformation,\nwhich deforms parallels on the checkerboard, and the simila rity, which preserves angles.\nWith this decomposition, we have obtained a quadratic appro ximation of optical \ufb02ow for\ntwo consecutiveimages,wherethequadratictermisonlydue to thechangeofopticalaxis\ndirection. Remarkthat weonlyneedcondition(3)forapprox imatingequation(1) by \u03c8.\n4 Camera motionestimation\nLetfandgbe two adjacent images in a video sequence. In this section, w e propose a\nmethod for estimating camera motion between fandg, based on camera motion decom-\npositionandoptical\ufb02owquadraticapproximation.\n104.1 Algorithm\nOdobezandBouth\u00b4 emyproposein[20]amethodfordeterminat ing2Dparametricmotions\nbetweentwoimages. Theyuseconstant,af\ufb01neorquadraticmo dels. Theirmethodisrobust,\nmultiresolutionandonlyusesspatialandtemporalgradien tsofintensity. Thesoftware,de-\nveloppedbytheauthors,isavailableattheaddress http://www.irisa.fr/Vista/Motion2D .\nLetusnowdescribebrie\ufb02ytheiralgorithm. Theoptical\ufb02owa tapoint(x,y)isassumed\nto be parametric, denoted u\u0398(x,y), where\u0398is the set of parameters. Several models are\nproposed,themostgeneralhas12parameters\nu\u0398(x,y) =/parenleftbiggc1\nc2/parenrightbigg\n+/parenleftbigga1a2\na3a4/parenrightbigg/parenleftbiggx\ny/parenrightbigg\n+/parenleftbiggq1q2q3\nq4q5q6/parenrightbigg\uf8eb\n\uf8edx2\nxy\ny2\uf8f6\n\uf8f8.\nThe displacement frame difference (DFD) associated to a par ametric motion model at the\npoint(x,y)isde\ufb01nedwith\nDFD(\u0398,\u03be)(x,y) =g((x,y)+u(x,y))\u2212f(x,y)+\u03be\nwhere\u03beis a global intensity shift to account for global illuminati on change. The set of\nparametersisthusestimatedbyminimizingthefollowingfu nction\n/summationdisplay\n(x,y)\u2208f\u03c1(DFD(\u0398,\u03be)(x,y),\u0393)\nwhere the function \u03c1is called an M-estimator since its minimization correspond s to the\nmaximum-likelihood estimation if \u03c1is considered as the opposite log-likelihood of the\nmodel. The authors choose a function bounded for high values in order to eliminate the\ncontributionofoutliers. TheyusetheTuckey\u2019sbiweightfu nctionde\ufb01nedas\n\u03c1(t,\u0393) =\uf8f1\n\uf8f2\n\uf8f3t2\n2(\u03934\u2212\u03932t2+t4\n3)if|t|<\u0393,\n\u03936\n6otherwise.\nThe minimization of \u03c1is performedusing an incremental and multiresolutionsche me de-\nscribedin[20]. Thismethodisaccurateandhasa lowcomputa tionalcost.\nSeveral models are proposed in the software but none corresp onds to our optical \ufb02ow\napproximation. Thus,we haveaddedthe followingmodeltoth esoftware\nu\u0398(x,y) =/parenleftbigg\nc1\nc2/parenrightbigg\n+/parenleftbigg\na1a2\n\u2212a2a1/parenrightbigg/parenleftbigg\nx\ny/parenrightbigg\n+/parenleftbigg\nq1q20\n0q1q2/parenrightbigg\uf8eb\n\uf8edx2\nxy\ny2\uf8f6\n\uf8f8.\nOncethesix parameters (c1,c2,a1,a2,q1,q2)areestimated,we converttheminto \u03b1,\u03b2,\u03b8,\nA,B,Cbyidentifyingthepreviousexpressionwiththequadraticf ormulagivenintheorem\n2 \uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\u03b8=\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\u2212arctan(q1/q2)ifq2>0\n\u2212arctan(q1/q2)+\u03c0ifq2<0\n\u03c0/2 ifq2= 0andq1>0\n\u2212\u03c0/2 ifq2= 0andq1\u22640.\n\u03b1=/radicalbig\nq2\n1+q2\n2\n\u03b2=a2\nA=c1+\u03b1sin\u03b8\nB=c2\u2212\u03b1cos\u03b8\nC=\u2212a1.\n114.2 Results\nThe performances of our method are illustrated through came ra motion estimations on\nsynthetic and real sequences, and some applications of thes e estimations. The context for\napplicatingourmethodisgivenbycondition(3)\n/parenleftbigg1\nZinf\u22121\nZsup/parenrightbigg\n/bardblt/bardbl2(L+1)\n3\u2264\u03b5,\nwith\u03b5<10\u22122. Thismeansthatforagivenimagesize,theproductoftransl ationnormand\nvariations of inverse of depth must be small enough. We do not need condition (4) since\nwe onlyusethe deformation \u03c8.\n4.2.1 Syntheticsequences\nWe \ufb01rst estimate camera motion on sequences, that we have cre ated from an image, con-\nsideredasorthogonalto theopticalaxisanddeformedwiths etsofsixparameters( \u03b8,\u03b1,\u03b2,\nA,B,C). These sets are randomly generated with respect to values g iven in table 1. The\nangleofviewisequalto 90\u25e6. Threesequencesof200imagesaresynthesized;the\ufb01rstone\nis generated with translations, the second one with rotatio ns and the third one with plain\nmotions. The initial image is shown on \ufb01gure 5. We assumed tha t depth is constant and\napplyformula(6)ontheimagewitha bilinearinterpolation .\nFigure5: Initialimagefortest sequences.\nTranslation Axisrotation Rotationangle\ndirection direction error\nerror error absolute relative\nPlain\nmotions 9.7\u25e617.3\u25e60.03\u25e62.2%\nPure\ntranslations 4.5\u25e6- 0.01\u25e6-\nPure\nrotations - 18.2\u25e60.002\u25e60.1%\nTable2:Resultsofcameramotionestimationson3syntheticsequenc esof200images. The\nerrors areaveragederrors computedovereachsequence.\nCamera motion results are shown on table 2. Whatever the type of camera motion,\nthe estimations of translation direction are correct up to a few degrees and the estimated\nrotation directionup to ten or twenty degrees. These last er rorsmay seem to be important\nbut we must notice that the change of optical axis direction i s hard to estimate, as small\nrotation and small translation can produce very similar res ults on images. For example, a\n12smalltranslationwithdirection iandasmallrotationwithaxis jproduceverycloseeffects\nonimages. Theestimationsofrotationanglearemoreaccura te;theyarecorrectuptoafew\nhundredthsdegreesforrotationanglesof1or2degrees. Ins um,obtainedresultsarerather\ngood, better when motions are reduced to a translation or a ro tation. Moreover, the scene\nwas quite complicated and the method is very fast: it takes 7. 7 seconds for a sequence of\n200imageswith 284\u00d7188pixels,withaprocessorPentiumM1.8GHz.\nRobustness Figure6showstherobustnessofthealgorithmtoimpulseorg aussiannoise.\nWe addvariousamountsofimpulseorgaussiannoisetotheseq uenceproducedwithcom-\npletemotions. Graphsploterrorsintheestimatesasafunct ionofnoiselevel,averagedover\nthe 200 images at each noise level. For both types of noise, th e errors do not increase a\nlot: theyremainclosetoerrorscomputedwithoutnoise,les sthan15degreesfortranslation\ndirection,atmostfewtenthsdegreesfortheangleofrotati on(forimpulsenoise). Thusthe\nmethodis robust,thanksto the useof M-estimator: it provid esgoodresultsevenwhen the\namountofimpulsenoiseis important.\nDepths in\ufb02uence In this paper, we have approximated the deformation (equati on (1))\nbetweengandfby\u03c8, providedthatcondition(3)wasveri\ufb01ed,with \u03b5<10\u22122\n/parenleftbigg1\nZinf\u22121\nZsup/parenrightbigg\n/bardblt/bardbl2(L+1)\n3\u2264\u03b5.\nThe smaller is/parenleftBig\n1\nZinf\u22121\nZsup/parenrightBig\n/bardblt/bardbl2(L+1)\n3, the more accurate is the approximation. For a\ngiven scene, further the camera is from the scene, smaller is the previous expression and\nbetter is the estimation. This fact is illustrated with moti on estimation on synthetic se-\nquencesSOFA5 and SOFA6 (Sequencesfor Optical Flow Analysi s, courtesy of the Com-\nputer Vision Group, Heriot-Watt University). Each sequenc e, which each contains 20 im-\nages, is given with internal and external camera parameters , and camera motion. Motions\nare basic: a translation of direction kfor SOFA5 and a rotation with axis kfollowed by a\ntranslation with direction kfor SOFA6. Images of the two sequences are shown on \ufb01gure\n7. Resultsaregivenontables4and5; theevaluationof/parenleftBig\n1\nZinf\u22121\nZsup/parenrightBig\n/bardblt/bardbl2(L+1)\n3isalso\ncomputed(inunitsoffocallength)ontable3.\n1\nZinf\u22121\nZsup/parenleftbigg1\nZinf\u22121\nZsup/parenrightbigg\n/bardblt/bardbl2(L+1)\n3\nImage10.0062 0.0076\nImage100.0112 0.0137\nImage200.0293 0.0357\nTable 3:Relative variations of inverse of depths in sequences SOFA5 et SOFA6. Depths\nZinfandZsup,/bardblt/bardblandLareexpressedin unitsoffocallengthin thecamerasystem.\nAs the camera comes close the scene, differences in table 3 in crease in time. Remark\nthatwehave L\u22648;theangleofviewisequalto 45\u25e6. Tables4and5giveerrorsinmotion\nestimationbetweenconsecutiveimagesat threeinstants: a t the beginningof the sequence,\nat the middle and at the end. The estimation method is the same as previously used: we\nassumeno aprioritypeofmotion. ForSOFA5,thetranslationdirectionestima tesarevery\ngood,betterthanonprevioussyntheticsequences. Thisisd uetothemotionsimplicityand\nto the \ufb01xity of optical axis. However, we observe that when th e camera comes close the\nscene,thetranslationestimationerrorandtherotationan gleestimation(thatshouldbenull)\n13 9.5 10 10.5 11 11.5 12 12.5 13 13.5 14 14.5 15\n 0  5  10  15  20  25  30PSfragreplacementsEstimationof translation directionError (degrees)\nGaussiannoise\nImpulse noiseEstimationof rotation angle\nEstimationof rotation axisdirection 9.5 10 10.5 11 11.5 12\n 0  5  10  15  20  25  30  35  40  45  50PSfragreplacementsEstimationof translation directionError (degrees)\nGaussiannoiseImpulse noise\nEstimationof rotation angle\nEstimationof rotation axisdirection\n 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2\n 0  5  10  15  20  25  30PSfragreplacements\nEstimationof translation direction\nError (degrees)Gaussiannoise\nImpulse noiseEstimationof rotation angle\nEstimationof rotation axisdirection 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1\n 0  5  10  15  20  25  30  35  40  45  50PSfragreplacements\nEstimationof translation direction\nError (degrees)\nGaussiannoiseImpulse noiseEstimationof rotation angle\nEstimationof rotation axisdirection\n 17 18 19 20 21 22 23 24 25 26\n 0  5  10  15  20  25  30PSfragreplacements\nEstimationof translation direction\nError (degrees) Gaussiannoise\nImpulse noiseEstimationof rotation angleEstimationof rotation axisdirection\n 17 17.5 18 18.5 19 19.5 20 20.5 21 21.5\n 0  5  10  15  20  25  30  35  40  45  50PSfragreplacements\nEstimationof translation direction\nError (degrees)\nGaussiannoiseImpulse noise\nEstimationof rotation angleEstimationof rotation axisdirection\nFigure 6: Camera motion estimation errors, averaged over 200 images o f the noisy se-\nquence. Impulse noise level of 10 means that 10%of pixels values are randomly chosen\nwith a uniform variable distributed on all gray levels. Gaus sian noise level of 10means\nthatwe addto theimagesagaussiannoisewith standarddevia tion10.\nslightlyincrease. ForSOFA6,thetranslationdirectiones timatesarealwaysverygood;but\nthe estimation errors on axis and angle of rotation increase signi\ufb01cantly when the camera\ncomesclosethescene.\nAlthough errors increase when we get close to the scene (beca use we then are away\nfromthe de\ufb01nedcontext),ourmethodallowstoconcludefors implemotions(forexample\nwhentheopticalaxisis\ufb01xed)evenifcondition(3)isnotver i\ufb01edwith\u03b5<10\u22122.\n4.2.2 Applicationson realsequences\nAs we have no real sequences with given camera motion and inte rnal camera parameters,\nwe illustrate the quality of camera motion estimation with t wo applications of estimation\nresults.\nThe \ufb01rst use is mosaicing. In our framework, we suppose that t wo successive images\narelinkedbyaplanartransformation,thustheknowledgeof cameramotionbetweenthese\n14Figure 7: At the top, images 1 and 2 of SOFA5 and SOFA6. At the middle, ima ges 19 and\n20ofSOFA5andatthe bottom,images19and20ofSOFA6.\nTranslation Rotation\ndirection angle\nerror error\nBetween\nimages1and20.12\u25e60.0005\u25e6\nBetween\nimages10and110.17\u25e60.0018\u25e6\nBetween\nimages19and200.55\u25e60.019\u25e6\nErrors\naverage 0.42\u25e60.014\u25e6\nTable 4:Estimation errors on SOFA5. Camera motion is constant on the sequence: it is a\ntranslationofdirection k(thecameracomesclosethe scene).\ntwoimagesallowstoregisteroneimagetotheother. Withthe estimationofcameramotion\non a whole sequence, we can compute the motion between two ima ges distant in time, by\ncomposingdisplacementestimationsintheregistrationgr oup. Thus,bychoosinganimage\nviewpoint and registering some images distant in time on it, we obtain a bigger image\nthat we could observe from the image viewpoint, but with a lar ger vision \ufb01eld. Figures 8\nand 9 show two panoramas, computed with the estimated camera motion on a real video\nsequenceofan of\ufb01ce. Remarkthat the mosaicingis theoretic allypossible if the viewpoint\ndoes not change (when there is no translation) or when the cam era \ufb01lms a planar scene.\nOur movie does not exactly verify the hypothesis of pure rota tion because although the\ncamera translation is very small between adjacent frames, i t may be signi\ufb01cant between\ntwoimagesdistantintimeandobviously,thesceneisnotpla nar. But asthesceneisrather\nfarfromthecameralocation,registrationsarecorrect.\nThe second use is augmented reality. It consists in adding an object in a sequence in\nsuch a way it appears to be present in the scene. In our framewo rk, the application is\n15Translation Rotationaxis Rotationangle\ndirection direction error\nerror error absolute relative\nBetween\nimages1and20.23\u25e60.001\u25e60.051\u25e62.5%\nBetween\nimages10and110.38\u25e60.491\u25e60.068\u25e63.4%\nBetween\nimages19and200.97\u25e61.08\u25e60.094\u25e64.7%\nErrors\naverage 0.39\u25e60.269\u25e60.069\u25e63.4%\nTable 5:Estimation errors on SOFA6. Camera motion is constant on the sequence: it is\na rotation of axis kfollowed by a translation of direction k(the camera comes close the\nscene).\nFigure 8: At the top, scenes 20, 35 and 50 of the of\ufb01ce sequence; at the bo ttom, recon-\nstructedpanoramicviewonviewpoint35.\nsimpli\ufb01ed since we insert in the of\ufb01ce sequence a planar obje ct, which is a poster. This\nposteris\ufb01rstinsertedonthemainplanarregionofthescene ,roughlyparalleltotheretinal\nplane. Next, it is deformed with the projective application 6 associated to the estimated\ncameramotion. Exampleframesfromtheaugmentedsequencea representedon\ufb01gure10.\nThis experience shows that the camera motion is accurately e stimated: the poster moves\nwiththesamemotionasthebackgroundofthescene. Moreprec isely,theposterorientation\nfollowstheorientationofthebackground(camerarotation sarecorrectlyestimated)andits\npositionisplausible.\nLet us recall that our goal is not mosaicing nor augmented rea lity: these two applica-\ntions are utilizations of estimated camera motions and illu strate the quality of our motion\nestimationresultsin ourframework.\n16Figure 9: At the top, scenes 10, 30, 60, 70 and 80 of the of\ufb01ce sequence; a t the bottom,\nreconstructedpanoramicviewonviewpoint60.\nFigure10: Replacementofthenoticeboardbyacinemaposter. Atthetop : theinsertionof\ntheposteronthe\ufb01rstimage. Atthemiddle,images 10,20,30,40et45ofthenewsequence\nobtainedby deformingthe posterwith the estimationsof cam era motionsandpasting it in\nthesequence.\n5 Conclusion\nIn this paper, we have proposed a new global method for the pro blem of egomotion esti-\nmation, well-adaptedto adjacentframesas producedby a cam erathat \ufb01lms a static scene,\nwhenvariationsofinverseof scenedepthsandtranslationa resuf\ufb01cientlysmall. Thiscon-\n17text is theoretically limited, but as the translation is ver y small between two acquisitions,\nit is not so restrictive. In this context, the method is very f ast : \ufb01rst because we do not\nhavetocomputeoptical\ufb02owormatchpointsasitisadirectme thod,secondbecauseofthe\nmultiresolutionschemeinthesoftwareMotion2D,\ufb01ttedtoo urquadraticapproximationof\noptical\ufb02ow. It isalso robust,thankstotheuse ofan M-estim ator. Moreover,themodeling\nof camera motion in the registration group allows to compose image deformations and to\nobtain camera motion between two images distant in time in a s equence. At last, as it is a\nglobal method, it is robust to a moving object in the scene, pr ovided its size is limited in\ncomparisontothe imagesize.\nA Proofof theorem1\nLet0< Zinf\u2264Z0\u2264Zsupand(x,y)belong toK. We denote \u03b4=1\nZ(x,y)\u22121\nZ0. Thus,\nwe canwriteformula1 \uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032=u1\n0\u2212\u03b4/an}bracketle{tt,R(i)/an}bracketri}ht\nv0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht\ny\u2032=u2\n0\u2212\u03b4/an}bracketle{tt,R(j)/an}bracketri}ht\nv0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht\nwhere \uf8f1\n\uf8f2\n\uf8f3u1\n0=a1x+a2y+a3\u2212/an}bracketle{tt\nZ0,R(i)/an}bracketri}ht\nu2\n0=b1x+b2y+b3\u2212/an}bracketle{tt\nZ0,R(j)/an}bracketri}ht\nv0=c1x+c2y+c3\u2212/an}bracketle{tt\nZ0,R(k)/an}bracketri}ht.\nBy applyingTaylor\u2019sformulaon \u03b4about0with integralformofremainder,weobtain\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032=u1\n0\nv0+/integraldisplay\u03b4\n0/an}bracketle{tt,R(k)/an}bracketri}htu1\n0\u2212/an}bracketle{tt,R(i)/an}bracketri}htv0\n(v0\u2212z/an}bracketle{tt,R(k)/an}bracketri}ht)2dz=u1\n0\nv0+\u03b4/an}bracketle{tt,R(k)/an}bracketri}htu1\n0\u2212/an}bracketle{tt,R(i)/an}bracketri}htv0\nv0(v0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht)\ny\u2032=u2\n0\nv0+/integraldisplay\u03b4\n0/an}bracketle{tt,R(k)/an}bracketri}htu2\n0\u2212/an}bracketle{tt,R(j)/an}bracketri}htv0\n(v0\u2212z/an}bracketle{tt,R(k)/an}bracketri}ht)2dz=u2\n0\nv0+\u03b4/an}bracketle{tt,R(k)/an}bracketri}htu2\n0\u2212/an}bracketle{tt,R(j)/an}bracketri}htv0\nv0(v0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht)\nthatimplies\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3/vextendsingle/vextendsingle/vextendsingle/an}bracketle{tt,R(k)/an}bracketri}htu1\n0\u2212/an}bracketle{tt,R(i)/an}bracketri}htv0\nv0(v0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht)/vextendsingle/vextendsingle/vextendsingle\u2264 /bardblt/bardbl|u1\n0|+|v0|\n|v0|/vextendsingle/vextendsingle/vextendsingle1\nv0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht/vextendsingle/vextendsingle/vextendsingle\n/vextendsingle/vextendsingle/vextendsingle/an}bracketle{tt,R(k)/an}bracketri}htu2\n0\u2212/an}bracketle{tt,R(j)/an}bracketri}htv0\nv0(v0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht)/vextendsingle/vextendsingle/vextendsingle\u2264 /bardblt/bardbl|u2\n0|+|v0|\n|v0|/vextendsingle/vextendsingle/vextendsingle1\nv0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht/vextendsingle/vextendsingle/vextendsingle.\nSince(x,y)\u2208K\u2286[\u2212L\n2,L\n2]2, wehave,withthehypothesis2\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3|u1\n0|+|v0|\n|v0|\u2264/vextendsingle/vextendsingle/vextendsingleu1\n0\nv0\u2212x/vextendsingle/vextendsingle/vextendsingle+|x|+1\u2264L+1\n|u2\n0|+|v0|\n|v0|\u2264/vextendsingle/vextendsingle/vextendsingleu2\n0\nv0\u2212y/vextendsingle/vextendsingle/vextendsingle+|y|+1\u2264L+1.\nMoreover,asthehypothesis1implies\n/vextendsingle/vextendsingle/vextendsingle1\nv0\u2212\u03b4/an}bracketle{tt,R(k)/an}bracketri}ht/vextendsingle/vextendsingle/vextendsingle\u22644\n3,\nthus\nmax/parenleftbigg/vextendsingle/vextendsingle/vextendsinglex\u2032\u2212u1\n0\nv0/vextendsingle/vextendsingle/vextendsingle,/vextendsingle/vextendsingle/vextendsingley\u2032\u2212u2\n0\nv0/vextendsingle/vextendsingle/vextendsingle/parenrightbigg\n\u2264\u03b4/bardblt/bardbl4(L+1)\n3.\n18Now,if /parenleftbigg1\nZinf\u22121\nZsup/parenrightbigg\n/bardblt/bardbl2(L+1)\n3\u2264\u03b5,\nthen,forZ0suchthat1\nZ0=1\n2/parenleftBig\n1\nZinf+1\nZsup/parenrightBig\n,we have\n\u2200(x,y)\u2208K,/vextendsingle/vextendsingle/vextendsingle1\nZ(x,y)\u22121\nZ0/vextendsingle/vextendsingle/vextendsingle/bardblt/bardbl4(L+1)\n3\u2264\u03b5,\nthatimplies\n\u2200(x,y)\u2208K,max/parenleftbigg/vextendsingle/vextendsingle/vextendsinglex\u2032\u2212u1\n0\nv0/vextendsingle/vextendsingle/vextendsingle,/vextendsingle/vextendsingle/vextendsingley\u2032\u2212u2\n0\nv0/vextendsingle/vextendsingle/vextendsingle/parenrightbigg\n\u2264\u03b5.\nB Proof oftheorem 2\nLetD= (\u03b8,\u03b1,\u03b2,A,B,C )beacameramotion. Therotationmatrix Ris equalto\n/parenleftBiggcos\u03b2\u2212(1\u2212cos\u03b1)sin\u03b8sin(\u03b8\u2212\u03b2)\u2212sin\u03b2+(1\u2212cos\u03b1)sin\u03b8cos(\u03b8\u2212\u03b2) sin\u03b8sin\u03b1\nsin\u03b2+(1\u2212cos\u03b1)cos\u03b8sin(\u03b8\u2212\u03b2) cos\u03b2\u2212(1\u2212cos\u03b1)cos\u03b8cos(\u03b8\u2212\u03b2)\u2212cos\u03b8sin\u03b1\n\u2212sin\u03b1sin(\u03b8\u2212\u03b2) sin \u03b1cos(\u03b8\u2212\u03b2) cos \u03b1/parenrightBigg\nthatwe alsodenote\nR=\uf8eb\n\uf8eda1b1c1\na2b2c2\na3b3c3\uf8f6\n\uf8f8.\nThecoef\ufb01cientsof Rverify,byusingTaylorexpansionsin \u03b1and\u03b2\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3a1= 1+ka1, k a1=o(\u03b2)+o(\u03b1), |ka1| \u2264\u03b22/2+\u03b12/2(1+|\u03b2|)\na2=\u03b2+ka2, k a2=o(\u03b22)+o(\u03b1),|ka2| \u2264\u03b23/6+\u03b12/2(1+|\u03b2|)\na3=\u2212\u03b1sin\u03b8+ka3, ka3=o(\u03b12)+o(/radicalbig\n|\u03b1\u03b2|),|ka3| \u2264\u03b13/6+|\u03b1|(|\u03b2|+\u03b22/2)\nb1=\u2212\u03b2+kb1, k b1=o(\u03b22)+o(\u03b1),|kb1| \u2264\u03b23/6+\u03b12/2(1+|\u03b2|)\nb2= 1+kb2, k b2=o(\u03b2)+o(\u03b1), |kb2| \u2264\u03b22/2+\u03b12/2(1+|\u03b2|)\nb3=\u03b1cos\u03b8+kb3, kb3=o(\u03b12)+o(/radicalbig\n|\u03b1\u03b2|),|kb3| \u2264\u03b13/6+|\u03b1|(|\u03b2|+\u03b22/2)\nc1=\u03b1sin\u03b8+kc1kc1=o(\u03b12), |kc1| \u2264 |\u03b1|3/6\nc2=\u2212\u03b1cos\u03b8+kc2kc2=o(\u03b12), |kc2| \u2264 |\u03b1|3/6\nc3= 1+kc3kc3=o(\u03b1), |kc3| \u2264 |\u03b1|2/2.\nAccordingto thede\ufb01nitionoftheapplication \u03c8,we have\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032\u2212x=x+\u03b2y\u2212\u03b1sin\u03b8+A+o(\u03b1)+o(\u03b2)+o(/radicalbig\n|\u03b1\u03b2|)\n\u03b1sin\u03b8x\u2212\u03b1cos\u03b8y+1+C+o(\u03b1)\u2212x\ny\u2032\u2212y=y\u2212\u03b2x+\u03b1cos\u03b8+B+o(\u03b1)+o(\u03b2)+o(/radicalbig\n|\u03b1\u03b2|)\n\u03b1sin\u03b8x\u2212\u03b1cos\u03b8y+1+C+o(\u03b1)\u2212y,\nthatis\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032\u2212x=/parenleftBig\nx+\u03b2y\u2212\u03b1sin\u03b8+A+o(\u03b1)+o(\u03b2)+o(/radicalbig\n|\u03b1\u03b2|)/parenrightBig\n(1\u2212C\u2212\u03b1sin\u03b8x+\u03b1cos\u03b8y+o(\u03b1)+o(C))\u2212x\ny\u2032\u2212y=/parenleftBig\ny\u2212\u03b2x+\u03b1cos\u03b8+B+o(\u03b1)+o(\u03b2)+o(/radicalbig\n|\u03b1\u03b2|)/parenrightBig\n(1\u2212C\u2212\u03b1sin\u03b8x+\u03b1cos\u03b8y+o(\u03b1)+o(C))\u2212y.\n19Thatimplies\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x\u2032\u2212x=\u2212Cx+\u03b2y\u2212\u03b1sin\u03b8+A\u2212\u03b1sin\u03b8x2+\u03b1cos\u03b8xy+o(\u03b1)+o(\u03b2)+o(C)\n+o(/radicalbig\n|\u03b1\u03b2|)+o(/radicalbig\n|C\u03b2|)+o(/radicalbig\n|C\u03b1|)+o(/radicalbig\n|\u03b1A|)+o(/radicalbig\n|CA|)\ny\u2032\u2212y=\u2212Cy\u2212\u03b2x+\u03b1cos\u03b8+B\u2212\u03b1sin\u03b8xy+\u03b1cos\u03b8y2+o(\u03b1)+o(\u03b2)+o(C)\n+o(/radicalbig\n|\u03b1\u03b2|)+o(/radicalbig\n|C\u03b2|)+o(/radicalbig\n|C\u03b1|)+o(/radicalbig\n|\u03b1B|)+o(/radicalbig\n|CB|).\nFurthermore,\n/vextendsingle/vextendsinglex\u2032\u2212x\u2212/parenleftbig\n\u2212Cx+\u03b2y\u2212\u03b1sin\u03b8+A\u2212\u03b1sin\u03b8x2+\u03b1cos\u03b8xy/parenrightbig/vextendsingle/vextendsingle\n=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\u2212c1x2\u2212c2xy+(a1\u2212c3\u2212C)x+a2y+a3+A\u2212(c1x+c2y+c3+C)(A\u2212Cx+\u03b2y+\u03b1cos\u03b8xy\u2212\u03b1sin\u03b8x2\u2212\u03b1sin\u03b8)\nc1x+c2y+c3+C/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.\nBy usingboundsof |ka1|,|ka2|,...,|kc3|andthehypothesis1, weget\n/vextendsingle/vextendsinglex\u2032\u2212x\u2212/parenleftbig\n\u2212Cx+\u03b2y\u2212\u03b1sin\u03b8+A\u2212\u03b1sin\u03b8x2+\u03b1cos\u03b8xy/parenrightbig/vextendsingle/vextendsingle\n\u22644\n3/vextendsingle/vextendsinglex2(\u2212c1+Cc1+\u03b1sin\u03b8c3+\u03b1sin\u03b8C)\u2212y2\u03b2c2+\nxy(\u2212c2+Cc2\u2212\u03b2c1\u2212\u03b1cos\u03b8c3\u2212\u03b1cos\u03b8C)+x2y(\u2212c1\u03b1cos\u03b8+c2\u03b1sin\u03b8)+\nx3(\u03b1sin\u03b8c1)\u2212xy2c2\u03b1cos\u03b8+x(a1\u2212c3\u2212C\u2212Ac1+c1\u03b1sin\u03b8+Cc3+C2)+\ny(a2\u2212Ac2+c2\u03b1sin\u03b8\u2212\u03b2c3\u2212\u03b2C)+a3+A(1\u2212c3\u2212C)+\u03b1sin\u03b8(c3+C)/vextendsingle/vextendsingle.\nAs(x,y)\u2208[\u2212L/2,L/2]2,we obtain\n/vextendsingle/vextendsinglex\u2032\u2212x\u2212/parenleftbig\n\u2212Cx+\u03b2y\u2212\u03b1sin\u03b8+A\u2212\u03b1sin\u03b8x2+\u03b1cos\u03b8xy/parenrightbig/vextendsingle/vextendsingle\n\u2264/bracketleftBig\nL32\u03b12\n3+L2/parenleftBig\n4|C\u03b1|\n3+2|\u03b2\u03b1|\n3+4|\u03b1|3\n9/parenrightBig\n+L/parenleftBig\n\u03b12/parenleftBig\n2+|\u03b2|+|C\u22121|\n3/parenrightBig\n+4|A\u03b1|\n3+2|\u03b2C|\n3+\u03b22\n3+2C2\n3+|\u03b2|3\n9/parenrightBig\n+|\u03b1|/parenleftBig\n2\u03b22\n3+4|\u03b2|\n3+4|C|\n3+2|\u03b1A|\n3+8\u03b12\n9/parenrightBig\n+4|AC|\n3/bracketrightBig\n.\nByasimilarway,webound/vextendsingle/vextendsingley\u2032\u2212y\u2212/parenleftbig\n\u2212Cy\u2212\u03b2x+\u03b1cos\u03b8+B\u2212\u03b1sin\u03b8xy+\u03b1cos\u03b8y2/parenrightbig/vextendsingle/vextendsingle\nbyreplacing AwithB."}
{"category": "abstract", "text": "In this paper, we propose a global method for estimating the m otion of a camera\nwhich\ufb01lmsa static scene. Our approach is direct,fast and ro bust, anddeals withadja-\ncentframes of a sequence. Itis basedona quadratic approxim ation ofthe deformation\nbetween two images, in the case of a scene with constant depth in the camera coordi-\nnate system. This condition is very restrictive but we show t hat provided translation\nand depth inverse variations are small enough, the error on o ptical \ufb02ow involved by\nthe approximation of depths by a constant is small. In this co ntext, we propose a new\nmodel of camera motion, that allows to separate the image def ormation in a similar-\nity and a \u201cpurely\u201d projective application, due to change of o ptical axis direction. This\nmodel leads to a quadratic approximation of image deformati on that we estimate with\nanM-estimator; wecan immediatlydeduce camera motionpara meters.\n1 Introduction\nTheestimation ofcameramotionplaysa crucialrolein manyd omainsofcomputervision\nsuchastherecoveryofscenestructure,medicalimaging,au gmentedrealityandsoon. This\nis a dif\ufb01cult task since the motion of a pixel between two imag es dependsnot only on the\nsix parameters of camera motion between the two successive i mage captures, but also on\nthedepthat thecorrespondingpointin thestatic scene. Exi stingmethodscan beclassi\ufb01ed\nas features correspondences-basedapproaches, which are l ocal, optical \ufb02ow methods and\ndirectmethods,whichareglobal.\nAmong all proposed methods using features correspondences , one can mention re-\ncursive techniques based on extended Kalman \ufb01lters [1, 2] wh ich track camera motion\nand estimate the structure of the scene. The essential matri x, which was \ufb01rst de\ufb01ned by\nLonguet-Higgins in [3], is often estimated, as only a few cor respondences in two images\nare suf\ufb01cient; the number of required correspondences is di scussed by Faugeras et al. in\n[4, 5, 6]. Inthe case of an uncalibratedcamera,the analogou sapproachis describedin [7]\nwiththe fundamentalmatrix.\nTheuseofoptical\ufb02owavoidsthechoiceof\u201cgood\u201dfeatures;m anyauthorsusethebasic\nbilinearconstraintlinkingoptical\ufb02ow,cameravelocitie sanddepthsofprojectedpoints;in\n[8], Bruss and Horn apply an algebraic computation to remove depth from the bilinear\nconstraintandusenumericaloptimizationtechniques. Hee gerandJepson,in[9],decouplethetranslationalvelocityfromtherotationalvelocityan duselinearsubspacemethods. Ma\netal. in[10]andBrooksetal. in[11]useadifferentapproac hwiththeepipolardifferential\nconstraint"}
{"category": "non-abstract", "text": "image com pression by orders of m agnitude , signal com pression including sound as  \nwell, im age analys is in  a m ultilay ered de tailed analys is, patte rn recognition  and  \nmatching and rapid d atabas e searching (e.g.  face recog nition), motion analys is, \nbiom edical applications e.g. in MRI and CAT s can im age analysis and com pression, \nas well as h ints on the link of these ideas  to the way how biological m emory m ight \nwork leading to new points of  view in neural com putati on.  Comm ercial applications \nof immediate interest are the com pression of images at the source (e.g. photographic \nequipm ent, scanners, satellite im aging system s), DVD fil m com pression, pay-per-\nview downloads acceleration and many others  identif ied in  the p resent paper at its  \nconclusion and future work section. \n \n \nPage 1 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n1. Introduction: image compression and the Perceptron Algorithm \n \nImage com pression  nowadays is a crucial as pect in m any comm ercial a nd scien tific \napplic ation s, where the prim ary aim is the re duction of size of an im age f ile so  as to \nminimize the storage an d transm ission require ments.  Most algorithm s of commercial  \nvalue and standard operate at a bitwis e manner, e.g. JPEG, TIFF with various \ncomponents for com pression, etc. \n \nThe point of  view adopted in this w ork is to look at an im age \u201cas an im age\u201d and apply \nprinciples related to how hum ans perceive  a field  of view.  For exam ple, m ammalian \nreactions w hen presen ted with a rap idly chan ging scene are to perceiv e in high speed \nsituations as  \u201cfriend of foe\u201d, and to  reac t acco rdingly (albeit erroneous ly at tim es).  \nWhere m ore deta il is stu died in  a field, it appears that biologic al processing requires \nmore tim e to react.  For exam ple, in cases where one individual can afford this, m ore \ntime is spen t study ing a  scene  to re solve and  process finer detail.  This is contr asted \nwith high speed changes in the field which are p erceived m uch fa ster, as m entioned, \nalthough motion blurring restricts the percepti on of finer detail.  In other words for \nfaster processing of changing scenes speed of reaction is param ount and for this a \nblurred im age seem s to suffice. \n \nMotivated by this, we conducted some experi mental steps of a new algorithm  which is  \ndescrib ed mathem atically in the f ollowing sim ple schem a: \n \nN i ,,...2,1_Blur Compressed)( Image Image); Blur n( Compressio _Blur Compressed); icient blur_coeff, noise, Image lurring( Gaussian_B Blur\n(i) (i) 1)(i(i) (i)(i) (i) (i) (i)\n=\u2212 ===\n+   (1) \n \nThe algorithm  in equations (1) in our im plem entation u ses the following details, \nimplemented in GIMP version 2.0 produced  by the GNU free software organisation: \n \n\u2022 \u201c \u201d is the standard Gaussian  blurring function pro vided \nin GIMP. .,.) lurring(., Gaussian_B\n\u2022 \u201cnoise\u201d is a sm all am ount of \u201cSpread\u201d added to the im ages prior blurring \nwhich is  used as  a heu ristic techniq ue to reduce artefact s introduced by high \nJPEG com pression (see below).  If the noi se of the current working im age is \nalready perceived as \u201chigh\u201d then no further noise is added to the im age prior to \nblurring. \n\u2022 \u201c \u201d is th e standard deviation in pi xels used in th e application of \nGaussian blurring.  This initially is se t to som ething rathe r high, \napproxim ately  used as \u00bd of the m aximum dimension of th e image (vertical o r \nhorizontal) in pixels, and it is reduced  by a reduction factor greater than 1 \nfrom  one iteration to the next, usually  we use a factor of 2, for exam ple \nbuilding a sequence of blur coefficients in the form  of {1000, 500, 250, 125,  \n60, 30, 15, 8, 4, 2, 1}, and usually stopping at a blur coefficient of 1. icient blur_coeff\n\u2022 \u201cCom pression(.)\u201d is a standard functi on used from the options in GIMP.  \nAlthough the original im age and all working im ages are used in \u201craw\u201d \nformat (TIFF uncom pressed is used in our exam ples), the stored blurred (i) Image\nPage 2 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nimages and the ones  subtracted from the work ing im age are low grade J PEG \nof a few Kbytes in s ize.   \n\u2022 The subtraction operation,  \u201c(-)\u201d, in the outlined algorithm  above uses any \nsuitab le \u201csu btraction\u201d o peration to  take away  the com pressed low grade \nblurred im age f rom the working im age, resu lting in a new working im age.  In \nGIMP for (-) we use  the  \u201cGrain Ex tract\u201d opera tion in the \u201c Layers\u201d  \nsuperposition m ode selection. \n \nUpon com pletion, the operation of the algorithm results in N low grade com pressed \nimages, , and a high grade working im age, , which \nserves as a base im age.  The reconstr uction without any loss whatsoever of the \norigin al im age can be co nstruc ted by  the f ollowin g equation : (i) _Blur Compressed)( ImageN\n \n\u2211\n=+ + =N\ni11)(N (i) (1) Image)( _Blur Compressed Image      ( 2) \n \nWhere the (+) is som e appropriate addition of the im ages i n equation (2), reversing \nthe opera tion (-).  The s ame operation is im plied in the sum mation sym bol.  In G IMP \nwe have use d the rev erse of \u201cGrain Extrac t\u201d which is \u201cG rain Merge\u201d. \n \nNow, since the above equation reverses the entire process, the original im age is  \nreconstructed flawlessly.  The com pressed bl ur im ages can  be as  small as  desired,  \nusing the  JPEG operatio n com pression ratio/qua lity to de rive an acc eptable size f ile \nfor the \u201clay er\u201d gener ated.  Each com pressed blurred im age w ill be ref erred to f rom \nhere on as a layer, while their collection along  with the f inal base  image w ill be \nreferred to as the  Gaussian Blu rring deco mposition  stack.  The prob lem arises w ith \nthe ba se im age w hich is still in unc ompressed form , and any lossy  com pression on it \nwill redu ce the quality of  the recon structed im age.  It is no ted that if  one is  working in  \nraw format (e.g. TIFF) the base im age w ill be of  the sam e size as the o rigina l image \nand hence storing it as is would defy th e purpose of com pression; for analysis \npurposes though (see later discussion) this m ight be desired. \n \nTo control this aspect of loss in reconstruction, two re medies are possible at least  \n(when signif icant overall com pression is the aim ): \n \n\u2022 Increase the num ber of N compressed blurred layers, by reducing the factor of \nblurring coefficient, so as to produce more closely packed compressed blurred \nimages, and \n\u2022 Com press th e final b ase im age but w ith a higher quality comp ression  so as to  \nreduce artef acts introdu ced in it, if J PEG is used for it as well for exam ple. \n \n \n2. General signal compression wi th the \u201cPerceptron Algorithm\u201d \n \nQuite clearly, the ideas above can be used to compress any ki nd of signal data series.  \nFor exam ple, im ages can be viewed as tw o-dim ensional collections of inform ation, \nwhile for exam ple audio files can  be viewed as one-dim ensional collections o f \ninformation w ith x-coe fficient the tim e axis .  A similar schem a to equations (1)  is \noutlin ed for CD audio f iles (gener ic term  used here) and the a pplica tion f or exam ple of \nMP3 com pression for the com pression of the blurred  signal.  Gaussian blurring, as \nPage 3 of 32 V. S. Vassiliadis  The Perceptron Algorithm \ndiffusion of datum  point values, can be applie d in the sam e fashion as in equations (1) \nto the aud io data se ries: \n \n \nN i ,,...2,1_Blur Compressed)( CDaudio CDaudio); Blur n( Compressio _Blur Compressed); icient blur_coeff, noise, CDaudio lurring( Gaussian_B Blur\n(i) (i) 1)(i(i) (i)(i) (i) (i) (i)\n=\u2212 ===\n+  (1) \n \nFor Com pressed_Blur(.) function we could us e here MP3 low-sam pling com pression.   \nThe reconstruction of the signal w ould re quire the sim ultaneous un-compression of \nthe low grade MP3 files in the stack, along with that of the base file as well, so as to \nreconstruct point-by-point the original sign al values.  Of course, again  the issue o f \nappropriate com pression of the base file arises , but this m ay be controlled in the same  \nway as suggested in the discussion for im age reconstruction above.  The rem ainder of \nthe paper f ocuses exclu sively on im age com pression and an alysis.  Futu re work w ill \nfocus also on the general prop erties  of Gaussian Blurring  and analy sis for general typ e \nsignals. \n \n3. Example case studies \n \nTwo case studies are presented in this se ction highlighting the observed properties \nwhen the algorithm  is applied to re al world im ages.  The first case s tudy shows the \ndecom position stack and the resulting rec onstruction of the im age in two ways: \nbottom  up, and top down, for  an original im age which is noise-free at larg e.  The \nsecond application works on a noisy original  image, which shows how the algorithm \nseem s to ac cumulate all noisy elements in the second half (bottom  half)  of the stack, \nlending itself potentially as an ideal noise  reduction schem e where noise is reduced \npredom inantly in th e lay ers where it is accum ulated. \n \n3.1 Decomposition o f a noise-free image \n \nThis im age was produced in 16-bit col our depth TIFF from CANON EOS20D,  by \nconverting it from  its RAW  format (the CANON RAW is 12-bit co lour depth).  T he \nTIFF was mapped to 8-bit colour depth as use of JPEG compression s restricts colou r \ndepth to 8-bit. \n \nThe f irst sh own below  in Table 1 is  the decom position of  the origina l image into the \nPerceptron Algorithm  stack. \nPage 4 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nTable 1: Decomposition of a noise-free image \n \n \nBlur 1: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 250 \nfile size = 7 2.2 Kbytes \n \nIt is no ted that the first blurred layer of \nthe Per ceptron A lgorithm effectively \nremoves all the colour from the im age, \nwhile a ll remaining im ages in  the stack  \nbecom e increasing ly greyer.  \n \n \nBlur 2: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 250 \nfile size = 6 5.5 Kbytes \n  \n \n \nPage 5 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nBlur 3: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 250 \nfile size = 6 5.7 Kbytes \n  \n \n \nBlur 4: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 150 \nfile size = 7 3.6 Kbytes \n \nThis blur layer begins to show the thick \nedges detail, e.g. the bars in the window \nand the thick branches  of the plan t.  This \ncontinues for layers Blur 5 and Blur 6.  \n \n \nBlur 5: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 100 \nfile size = 8 9.6 Kbytes \n  \n \n \nPage 6 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nBlur 6: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 70 \nfile size = 8 6.5 Kbytes \n  \n \n \nBlur 7: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 50 \nfile size = 8 4.1 Kbytes \n \n \nThe edges shown here begin to indicate \nfiner detail both around the bars of the \nwindow as well as the outline of the \nleaves of the tree and finer bran ches.   \n \n \nBlur 8: \n \nParam eters in pixel va lues: \nspread = 30, diffusion = 30 \nfile size = 115.0 Kbytes \n \nThis level of the decom position along \nwith Blur 9 and Blur  10 begin to focus on \nfiner edge details.  \n \n \nPage 7 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nBlur 9: \n \nParam eters in pixel va lues: \nspread = 20, diffusion = 20 \nfile size = 116.0 Kbytes \n  \n \n \nBlur 10: \n \nParam eters in pixel va lues: \nspread = 10, diffusion = 15 \nfile size = 308.0 Kbytes \n \nThis blur layer is dom inated by very fine  \ndetail edges.   Below  the im age is copied  \nand enhanced in contrast to highlight this \nfeature. \n \n \n  \n \n \nPage 8 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nBlur 11: \n \nParam eters in pixel va lues: \nspread = 5, diffusion = 7 \nfile size = 174.0 Kbytes \n \nThis blur layer shows the edges \nbeginning to show halos as the \ninformation dif fuses into  the  grey  \nbackground.  \n \n \nBlur 12: \n \nParam eters in pixel va lues: \nspread = 3, diffusion = 5 \nfile size = 152.0 Kbytes \n \nFrom  this level of blurring, and up to \nBlur level 13 and the Base im age the \ninform ation becom es increas ingly  \ndiffused, to the point of becoming a \n\u201cwhisper\u201d above the 50% grey level.  \n \n \nBlur 13: \n \nParam eters in pixel va lues: \nspread = 2, diffusion = 3 \nfile size = 262.0 Kbytes \n  \n \n \nPage 9 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nBase: \n \nfile size = 1.49 Mbytes \nJPEG quality 95% in GIMP \n  \n \n \n \nThe original TIFF file size was 23.4 Mbytes, the com pressed stack as in Table 1 is of \ntotal size 3.12 Mbytes.  The com pression ratio here is 7.5 tim es.  The original im age, \nthe reconstructed im age, and their di fference is shown in Table 2 below. \n \nPage 10 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nTable 2. Image comparisons for Case 1. \n \n \nOrigina l image  \n \n \nReconstructed im age from  the stack \nshown in Table 1  \n \n \nPage 11 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nDifference im age  \n \n \n \nUpon close exam ination the difference im age is not com pletely black, as expected, \nbecause the base im age has been com pressed using the JPEG algorithm  which is \nlossy.   \n \nAn idea to o vercom e this problem  is to f urther store ano ther sm aller stack of Gaussian \nBlur decom position  working on th e difference of  the recon structed im age f rom the \noriginal TIF F image.  The secondary stack can be sm all, and can be used to further  \nadd correction to the reconstructed im age. \n \nIt is a lso n oted th at th e recon structed image is m ore equalis ed in  contr ast and \nexposure, which is the result of the blurring ope rations.  It is also noted that there is \nsome over-sharpening, with som e halos ap pearing around prom inent edges.  This is \nthe result of the com pression of the fina l base im age, whic h no m atter how alm ost \ngrey it is, it still retains enough inform ation it appears to influence the final result. \n \nFinally,  the size  of the interm ediate  blur le vels was not reduced by as much as it is \npossible to achieve as this was the very  first experim ent conducted with the new \nalgorithm  in order to gain understanding of its operation.  It would be possible to \nreduce m ore dram atically the size of the bl urred JPEG files, as they are of no \nimportant significance in them selves indivi dually, e.g. by up to 2-4 tim es, thus  \ngaining overall com pression ratios of  15 to 30 ti mes.  This is dem onstrated in the n ext \ncase study. \n \nThe reconstruction of the im age can be done in any desirable order, top to bottom , \nbottom  to top, or in a random  fashion.  Be low, in Table 3, the reconstruction from \nbottom  to top is shown, while Table 4 show s the reconstruction from  top to bottom . \n \nPage 12 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nTable 3. Reconstruction bottom to top (vie wed left to right, top to bottom, i. e. \nrow-wise). \n \n \n  \n  \n  \n \n \n  \n  \n  \n \n \n  \n  \n  \n \n \n  \n \n   \n \nPage 13 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nTable 4. Reconstruction top to bottom (v iewed left to right, top to bottom, i. e. \nrow-wise). \n \n \n  \n  \n  \n \n \n  \n  \n  \n \n \n  \n  \n  \n \n \n  \n \n  \n  \n \n \n \nThe top to  bottom reconstruc tion sta rts creating the de tail of  the image and adds th e \ncolour finally to im ages that are focused al most  all the way (also revealing \nPage 14 of 32 V. S. Vassiliadis  The Perceptron Algorithm \ninform ation at each lev el). The top to bo ttom proceeds by unfocused images in full \ncolour that are progressively becom ing m ore focused.  Observing the latter sequence  \nas it unfolds one has the feeli ng of moving from  outside into the im age, i.e. it creates a \nstereo scopic percep tion of the im age as it focuses. \n \n3.2 Decomposition of a noisy image \n \nThis im age was produced in 16-bit colour depth TIFF from  scanning of a printed \nphotograph. The TIFF was m apped to 8-bit co lour depth as use of JPEG com pressions \nrestricts colour depth to 8-bit.  Table 5 shows its decom position. \n \nTable 5. De composition of a noisy image (vie wed top to bottom, rig ht to left, i. e. \ncolumn-w ise). \n \n \nBlur 1: Diffusion 1000 \nFile size: 15 Kbytes \n \nBlur 8: Diffusion 16 \nFile size: 15 Kbytes \n \n \nBlur 2: Diffusion 500 \nFile size: 14 .8 Kbytes \n \nBlur 9: Diffusion 8 \nFile size: 15 .6 Kbytes \n \n \nBlur 3: Diffusion 250 \nFile size: 14 .9 Kbytes \n \nBlur 10: Diffusion 4 \nFile size: 15 .7 Kbytes \n \n \nBlur 5: Diffusion 125 \nFile size: 15 Kbytes \n \nBlur 11: Diffusion 2 \nFile size: 17 .1 Kbytes \n \nPage 15 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n \nBlur 6: Diffusion 60 \nFile size: 14 .9 Kbytes \n \nBlur 12: Diffusion 1 \nFile size: 16 .4 Kbytes \n \n \nBlur 7: Diffusion 30 \nFile size: 15 Kbytes \n  \nBase: JPEG  quality 50%  \nFile size: 229 Kbytes \n \n \n \nThe original im age had a size of 5.99 Mbytes in TIFF for mat, the total G aussian Blur \ndecom position stack presented has a size of  0.399 Mbytes.  The com pression factor \nachieved is 15.  The major obstacle in achieving better compression was the base \nimage.  As t he image originally was very  noisy (degraded negative film  of 1000 ISO) \na lot of noise has accumulated in the base image.  Noise reduction techniques can be \nemployed on the noisiest levels (from levels 7-8 downwards) so  as to further assis t in \nthe com pression of the base im age, and to achieve far better com pressions even i n \nsuch noisy cases. \n \nSome smoothing (further blurri ng of levels 9 through to 12, with m ild param eters of \n10 pixels dif fusion, applied on the decom posed im ages) was used as an experim ental \napproach to reduce the n oise of the reconstr ucted image.  The results alo ng with the \noriginal, reconstructed, di fference and noise reduced im ages are show n in Table 6 \nbelow. \n \nAnother obs ervation abo ut the nature of the noise accum ulated in  lower levels of th e \ndecom position and the final base im age, is that the noise is not in grey m ode, but \nrathe r it is R GB (colour speckles ).  Perhaps turning the base into greyscale could also \nhelp in redu cing the p erceived no ise in the recon structed im age. \nPage 16 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nTable 6. Comparisons in the recon struction of a noisy ima ge. \n \n \nOrigina l image \n \nIt is no ted that the h ighlights of  the image, predom inantly at the roof  tiles and  \nthe bench  are \u201cblown\u201d (s aturated),  with also som e specular highlights  blown on \nthe gras s area \n \n \n \n \nReconstructed im age \n \nThe reconstructed im age is al most identical to the original, w ith equal amount of \nnoise pe rceptually.  It is  notable though that a differen ce he re is th at the blown \nhighlights have been \u201cequalised\u201d by a ssigning them  the colou r tha t is \nrepresentative of nearby regions, nam ely the blown highlights in the tiles have \nturned reddish which was the approxim ate colour of the origin al scene.  Of \ncourse th is constitute s dist ortion of an original i mage, but then again, blown \nhighligh ts are considered  to be \u201cdead \u201d pixels w ith no inf ormation con tent in th e \nfirst place. \n \n \n \nPage 17 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n \nDifference im age \n \nThe difference is  exclusively lo cated  in the blow n highlight s region. The region \ncontains som e infor mation in term s of c ontrast differences that constitute the \nedges of the tiles and the bench whic h are s omewhat \u201cwashed out\u201d in th e \nreconstructed im age.  A finer tuning, in an iterative pred ictor-correcto r mode, in \nthe decom position could preserve  that detail if required. \n \n \n \nPage 18 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n \nNoise reduced im age \n \nTo reduce the noise in the reconstru cted im age, the layers num bered 9, 11, and \n12 were blurred  with a Gaussian  diffusion coe fficient of 10 pixels in the \ndecom position.  Layer 10 was not blurred as it was found t o contain vital edge \ninform ation although it was also relatively noi sy.  Further to this, the base im age \nwas converted into greyscale to remove  com pletely the RGB speckles observed \nthere.   It was then  blurred with a G aussian  diffusion coefficient com parable to \nthe observed noise, set at 3 pixels. The base from  its orig inal un compressed \nTIFF stored  separately,  was com pressed us ing JPEG at quality of 50% resulting \nin a file size of 101 Kbyt es. \n \nThe overall enhanced im age decompos ition (accounting only the difference in \nthe new g rey base im age compression  dif ference f rom the o rigina lly \nreconstructed im age) yields a total decom position/com pression size of 0.271 \nMbytes.  The corresponding com pression fact or from  the original T IFF im age is  \n22.1, without signifi cant loss of quality due to the dominant noisy e lement in th e \norigin al im age.   \n \nTo further enhance the im age, it is possi ble to strengthen the contribution of the  \ndecom position layers carrying edge inform ation, after som e more sophisticated \nnoise reduction algorithm is applied to them  (perhaps in greyscale variants of \nthe layers).  This will have to be don e prior to compression of the blurred layer \ninto lower quality JPEG for mat.  In e ffect, the proposed procedure is a m ore \nsophisticated extension of th e \u201cUnsharp Mask\u201d technique. \n \nWhere absolute quality of the im age is  param ount and no com pression is of \ninterest, the n the blu rred layer s can be stored in raw form at (uncom pressed) so \nas to allow  future spec ialised algor ithms to focus on the im age enhancement and \nanalys is asp ects offered by the Perceptron Algorithm . \n \n \n \n \nPage 19 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n4. Multichannel image deco mposition and related considerations \n \nMost im ages of  interest in the m odern digital m edium  are com posites, i.e. they resu lt \nfrom  the superposition of si milarly capture d im ages, either in the sam e window of \nfrequency sam pling, or in fact resulting from the superposition of different spectral \nwindows\u2019 sampling, such as RGB i mages obt ained with optical capture devices.  The \ndiscussion is by no m eans exclusive to im ages, and can be generalised to any such \nform of signals.  To hig hlight the is sues of  com posite s ignals w e focus in this sec tion \nexclus ively on RGB photograph ic images. \n \nThe f irst o bservation is rela ted to  the Perceptron Algorithm  application in the 3 \nchannels separately.  As different densitie s of infor mation are captu red in the three \nchannels, it stands to  reas on that a further improvem ent in the efficiency of the \nalgorithm , both for com pression  and image anal ysis, would be to apply it separately to \neach chann el thus  treating the R-G-B channels  as three s eparate im ages (which they \nare). \n \nFor digital cam eras, the most favourite (and ch eapest) way to construct a sensor is the \nBayer arr ay, where R, G , B, sensitive pixe ls are arr ayed in a checker board f ashion.  \nThere are twice as m any green p ixels as  ther e are individually red or blue, such that \neach red or blue sen sor pixel is surrounded by 4 green  ones.   The co lour distribu tion \nto all pixels is conducted via som e form of appropriate inter polation over the whole \narray thus yielding a full resolution in RGB over the whole sensor m atrix which is in \neffect interp olated,  except for th e channel value where th ere is an  actual R-G-B pixel \nsensor sam pling the colour lum inance in that location.  It is  possi ble to envisage that  \nthe Percep tron Algorith m can serve as a superi or inte rpola tor if the pix el value s are \nnot interpolated but create a spars e image for the R, G, and B channels s eparately.   \n \nThe G aussian Blurr ing operations w ill sp read the valu es over the en tire matrix if \napplied  to each chann el sepa rately and th en the three (potentia lly also  much m ore \nefficient decom positions) are superimposed wh en the im age is to be com posed as a \nwhole.  This m ight also allow a m uch m ore significant compression  of the im age at \nthe source (i.e. the cam era, w hich w ith a lit tle m ore proces sing capability w ill stor e a \nsmoother im age, w ith significantly reduced s ize in a \u201closs less\u201d fashion on  the m emory \ncard, whose capacity is u sed in  the most efficient m anner). \n \nA final issue discussed in this section, con cerns the RGB distributi on of values for the \nPerceptron Algorithm .  Som e colour sepa ration is observed as the layers are \ndecom posed, and it was m entioned that the very  first layer s eems to take  away all of  \nthe co lour from  the im age, yielding  mostly grey images in later  layers.  We turn ou r \nattention here on the final base im ages as  produced by the algorithm .  Table 7 below \nshows som e data collected from  a few decom positions. \n \nPage 20 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nTable 6. RGB analysis information for base  images obtained w ith the Perceptro n \nAlgorithm \n \nBase Mixed image RGB information w ith \nhigh blur (1000 diffusion or more) RGB tonality \ndistributions \nPage 21 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nNoisy Image  \n \nImage source is \nan EPSON \nPRO \n35mm/plate n \nscanner. \n \n \nThe blended  image average is ind eed a \n50% grey im age, as shown in the table \nabove. \n \nHowever, the histogram s to the right \nindicate th at the spread o f effectively \nwhat is nois e in the b ase image is \nsimilar f or the G  and B channels, bu t it \nis more spread for the R channel. \n \nIn a way this shows that noise \nalthough m ostly attributed to the B \nchannel, in s ome cases it can m anifest \nitself in any of the other ones. \n \nAlso noise in the over all image is \nshown in the overall value \n(lum inance) channel on the top in the \nright.  The s pread is centred at an \naverage v alue which is s hifted slightly \nto the highlights.  This indicates that \napplication of the Perceptron \nAlgorithm  will make the base not g o \nmore grey, but as one new blur is \nsubtracted from  the working base it \nwill shif t the noise in o ther locations.  \nClearly such  cases call f or a dedicated \napplication of a noise reduction \nalgorithm  at each workin g base lay er \nas it is de tected to be no isy. \n \nFurtherm ore, an edge detection \nalgorithm  can be applied to a base \nimage to f ilter out poss ible edges.  \nNoise reduction m ethods applied to \nthe edges \u2019 image can remove noise \nand the greyness can be restored by \nsuitab le re-composition w ith the \norigin ally n oisy base im age. This m ay \nbe applied at other layers of the \ndecom position, obviously, to enhance \nboth clarity and allow futher deeper \ncompression without loss. \n \n \n \n \nPage 22 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nLow noise \nimage  \n \nImage source is \na CANON \nEOS20D 8.2 \nMpixel dSL R \ncamera. \n \n \nHere the overall lum inance \ndistribution is sim ilarly shifted to th e \nhighlights as the RGB individual \nchannels. \n \nThere is ine vitably som e noise in th e \nimage, but the shif t in th e base im age \ndistributions  is becaus e there ex ist still \nedges\u2019 inf ormation in the im age. \n \nInter estingly , there is s lightly m ore \nspread ag ain in the R (red) channel.  \nThis lead s one to cons ider whether the \nhypothesis f or considering the B \n(blue) chann el as the noisiest is \ncorrect. \n \n \n \n \n \n \n \nPage 23 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n6. Distance and siz e computation s based on focal and aperture \nblurring \n \nWhen taking a photograph, or  any other form  of im age, a focusing ap paratus  is used \nto obtain a sharply focused object onto the sensor (film , record ing media, even in \nother form s of signals such as sound). \n \nIn photography this is achieved by the com bination of the lens  and its aperture \nopening which is of varying diam eter.  The si ze of the object obtai ned is a function of \nthe focal len gth used and  the dis tance of the actua l objec t from the lens. \n \nThe blurring of objects in front and behind the object where we focus on depends on \nthe apertu re diam eter/o pening (iris) and th e lens com bination.  This blurring is in \neffect out of focus objects and the eff ect is ak in to Gaus sian diffusion.  \nDiagramm atically, shown as unequal cones of  focusing (in photography this is related  \nto the \u201chyp erfocals d istance\u201d and  related concepts and  phenom ena) is the s cene in \nfront of the object we focus on, the object itself and the s cene behind  it.  This is \nshown below in Figure 1 below. \n \n \n Focal plan eIris \nLens \nFigure 1.  Depth and size estim ations using blurring (out of focus regions) \n \nIt is proposed here that using the Perceptron Algorit hm (increm ental Gaussian \nBlurring) to produce depth inform ation and a sense of stereoscopic im age \nreconstruction; this can be coupled with the orientation of shadows manifested as \ncontrast regions identified in the decom position (m ention elsewhere in this paper). \n \nTo do this, w e need to c ombine the f ollowing inf ormation: \n \n(a) Focal dis tance Sensor \nScene in front blurred differentially to \nscene beh ind focal plan e Same amount of blur \nrectangle (at opposite \nsides) \nPage 24 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n(b) Aper ture size \n(c) Recorded ob ject size \n(d) Model the scene blurring: in fr ont and behind the focal plane \n(e) Apply the P erceptron Algorithm , observi ng that the m ost per sisten t edge s \nremaining at the base im age must be com ing from the most focused object , \nas its edg es will be the  most sharp ly def ined in the reco rded im age (or \nsignal, if equivalent principles ca n be found for any type of signal \nanalys is). \n \nA com bination with the orientation of sha dows in relation to the blurring m ight be \nable to p lace correctly parts  of the im age as in front or behind th e focal plane, thus \nproducing a stereoscopic analysis  of what is perceived as a flat im age to begin with.  \nNo illum ination w ill be perf ectly perpend icular, and r eflected light w ill cr eate \nshadows inevitably, thus it m ight be possible to exploit such prope rties to  recons truct \ndepth and size inform ation. \n \nThe w orking princip le is that blu rred parts of  an origina l image w ill blur f aster w hen \nGaussian blurring is applied and hence wi ll be subtracted  at earlier s tages of the \nPerceptron Algorithm .  Edge analys is of a top down recom posed im age at each stag e \nmay be able to identif y which edges are further from  the focal plane.  The a mount of \nGaussian blurring (diffusion coefficient) coul d be used as a m easure of distan ce from \nthe focal plane, along  with the oth er param eters of the im aging system  mentioned in \nthe list abov e. \n \nTo obtain more inform ation and be able to relate dif ferentially w hat is in f ront and \nwhat is b ehind, one m ay also consid er taking ph otos of  the s ame scene w ith m utliple \nfocal points (focusing at diffe rent depths and also com binining these with different \napertu re op enings per focal point, one fo r each  sho t), and  comparing th e \ndecom position of the images together. \n \nMotion also causes blurring such  that m oving objects will app ear b lurred in a specific \nway in the recorded im age.  Motion  analys is may com bine other form s of blurring, \nsuch as \u201cmotion blurring\u201d and \u201cradial blurring\u201d found in m ost standard im age \nmanipulation packages, to design algorit hms that analyse properties of m oving \ncaptured objects (e.g. es timate velocity, dire ction, recom pose their shap e by m oving \npixels \u201cback wards\u201d, etc.).   \n \nFinally, with the above observation, other fo rms of blurring appl ied to  still im ages \neither alone or in tandem  with each  other, could reveal oth er aspects  of infor mation \nimbedded in an im age.  For exam ple, if motion blurring is gene ralised  to inc lude a \nspherical angle orientation of the motion along with the perspective distortion when \nthis m otion is recorded on a two-dim ensional m edium , one could infer the spherical \nangle direction along w hich a recorded obj ect was m oving (in or out of the page,  \nalong with the angle of rotation on th e plane of the r ecording m edium ). \n \nFor exam ple, if the m otion is completely in the opposite directi on of  the spherical  \nmotion blurring analysis applie d (assum ing m otion in a stra ight line for the duration \nof the exposure), th en the object w ill be \u201c recomposed\u201d bringing the pixels back t o \nform sharp edges.  Then, if on top of this  Gaussian blurring anal ysis is applied it \nmight be possible to reconstruct inf ormation about its shape, si ze, and distance from \nthe observ ation point (cam era locatio n for exam ple). \n \nPage 25 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nIn a further extension, one m ight consider real-tim e tracking  system s that to  enhance \ntheir ability  to corre ct and perce ive motion utilise a com bination of  spheric al ang le \nblurring analysis along with  Gaussian blurring.  If two m oving im aging/sensor  \nsystem s are placed clos e to each  other then  their difference in dis tance from  each \nother can create a stereoscopi c depth perception along with  analysis of m otion and \ntrack ing.  T his might be the com bination of \u201chardware\u201d and \u201csof tware\u201d that m akes the \neyes (w ith the ability to vary  rapidly, at w ill, th e iris ape rture opening and  focal length \nof the lens) and the brain able to analyse such inform ation rapidly. \n \nSuch discussion is only presented here in broad term s, hoping it could form  the kernel \nfor future research in the field. \n \n \n6. Summary of observations and future ideas for research and \ndevelopment \n \nThe ideas sp ecifically are outlined in  summarised  form as: \n \n1. Digital image com pression w ithou t loss,  or with tunable loss depending on the \nprecis ion of  the reprod uction  device targeted.  The im age is d ecom posed in  a \nmathem atical-a lgorithm ic way, which is com putationa lly ch eap and sim ple, in to a \nmultilay ered array (e ither a stack or a tree s tructure) of  simpler im ages w hich can \nbe com pressed to the order of 10 K bytes e ach, indicatively.   Tunable los s can b e \nachieved by setting a thres hold tolerance for the differe nce of the reconstructed \nimage to that of the original, so that the algorithm m ay decelerate o r accelerate the \nreduction in  the blur co efficient so as to produce fewer or m ore layers in the \ndecom position to m atch the desired accuracy of reconstruction. \n \nThis idea can lead to an extens ion of  the JPEG  compression algor ithm to becom e \nmultilay ered to accom modate the  Gaussian Blurring s tack; it would also be  \ninteresting to see an ex tension of the JPEG protocol to  accommodate 16-bit colour \ndepth.  If the derived im age by any JPEG com pression uncom presses \nproportionately to the file size, then sin ce the operations of the reconstruction in \nthe Perceptron Algorithm are m ere additions  effectively, then since also the \nresulting siz e of the s tack is sm aller than 100%  JPEG quality , it stands  to reason  \nthat the new  com pression proposed would be fa ster in reconstructing compressed \nimages (and in general other types of co mpressed signals) than that  to p quality  \nJPEG  com pression r econstruc tion ( and potentia lly w ith m uch sm aller losses in \nquality). \n \n2. Dem onstrations are availab le where m anually an im age was com pressed from \nTIFF for mat of 23Mb (8-bit colour quan tisation, 8.2 Mpixels resolution) to \napproxim ately 30 tim es smaller size usi ng the standard ised image m anipulation  \nsoftware GIMP, produced by the GNU organisation.   \n \n3. Noise suppression and com pensation propertie s of the array of i mages, either in \ntransm ission , sto rage, o r when restoring o lder images.  The decom position -\ncompression m ethod assigns autom atically m ost of the noise of the original im age \ninto readily identif iable sub-laye rs of the decom posed im age, and in so doing it \nbecom es very easy to apply noise correction algorithm s only to the laye rs that a re \nautom atically identif ied as noisy.  Since ther e is an additiv e proper ty in the \nreconstruction of the im age from  the laye red stack  or tree-structure,  then the \nPage 26 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nnoise- reduce d or cor rected im age w ill not su ffer blurr ing or loss of  edge detail, to  \nthe extent th at ord inary m ethods will do when applied to the  entire original im age.   \n \nFor transm ission and storage erro rs, since th e image is deco mposed into a stack of \nlayers, if an error w ould occur at a ra ndom location in som e layer(s ) the c orrection \nwould com e from the additive r econstru ction by the superposition  of the \nremaining la yers.  In oth er words sin ce the to tal pixel lum inance and hue values \ncome from  the superposition of m any pixels  merged into a single pixe l, error in a  \nfew of them  can be com pensated  in this f ashion  quite ef fectively.  Fur ther to this, \ndue to the com pression capability of  the algorithm , if an im age is reduced in \nstorage size significan tly it can be affo rded to re-transm it it m ore than once  \ndepending o n the critica lity of  the a pplica tion ( e.g. transm ission f rom sate llites,  \nspace prob es, etc.). \n \n4. Exploiting the s moothing of the first layers  into highly blurred im ages a nd of the  \nlater laye rs where m ostly edges are predom inant, it m ight be possible to enlarge  \nsmaller im ages (in the  num ber of total pix els availab le) and obtain  a sim ilar \nquality higher resolution im age without jagg ed structures or ar tefacts appearing in \nthem .  This is likely to be so, becau se the blurred top layers carry little s tructural  \ninform ation so a sm ooth (e.g. cubic) inte rpolation m ight be very good for them .  \nSimilarly for \u201cedgy\u201d layers, in  the later stages, w hich are also alm ost 50% grey or \nnoisy in other areas, the edges can be en larged  relatively a ccura tely an d so the \noverall enlarged and recom posed i mage might be m uch smoother than other  \nmethods of enlargem ent working directly on the original im age.  In other words, a \ntunable layer-by-layer approach can pr oduce high quality enlargem ents.  Of \ncourse no new inform ation can be gene rated f rom a sm aller im age, but it is  \nexpected th at for repro duction purposes this m ethod will perform  very well.   \nCoupled with noise reduction properties, it  is possible to restore and re-exam ine \nolder im ages and film s. \n \n5. Structu ral analysis p roperties of imag es by either exam ination of decom posed \nlayers of the stack, or by partia l re-composition of the i mage.  Applicatio ns can be \nderived read ily for Mag netic Reson ance Im aging and CAT- scan im ages (m edical \nand general analytical im aging in general) which (a) can be com pressed \ndram atically  to facilitate  transm ission and storag e of medical record s, (b) can be \nanalysed au tomatically to identify st ructural problem s for  exam ple in m any \nmedical diagnostic situati ons (see below: pattern r ecognition applications). \n \n6. Exploitation of super-com pressed im ages to reduce storage needs by orders of  \nmagnitude, and to reduce transm ission speeds over wireless or cabled \ntransm ission s with applic ations in: \n \na. Internet decongestion by orders of  magnitude in the transm ission of \ndigital im ages and scanned docum ents. \nb. DVD & film  storage co mpression by orders of m agnitude without \nfurther im provem ent or dem and on s torage m edia. \nc. Pay-per-v iew applica tions w here en tire movies in the new  compressed  \nformat can be so compressed s o as to facilitate rapid  and cheap \ntransm ission to a hom e basis deli very of film s (m ovies) by cable  \ncompanies. \nd. Com pression of im ages at the so urce, for exam ple photographic, \nscanning, and im aging devices (cam eras, scanners, m edical im aging \nPage 27 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nsystem s, scientific im aging and not onl y restricted of course to visible \nlight im aging system s). \n \n7. Parallelisation schem es for the acceleration of th e com pression \n \ne. Splitting of the im age in parts, with overlapping or non-overlapping \nsegm ents (which can be autom atically  iden tified, depending on the \nneeds and seriousness of the applica tion) so as  to ass ign each sub-\nimage to a separate processor.   \nf. Adjustable, predic tor-corrector m ode of operation where portions of \nhigh detail require m ore analysis which is recognised autom atically, \nwhile others of s maller detail are analysed and com pressed m ore \nrapidly. \n \n8. Parallelisation schem es for the acceleration of th e recons truction \n \ng. In reverse order of items 5a & 5b above, the reconstruction can be \nproduced b y a parallel com puting system  which exploits the \ndecom position of an im age in subparts. \n \n9. Face recog nition and scene recog nition (p attern re cognition in gene ral), and  \naccelera ted databas e searching.   The decom position  of an  image in \u201c layers \u201d of \nvarying  com plexity, ea ch of which is m uch smaller and sim pler when com pared to \nthe original im age, offers a unique a nd very novel way for pattern recognition \n(with app lications in security  cam eras, stream ing real-time video, \ncomputer/robotic vision applications, motion analysis, etc.).  The database  \nsearch ing schem e presented is  based on the following steps: \n \nh. Since the d ecom position can be s tandardised to produ ce in  the sam e \nway a com pressed deco mposition o f an i mage, these deco mpositions \ncan be searched horizontally (at the same level of decomposition layer)  \nwith respect to a reference photogra ph/im age decom posed in the sam e \nfashion.  Th e com parison can be simp le, such as sim ple su btrac tion of \nthe pixel values of the corresponding \u201clayer\u201d of stored images with the \ncorresponding \u201clayer\u201d of the reference image.  A statistical m easure of \nsimilarity can be thus derived re adily, and po tential m atches can be \nselec ted out of  the origin al larger set stored in the  databas e. \ni. Once one tier of the comparison for the entire contents of t he database \nis conducted, the com parison w ill co llect the likely hits and analyse  \nthose for the next laye r, doing so recursivel y until the num ber of \nmatches is narrowed down to a few,  or to none. \nj. The search  can be do ne in para llel, f or exa mple by sp litting the \ncontents of the database and assign ing them  to different processors  \nwhile giv ing to all of them  the sam e reference (decom posed and \ncompressed) im age to se arch for.  A coordination step is conducted at \nthe conclusion of all the processors tasks for the sam e depth they have \nbeen assign ed. \nk. It is noted  that becau se of the way the Perceptron Algorithm  is \nconducted slight variati on in angle of presenta tion, colour and fine \ndetail, will not affect much the pa ttern recognition capabilities of the \nalgor ithm.  W here m ore deta il w ill be inv ariably co mpared and \nsearch ed for, m inor corrections fo r skewness,  rotation, etc. can be \nPage 28 of 32 V. S. Vassiliadis  The Perceptron Algorithm \napplied to o vercom e these problem s that can b een serious issues in  \npattern m atching. \n \nThe idea is dem onstrated in Figure 2 below. \n \n \nPage 29 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n \n IMAGE DATABAS E \nImage1 Image2 Image3 Image\u2026 ImageN\nFigure 2.  Database searching for patter n recognition schem atic operation. Reference im age, even partially \ndecom posed due to blurry original \nconditions of capture, but \nstandard ised as m uch as possible in \nthe w ay the databas e is \nstandardise d.PROCESSOR  \n2 PROCESSOR  PROCESSOR  \n1 3 \nImage decomposition is distributed to \nparallel p rocessors w ho split the da tabase \nand search in their ass igned subsection to \nfind m atches horizontally at the assigned \nlevel f irst. O nce com pleted m atches are \nfurther se arched for the n ext lev el. \nPage 30 of 32 V. S. Vassiliadis  The Perceptron Algorithm \n10. Medical applications and ne ural computing.  Ideas regard ing this are also \nbeing cons idered f or future work, ind icating her e that the m otivation f or the \nPerceptron Algorithm  was derived fr om observations and conjectures \nregarding how m emory seems to operate in the hum an brain (as m entioned in \nthe Introduction section).  From  this th e algorithm presente d was derived, and \nbased on its successful dem onstration in this work,  it is f urther conje ctured \nthat novel advancem ents can be researched regarding both hum an \nneurophysiology as well as the design of new analogue or digital com puting \ndevices and algorithm s motivated by new insigh ts.  In a way,  the successful \nfew first applications of our algorithm may serve as verification of the ideas \non neural structures (e.g . the hippocambus) for mem ory and processing in the \nbiological brain. \n \n11.  Depth analy sis inform ation.  The inform ation obtained by the decom position \nof the Perceptron Algorithm  is possible to be related to contra st differences at \nvarious levels of blurring s ubtrac tions.  If  this in formation ca n be related to the \nrelative p lacem ent of obj ects in depth, in relation to each other,  then it is \npossible to conjecture that  appropriate research in to the m ethodology can be \nused to reconstruct and enhance stereoscopic inform ation obtained from  two-\ndimensional im ages alone.  Already, upon close exam ination of the \nreconstructed im ages as they evolv e and the blur levels, cr eates a feeling of \nrelative depth of the objects pe rceiv ed in the field  of view. \n \n6. Conclusions \n \nThe purpose of this paper is to serve as an initial exposure to  the ideas underlying a \nnovel and very sim ple algorithm  for im age com pression and analysis.  The key \nconcept is the use of iterative Gaussian Bl urring and subtraction of the blurred im age \nfrom  the working im age until a nearly 50 % grey im age is obtained as a bas e.  \nCom pared to other m ethods that may exist,  Gaussian Blurring is  a very fast, very \nsimple, and efficient technique  which has been shown to yield prom ising results in \nthis w ork. \n \nIt is believe d that the id eas outlin ed here can have im mediate prof itable results by the \napplication of the algorithm in the fields identified above, to begin with in im age and \nsignal com pression. \n \nFurther to  these, the new approach  is b elieved that it can create  a chain-reaction of \ninnovations in m any fields, w ithout causing technological pre ssure for m ore advanced \nequipm ent, although it is also believed that it will lead to such innovations as well.   \n \nAs a final exam ple one m ight consider that since th e algo rithm can res tore \nimperfections in an im age, w ithin r eason, it co uld reduce signif icantly the cost of  \nconstruction of digital cam era sensor arrays (not only restricted to  optical sensors of  \ncourse) by relaxing the tight hom ogeneity in  their sensors, perhaps even producing  \nacceptable enlarg ements by lower num ber of pixels, and even by not so \nhomogeneously arrayed arrangem ents of sensor pixels.  \n \nAfter all,  the eye itse lf has a f inite num ber of rods and con es with \u201cimperfection s\u201d in \ntheir placement, and yet when we view s cenes we do not perceive jagged edges or \nhomogeneity defects.  It is  the conjecture of the auth or that the  correc tion in \nperception is \u201csoftware\u201d related.  Perhap s a step towards unders tanding these issues \nPage 31 of 32 V. S. Vassiliadis  The Perceptron Algorithm \nmight be the w ork prese nted in th is paper, along  with the ab ility to recog nise and s tore \nin m emory thousands of different shapes a nd patterns, and to discern them  even in \nblurry conditions. \n \nReferen ces \n \nThe work here is prim arily experim ental an d orig inal, stemm ing from  work i n \nretouching photographs in GIMP and Adobe PhotoshopTM, prim arily e xtending th e \nstandard technique of \u201cUnsharp  Masks\u201d in an iterative m anner as used in sharpening \nand \u201cLocal Contrast En hancem ent\u201d of i mages.   \n \nIn brief bibliograph ical search es no sim ilar approaches with  such a wide breadth of \nrecognisable possible applications and conn ections to both software and technolog y \nhas been found, and to do so would have pr oduced an enormous a mount of citations \nto sim ilar and parallel approaches, w hich in our opinion have nonetheless never been \nput toge ther in the w ay attem pted in  this w ork; this by no m eans does n ot recogn ise \nthe work of num erous researchers in the fi eld.  Future research stemm ing from  this \npaper will provide links with other techniques in the field of im age, audio, and signal  \nprocessing. \n \nHence no references are used in this  work which rep resents mostly an experim ental \napplic ation of  well-esta blished and sim ple tools, availab le in m ost off-t he-shelf im age \nmanipulation and processing software. \n \n \nPage 32 of 32"}
{"category": "abstract", "text": "A novel algorithm  for tunable com pression to within the preci sion of reproduction \ntargets, or storage, is proposed.  Th e new algorithm  is term ed the \u201cPerceptron \nAlgorithm \u201d, which utilises sim ple existi ng con cepts in a novel way, has m ultiple  \nimmediate comm ercial a pplica tion aspects as  well as  it opens up a m ultitude of  fronts \nin com putational science and technology.  Th e aim s of this paper are to present the \nconcepts underlying the algorithm , observati ons by its application to som e example \ncases, and  the iden tification of a m ultitude of  potential are as of applications such as"}
{"category": "non-abstract", "text": "Face recognition, Principal Component Analysis, Log-Gabo r \ufb01lters,\nFacial expression\n1 Introduction\nFace recognitionis a di\ufb03cult problem, because the accuracy of reco gnition can\nbe a\ufb00ected by many factors: di\ufb00erent environment and lighting con ditions,\ndi\ufb00erent input devices and their parameters, changes of the face itself (due to\nthe change of expression, make-up, face rotation or aging). In t his article we\ninvestigate the problem of recognizing faces with di\ufb00erent facial ex pressions.\nEmail address: vperlib@mmlab.ktu.lt (Vytautas Perlibakas).\nPreprint submitted to 15 October 2018Traditional greyscale pattern -based recognition methods (like Pr incipal Com-\nponent Analysis (PCA) [1]) do not solve the problem of recognizing fac es with\ndi\ufb00erent expressions. This problem becomes obvious if we perform t raining\nand store in the database faces with one expression (for example, neutral) and\nthen need to recognize faces with another expression (for examp le, smiling).\nThe accuracy of recognition in such case is usually lower than the acc uracy\nof recognizing faces with the same expressions. In order to improv e the recog-\nnition accuracy of expression variant faces we can detect some fa cial features\n(for example, around lips, eyes, nose) and then compare these co rresponding\nfeatures of di\ufb00erent faces. Such feature detection is used in the Elastic Bunch\nGraphMatching (EBGM) andGabor wavelets -based facerecognitio n method\nthat was developed by Wiskott and his colleagues [2]. Penev and Atick [3 ] de-\nveloped Local Feature Analysis (LFA) -based face recognition algo rithm that\ndetects features (not necessarily related with anthropometrica l features like\neyes or lips) in any part of the face and uses them for recognition. E xpression\nvariant faces also can be compared by using optical \ufb02ow -based algo rithm [4],\nand the accuracy of recognition can be improved by appropriately w eighting\nimage pixels, regions or features that are more or less a\ufb00ected by t he changed\nexpression [5].\nIn this article we investigate how the accuracy of face recognition is a\ufb00ected\nby the change of facial expression and propose a masked log-Gabo r features\nand Principal Component Analysis -based method that could increas e the ac-\ncuracy of recognizing faces with di\ufb00erent expressions. Extensive recognition\nexperiments using a large database of facial images and comparison with the\nresults of other researchers showed that our method achieves v ery high recog-\nnition accuracy.\n2 Face recognition using masked log-Gabor features and PCA\n2.1 Feature extraction using log-Gabor \ufb01lters and sliding w indow algorithm\nFor face recognition we used greyscale facial images and performe d feature\nextraction by using the log-Gabor \ufb01lters that were proposed in [6] f or coding\nofnaturalimages. Theexperiments showed, thatthese \ufb01ltersar emoresuitable\nfor image coding than the traditional Gabor \ufb01lters. The log-Gabor \ufb01 lter in the\nfrequency domain and polar coordinates can be calculated using the following\nequation [7]:\nG(f,\u03b8) = exp/parenleftBigg\n\u2212ln2(f/f0)\n2\u00b7ln2(k/f0)/parenrightBigg\n\u00b7exp/parenleftBigg\n\u2212(\u03b8\u2212\u03b8o)\n2\u00b7\u03c32\n\u03b8/parenrightBigg\n(1)\n2heref0is the centre frequency of the \ufb01lter, kdetermines the bandwidth of\nthe \ufb01lter, \u03b8ois the orientation angle of the \ufb01lter, and \u03c3\u03b8=\u25b3\u03b8/s\u03b8wheres\u03b8-\nscaling factor, \u25b3\u03b8- orientation spacing between \ufb01lters. For face recognition\nwe generated multiple log-Gabor \ufb01lters Gno,nsof di\ufb00erent orientations noand\nscalesnsusingthefollowingrelationships: f0= 1/\u03bb,\u03bb=\u03bb0\u00b7s(ns\u22121)\n\u03bb,k/f0=\u03c3f,\n\u03c3f(\u03b2) = exp(\u22120.25\u03b2/radicalBig\n2\u00b7ln(2)) [8], ns= 1,...,Ns;\u03b8o=\u03c0(no\u22121)/No,\u25b3\u03b8=\n\u03c0/No,no= 1,...,No. Here\u03bb0is the wavelength of the smallest scale \ufb01lter,\ns\u03bbis the scaling factor between successive \ufb01lter scales, \u03b2is the bandwidth\nof the \ufb01lter in octaves, Nsis the number of scales, and Nois the number of\norientations. Using a chosen two-dimensional log-Gabor\ufb01lter Gno,nsin Fourier\nspace we perform \ufb01ltering (convolution), magnitude calculation and masking:\nVno,ns=abs(IFFT2(Gno,ns.\u2217FFT2(I))).\u2217mask, (2)\nhere \u201d.\u2217\u201d - array multiplication, I- normalized (derotated, cropped, resized,\nmasked) face image, Gno,ns- log-Gabor \ufb01lter of a chosen orientation and scale\nin Fourier space (the size of the \ufb01lter array is the same as the size of the\ntwo-dimensional image I),FFT2 - two-dimensional Fast Fourier Transform,\nIFFT2 - inverse FFT2,mask- binary mask for masking log-Gabor magni-\ntude image (the same as is used for masking greyscale face image Iin order to\nleave only the internal part of the face), Vno,ns- masked log-Gabor magnitude\nimage. It must be noted that for the selected image size we can pre- calculate\nthe log-Gabor \ufb01lters (of di\ufb00erent sizes and orientations) only once and store.\nAfter image \ufb01ltering with multiple log-Gabor\ufb01lters ( Nsscales and Noorienta-\ntions) we get a very large number of log-Gabor features (magnitud e values in\nallNs\u00b7Nomagnitude images as it is shown in Fig. 2). The size of each magni-\ntude image Vis the same as the size of facial image I. In order to reduce the\nnumber of features we use sliding window algorithm that is illustrated in Fig.\n1. Rectangular window of a chosen size is slided over the magnitude ima ge\nVno,1(scalens= 1) using a chosen sliding steps. In each window we \ufb01nd one\nmaximal magnitude value and remember the location (coordinates in im age\nVno,1) of this value. Features at all other scales ns= 2,...,Nsof the same ori-\nentation noare extracted at the same locations without using sliding window\nas it is shown in Fig. 1. The same feature\u2019s \ufb01nding procedure is repeat ed for\nallNoorientations. When we perform face recognition, the log-Gabor fe atures\n(found using sliding window) for each image from the database of fac es are\ncalculatedonlyonceandstored. Ifthemagnitudeimageismasked, w eperform\nsearch only in an unmasked image part. Then all extracted log-Gabo r features\n(magnitude values) are stored in a one-dimensional vector Xand passed to\nthe Principal Component Analysis -based recognition method.\nIn order to increase the accuracy of recognizing expression varia nt faces we\nused masking of log-Gabor magnitude images. Using masks we eliminate d the\nregions of magnitudes that were mostly a\ufb00ected by changed facial expressions.\n3VSliding\u25a1window\nVertical\u25a1sliding\nstep\nVertical\u25a1overlapThe\u25a1same\nlocations\nfor\u25a1all\u25a1other\nscalesn\u25a1=1,...,N\u25a1;o o n =1s n\u25a1;\u25a1n =2,...,No s sV\nFound\u25a1locations\nof\u25a1maximal\nmagnitudes\nat\u25a1n =1\u25a1scale\u25a1and\nn\u25a1\u25a1orientations\no\nFig. 1. Selection of Log-Gabor magnitude features using sli ding window algorithm.\nn\u25a1=1on =1s n =2s n =3s n =4s\nn\u25a1=2o\nn\u25a1=3o\nn\u25a1=4o\nn\u25a1=5o\nn\u25a1=6o\n.\u25a1.\u25a1.\n.\u25a1.\u25a1.\n.\u25a1.\u25a1.Calculation\u25a1of\nvariance image \u2019s\nAveragingCalculation\u25a1of\nvariance image \u2019s\nThresholdingAverage\nmage\nof\nvariancesi Binary\nmaskImage (1)\nof\nvariancesImage (P)\nof\nvariancesPerson\u2019s\u25a11\nimagesPerson\u2019s\nimagesP\nFig. 2. Log-gabor magnitude images of\nNo= 6 orientations and Ns= 4 scales\n(dark regions mean high magnitudes).Fig. 3. Calculation of variances and mask-\ning images.\nThese changes were evaluated by calculating magnitude variances b etween im-\nageswithdi\ufb00erent expressions ofthesameperson. Varianceimage sofdi\ufb00erent\npersons were averaged and thresholded (by average). After th resholding ob-\ntained binary image was used for masking. For calculation of variance images\nand masks we use not greyscale images, but the log-Gabor magnitud e images\nat\ufb01rstscale.Wecalculatemasksforeach\ufb01lterorientation.Masks werecreated\nfor scales ns= 1 and then used for all scales ns= 1,...,Nsof the same orien-\ntation. The number of masks is the same as the number of orientatio ns. Mask\ncalculation procedure is illustrated in Fig. 3. Masking of log-Gabor mag nitude\n4images is illustrated in Fig. 4. After masking we use sliding window algorith m\nand \ufb01nd the log-Gabor features in an unmasked parts of these mas ked images.\nFound features are used for PCA -based recognition.\nn\u25a1=1oLog-Gabor\u25a1(LG)\nmagnitude\u25a1images\nt\u25a1scale\u25a1n =1asExpression\nvariances\n(a) (b) (c) (d) (e) (f)OrientationsM\n(thresholded\nvariances)asks Masked LG\n(MLG)\nmagnitudesLocations\nof\u25a1MLG\nfeaturesLocations\nof\u25a1LG\nfeatures\nn\u25a1=2o\nn\u25a1=3o\nn\u25a1=4o\nn\u25a1=5o\nn\u25a1=6o\nSliding\nwindow\nalgorithmMasking\nof LG\nmagnitudes\nFig. 4. Features masking procedure: (a) Log-Gabor (LG) magn itude images at scale\nns= 1; (b) Pre-calculated variance images (dark regions mean h igh variances); (c)\nFeature masks (expression masks) that were calculated by th resholding variance im-\nages at average levels; (d) Masked log-Gabor (MLG) magnitud e images; (e) MLG\nfeature locations that are found using sliding window algor ithm (locations are de-\nnoted by black points); (f) LG feature locations that are fou nd if we omit feature\u2019s\nmasking steps (b)-(d).\n2.2 Face recognition using PCA of log-Gabor features\nIn this section we will present the main formulas of the Principal Comp onent\nAnalysis (PCA) -based recognition method. More information about PCA\ncould be found in [9].\nIn order to perform PCA, we calculate the covariance matrix C=1\nrr/summationtext\nj=1djdT\nj,\nhere centred data vector dj=Xj\u2212m, mean vector m=1\nrr/summationtext\nj=1Xj, data vector\nX= (x1,x2,...,xN)T, the number of data values is N, the number of data vec-\ntorsisr.Then we\ufb01ndtheeigenvectors ukandeigenvalues \u03bbkofthiscovariance\nmatrix:Cuk=\u03bbkuk. Eigenvectors that correspond to the largest eigenvalues\nare used to construct the transformation matrix T. This matrix is used to\ntransform any data vector Xto the PCA space: Y= \u039b\u22121/2T(X\u2212m),here\n5\u039b is a diagonal matrix (of the corresponding eigenvalues) that perf orms data\n\u201dwhitening\u201d [10].\nFace recognition is performed by comparing feature vectors Yof di\ufb00erent\nfacial images. For comparison we used the cosine -based distance m easure\nd=\u2212cos(Y1,Y2), hereY1andY2are PCA feature vectors of the compared\nfacial images X1andX2. Unknown faces are rejected by using some manu-\nally chosen distance\u2019s threshold. When we perform traditional PCA - based\nface recognition, data vector Xcontains greyscale values of the image pixels.\nWhen we perform log-Gabor PCA -based recognition, this vector co ntains the\nselected log-Gabor magnitude values (using expression masking and sliding\nwindow -based algorithm).\n3 Face recognition experiments and results\n3.1 Recognition performance measures\nFor comparison of face recognition methods we used Cumulative Mat ch Char-\nacteristic (CMC) and Receiver Operating Characteristic (ROC) - ba sed mea-\nsures that we also used in [11]. We used the following measures: 1) Firs t one\nrecognition rate (First 1, [0 ,100%]). For example, First 1 = 70% means that\nin 70 % cases the persons (that we are looking for in the database) w ill be\nidenti\ufb01ed correctly by extracting from the database one most simila r photog-\nraphy. Larger First 1 values mean better result. 2) Rank (number ) of images\n(Cum, (0 ,100%]) that we must extract from the database in order to achieve\nsome cumulative recognition rate. For example, Cum(90%)=15% mea ns that\nwe need to extract 15% of images from the database in order to ach ieve not\nsmaller than 90% cumulative recognition rate. That is, the faces tha t we look\nfor appear between extracted 15% of images (not necessarily \ufb01rs t one) in 90%\ncases. Smaller ranks (at the same recognition rate) mean better r esult. 3)\nThe area (CMCA, [0 ,1004]) above Cumulative Match Characteristic (CMC)\nthat measures overall identi\ufb01cation accuracy. Smaller CMCA means better\nresult. 4) The area (ROCA, [0 ,1004]) below Receiver Operating Characteris-\ntic (ROC) that measures overall veri\ufb01cation accuracy. Smaller ROC A means\nbetter result. 5) Equal Error Rate (EER, [0 ,100%]). For example, EER=20%\nmeans that FAR=FRR=EER=20%, in 20% cases a non authorized pers on\nwas accepted as authorized (False Acceptance Rate), and also in 2 0% cases\nan access was not granted (rejected) for an authorized person (False Rejection\nRate). Smaller EER means better result. Graphical representatio n of the used\ncharacteristics and measures is presented in Fig. 5.\n610 20 30 40 50 60 70 80 90102030405060708090100\n100\nRank,\u25a1%\n( ) a (\u25a1\u25a1) b100%\u25a1cumulative\u25a1rec.,\u25a1rank\u25a1=\u25a140%\nCum(100) = 40\nCMCA\nRecognition\u25a1rate,\u25a1%First\u25a11\u25a1recognition\u25a1=\u25a170%90%\u25a1cum.\u25a1rec.,\u25a1rank\u25a1=\u25a115%\nCum(90) = 15\n10 20 30 40 50 60 70 80 90102030405060708090100\n100\nFAR,\u25a1%EER\u25a1=\u25a120%\u25a1\u25a1(\u25a1FAR\u25a1=\u25a1FRR\u25a1)\nROCAFRR,\u25a1%\nFig. 5. Recognition performance characteristics: (a) Cumu lative Match Character-\nistic (CMC); (b) Receiver Operating Characteristic (ROC).\n3.2 Face normalization and feature extraction\nFor recognition we used greyscale facial images with manually selecte d centres\nof eyes and the tip of chin. Initial images were denoised using Gaussia n \ufb01lter\nwith\u03c3= 0.5 and window size 5x5 pixels. Then using manually selected points\nwe performed geometrical normalization. Faces were derotated ( in order to\nmake the line connecting eye centres horizontal), cropped, resize d (to the\nsize of 128x128 pixels), masked. For rotation and resizing we used b icubic\ninterpolation. For masking we used an ellipse with central point (64.5,4 5.5),\nhorizontal axis of 120 pixels, and vertical axis of 160 pixels. Then fo r an\nunmasked part of the image we performed histogram equalization (2 56 levels).\nImage normalization procedure is presented in Fig. 6. For illustration we used\nan image from our personal archive. The same face normalization pr ocedure\nwas used in all our recognition experiments.\nAfterimagemasking usingelliptical maskareleft12646unmasked pixe ls(grey\nvalue features) of 16384 (128x128). These grey value features are used for\ntraditional PCA -based recognition method. For log-Gabor PCA (LG PCA)\nwe leave (using sliding window) 19704 log-Gabor magnitude features. Slid-\ning window size is 4x4 pixels, sliding step is 4 pixels. For masked log-Gabor\nPCA (MLG PCA) we leave 15120 log-Gabor magnitude features (tota l at all\norientations and scales). Log-Gabor \ufb01lters were generated using the following\nparameters: \u03bb0= 5,s\u03bb= 1.6,\u03b2= 1,\u03c3f(\u03b2) = 0.745,Ns= 4,s\u03b8= 1.5,No= 6.\nLog-Gabormagnitude images were masked using the same elliptical ma sk that\nwas used for image normalization. For calculation of expression mask s we used\nfacial images of 117 persons from the AR [12] database (\ufb01rst sess ion). We\n7(a) (b) (c) (d)d20.4d20.9d10.9d1\nd1\nFig. 6. Image normalization: (a) Initial image with selecte d eyes and chin; (b) De-\nnoised, derotated image, and cropping schema; (c) Cropped, resized and masked\nimage; (d) Normalized image after histogram equalization.\nused facial images with three expressions (neutral, smiling, angry) per person.\nFace recognition experiments were performed using the following 3 m ethods:\n1) PCA with grey value images (notation \u201dPCA\u201d); 2) PCA with log-Gabo r\nfeatures and without expression masks (notation \u201dLG PCA\u201d); 3) P CA with\nmasked log-Gabor features (notation \u201dMLG PCA\u201d). All these meth ods used\nthe same normalized image patterns and the same implementation of P CA\nwith cosine -based distance measure between \u201dwhitened\u201d feature vectors. For\nface recognition experiments we used the AR [12] and FERET [13] dat abases.\n3.3 Recognition experiments using AR database\nThe AR face database [12] was created by A. Martinez and R. Benav ente at\nComputer Vision Center, Purdue University in 1998. This database c ontains\nfacial photographs of 126 persons with strictly controlled facial e xpressions\nand lighting. The size of images is 768x576 pixels. Images of each pers on were\ncaptured in two sessions that were separated by two weeks time. F rom this\ndatabase we selected images of 117 persons (64 men, 53 women) wit h the\nfollowing three controlled expressions: neutral expression (NE), smile (SM),\nanger (AN). Images with expression numbers 1, 2, 3 of the \ufb01rst se ssion we\ndenote by NE1, SM1, AN1, and images with expression numbers 14, 1 5, 16 of\nthe second session we denote by NE2, SM2, AN2. Facial images of th e \ufb01rst\nsession were used for training and registration (known faces) and for creation\nof variances -based expression masks. Facial images of the secon d session were\nused as an unknown faces that we wish to recognize. All images we co nverted\nto greyscale format. For recognition we used 100 PCA features.\n8At \ufb01rst we performed recognition experiments using NE1, SM1, AN1 images\nfor training and registration, and using NE2, SM2, AN2 as an unknow n faces.\nThe results of experiments showed (Table 1), that the best recog nition accu-\nracy is achieved if we can ensure that the expressions of training, r egistered,\nand unknown faces will be the same. Also we can see that the worst r ecog-\nnition accuracy is achieved in the following cases: 1. If we use smiling (S M1)\nfaces for training and registration, and then wish to recognize fac es with neu-\ntral (NE2) or angry (AN2) expressions; 2. If we use faces with ne utral (NE1)\nor angry (AN1) expressions for training and registration, and the n wish to\nrecognize faces with smiling expressions (SM2). In such hard cases LG PCA\nand MLG PCA methods allow to achieve 10-38% higher \ufb01rst one recogn ition\naccuracy than traditional PCA. In these cases MLG PCA achieves s lightly\nhigher recognition accuracy than LG PCA.\nAverage recognition results showed (Table 2, Fig. 7) that if we wish t o use\nthe same expressions for training and registration, and then reco gnize faces\nwith di\ufb00erent expressions, the best choice (with respect to \ufb01rst o ne recogni-\ntion accuracy) for training and registration are images with neutra l (NE1)\nor angry (AN1) expressions, and that we should not use faces with smiling\n(SM1) expressions. Also we can see that MLG PCA in most cases achie ves\nhigher average recognition accuracy than PCA and LG PCA.\n9Table 1. Recognition using AR database and the same expressi ons for training and registration (known faces).\nKnown Method Unknown faces\nfaces NE2 SM2 AN2\nFirst 1 CMCA ROCA |First 1 CMCA ROCA |First 1 CMCA ROCA\nNE1 PCA 94.87 161.44 63.01 68.38 264.08 169.25 80.34 228.29 123.36\nLG PCA 99.15 91.68 2.07 87.18 142.82 51.37 96.58 94.97 5.76\nMLG PCA 98.29 86.20 0.94 93.16 113.96 23.73 97.44 89.49 1.74\nSM1 PCA 63.25 321.79 231.94 96.58 89.85 2.47 45.30 618.01 528.01\nLG PCA 83.76 184.45 96.67 99.15 85.84 0.96 70.94 211.85 114.96\nMLG PCA 92.31 126.74 31.39 99.15 86.57 0.79 83.76 139.89 52.29\nAN1 PCA 87.18 226.82 131.45 54.70 738.91 642.74 94.87 112.50 17.58\nLG PCA 97.44 92.41 3.86 80.34 142.82 48.72 98.29 87.66 1.02\nMLG PCA 97.44 93.14 4.31 90.60 132.59 31.71 98.29 86.20 0.98\n10Table2. Average recognition results (of 3) if weusethesame expressionsfortraining\nand registration (known faces), but use di\ufb00erent expression s for recognition (NE2,\nSM2, AN2).\nKnown Method Average recognition results\nfaces Cum(100) First 1 CMCA ROCA EER\nNE1 PCA 57.55 81.20 217.94 118.54 4.56\nLG PCA 14.25 94.30 109.82 19.73 1.42\nMLG PCA 10.54 96.30 96.55 8.80 0.85\nSM1 PCA 40.46 68.38 343.22 254.14 6.84\nLG PCA 20.80 84.62 160.71 70.86 2.85\nMLG PCA 14.53 91.74 117.73 28.16 2.28\nAN1 PCA 62.68 78.92 359.41 263.92 6.55\nLG PCA 10.54 92.02 107.63 17.87 1.14\nMLG PCA 15.38 95.44 103.98 12.33 0.85\nFig. 7. Average \ufb01rst one recognition accuracy using AR datab ase and di\ufb00erent\nexpressions for training.\n113.4 Recognition experiments using FERET database\nBecause the AR database is relatively small (small number of di\ufb00eren t per-\nsons), we also performed recognition experiments using much large r FERET\ndatabase [13] containing greyscale photographsof 1196persons . This database\nwas collected in 1993-1996 by the researchers from George Mason University\nduring the FERET (FacE REcognition Technology) program. As far a s we\nknow, this is the largest database of face photographs (of di\ufb00ere nt persons)\nin the world that is publicly available for face recognition research pur poses.\nFor training we used 1196 greyscale images (the size of each image is 2 56x384\npixels) from the faset of this database. The same 1196 faimages were used\nas a gallery (known persons), and 1195 images from the fbset were used as\na probe (unknown persons that we wish to recognize). Facial image s from\nthese sets have di\ufb00erent expressions, but these expressions ar e not strictly\ncontrolled. For log-Gabor feature masking we used binary masks th at were\ngenerated from the AR database\u2019s faces.\nAt \ufb01rst we performed recognition experiments using di\ufb00erent numb er (100-\n1100) of PCA features and compared the results of PCA, LG PCA, a nd MLG\nPCA methods. These results are presented in Table 3. The results s howed\nthat using MLG PCA or LG PCA we can achieve much better recognition\nresults than using traditional PCA with respect to all measured cha racteris-\ntics (Cum(97)-Cum(100), \ufb01rst one recognition, CMCA, ROCA, EER ). The\nresults also showed that MLG PCA is better (or not worse) than LG P CA\nwithrespecttoCum(97)-Cum(99),ROCA,EER.TheMLGPCAalsoac hieves\nhigher \ufb01rst one recognition accuracy than LG PCA. But LG PCA is sligh tly\nbetter than MLG PCA with respect to Cum(100) and in some cases wit h\nrespect to CMCA. First one recognition accuracies of compared me thods us-\ning di\ufb00erent number of PCA features are also presented in Fig. 8. Th is \ufb01g-\nure illustrates that MLG PCA achieves higher accuracy than other c ompared\nmethods. Also we can see that \ufb01rst one recognition accuracy of tr aditional\nPCA -based recognition method (using cosine -based distance meas ure be-\ntween \u201dwhitened\u201d feature vectors) decreases if we use larger nu mber of PCA\nfeatures. The same e\ufb00ect was observed in [11] using other databa se and image\nnormalization procedure.\nBecause in real life situations it is not always possible to use large numb er\n(more than 1000) of training images, we tested our face recognitio n methods\nusing 400 fatraining images and 380 PCA features. We performed 50 exper-\niments with di\ufb00erent training sets of 400 faimages and calculated average\nrecognition results. These training sets for all compared methods were the\nsame. These average results and standard deviations are presen ted in Table 4.\n12Table 3\nComparison of PCA, LG PCA and MLG PCA methods using di\ufb00erent nu mber of\nPCA features. For experiments was used the FERET database (1 196fa, 1195fb)\nand 1196 fatraining images.\nMethod Feat. Cum(97) Cum(98) Cum(99) Cum(100) First 1 CMCA R OCA EER\nnum.\nPCA 100 1.59 2.93 6.44 36.87 83.85 37.27 28.77 2.26\nLG PCA 100 0.75 1.17 2.26 10.70 88.54 15.89 7.42 1.34\nMLG PCA 100 0.42 0.50 1.25 10.87 92.89 13.64 4.86 0.92\nPCA 200 1.17 2.34 7.19 84.20 86.44 40.21 31.59 2.01\nLG PCA 200 0.25 0.33 0.75 4.77 93.72 10.87 2.10 0.59\nMLG PCA 200 0.17 0.25 0.59 8.11 96.07 10.34 1.68 0.50\nPCA 300 1.34 2.84 5.52 93.65 87.87 39.36 29.91 1.92\nLG PCA 300 0.17 0.25 0.42 6.52 95.82 10.00 1.28 0.50\nMLG PCA 300 0.08 0.17 0.33 1.67 97.15 9.09 0.49 0.33\nPCA 400 1.67 3.09 12.12 92.73 88.03 52.94 42.79 2.09\nLG PCA 400 0.17 0.17 0.33 1.76 96.90 9.08 0.57 0.33\nMLG PCA 400 0.08 0.17 0.25 1.59 97.74 8.90 0.34 0.33\nPCA 500 2.01 4.10 16.64 74.50 87.53 58.73 48.26 2.34\nLG PCA 500 0.08 0.17 0.25 1.17 97.57 8.85 0.36 0.25\nMLG PCA 500 0.08 0.08 0.17 1.67 98.24 8.80 0.26 0.33\nPCA 600 2.59 5.94 13.55 82.86 86.03 67.05 56.21 2.59\nLG PCA 600 0.08 0.17 0.17 0.84 97.74 8.72 0.24 0.25\nMLG PCA 600 0.08 0.08 0.17 1.92 98.58 8.69 0.16 0.17\nPCA 700 4.35 8.86 22.41 95.07 85.02 88.53 77.02 3.18\nLG PCA 700 0.08 0.08 0.25 0.75 98.24 8.68 0.22 0.25\nMLG PCA 700 0.08 0.08 0.17 1.59 98.74 8.69 0.15 0.17\nPCA 800 5.35 10.28 28.51 93.14 83.10 109.43 97.12 3.51\nLG PCA 800 0.08 0.08 0.17 0.75 98.41 8.68 0.20 0.25\nMLG PCA 800 0.08 0.08 0.17 1.59 98.83 8.68 0.14 0.17\nPCA 900 7.02 12.63 38.13 92.89 82.43 123.39 110.99 3.93\nLG PCA 900 0.08 0.08 0.17 0.50 98.49 8.58 0.14 0.17\nMLG PCA 900 0.08 0.08 0.17 1.34 98.91 8.64 0.13 0.17\nPCA 1000 9.28 16.22 45.48 90.80 80.42 143.82 130.79 4.27\nLG PCA 1000 0.08 0.08 0.17 0.75 98.58 8.62 0.14 0.17\nMLG PCA 1000 0.08 0.08 0.17 0.75 98.49 8.61 0.10 0.17\nPCA 1100 12.63 26.25 48.75 91.47 78.83 166.00 152.41 4.77\nLG PCA 1100 0.08 0.08 0.17 0.92 98.66 8.65 0.13 0.17\nMLG PCA 1100 0.08 0.08 0.17 1.00 98.83 8.67 0.12 0.17\n137880828486889092949698100\n100 200 300 400 500 600 700 800 900\n1000 1100Number\u25a1of\u25a1PCA featuresFirst1 rec., %MLG\u25a1PCA\nLG\u25a1PCA\nPCA\nFig. 8. First one recognition accuracy using FERET database and di\ufb00erent number\nof PCA features.\nTable 4\nAverage recognition results of 50 experiments using FERET d atabase (1196 fa,\n1195fb), 400fatraining images, 380 PCA features.\nMethod Cum(97) Cum(98) Cum(99) Cum(100) First 1 CMCA ROCA EE R\nPCA 2.59 4.62 11.21 70.66 83.83 52.09 43.92 2.69\n\u00b10.46 \u00b10.97 \u00b12.67 \u00b117.33 \u00b10.90\u00b15.70\u00b15.86\u00b10.20\nLG PCA 0.23 0.35 0.73 5.72 94.45 10.88 2.31 0.70\n\u00b10.05 \u00b10.08 \u00b10.19 \u00b12.47\u00b10.56\u00b10.56\u00b10.45\u00b10.08\nMLG PCA 0.16 0.20 0.45 5.60 96.59 9.97 1.46 0.56\n\u00b10.03 \u00b10.04 \u00b10.11 \u00b13.55\u00b10.40\u00b10.53\u00b10.43\u00b10.07\nThe best results (Table 4) were achieved using MLG PCA. Even using s mall\nnumber of training images this method achieved enough high \ufb01rst one recog-\nnition accuracy ( >96%) and low EER (0.6%). Traditional PCA achieved 12%\nlower \ufb01rst one accuracy and 2% higher EER.\n3.5 Comparison of our results with the results of other resea rchers\nFor comparison we used the FERET database, which has become a de facto\nstandard for evaluating face recognition technologies [14]. We comp ared the\nresults of di\ufb00erent face recognition methods that were tested us ing all images\n14(not subsets) from the FERET fa(1196 images) and fb(1195 images) sets.\nFor comparison we present only the best published results of other researchers\nthat we were able to \ufb01nd. More results of other researchers could be found in\n[14], [15], [16]. The selected results are summarized in Table 5.\nNow we will brie\ufb02y describe the methods that we selected for compar ison.\nMIT 1996 (Massachusetts Institute of Technology) method [17] u ses dual (in-\ntrapersonal and extrapersonal) PCA and Bayesian MAP (maximum a poste-\nriori) similarity measure. For training are used image pairs of the same and\nof di\ufb00erent persons. UMD 1997 (University of Maryland) method [18 ], [19]\nuses a combination of Principal Component Analysis (PCA) and Linear Dis-\ncriminant Analysis (LDA) methods and weighted Euclidean distance be tween\nfeature vectors. For LDA training are used multiple ( >2) images of the same\nperson. For training are used more than 1000 images of more than 4 00 per-\nsons. For feature extraction are used 300 eigenvectors. USC 19 97 (University\nof Southern California) Elastic Bunch Graph Matching (EBGM) metho d [2],\n[20] detects speci\ufb01ed facial features (48 graph nodes) and extr acts Gabor Jets\nusing 40 Gabor \ufb01lters (5 scales, 8 orientations). Geometrical relat ionships be-\ntween features and the values of Gabor Jets (at graph nodes) ar e used for\ncomparison of faces. At \ufb01rst this method is trained using 70 facial im ages\nwith manually selected features, and then these features are det ected auto-\nmatically. For comparison are used more than 1900 features. CSU E BGM\nStandard [16] and CSU EBGM Optimised [21] face recognition methods were\ndeveloped at Colorado State University. These methods use the sa me theo-\nretical background as the method of USC, but di\ufb00erent features and di\ufb00erent\ntraining images. For training are used 70 facial images with manually se lected\nfeature landmarks. Features are extracted using 80 Gabor \ufb01lter s (8 orienta-\ntions, 5 frequencies, 2 phases). For recognition are used graphs with 80 nodes\n(25 landmarks, 55 interpolated points). The total number of feat ures is larger\nthan 6000. It is important to note that EBGM -based methods for c ompar-\nison usually use a whole human head (not only the internal part of the face\nwithout hair and facial contour). CSU PCA MahCosine method [16] us es tra-\nditional PCA and cosine -based distance measure between \u201dwhitene d\u201d feature\nvectors, 501 training images, and 300 PCA features. Gabor featu res -based\nmethod [22] extracts features by using 40 Gabor \ufb01lters and then u ses slid-\ning window -based algorithm for \ufb01nding high-energized points. Then m ultiple\ncomplex-valued feature vectors are constructed by storing the coordinates of\nthe points and 40 \ufb01lter responses at these points (each feature v ector has 42\ncomponents). Recognition is performed by calculating similarity meas ures be-\ntween multiple vectors and then by calculating the overall similarity me asure.\nHaar+AdaBoost method [23] uses Haar-like features and adaptive boosting\n-based training and feature selection. This method is trained using 3 98 pairs\nof images from the FERET faandfbsets. After training are selected 400\nfeatures. Gabor+AdaBoost method [24] for feature extraction uses Gabor \ufb01l-\nters (5 scales, 8 orientations). Adaptive boosting -based training is used in\n15Table 5. Comparison of di\ufb00erent face recognition methods usi ng FERET database (1196 fa, 1195fb).\nMethod and its authors Cum(97) Cum(98) Cum(99) Cum(100) First 1 CMCA ROCA EER\nMIT 1996 (Dual PCA + Bayes MAP) [17] 0.33 1.09 23.33 99.83 94.81 84.14 2 03.25 4.77\nUMD 1997 (PCA+LDA) [18], [19] 0.17 0.33 0.84 75.92 96.23 18.91 14.37 1.09\nUSC 1997 (EBGM) [2], [20] 0.25 0.33 3.09 50.67 94.98 27.94 57.52 2.51\nCSU EBGM Standard [21] 1.34 2.42 9.2 37.54 88.37 34.26 - -\nCSU EBGM Optimised [21] - - - - 89.80 - - -\nCSU PCA MahCosine [16] 2.26 4.43 10.28 60.45 85.27 48.90 - -\nGabor features [22] - - - - 96.30 - - -\nHaar+AdaBoost [23] - \u223c0.42 \u223c1.17 - \u223c94.00 - - \u223c1.00\nGabor+AdaBoost [24] - - - - \u223c95.20 - - -\nOur MLG PCA (train 400 fa, 380 PCA feat.) 0.16 0.20 0.45 5.60 96.59 9.97 1 .64 0.56\naverage values of 50 experiments\nOur MLG PCA (train 1196 fa, 300 PCA feat.) 0.08 0.17 0.33 1.67 97.15 9.09 0.49 0.33\nOur MLG PCA (train 1196 fa, 900 PCA feat.) 0.08 0.08 0.17 1.34 98.91 8.64 0.13 0.17\n16order to reduce the number of intrapersonal and extrapersona l features. For\nrecognition are selected 700 features.\nThe comparison showed (Table 5) that our MLG PCA method can achie ve\nhigher \ufb01rst one recognition accuracy (up to 98.91% using 1196 fatraining\nimages and 900 PCA features) and lower EER (0.17%) than many othe r\ncompared methods. Even using only 400 fatraining images and 380 PCA\nfeatures our MLG PCA method achieves enough high recognition acc uracy\n(First 1 = 96.59%, EER = 0.56%). The best methods of other researc hers are\nGabor features [22] (First 1 = 96.30%), UMD 1997 (PCA+LDA) [18], [19 ]\n(First 1 = 96.23%, EER = 1.09%), Gabor+AdaBoost [24] (First 1 = 95.20 %),\nand Haar+AdaBoost [23] (First 1 = 94%, EER = 1%). CMCA values of ou r\nmethod are about two times smaller than of other methods, and ROC A values\nof our method are also several times smaller than of other methods . In order\nto achieve 99% cumulative recognition rate (using 900 features) we need to\nextract from the database only 0.17% of images (that is 1196*0.17/ 100 = 2\nimages), and in order to achieve 100% cumulative recognition rate we need\nto extract 16 images (1.34%). Using CSU EBGM Standard method we n eed\nto extract 449 images of 1196 (37.54%) in order to achieve 100% cum ulative\nrecognitionaccuracy,andusingUSC1997(EBGM)[2],[20]methodwe needto\nextract 606 images (50.67%). Using the proposed method we also ca n achieve\n>90% recognition accuracy using larger windows than 4x4 pixels and sm aller\nnumber of log-Gabor features ( <10000), but we prefer to use larger number of\nlog-Gabor features (15120) and then reduce the number of feat ures to several\nhundreds by using PCA, because it allows to achieve >96% \ufb01rst one recog-\nnition accuracy. Because our traditional PCA using small number of training\nimages (400 fa) and features (380 PCA features) achieves similar recognition\naccuracy (84%) as CSU\u2019s [16] implementation of the same method (CS U PCA\nMahCosine [16], 501 mostly fatraining images, 300 PCA features, 85% \ufb01rst\none accuracy), we decided to test if it is possible to achieve high reco gni-\ntion accuracy using only log-Gabor features and di\ufb00erent distance measures\n(Euclidean, Manhattan, cosine-based) without using PCA. But the results of\nthese experiments showed that MLG PCA achieves much higher reco gnition\naccuracy.\n174 Conclusions and future work\nIn this article we proposed a novel face recognition method based o n masked\nlog-Gabor features and Principal Component Analysis. The experim ents with\nthe AR and FERET databases showed that using the proposed MLG P CA\nmethod we can increase the accuracy of recognizing faces with di\ufb00e rent ex-\npressions. The experiments showed that using log-Gabor feature s, expression\nmasking, sliding window -based feature selection method, Principal C ompo-\nnentAnalysis, \u201dwhitening\u201d,andcosine-baseddistancemeasurewe canachieve\nvery high recognition accuracy (98.91%) and low Equal Error Rate ( 0.17%)\nwith the FERET database containing facial photographs of 1196 pe rsons. The\nresults of our algorithm are among the best results that were ever achieved\nusing this database.\nIn the future we are going to investigate other methods of extrac ting and\ncomparing facial regions in order to achieve higher recognition accu racy (of\nfaceswithdi\ufb00erent expressions) thanwealreadyachieved. Alsowe aregoingto\ninvestigate the possibilities to optimise the parameters of log-Gabor \ufb01lters in\norder to increase the accuracy of the proposed method and redu ce the number\nof features.\n5 Acknowledgements\nPortionsoftheresearch inthispaper usetheFERETdatabaseoff acialimages\ncollected under the FERET program."}
{"category": "abstract", "text": "In this article we propose a method for the recognition of fac es with di\ufb00erent facial\nexpressions. For recognition we extract feature vectors by using log-Gabor \ufb01lters\nof multiple orientations and scales. Using sliding window a lgorithm and variances\n-based masking these features are extracted at image region s that are less a\ufb00ected\nby the changes of facial expressions. Extracted features ar e passed to the Principal\nComponent Analysis (PCA) -based recognition method. The re sults of face recog-\nnition experiments using expression variant faces showed t hat the proposed method\ncould achieve higher recognition accuracy than many other m ethods. For develop-\nment and testing we used facial images from the AR and FERET da tabases. Using\nfacial photographs of more than one thousand persons from th e FERET database\nthe proposed method achieved 96.6-98.9% \ufb01rst one recogniti on rate and 0.2-0.6%\nEqual Error Rate (EER).\nKey words"}
{"category": "non-abstract", "text": "//www.cs.smith.edu/\u02dcnhowe/research/code/#fgseg .\n1. Introduction\nMany computer vision applications require the segmenta-\ntion of foregroundfrom backgroundas a prelude to further\nprocessing. Although dif\ufb01cult in the general case, the task\ncanbegreatlysimpli\ufb01ediftheobjectorobjectsofinterest in\ntheforegroundmoveacrossa static background. Suchsitu-\nationsarise orcan beengineeredin a wide varietyof appli-\ncations,includingsecurityvideos,video-basedtracking and\nmotion capture, sports ergonomics, and human-computer\ninteractionsviainexpensiveworkstation-mountedcamera s.\nAll of these applications rely on or would bene\ufb01t from\nhigh-quality foreground segmentation. Unfortunately, ex -\nistingmethodssometimesproveunreliableanderror-prone .\nFurthermore, the results can vary greatly between succes-\nsive frames of a video. This paper introduces a new way\nto compute the foreground segmentation that makes fewer\nerrorsandcanbetemporallystabilizedfromframetoframe.\nTraditionally, researchers solve the foreground segmen-\ntation problemwith static backgroundthrougha procedure\ncalledbackground subtraction . In this approach, the com-puter builds a model of the static background, either off-\nline or updated dynamically after each frame in the video\nstream, and then compares the next frame with the back-\nground model on a per-pixel basis. Pixels that differ suf-\n\ufb01ciently from the background model may deemed part of\nthe foreground, at least in the ideal case. If the calculatio n\nwerefreeofnoise,onecouldsimply\ufb01xathresholdandde-\nclareanythingsuf\ufb01cientlydifferentfromthebackgroundt o\nbepartoftheforeground.\nUnfortunately, a number of confounding factors make\nperfectbackgroundsubtractionunattainable. Camera nois e\n(particularly in inexpensive CCD cameras) ensures that\neven background pixels will not exhibit constant values\nfrom frame to frame, but will instead show a distribu-\ntion around some characteristic value. If the background\ncontains non-static elements (such as vegetation, cloth, o r\ngravel) then the variance in the measurements of back-\nground pixels may be quite large. Moving objects in the\nforeground may cause shadows and re\ufb02ections to fall on\nbackground areas, changing their appearance signi\ufb01cantly .\nThe foreground objects may lack suf\ufb01cient contrast with\nthe background areas they obscure, either through deliber-\nate camou\ufb02age or by chance. In consequence, comparison\nof a pixel in a given frame with the background model for\nthatpixelcannotde\ufb01nitivelyclassifythepixelaseitherf ore-\ngroundorbackgroundwithoutsomepotentialforerror.\nErrors at a single pixel may be mitigated by aggregat-\ningtheresultsoversomelocal neighborhoodofpixels. Re-\nsearchershave traditionallytaken thisapproachto cleani ng\nup the errors in the thresholded image. A combination of\nmorphological operations on the binary thresholded image\nremovesisolatedforegroundandbackgroundpixels,gener-\natingabetterapproximationtothesilhouettesofthemovin g\nobjectsintheforeground.Unfortunately,thesameapproac h\ncanobliteratedetailsattheedgesofthesilhouette.\nThis work departs from the standard practice by us-\ning an algorithm based upon the minimum graph cut to\nseparate the foreground from the background. The al-\ngorithm presented herein uses information that would be\nthrown away by thresholding to construct a graph incor-\n1porating all the differences measured between the current\nframe and the background model. Links in this graph re-\n\ufb02ect the connectivity of the pixels in the image, allowing\neach pixel to affect those in its local neighborhood. (De-\ntails of the graph construction appear below.) Segmenting\nthe graph using a standard graph-cut algorithm produces a\nforeground-backgroundsegmentationthat can correctloca l\nerrors without introducing larger global distortions. Qua l-\nitatively, the results using the new technique look cleaner\nandmore correct;the quantitativetests in Section 4.1show\nthatthemethodproducesfewererrorsthandocurrentprac-\nticeswhencomparedtohuman-segmentedgroundtruth.\nTheremainderofthispaperconductsanin-depthlookat\ntheoldandnewmethodsforforegroundsegmentationusing\nbackground subtraction. Section 2 describes the two algo-\nrithms that Sections 3 and 4 compare experimentally. Sec-\ntion5 concludeswith a discussionof the numerousties be-\ntweenthisworkandotherefforts,andsome\ufb01nalthoughts.\n2. Algorithmic Details\nMorphologicaloperationsformthebasisofthestandardap-\nproach for cleaning up noise after background subtraction\nand thresholding. In particular, the morphological tech-\nniques commonly applied consist of the two basic opera-\ntionsdilationanderosionapplied in various combinations.\nDilation expands the foreground of the image, adding a\npixeltotheforegroundifanyofitsneighborswithinaspec-\ni\ufb01ed neighborhood of radius r(called the structuring ele-\nment) are already part of the foreground. Erosion expands\nthe background, removing a pixel from the foreground if\nany of its neighborsare background. These two operations\nmay be combined; a dilation followed by an identical ero-\nsion is called a closing, and \ufb01lls in holes in the foreground\nsmaller than the neighborhooddiameter. Likewise, an ero-\nsion followed by an identical dilation is called an opening,\nand may be used to eliminate isolated foreground pixels.\nSuch operations are well studied, and more details may be\nfoundinreferencetexts[6].\nNoiseinthebackground-subtractedimagetendstomake\nsome foreground pixels look like background, and vice\nversa. Amorphologicalclosingfollowedbyanopeningad-\ndresses these sources of error: the closing \ufb01lls in the the\nmissing foreground pixels (assuming that enough of their\nneighborsarecorrectlyidenti\ufb01ed),andtheopeningremove s\nextraneous foreground pixels surrounded by background.\nCare must be taken in choosing the radius for these opera-\ntions. Iftheradiusistoosmall,thenlargerclustersofnoi sy\npixelswill remain uncorrected;if too large, then legitima te\ndetailintheforegroundsilhouettewill belost.\nWith particularly noisy background-subtracted images,\nmislabeled background pixels may become so numerous\nand closely spaced that the initial closing operation \ufb01lls i nthe gaps between them. Increasing the threshold \u03c4for the\ninitial foreground-background segmentation prevents thi s\nundesirableeffect by biasing the initial labeling away fro m\nthe foreground. In otherwords,the higherthresholdcauses\nmore foregroundpixels to be classi\ufb01ed as backgroundthan\nbackground pixels as foreground. Performing the initial\nclosing operation corrects for this bias by closing the gaps\nbetweenthecorrectlylabeledforegroundpixels.\n2.1. GraphCutsforForegroundSegmentation\nUnlike the morphological approach, the graph-cut algo-\nrithm begins by building a graph based upon the image.\nEachpixel pijintheimagegeneratesacorrespondinggraph\nvertexvij. Two additional vertices form the source and\nsink, representing the foreground and the background re-\nspectively.\nFigure 1 illustrates the graph formed for a small 3\u00d73\nportion of the image plane. A typical vertex in the graph\nlinkstoexactlysixothernodes: thesourceandthesink,plu s\nthe the vertices of its four-connected neighbors. Vertices\ncorrespondingto pixels on the edge of the image will have\nfewer neighborlinks, and the source and the sink will each\nconnect to all the pixel vertices. The weights of the links\nbetweenthepixelverticesandthesource sandsinktderive\ndirectly from the difference between the current frame and\nthebackgroundatthe correspondingpixel, \u03b4ij:\nw(s,pij) =\u03b4ij (1)\nw(pij,t) = 2\u03c4\u2212\u03b4ij (2)\nThe neighbor links (between pixel vertices) all have iden-\ntical weights, equal to \u03c4times a second parameter \u03b1(typ-\nically taking on values close to 1.0). The parameter \u03c4in\nthe latter equation plays an analogous role to the thresh-\nold in the morphological algorithm, corresponding to the\nlevel above which the pixel associates more strongly with\ntheforegroundthanthebackground.\nThe value of \u03b1controls how strongly neighboring pix-\nels tendto group. If \u03b1is low, then neighboringpixelsbond\nweaklyandtheendresultwill lookmuchlikethat obtained\nbysimplythesholdingtheoutputfromthebackgroundsub-\ntraction. Conversely,high \u03b1causes pixels to bondstrongly\nwith neighboring pixels, and the output will contain larger\nclusters of homogeneity. Noisy inputs thus tend to require\nlargervaluesof \u03b1,inordertosmoothoverthelargerclusters\nofnoisypixels.\nOnce constructed, standard methods based upon graph\n\ufb02owwill\ufb01ndanoptimal(minimumcost)cutseparatingthe\nsource from the sink. Andrew Goldberg has kindly made\noptimized code for this computation available on the web\n[2]. Eachnodein thegraphwill lie ononeside orthe other\noftheoptimalcut,remainingconnectedsolelytothesource\n2Figure 1: Graph construct embeddedin image plane. Each\npixel corresponds to a node, and all pixel nodes are con-\nnectedtothesourceandthesink.\nor to the sink. The algorithm labels those nodes still con-\nnected to the source as foreground,and those connected to\nthesinkasbackground.\nIfdesired,onemayconstructagraphtorepresentseveral\nframes of video at once. In this case, a typical pixel ver-\ntex connects to six neighbors(four spatial plus two tempo-\nral), but otherwise the construction remains the same. Us-\ning multiframegraphscan impose consistencyof the result\nfromoneframetothenext,buttrialexperimentsindicateno\nbene\ufb01tin termsof overallaccuracy. Therefore,the remain-\nderofthispaperfocusesonsingle-framegraphs.\n3. Synthetic Results\nApreliminarysetofexperimentsmeasurestheperformance\nofthetwoalgorithmsonarti\ufb01cialdata. Usingarti\ufb01cialdat a\nallowscarefulcontrolofexperimentalconditions. Figure 2a\nshows the test pattern used, containing gradations in detai l\nfrom coarse to \ufb01ne. The right-hand portion of the image\ncontainslinesonepixelinwidthspacedasinglepixelapart ,\nandsuccessive portionsto the left doublethe width of both\nthe lines and the gaps. Figures 2b and 2f show two images\nusedasinputtothealgorithms,formedbytakingtheground\ntruthandaddingnoise. Noiseateachpixelissampledinde-\npendentlyfroma normaldistributionof knownvariance,to\ngenerate an input with known signal-to-noise ratio (SNR).\nFigures2c-2e and 2g-2i showsthe results generatedfor the\ntwoinputesshownin2band2f.\nTable 1 gives the error rate on the best performance of\neach algorithm for a number of different SNR values, in-\ncluding those illustrated in Figure 2. As a baseline, it also\ngives the performance achieved by simply thesholding the\ninputimage. Inadditiontotheerrorrates,theparameterva l-\nues used to achieve the result also appear (except for \u03c4on\nthegraphalgorithm,whichisalways0.5). Thevaluesgiven(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 2: Segmentation results for synthetic data. (a)\nGround truth; (b-e) SNR = 2.0 results: input signal, con-\ntrol (thresholded), morphological, graph; (f-i) SNR = 0.5\nresults: input signal, control (thresholded), morphologi cal,\ngraph.\n3Table1: Errorratesforthe test patternsofFigure2.\nSNRThresh Morph \u03c4 r Graph \u03b1\n42.3% 2.3%0.50 0 0.0% 0.8\n215.8% 10.3% 0.70 1 2.8%0.87\n1.522.8% 13.5% 0.75 1 5.9%0.95\n130.9% 19.6% 0.95 1 11.2% 1.4\n0.834.3% 23.8% 1.05 1 14.0% 1.8\n0.6537.3% 27.5% 1.20 1 16.7% 2.1\n0.540.1% 31.4% 1.45 1 19.8% 2.5\ncorrespondto the best result for a particularalgorithmon a\nparticularinput.\nThegraphcutalgorithmperformsmarkedlybetteronthe\nsynthetic data than either the control or the morphological\nmethod at all signal-to-noise ratios. The error rate for the\nmorphologicaltechniquesis always better than that for the\ncontrol,exceptat thehighestSNR(wheretheytie).\nExamination of the graphical output in Figure 2 shows\nthatatSNR=2.0bothtrialalgorithmsdofairlywell,butthe\ngraphcutresultdisplayscleaneredgesandcapturesmoreof\nthe \ufb01ne detail at the highest resolution. The morphological\nresult loses the details of the highest-resolution section at\nthe right of the test image. At the SNR = 0.5, where the\noutlinesof the inputtest patterncan barelybe discernedby\nhuman eyes, the graph cut result still looks reasonable for\nthe areas with coarser detail. Both algorithms lose the de-\ntailsin the two sectionsofhighestresolution. Interestin gly,\nincreasing \u03b1inthegraphcutalgorithmcanfurtherimprove\nresultsonthelow-resolutionsegments,albeitattheexpen se\noffurtherdegradationathighandmediumresolution.\n4. Experiments WithReal Images\nThe main set of experiments explores the performance of\nthe algorithms on video taken under real world conditions.\nThethreevideoclipsusedcoverarangeinqualityandsub-\nject: from color clips shot both indoors and outdoors with\n\ufb01xed cameras, to a low-quality grayscale video of a ballet\ndancer with a panning camera and compression artifacts.\nThe former represent relatively easy conditions, while the\nlatter presents a stiffer challenge for background subtrac -\ntion. Table2 givesmoredetailsoneachclip.\nEach clip must undergo extensive preprocessing before\nreachingthestage wherethealgorithmsontrialmaybeap-\nplied. Althoughsomemay objectto the processingchoices\nmade here (particularly the use of static background mod-\nels), changes in these choices would amount only to a\nchange of the input to both trial algorithms, akin to choos-\ningdifferentvideosfor the tests. Neither of the approache s\nunderconsiderationprecludesalgorithmicimprovementst o\nthepreprocessingstages,butmorecomplicatedpreprocess -Table2: Detailsonthevideoclipsusedfortesting.\nClip #2: Outdoor clip, regu-\nlar motion, some re\ufb02ection off\nglass. Fixed camera. 124\nframes.\nClip #1: Indoor clip with some\nshadowing, re\ufb02ections off the\n\ufb02oor. Some low contrast por-\ntions. Fixed camera. 160\nframes.\nClip#3: GrayscaleMPEGvideo\nof a dancer. Low contrast. Pan-\nningcamera. 99frames.\ning introduces potentially confounding design decisions.\nTherefore, for simplicity and replicability, the experime nts\navoid sophisticated preprocessing where possible. For the\n\ufb01rst twoclips, theexperimentseschewthedynamicallyup-\ndated background models used by most current systems\n(e.g.,W4[3]) in favor of a static background model com-\nputed over the entire clip. (A static background model is\nalso comparableto a dynamicmodel that has been allowed\ntoequilibrate.)\nTo prepare a video for background subtraction, a back-\nground model is built using crudely robust statistical tech -\nniques. The model builder takes the pixel color from every\nfourthframe,andthrowsoutthedataaboveandbelowapair\nof thresholds(say the 25th and the 75th percentiles). From\nthe remaining numbers it estimates the mean and variance\nof each pixel\u2019s color, assuming a normal distribution. This\napproachprovideseffective robustness to occlusionsof th e\nbackgroundon a small fraction of the video frames. (More\ncomplicated pixel modeling based upon mixtures of Gaus-\nsians can be used, but again this would only confound the\ncomparisonofthetwoalgorithmsundertrial.)\nForthethirdclip,thecameramovementnecessitatesspe-\ncial treatment. Before building the background model de-\nscribed above, each frame must be registered with a can-\nvas representing the entire background. A least-squares\n\ufb01t based on frame-to-frame optical \ufb02ow generates an ap-\nproximate set of initial registrations, expressed as af\ufb01ne\ntransforms. The computer then builds up the canvas frame\nby frame, using function minimization to \ufb01nd the af\ufb01ne\ntransformyieldingthesmallest disparitiesbetweenthene w\nframe and the median values on the existing canvas. The\npixelvaluesfortheframethengetinterpolatedontothecan -\n4Figure3: RegisteredmedianbackgroundmodelforClip#3,\nDancer. The camera pans from right to left, following the\ndancer.\nvas, yieldingnew medianvalues. Figure 3 showsthe result\nofthisprocess.\nWith the completed model (and registrations, if neces-\nsary) in hand, each frame can be compared with the mean\nbackground image. The difference at each pixel, normal-\nized by the variance at that pixel, forms the raw data input\nto the two \ufb01nishing algorithms. Typically the calculations\nmight be carried out on each of the red, green, and blue\n(RGB)componentsofanimage,andthedifferencesineach\ncomponent summed together. However, several modi\ufb01ca-\ntionstotheprocessprovidenecessarytolerancetoshadows\nand lighting changes. Making the comparisons in the hue-\nsaturation-value (HSV) color space causes disparities due\ntoshadowstoshowupprimarilyinonechannel,namelythe\npixel value ( V). Areas in shadow display lower Vcompo-\nnent values than the unshadowed background,but are sim-\nilar in the other two components. Therefore, discounting\nsmall decreases (less than 5% of the total range) in Vef-\nfectively ensures that shadowed background areas do not\nfalselyappeartobelongto theforeground. Thismethodef-\nfectivelymirrorsotherrecentresults[4],althoughthatw ork\ndiffers super\ufb01cially by processing images in RGB color\nspace.\nA suitable criterion must be chosen for grading the seg-\nmentation results. The total fraction of pixels in the image\ndifferingfromgroundtruthappearsanattractiveerrormea -\nsure at \ufb01rst glance, but a closer look reveals \ufb02aws in this\ncriterion. Figure 4d shows a segmentation created using\nmorphological techniques with very large structuring ele-\nment(a disk ofradius10). Althoughit capturesfewdetails\nof the subject \ufb01gure\u2019s outline accurately, its error over th e\nwholeframeis1.25%. Figure4eand4fshowalternateseg-\nmentations that look more faithful to the exact outlines of\nthe groundtruth. Yet these have higher whole-frameerrors\nof2.24%and2.59%,respectively,largelyduetonoiseatthe\nedges of the frame and other areas separate from the main\n\ufb01gure.\nInordertorewardsegmentationssimilartoFigure4eand\n4f,theexperimentsemployamodi\ufb01ederrormeasurefocus-\n(a) (b)\n(c) (d)\n(e) (f)\nFigure4: Effectsofdifferenterrormeasurements. (a)Orig -\ninal image. (b) Ground truth. (c) Difference from back-\nground. (d) Best morphological segmentation by whole-\nimage criterion. (e) Best graph and (f) morphological seg-\nmentationsusingtheconnectedcomponentscriterion.\ning speci\ufb01cally on the moving \ufb01gure in the clip. The mea-\nsure \ufb01rst computes the connected components of the seg-\nmented foreground and identi\ufb01es all the components that\noverlap with ground truth (as identi\ufb01ed by a human oper-\nator tracing the \ufb01gure\u2019s outline in Photoshop). Pixels in\nthe selected components that do not overlap with ground\ntruthcountasfalsepositives,whilegroundtruthareaside n-\nti\ufb01ed as background by the segmentation algorithm count\nasfalse negatives. Combiningthenumberoffalse positives\nandfalse negatives,then scalingby the numberof pixelsin\nthegroundtruthyieldstheerrormeasurementfortheframe.\n4.1. Real ImageResults\nTable3showstheerrorratesofboththemorphologicaland\ngraphalgorithmson the videoinputs, after tuningthe algo-\nrithm parameters for best results. (Interestingly, althou gh\nlarger stucturing elements were tested for the morphologi-\ncal operations, small radius-one disk-shaped elements giv e\nthe best results on all three clips.) Examiningthe numbers,\noneseesthatthequalityofthevideoinputformsthelargest\nfactor determiningthe error, with far more mistakes for ei-\n5Table3: Summaryofforegroundsegmentationresults(con-\nnectedcomponentserrorcriterion).\nClip Morph Graph\n1. Outdoor 0.164 0.161\nParams: \u03c4= 20.3,r= 2\u03c4= 16.2,\u03b1= 0.94\n2. Indoor 0.154 0.133\nParams: \u03c4= 5.21,r= 1\u03c4= 4.87,\u03b1= 0.81\n3. Dancer 0.541 0.532\nParams: \u03c4= 2.26,r= 1\u03c4= 2.15,\u03b1= 0.97\nther approach on the Dancerclip. Nevertheless, the graph-\nbased algorithm performs signi\ufb01cantly better (in a statist i-\ncalsense)oneverycliptested,accordingtoapairedsample\nt-test.\nAlthough the numeric differences appear small, their\npsychological importance can be large, with the numbers\nunable to tell the entire story. The presence or absence of\na body part such as a forearm may alter the error by as lit-\ntle as 0.04 or so, while a one-pixel shift distributed evenly\naround the entire the segmentation boundary can alter the\nerror by 0.12 or more. To illustrate what the error values\ncannot, Figures 5\u20137 show sequences of frames from each\nclip, spread evenly across time. (Frames where the subject\nisenteringorexitingthescreenare notshown.)\nCompared side by side, the graph based result (right\ncolumn) typically appears \u201ccleaner\u201d than the morphologi-\ncal result, and adheres more faithfully to the contours of\nthe ground truth. The Indoorclip best displays the advan-\ntages of the graph algorithm, showing a smoother bound-\nary and less frequent inclusion of background. Although\nthe graph algorithm does better on all three clips, Outdoor\nis mostly too easy to show up the differences, and Dancer\nis too hard. The errors evident in the Dancerclip demon-\nstrate that any contrast-based algorithm will fail where th e\ninput is deceptive (due to low contrast between foreground\nandbackground,heavy shadowing,re\ufb02ections, noise, etc.) .\nHigh quality input remains crucial, regardless of the algo-\nrithmapplied.\n5. Related Workand Conclusions\nGraph cuts can produce a cleaner foreground segmenta-\ntion based upon frame-by-framecomparisons with a back-\ngroundmodel. Thisresultmaynotsurprisethosewhohave\nbeen following the use of graph-based methods for other\napplications. This paper combines advances from several\ndifferentresearchthreadsthat havenot previouslybeenap -\nplied to the speci\ufb01c problem of foreground segmentation.\nGiven the status of foregroundsegmentation as the precur-\nsor to a host of other applications, any advance which can\nimprove the segmentation quality may have wide-ranging\n(Truth) (Morph.) (Graph)\nFigure5: Asequenceofframesfromthe Outdoorclip. The\nextraspotin earliertimeframesisare\ufb02ection.\n6(Truth) (Morph.) (Graph)\nFigure 6: A sequence of frames from the Indoorclip. The\nsubject\u2019sshirtprovideslowcontrastwith thebackground.\n(Truth) (Morph.) (Graph)\nFigure 7: A sequence of frames from the Dancerclip.\nThe dancer\u2019s costume blendswith the backgroundin many\nplaces.\n7effects. The experimental results show that the borrowed\ntechniques produce excellent results, as they have in other\n\ufb01elds.\n5.1. PreviousWork\nAlthough the use of a 2-way cut for foreground segmen-\ntation is novel, other kinds of graph-based methods have\nreceived considerable attention recently for segmentatio n\napplications. In particular, methods based upon the mini-\nmum normalized cut (n-cut) have achieved notable success\nin general segmentation problems [8]. General segmenta-\ntion (with the goal of subdividing any image into coher-\nent regions without a prioriknowledge of its contents) is\na much more dif\ufb01cult problem than foreground segmenta-\ntion with a static background,as explored herein. Not sur-\nprisingly therefore, n-cut algorithms for general segment a-\ntion differ from this paper\u2019s approach, typically employin g\na fully-connected graph requiring approximation methods\nto solve [7]. By contrast, the algorithm described herein\nuses a graph that represents only local connections among\npixels. Because the number of links remains linear in the\nnumberofnodes,thislocal-onlygraphcanbesolvedeasily\nand exactly. Furthermore, the advantages of the normal-\nizedcutoverthestandardcutdonotapplyinthelocal-only\ncase, because the energy of a cut relates only indirectly to\nthenumberofnodesoneitherside.\nA more direct connection to the current work may be\ndrawn from research on image correspondance for stereo\nvision and visual correspondence [1, 9]. Again, the stere-\nopsis problem is more dif\ufb01cult than foreground segmenta-\ntion,requiringaselectionamongmultiplehypothesizeddi s-\nplacementsateachpixel. Theworkmentionedabovethere-\nfore employs algorithmsto approximatea multiway cut on\na graph representing the image. The algorithm herein em-\nbodiesaspecialcaseofsuchasituation,wheretheexistenc e\nofonlytwo categoriesto distinguish(foregroundandback-\nground)allowstheuseofanexact2-waycut solution.\nIntheprocessingofthevideoframespriortothethresh-\noldingstep,thisworkfollowscurrentthestateoftheart. I n\nparticular,itbuildsprobabilitymodelsforthedistribut ionof\nmeasured color values at each pixel [5]. It further employs\ntechniquesto eliminate interference from shadows; simila r\nmeasures were recently described elsewhere [4]. The au-\nthorsofthelatterworknotethattheirsegmentationproces s\nrunsinrealtime,whichisachallengeforthenewalgorithm\ndue to the graph cut step. Theoretical bounds on this step\nareO(n2logn), althoughforsome problemclasses the ac-\ntualperformancecanbequadraticorbetter[2]. Empiricall y,\nit appears that real time processing is still possible at low -\nered resolution, and this stricture should ease with time as\nprocessorspeedsincrease.5.2. FinalThoughts\nUsing graph cuts for foreground segmentation produces\ncleanerandmoreaccurateresultsthanthecurrentlyprevai l-\ning approach based upon morphological operations. The\ngraph-based technique appears better at overcoming the\neffects of noise by aggregating information from a local\nneighborhood around each pixel, while remaining true to\nthe underlyingdata. On test using synthetic data, it cut the\nerror rate by at least a third over the current methods for\nthe noisiest input, and by a greater factor for the less noisy\ncases. On real data, the method signi\ufb01cantly reduces er-\nrorovercurrentmethods,althoughit cannotmagicallycure\nproblems associated with low quality input data. The one\ndisadvantage of the graph-based method is its speed; em-\npiricallyit runsmoreslowly or at lower resolutionthan the\nmorphologicaloperations.\nGiventherangeofapplicationsthatusebackgroundsub-\ntraction,adoptionofthenewtechniqueseemslikelytopro-\nvidesigni\ufb01cantbene\ufb01tsinanumberofareas. Inadditionto\ncurrent uses for backgroundsubtraction, the higher \ufb01delit y\nof the graph-cutmethodmay openup new applicationsnot\nhitherto feasible because they require highly reliable inp ut.\nInanycase,graphcutsforforegroundsegmentationdeserve\naplacein theresearchscientist\u2019sbagoftools."}
{"category": "abstract", "text": "For many tracking and surveillance applications, back-\nground subtraction provides an effective means of\nsegmenting objects moving in front of a static background.\nResearchers have traditionally used combinations of\nmorphological operations to remove the noise inherent\nin the background-subtracted result. Such techniques\ncan effectively isolate foreground objects, but tend to\nlose \ufb01delity around the borders of the segmentation,\nespecially for noisy input. This paper explores the\nuse of a minimum graph cut algorithm to segment\nthe foreground, resulting in qualitatively and quanti-\ntiatively cleaner segmentations. Experiments on both\narti\ufb01cial and real data show that the graph-based\nmethod reduces the error around segmented foreground\nobjects. A MATLAB code implementation is available at\nhttp"}
{"category": "non-abstract", "text": "total variation, motion detection, active contour models.\n1 Introduction\nSegmentation of moving objects from a video sequence is an im portant task whose\napplications cover domains such like video compression, vi deo surveillance or object\nrecognition. In video compression, the MPEG-4 video coding standard is based on the\nrepresentation of the scene as di\ufb00erent shapes-objects. Thi s representation simpli\ufb01es\nthe scene and is used for the encoding of the sequence.\nTherearedi\ufb00erentwaystoperformmovingobjectssegmentati on, usingdi\ufb00erentmath-\nematical techniques. For Markov Random Fields based method s, we refer to the works\nof Bouthemy ([6], [5]) and for maximum likelihood based meth ods, to the works of\nDeriche and Paragios ([17]). For variational techniques, w e refer to the works of De-\nricheet al.([3]) and Barlaud et al.([2]). At last, mathematical morphology has been\nmore and more used these last ten years, see the works of Salem bier, Serra and their\nteams ([4]).\nIn this paper, based on the former work [31] concerning movin g object segmenta-\ntion, we focus on two di\ufb00erent techniques, the \ufb01rst one relyin g on the recent result\nof [11] (the same results were derived independently, and pr eviously, by Darbon and\nSigelle [14, 15] in a probabilistic setting) and the second o ne is the use of graph cuts\n(Boykov, Veksler, and Zabih [8], Kolmogorov and Zabih [25]) .\nThe result of [11] states that solving the Rudin-Osher-Fate mi Total Variation regular-\nization problem [32] and thresholding the result at the leve l\u03b1gives the region that is\nsolution of the shape optimization problem 7. The idea of the proof relies on the fact\n2that the total variation of a function can be reconstructed f rom the perimeters of its\nlevel sets: it is the famous coarea formula . Former works rely also on the coarea for-\nmula: in [18], the authors propose to use it to propose a new sc heme for TV di\ufb00usion\nand improve its e\ufb03ciency in [19] using a level set decomposit ion of the image; Chan,\nEsedoglu and Nikolova in [13] solve a Mumford-Shah/Chan-Ve se ([28],[12]) problem\nwith \ufb01xed means by a TV-regularization and state also an equi valence result between\nsome special shape optimization problem and a TV regulariza tion one with L1norm\ndata \ufb01delity term.\nIn this paper, we use the framework of [11] in the case of a non- homogeneous total\nvariation functional, corresponding to a weighted anisotr opic perimeter like the one\nstudied in [31]. The outline is the following : in the \ufb01rst par t we present the energy\nused to segment moving objects in the image in the second part and we expose formal\nmathematic arguments for the use of TV regularization. It is followed by a mathe-\nmatical part about TV regularization and results about the e quivalence with solving a\nclass of shape optimization problems, and by a part where we p resent graph cuts and\ntheir use for our functional. It is followed by an experiment al part where we show the\nresults obtained. The last part is dedicated to an automatic moving objects detection\nperformed by a contrario statistical methods on the result obtained by total variati on\nregularization (previous parts). We compare it to the previ ously shown methods.\n2 A shape optimization problem for moving object de-\ntection\n2.1 The functional\nOncewehavedeterminedtheoptical \ufb02ow, wekeepitfortheseg mentation purpose. We\nwill denote \u2126 the moving region and Dthe image domain. As a moving object should\nbe characterised by a su\ufb03ciently large \ufb02ow magnitude, it see ms natural to incorporate\n/integraltext\n\u2126\u03b1\u2212 |v|(x)dxto the energy we want to minimize, where \u03b1\u2212 |v|(x) have to take\ndi\ufb00erent signs on the image domain, otherwise the solution of the shape optimization\nproblem will betrivial. As we want the boundaryof \u2126 to remain stable in the presence\n3of noise or spurious variations, we also penalize the total l ength of this boundary (that\nis, the perimeter of \u2126) in our functional. Finally, as thresh olding the optical \ufb02ow will\nnot give exact object contours (due to the temporal integrat ion), we add a weighted\nperimeter which integrates a function of the gradient (here gI=1\n1+|\u2207I|2) along the\nboundary. It gives the functional\n/integraldisplay\n\u2126\u03b1dx+/integraldisplay\nD\\\u2126|v|dx+\u03bb/integraldisplay\n\u2202\u2126gI(x)dS+\u00b5/integraldisplay\n\u2202\u2126dS (2)\nwheredSdenotesthearclengthvariation alongtheboundary. Forsim plicity notations,\nwe will denote \u03bbgI+\u00b5byg. Finally, our functional is\n/integraldisplay\n\u2126\u03b1dx+/integraldisplay\nD\\\u2126|v|dx+/integraldisplay\n\u2202\u2126g(x)dS (3)\nWithin the framework of shapesensitivity analysis (see Mur at and Simon [29], Delfour\nand Zolesio [16]), one can compute the shape derivative of this functional and obtain\nthe steepest gradient descent. Combining it to the famous le vel set method (Osher,\nSethian, [30]), we would obtain\n\u2202u\n\u2202t=|\u2207u|/parenleftbigg\n|v|\u2212\u03b1+div/parenleftbigg\ng\u2207u\n|\u2207u|/parenrightbigg/parenrightbigg\n.\nAnother similar method is to use uas the unknown of the functional and not \u2126 : the\nintegral over \u2126 ( resp.D\\\u2126) is replaced by integrals over Dwith the weight H\u01eb(u)\n(resp.1\u2212H\u01eb(u)) and the boundary term by the integral over Dwith the weight\n|\u2207(H\u01eb(u))|. Let notice that a parameter \u01ebis needed in this method for computing\n\u03b4\u01ebandH\u01ebwhich areC\u221eregularizations of Dirac and Heaviside distributions. The\nobtained PDE, leading to the same curve motion than the previ ous one, is\n\u2202u\n\u2202t=\u03b4\u01eb(u)/parenleftbigg\n|v|\u2212\u03b1+div/parenleftbigg\ng\u2207u\n|\u2207u|/parenrightbigg/parenrightbigg\n.\nThat was done in [31], unfortunately, if we want to adjust the value of\u03b1in a suitable\nway, wehavetorecomputetheresultbythispartialdi\ufb00erenti al equation asmanytimes\nas necessary. We overcome this problem by using the equivale nce between solving the\nROF model with a weighted total variation andsolving of (2) f or all thepossiblevalues\nof\u03b1.\n4In [11], functionals do not involve standard perimeter but a di\ufb00erent anisotropic one.\nThis is for theoretical reasons explained in [11] : the discr ete total variation does not\nsatisfy the coarea formula which is needed in the main result of [11]. In fact, the\ntheory can be developped with the isotropic total variation in the continuous setting,\nand results could still be (approximately) computed.\nThus we slightly modify the functional to \ufb01t in the framework given in [11] ( \u03bddenotes\nthe outside normal to the boundary and | \u00b7 |1the 1-norm : |(a,b)|1=|a|+|b|,R\u03c0\n4\ndenotes the rotation of angle\u03c0\n4)\nE(\u2126) =/integraldisplay\n\u2126\u03b1dx+/integraldisplay\nD\\\u2126|v|dx+1\n2/integraldisplay\n\u2202\u2126g(x)(|\u03bd|1+|R\u03c0\n4(\u03bd)|1)dS. (4)\nThis is a change of metric : the standard length and its weight ed counterpart are\nreplaced by what it is usually called \u201cManhattan\u201d or \u201ctaxica b\u201d length. We could keep\nonly/integraltext\n\u2202\u2126g(x)|\u03bd|1dSbut/integraltext\n\u2202\u21261\n2g(x)(|\u03bd|1+|R\u03c0\n4(\u03bd)|1)dSis useful to not overestimate\nthe length of diagonal linear parts of the boundary of \u2126. We in troduce the weighted\nisotropic and anisotropic total variations\nTVg(u) :=/integraldisplay\nDg|Du|andTV1,g(u) :=1\n2/integraldisplay\nDg(|Du|1+|R\u03c0\n4(Du)|1),\n(notation 1 refers to the 1-norm of the normal and gto the weight function) so that\nTV1,g(\u03c7\u2126) =/integraltext\n\u2202\u21261\n2g(x)(|\u03bd|1+|R\u03c0\n4(\u03bd)|1)dSandTVg(\u03c7\u2126) =/integraltext\n\u2202\u2126g(x)dSarerespectively\nthe anisotropic weighted perimeter and the weighted perime ter. We denote \u039b g(\u2202\u2126) =\nTV1,g(\u03c7\u2126) andLg(\u2202\u2126) =TVg(\u03c7\u2126): these two perimeters satisfy\nc1Lg(\u2202\u2126)\u2264\u039bg(\u2202\u2126)\u2264c2Lg(\u2202\u2126)\nwithc1=1+\u221a\n2\n2,c2=1\u221a\n2\u2212\u221a\n2, and thus if the boundary of \u2126 has a \ufb01nite Lg, it has\n\ufb01nite \u039b g, and conversely.\nAt last, we rewrite our functional in discrete setting, as th is will be in the rest of the\npaper\nE(\u03b8) =/summationdisplay\ni,j(\u03b1\u2212|v|i,j)\u03b8i,j+1\n2/summationdisplay\ni,jgi,j(|\u03b8i+1,j\u2212\u03b8i,j|+|\u03b8i,j+1\u2212\u03b8i,j|)\n+1\n2\u221a\n2/summationdisplay\ni,jgi,j(|\u03b8i+1,j+1\u2212\u03b8i,j|+|\u03b8i\u22121,j+1\u2212\u03b8i,j|).\n5Let us observe that the weight gi,jcould be di\ufb00erent on each edge (connecting two\nneighboring pixels) of the grid and that the choice we have ma de is quite arbitrary.\nHowever, we did not observe a signi\ufb01cant change in the output when weighing the\nedges in a di\ufb00erent way.\n2.2 Remarks about the minimization\nAs we have seen, a functional like (3) is usually minimised us ing shape sensitivity\nanalysis [29, 33, 16], classical calculus of variation (see for example [9]) or heaviside\nfunction techniques (Chan-Vese, [12]). All of those are gra dient-descent methods. In\n[11, 14, 15], it is shown that the solutions of the discrete sh ape optimization problem\nmin\n\u03b8,\u03b8i\u2208{0,1}\u03bbJ(\u03b8)+/summationdisplay\ni(\u03b1\u2212fi)\u03b8i\n(iis an index of the pixel number and \u03b8plays the role of the characteristic function\nof the shape, fis a data function [in our problem it is the optical \ufb02ow norm] a ndJis\na total variation, though it could be another function satis fying the same properties,\nthis will be described in section 3) can be obtained by comput ing the solution of the\nRudin-Osher-Fatemi total variation regularization probl em\nmin\nu1\n2\u03bb/ba\u2207dblu\u2212f/ba\u2207dbl2+J(u)\nand just threshold the result \u02dc uat the level \u03b1. This has two advantages over classical\nsnakes methods like the ones cited above. First, it gives a gl obal minimum of the\nshape optimization problem, which is not necessarily the ca se of the classical snakes\nmethods, since the gradient descent may be trapped into loca l minima. Secondly, if\nwe want to \ufb01nd the most appropriate value of \u03b1, we have just to compute oncethe\nsolution of the ROF problem and to threshold at di\ufb00erent level s in order to decide the\nvalue we keep; by any other method, we would be obliged to repe at the minimization\nas many times as the number of values of \u03b1we would like to compare. With the\nprojection algorithm for computing the solution of the ROF p roblem (see section 3.2),\nwe inherit of another slighter advantage : we avoid introduc ing additional parameters\nwhich are required to approximate either the total variatio n in usual solving by PDE,\n6or Dirac and Heaviside functions (see [12] for details).\nIt is known since Greig, Portehous ans Seheult [20] that ener gies (3) and (4) can\nbe exactly minimized. More recently, Kolmogorov and Zabih i n [25] proposed the\n\u201d\u2018graph cuts\u201d\u2019 algorithm as a way to minimize such type of ene rgies. We will detail\nabout it in the section 4. It leads to a global minimum, but the second advantage\nof TV regularization does not occur here : we have to compute t he solution of the\nshape optimization problem as many times as necessary if we w ant to optimize the \u03b1\nparameter. As a single graph cut computation requires appro ximately 0.5 second and\nthe ROF solution about 1 minute (on an image of size 256 \u00d7256 on a laptop equiped\nwith a 1.8 GHz Pentium 4 and 1 Gb of RAM), graph cuts are better f or a computation\nfor a \ufb01xed value of \u03b1, but if we want to choose many di\ufb00erent values of \u03b1, the ROF\nsolution computation should be more indicated.\n3 On the equivalence of total variation regularization and\na class of shape optimization problems\nIn this section, we will use the following notations : | \u00b7 |denotes the euclidean norm\n|(a,b)|=\u221a\na2+b2,|\u00b7|pdenotes the p-norm|(a,b)|p= (|a|p+|b|p)1/pand|\u00b7|\u221edenotes\nthe\u221e-norm|(a,b)|\u221e= sup(|a|,|b|)\n3.1 Settings\nIn this section, we recall the main results obtained in [11]. The problem considered is\nmin\n\u03b8\u2208X,\u03b8i\u2208{0,1}\u03bbJ(\u03b8)+/summationdisplay\ni(\u03b1\u2212fi)\u03b8i(P\u03b1)\nwhereXis the space of functions de\ufb01ned on the Npixels of the image grid ( idenotes\nthe pixel index and fis still a data function). The function J:X\u2192R+satis\ufb01es four\nproperties.\n\u2022Convexity : J(tu+(1\u2212t)v)\u2264tJ(u)+(1\u2212t)J(v) for anyu,v\u2208Xandt\u2208[0,1],\n\u2022lower semicontinuity,\n7\u20221-homogeneity : J(tu) =tJ(u) for anyt\u22650 andu\u2208X,\n\u2022it satis\ufb01es also the generalized co-area formula\nJ(u) =/integraldisplay+\u221e\n\u2212\u221eJ(1u>t)dt (5)\nwhere1u>tdenotes the indicator function of the upper level set of u.\n3.1.1 Main theorem and extensions\nWe consider the Rudin-Osher-Fatemi TV regularization prob lem\nmin\nu\u2208XJ(u)+1\n2\u03bb/ba\u2207dblu\u2212f/ba\u2207dbl2(6)\nand the discrete shape optimization problem\nmin\n\u03b8\u2208X,\u03b8i\u2208{0,1}\u03bbJ(\u03b8)+/summationdisplay\ni(\u03b1\u2212fi)\u03b8i (7)\nThe main theorem of [11] states an equivalence between solvi ng (6) and thresholding\nthe result at threshold \u03b1and solving (7). As we are concerned only with solving (7),\nwe give only the part of the theorem which states that thresho lding the solution of\nthe discretized ROF model gives a solution of the shape optim isation problem.\nTheorem 1 ([11]) Letwsolve (6). Then, for any s\u2208R, bothws\ni=1wi>sand\n\u00af\u00afws\ni=1wi>ssolve (7). If ws=\u00af\u00afws, then the solution of (7) is unique.\nIn [11], it is the discrete Manhattan total variation that is used\nJ(u) =/summationdisplay\ni,j|ui+1,j\u2212ui,j|+|ui,j+1\u2212ui,j|\nwhich is dicretized from the continuous 1-TV introduced in t he previous section. If\nwe want a more isotropic and\u03c0\n4-rotationnally invariant Manhattan TV, we may take\ndiagonal terms into account\n1\n2/summationdisplay\ni,j|ui+1,j\u2212ui,j|+|ui,j+1\u2212ui,j|+1\n2\u221a\n2/summationdisplay\ni,j|ui+1,j+1\u2212ui,j|+|ui\u22121,j+1\u2212ui,j|,\nwhich is discretized from1\n2/integraltext\nD|\u2207u|1+1\n2/integraltext\nD|\u2207u\u00b7e1|+|\u2207u\u00b7e2|wheree1= (\u221a\n2\n2,\u221a\n2\n2) and\ne2=e\u22a5\n1. The second term can be seen as a Manhattan TV in another basis , actually\n8it is exactly/integraltext\nD|R\u03c0\n4(\u2207u)|1whereR\u03c0\n4is the rotation of angle\u03c0\n4. The discrete standard\nTV\nTV1,g(u) =/summationdisplay\ni,j/radicalBig\n|ui+1,j\u2212ui,j|2+|ui,j+1\u2212ui,j|2\ndo not \ufb01t in the frame described here since it does not satisfy the generalized coarea\nformula, though being the discretized version of the total v ariation in the standard\nde\ufb01nition given in the previous section.\nAs the Theorem 1 is stated for any function Jsatisfying the four conditions given\nabove and the Manhattan discrete TV satisfy them. It is strai ghtforward to extend it\nto a g-weighted Manhattan TV\n/summationdisplay\ni,jgi,j(|ui+1,j\u2212ui,j|+|ui,j+1\u2212ui,j|)\nthen to the more isotropic\nTV1,\u03c0\n4,g(u) =1\n2/summationdisplay\ni,jgi,j(|ui+1,j\u2212ui,j|+|ui,j+1\u2212ui,j|)\n+1\n2\u221a\n2/summationdisplay\ni,jgi,j(|ui+1,j+1\u2212ui,j|+|ui\u22121,j+1\u2212ui,j|) (8)\nin which we are concerned in this paper.\n3.2 The projection algorithm of [10]\nIn [10], a new algorithm for computing the solution of (6) was proposed. It is based\non duality results and consists in \ufb01nding the projection of fonto a convex set. Let us\ndescribe how it works on the energy we are interested in. Here we follow the calculus\nof [11] which generalize well to the g-weighted Manhattan TV\nThe energy considered is thus\nTV1,\u03c0\n4,g(u) =1\n2/summationdisplay\ni,jgi,j(|(\u2207xu)i,j|+|(\u2207yu)i,j|)+1\n2/summationdisplay\ni,jgi,j(|(\u2207xyu)i,j|+|(\u2207yxu)i,j|)\nwhere we have rewritten the expression of 8. By now, we denote \u2207w= (\u2207xw,\u2207yw)\nand\u2207\u2032w= (\u2207xyw,\u2207yxw).\nFrom discrete gradients, we get the de\ufb01nition of discrete di vergence div = \u2212\u2207\u2217\n(div\u03be,w)X=\u2212(\u03be,\u2207w)X\u00d7X,\u2200w\u2208X,\u03be\u2208X\u00d7X,\n9and similarly with is rotated counterpart div\u2032=\u2212(\u2207\u2032)\u2217\n(div\u2032\u03be,w)X=\u2212(\u03be,\u2207\u2032w)X\u00d7X,\u2200w\u2208X,\u03be\u2208X\u00d7X,\nIn [11], it is stated that the solution of\nmin\nw\u2208X/summationdisplay\ni,j|(\u2207w)i,j|+1\n2\u03bb/ba\u2207dblw\u2212w0/ba\u2207dbl2\n(where|(\u2207w)i,j|is the euclidean norm of ( \u2207w)i,j) is given by \u00af w=w0\u2212\u03bbdiv\u00af\u03bewhere\n\u00af\u03beis a solution to\nmin{/ba\u2207dbl\u03bbdiv\u03be\u2212w0/ba\u2207dbl2|\u03be\u2208X\u00d7X,|\u03be| \u22641}.\nMoreover, one has \u00af\u03bei,j\u00b7(\u2207\u00afw)i,j=|\u2207\u00afw|i,jfor all (i,j). As this duality problem relies\non the property\n\u03be\u00b7\u2207w\u2264 |\u03be|p|\u2207w|q\nwith1\np+1\nq= 1 withp\u2208[1,+\u221e] (forp=\u221e,q= 1 and conversely) and as we have\nq= 1 for Manhattan TV, the constraint |\u03bei,j| \u22641 is replaced by |\u03bei,j|\u221e\u22641, that is to\nsay|\u03bex\ni,j| \u22641 and|\u03bey\ni,j| \u22641. Forg-\u201cweighted\u201d Manhattan TV, as we want to realize\n\u03be\u00b7\u2207w\u2264 |\u03be|\u221e|\u2207w|1\u2264g|\u2207w|1,\nthe constraints become |\u03bex\ni,j| \u2264gi,jand|\u03bey\ni,j| \u2264gi,j. If we consider the full TV1,\u03c0\n4,g,\nwe have the part of Manhattan TV expressed in the basis ( e1,e2). This leads to\nanother vector \ufb01eld \u03b7wich satis\ufb01es the same properties as \u03be. All the constraints can\nbe renormalized by the function gequivalently : we replace div( \u03be) by div(g\u03be) and\n|\u03be| \u2264gby|\u03be| \u22641, and for\u03b7in the same way. Let introduce the compact set (the\noverlining denotes the closure)\nK={div(g\u03be)+div\u2032(g\u03b7)|(\u03be,\u03b7)\u2208A2}\nwhere\nA={p= (px,py)\u2208X\u00d7X,|px\ni,j| \u22641,|py\ni,j| \u22641}.\nFrom the de\ufb01nition of the total variation TV1,\u03c0\n4,g, we get\nTV1,\u03c0\n4,g(w) = sup\n|\u03be|\u221e\u22641(w,div(g\u03be))X+ sup\n|\u03b7|\u221e\u22641/parenleftbig\nw,div\u2032(g\u03b7)/parenrightbig\nX= sup\nv\u2208K(w,v)X.\n10Exactly in the same manner than in [11], it can be established that the solution of\nthe ROF problem is given by the orthogonal projection of wonto\u03bbKThis is for\nconstraints simplicity. Finally, the solution of\nmin\nw\u2208XTV1,\u03c0\n4,g(w)+1\n2\u03bb/ba\u2207dblw\u2212w0/ba\u2207dbl2(9)\nis given by \u00af w=w0\u22121\n2/parenleftbig\n\u03bbdiv(g\u00af\u03be)+\u03bbdiv\u2032(g\u00af\u03b7)/parenrightbig\nwhere (\u00af\u03be,\u00af\u03b7) is a solution to\nmin\n(\u03be,\u03b7)\u2208A2/ba\u2207dbl1\n2/parenleftbig\n\u03bbdiv(g\u03be)+\u03bbdiv\u2032(g\u03b7)/parenrightbig\n\u2212w0/ba\u2207dbl2(10)\nLet us mention that the div\u2032operator is di\ufb00erent from thediv oneas it is theconjugate\nof the gradient in the basis ( e1,e2). It is simply given by (denoting f= (f1,f2))\n(div\u2032f)i,j=1\u221a\n2(f1\ni,j\u2212f1\ni\u22121,j+1+f2\ni,j\u2212f2\ni\u22121,j+1).\nThe Karush-Kuhn-Tucker conditions yield the existence of L agrange multipliers \u03b11\ni,j\u2265\n0,\u03b12\ni,j\u22650,\u03b21\ni,j\u22650,\u03b22\ni,j\u22650 associated tot the constraints in (10) that are ( \u03be1\ni,j)2\u22641,\n(\u03be2\ni,j)2\u22641, (\u03b71\ni,j)2\u22641, (\u03b72\ni,j)2\u22641. These Lagrange multipliers satisfy\n\u2212\u03bbgi,j\u2207(\u03bb\n2(div(g\u03be)+div\u2032(g\u03b7))\u2212w0)i,j+2(\u03b11\ni,j\u03be1\ni,j,\u03b12\ni,j\u03be2\ni,j)T= 0\n\u2212\u03bbgi,j\u2207\u2032(\u03bb\n2(div(g\u03be)+div\u2032(g\u03b7))\u2212w0)i,j+2(\u03b21\ni,j\u03b71\ni,j,\u03b22\ni,j\u03b72\ni,j)T= 0\nwith either \u03b11>0 (and similarly for \u03b12,\u03b21and\u03b22) and\u03be1. Thus\n\u03b11\ni,j=1\n2\u03bbgi,j|\u2207x(\u03bb\n2(div(g\u03be)+div\u2032(g\u03b7))\u2212w0)|\n\u03b12\ni,j=1\n2\u03bbgi,j|\u2207y(\u03bb\n2(div(g\u03be)+div\u2032(g\u03b7))\u2212w0)|\n\u03b21\ni,j=1\n2\u03bbgi,j|\u2207xy(\u03bb\n2(div(g\u03be)+div\u2032(g\u03b7))\u2212w0)|\n\u03b22\ni,j=1\n2\u03bbgi,j|\u2207yx(\u03bb\n2(div(g\u03be)+div\u2032(g\u03b7))\u2212w0)|\nThen, we obtain a \ufb01xed-point algorithm similar to the one pro posed in [11]\nwn=1\n2/parenleftbig\n\u03bbdiv(g\u03ben)+\u03bbdiv\u2032(g\u03b7n)/parenrightbig\n\u2212w0\n(\u03ben+1\ni,j)x=(\u03ben\ni,j)x+gi,j\u03c4\n\u03bb(\u2207xwn)i,j\n1+gi,j\u03c4\n\u03bb|(\u2207xwn)i,j|\n(\u03ben+1\ni,j)y=(\u03ben\ni,j)y+gi,j\u03c4\n\u03bb(\u2207ywn)i,j\n1+gi,j\u03c4\n\u03bb|(\u2207ywn)i,j|\n(\u03b7n+1\ni,j)x=(\u03b7n\ni,j)x+gi,j\u03c4\n\u03bb(\u2207xywn)i,j\n1+gi,j\u03c4\n\u03bb|(\u2207xywn)i,j|\n(\u03b7n+1\ni,j)y=(\u03b7n\ni,j)y+gi,j\u03c4\n\u03bb(\u2207yxwn)i,j\n1+gi,j\u03c4\n\u03bb|(\u2207yxwn)i,j|\n11Following the convergence proof of [10], we obtain the conve rgence theorem\nTheorem 2 Let\u03c4\u22641\n8maxi,jgi,j. Then,\u03bbdiv(g\u03ben) +\u03bbdiv\u2032(g\u03b7n)converges to the or-\nthogonal projection of w0onto the convex set \u03bbKasn\u2192 \u221e, andwnconverges to the\nsolution of (9).\n4 How to minimize the energies with graphcuts\n4.1 Principle\nGreig, Portehous and Seheult proved in [20] that discrete en ergy minimization can be\nexactly performed. Graphcuts have been introduced in compu ter vision by Y. Boykov\nand his collaborators in [8] as an algorithm for this type of m inimization. They have\nbeen extended to many areas : stereovision [26], medical ima ging [7]... The idea is to\nadd a \u201csource\u201d and a \u201csink\u201d in such a way that to each point in th e image grid a link\nis created to either the source or the sink. A cost is assigned to the links so that the\nglobal cost be related to the energy. Finally, solving the en ergy minimization problem\nis equivalent to \ufb01nd a cut of minimal cost along the graph (sou rce-points-sink). This\nis achieved by \ufb01nding a \u201cmaximal \ufb02ow\u201d along the edges of the gr aph, due to a duality\nbetween min-cut and max-\ufb02ow problems, \ufb01rst observed by Ford and Fulkerson.\n4.2 Construction\nWe recall the energy is (we replace \u03bb+\u00b5gbygfor simplicity)\nJ(\u03b8) =/summationtext\n(i,j)(\u03b1\u2212|v|i,j)\u03b8i,j+1\n2/summationtext\ni,jgi,j/parenleftbigg\n|\u03b8i+1,j\u2212\u03b8i,j|+|\u03b8i,j+1\u2212\u03b8i,j|\n+\u221a\n2\n2|\u03b8i+1,j+1\u2212\u03b8i,j|+\u221a\n2\n2|\u03b8i+1,j\u22121\u2212\u03b8i,j|/parenrightbigg\nwhich gives, with simpler notations (we denote a pixel x= (i,j))\nJ(\u03b8) =/summationdisplay\nx(\u03b1\u2212|v|x)\u03b8x+/summationdisplay\nx,ywx,y|\u03b8y\u2212\u03b8x|\nThe coe\ufb03cients wx,yare given by w((i,j),(i\u00b11,j)) =w((i,j),(i,j\u00b11)) =g(i,j)and\nw((i,j),(i\u00b11,j\u00b11)) =w((i,j),(i\u22131,j\u00b11)) =\u221a\n2\n2g(i,j).\n12One can see that the weights wx,yare nonsymmetric : wx,y/\\e}atio\\slash=wy,xdue to the presence\nofgwhich has a dependency with respect to the pixel. However the re is no particular\nproblem to introduce nonsymmetric weights, as Kolmogorov a nd Zabih have shown in\n[25] that graphcuts can handle energies involving an intera ction term which satis\ufb01es\nEinter(0,0) +Einter(1,1)\u2264Einter(0,1)+Einter(1,0).\nThen, we build the graph G= (V,E) made of vertices V={i, i= 1,...,N}\u222a{t}\u222a{s}\nand whose edges are\nE={(x,y)|wx,y>0}\u222a{(s,x)|1\u2264x\u2264N}\u222a{(x,t)|1\u2264x\u2264N}.\nAs a cut of this graph de\ufb01ne a partition ( Vs,Vt) of the graph into two sets, the \ufb01rst\none containing the source and the second one containing the s ink, the global cost of a\ncut is given by\nE(Vs,Vt) =/summationdisplay\ne=(a,b)\u2208E\na\u2208Vs,b\u2208VtC(e).\nSo what we would like to realize is E(Vs,Vt) =J(\u03b8). The construction is given by\nKolmogorov in [25], it consists in assigning the weight wx,yto an edge e= (x,y)\u2208 E\nin the image grid, the weight \u03b1+max iGito the edges ( s,x) and max iGi\u2212Gito the\nedges (x,t), then the equality between the global cost and the energy ho lds.\n5 Experimental results\nAll the experiments whose results are presented here were pe rformed on a laptop\nequiped with a 1.8GHz Pentium 4 and 1 Gb of RAM.\n5.1 Experiments with optical \ufb02ow\nFor the implementation, we have used the maxflow-v2.1 andenergy-v2.1 graphcuts\nimplementationofV.Kolmogorov, availableat http://www.cs.cornell.edu/People/vnk/software.html .\nType of capacities has been set to double, though shortorintleads to faster com-\nputation when quantized quantities are chosen in input.\nThe optical \ufb02ow is computed by the Weickert and Schn\u00a8 orr meth od [35] with a mul-\ntiresolution procedure (see [27]). As optical \ufb02ow computat ion has been improved since\n13the Weickert and Schn\u00a8 orr spatiotemporal model (using mixe d model combining local\nand global information, using intensity or gradient intens ity...), we emphasize that our\npurpose is not to obtain a very precise estimation of the opti cal \ufb02ow but to show how\nwe can improve this with the g-weighted term and thus to obtain a segmentation as\nclose as possible to the image edges. Figure 1 shows results o btained successively with\nTV1,g,TV1,g,\u03c0\n4and weighted standard perimeter TVg. One can see the result obtained\nwith Manhattan perimeter (diagonal neighbors) is quite com petitive with the one ob-\ntained with standard perimeter, especially it is more isotr opic, which is precisely what\nis aimed. Parameters are chosen from previous computations with classical snakes\n(see [31]). The values are set in relation with the range of va lue of the optical \ufb02ow\namplitude. For the weighted standard perimeter, the result is obtained in 0 .24 or 0.25\nsecond on all the images (of size 256 \u00d7256) of the sequence. For weighted Manhattan\nperimeter involving diagonal neighbors, the time is of 0 .11, 0.12 or 0.13 second. Same\ntimes are obtained with weighted Manhattan perimeter, thou gh it can reach 0 .09 or\n0.10 second on some images. All of these are obtained with the clock() C command.\nThe \ufb01gure 2 shows the results obtained by solving the ROF prob lem and thresh-\nolding the function. We emphasize again that it is a major adv antage over all previous\nway for solving this problem, since the function gives us all the solutions of the shape\noptimization problemsdependingon \u03b1. Thesegmentation shownon\ufb01gure3areindeed\nobtained simply by thresholding the function at the levels i ndicated (0 .5, 0.7 and 0.8).\nAs we had reasonable values of \u03b1from previous computations with classical snakes\n([31]), we just tried a few values, but one could choose \u03b1in a more sophisticated way,\nadapted to the histogram of the function solving the ROF mode l. Such parameter\noptimization could also be applied in the same way to a functi onal that was used by\nJehan-Besson, Barlaud and Aubert in [2] for video segmentat ion purpose (actually it\neven inspired the work [31])\nJ(\u2126) =/integraldisplay\n\u2126\u03b1dx+/integraldisplay\n\u2126|B\u2212I|(x)dx+\u03bb/integraldisplay\n\u2202\u2126dS\nwhereBrepresent a background image and Ithe current image in the movie. In the\n14Figure 1: Results obtained with graphcuts with the energy involving TV1,g(\ufb01rst image on\ntop left),TV1,g,\u03c0\n4(top right) and TVg(bottom). The initial data is the optical \ufb02ow norm v.\nParameters are \u03b1= 0.6,\u03bb= 0.2 and\u00b5= 10.\n15Figure 2: Results obtained (10th image of the sequence) with TV1,gandTV1,g,\u03c0\n4and the\noptical \ufb02ow norm as initial data. Parameters are \u03b1= 0.6,\u03bb= 0.2 and\u00b5= 10. Notice the\nsmoothness of the result on the right image in comparison to the one on the left image.\ndiscrete formalism which is used in this paper, it gives\n/summationdisplay\ni(\u03b1\u2212|B\u2212I|(i))\u03b8i+\u03bbTV1(\u03b8).\nIn this case it is a direct application of the previous work [1 1] (as before we have to\nmodify the perimeter to a Manhattan perimeter). The backgro und can be computed\nusing more or less sophisticated methods. We tried time medi an \ufb01lter and the method\nproposed by Kornprobst, Deriche and Aubert [3]. Some result s are shown on \ufb01gure 4\nfor\u03b1= 10,15,20,25 and\u03bb= 50.\nHere is the computational time (measured in seconds with the clock() C com-\nmand) of the algorithm (using the model described in [31]) on the ten \ufb01rst images of\nthe sequence for the total variation minimisation algorith ms (images are 256 \u00d7256,\nparameters are \u03b1= 0.6,\u03bb= 0.2 and\u00b5= 10). Iterations were stopped when the\nmaximum of the two residues between \u03benand\u03ben+1and between \u03b7nand\u03b7n+1become\nlower than 0 .002, a maximum of 2000 iterations being set to prevent the alg orithm to\nbecome too slow. The time step is \u03c4= 0.1. Such a value could be quite high, as we\nhave indicated the time step should be lower than1\n8maxi,jgi,j, but a simple trick is to\nwriteg= \u02dcgmaxg, which changes the regularization parameter from 1 to max g, and\n16Figure 3: In\ufb02uence of the \u03b1parameter. Results obtained (10th image of the sequence) with\ntotalvariationminimisation with TV1,g,\u03c0\n4and theoptical\ufb02ownormasinitialdata. Parameters\nare\u03bb= 0.2 and\u00b5= 10. From left to right and top to bottom : \u03b1= 0.5, 0.6, 0.7 and 0.8.\n17Figure 4: First image on the top : backgroundimage computed by time median \ufb01lt er. Results\nobtained (10th image of the sequence) with total variation minimisat ion (Manhattan with\nhorizontal, vertical and diagonal neighbors TV1,g,\u03c0\n4) for the Jehan-Besson\u2013Aubert\u2013Barlaud\nmodel (initial data: |B\u2212I|). Parameters are \u03bb= 50. From left to right and top to bottom :\n\u03b1= 10,15,20,25.\n18Projection algorithm :\ncomputational time with TV1,g,\u03c0\n4\ntime in seconds iteration number residue\n335.81 1739 0.001999\n324.57 1722 0.001999\n380.42 2001 0.002525\n312.18 1691 0.002\n314.08 1625 0.001999\n330.00 1786 0.001999\n379.77 2001 0.003226\n312.70 1698 0.001999\n371.66 2001 0.002088\nFigure 5: computational time of the TV regularization solving algorithm with TV1,g,\u03c0\n4. The\nresiduer= max(/ba\u2207dbl\u03ben+1\u2212\u03ben/ba\u2207dbll2,/ba\u2207dbl\u03b7n+1\u2212\u03b7n/ba\u2207dbll2).\nthus has no incidence over the time step condition, as this on e does not depend on\nthe regularization parameter. One could think the precisio n value is too low and leads\nto a quite heavy computational time, however, we have notice d that for a precision of\n0.01, the result is not su\ufb03ciently good for level sets extracti on (see \ufb01gure 5 where a\nresult is displayed for precisions 0 .01 and 0.002)\n6 Moving objects segmentation by a contrario detection\nThe method described here is inspired from previous works of Pelletier, Koep\ufb02er and\nDibos [21] and Caselles, Garrido and Igual [34]. The purpose is to decide whether a\npixel ismeaningful or not. In our case, the data is the solution of the ROF problem\nwith the particular choice of the weighted total variation. The meaningfulness is\ndecided between two hypothesis: H0\u201cthere is motion\u201dand H1\u201cthere is no motion\u201d.\nThe classical approach of hypothesis testing ( hypothesis testing model) is to suppose\n19Projection algorithm :\nProjection algorithm computational time with TV1,g\ntime in seconds iteration number residue\n52.17 446 0.001998\n61.32 530 0.001999\n67.54 584 0.002000\n47.59 412 0.001999\n49.50 429 0.001999\n54.33 473 0.001999\n66.56 553 0.001999\n58.20 495 0.002000\n56.25 461 0.001996\n60.58 484 0.001999\nFigure 6: computational time of the TV regularization solving algorithm with TV1,g. The\nresiduer= max(/ba\u2207dbl\u03ben+1\u2212\u03ben/ba\u2207dbll2,/ba\u2207dbl\u03b7n+1\u2212\u03b7n/ba\u2207dbll2).\nFigure 7: Results obtained (10th image of the sequence) with total variation minimisation\n(Manhattan with horizontal, vertical and diagonal neighbors) for two di\ufb00erent precisions :\n0.002 (left image) and 0 .01 (right image). Parameters are \u03bb= 0.2 and\u00b5= 10.\n20thatH0is true and to have a look at the observations under this assum ption. Another\napproach ( a contrario model) consists in deciding under the assumption that H1is\ntrue. Thiswas introducedin[1] as astatistical methodtopr ovideadecision tool which\nsimulates the Gestalt laws. The basic principle ( Helmholtz principle), is is based on\nthe fact that every large deviation from the noise should be p erceptible and thus is\ndecided to be meaningful.\nAround a pixel, we design a neighborhood N(x) of sizeN=n\u00d7nand we de\ufb01ne the\nrandom variable\nEx=1\nN/summationdisplay\ny\u2208N(x)\u03c8(|\u02dcV(y)|)\nwhere\u03c8:R\u2192[0,1] is a function designed to renormalize the data between zer o\nand one. The pixels {y\u2208N(x)}are assumed to be \u201cindependent\u201dand \u02dcVdenotes the\nrandom variable associated to the solution of the ROF proble m with the optical \ufb02ow\namplitude as initial data.\nLetExthe observed value of Ex. There is motion if Exis su\ufb03ciently high. Then\nunder the assumption that H1is true, the rejection test is [ Ex\u2265\u03b4], \u03b4 >0. But we\ndo not compute the value of \u03b4for a given level of meaningfulness as it is usually done\nin hypothesis testing, we compute the probability P[Ex\u2265Ex|H1] which is the motion\nprobability for the observed value Ex. For its evaluation, we need the Hoe\ufb00ding\u2019s\nTheorem [24], once we have estimated the mean of the random va riable\u03c8(|\u02dcV(y)|)\nfrom the observed values.\nTheorem 3 (Hoe\ufb00ding 1963) LetY1,...,YNbe independent variables with \u00b5i=\nE(Yi)\u2208(0,1)andP[0\u2264Yi\u22641] = 1for alli= 1,...,N. Let\u00b5=\u00b51+...+\u00b5n\nN. Then, for\n0<t<1\u2212\u00b5and\u00afY=Y1+...+YN\nN,\nP[\u00afY\u2212\u00b5\u2265t]\u2264exp(\u2212NH(\u00b5+t,\u00b5))\nwhereH(x,y) =xlog(x\ny)+(1\u2212x)log(1\u2212x\n1\u2212y)\nThen we de\ufb01ne the expected number of false alarms\nDe\ufb01nition 1 (NFA of a pixel) The number of false alarms is de\ufb01ned as:\nNFA(x) =NtotP[Ex\u2265Ex|H1]\n21whereNtotis the total number of pixels in the image.\nThe rejection of H1is decided if the NFA is lower than a parameter \u01eb. For the\nestimation of \u00b5, we simply compute the empirical mean of Exover the entire image:\n\u02c6\u00b5=1\nNtot/summationdisplay\niExi.\nUsing the Hoe\ufb00ding formula, a su\ufb03cient condition of rejectio n is then\nH(Ex,\u02c6\u00b5)\u22651\nNlog(Ntot\n\u01eb)\nfor \u02c6\u00b5<Ex<1.\nThe main reservation of the application of this framework to the solution of the ROF\nproblem is that the independency of this quantity over a neig hborhood is not veri-\n\ufb01ed. However, we would like to emphasize that the dependency should exist only on\nthe part of level lines included in the neighborhood. The TV r egularization does not\nsmooth accross the edges but along the edges. The second reas on of this use of the\nHoe\ufb00ding formula is that practically, we do not notice any pro blem to apply this.\nA post-treatment is donein orderto take into account the fac t that the region detected\nshould slightly surround the true motion region, due to the n eighborhood constructed\naround each pixel. We simply erode the mask obtained by a radi us of half the neigh-\nborhoodradius. At the end, we can compute the level set of the ROF problem solution\nwhich has the minimal di\ufb00erence with the result obtained with thea contrario detec-\ntion.\nWe present results on \ufb01gures 6 and 7. On the \ufb01gure 6 ( resp.7), the observation is the\nresult of the ROF problem with weighted TV and optical \ufb02ow nor m as initial data\n(resp.di\ufb00erence image B\u2212I); on the left image is the result obtained from the a\ncontrario detection (plus erosion), the closer level set is shown on th e right image. We\ncan notice the a contrario method is not able to discriminate between two moving cars\nin the image.\n22Figure 8: a contrario detection with the optical \ufb02ow magnitude regularized by TV1,g,\u03c0\n4. The\nneighborhood radius is N= 3. The\u01ebparameter is set to one as it is usually done. The left\nimage is the basic result of the a contrario detection erodedwith a radiusof 1. The rightimage\nis one level set of the ROF solution which looks like best the a contrario detection result.\nFigure 9: a contrario detection with the di\ufb00erence image between the current image and t he\nbackground regularized by TV1,g,\u03c0\n4. The neighborhood radius is N= 3. The\u01ebparameter is\nset to one as it is usually done. The left image is the basic result of the a contrario detection\neroded with a radius of 1. The right image is one level set of the ROF so lution which looks\nlike best the a contrario detection result.\n237 Conclusion\nIn this paper, we have extended the main result of [11] in orde r to handle shape\noptimization functionals involving weighted anisotropic perimeter. It states that all\nthe solutions of some shape optimization problems dependin g on a parameter \u03b1are\u03b1-\nlevel sets of the solution of the Rudin-Osher-Fatemi proble m. Thus the algorithm used\nfor total variation regularization \u2014 as in [11] \u2014 allows to co mpute all the solutions\nfor di\ufb00erent values of \u03b1in one pass. This is in our opinion the main advantage\nover classical snakes methods like Chan and Vese one in this p articular type of shape\noptimization.\nOn the other hand, we have also minimized the discrete versio n of the functional with\ngraph cuts techniques. The main advantage of this method is t hat it is very fast, and\nwhenever the advantage of the TV-minimization algorithm do es not occur when we\nemploy graph cuts, even a great number of computations of the algorithm leads to\na very competitive computational time (close to a single com putation of a classical\ncontinuous snake algorithm).\nWe have used these both methods on two video segmentation mod els : one introduced\nin [31] in which weighted perimeter is involved and a previou s one introduced by\nJehan-Besson, Barlaud and Aubert [2]. We would like to empha size that the general\nmodel studied inthe theoretical part of the papercovers man y applications. Onecould\nthink for example about segmentation with shape priors, usi ng a perimeter weighted\nby a distance to the prior. Such models have been used by Freed man and Zhang [22],\nor by Gastaud, Jehan-Besson, Barlaud and Aubert [23]...\nWe have also proposed o use an a contrario method for region \ufb01nding with the result\nof TV minimization process but without extracting a level se t at a prede\ufb01nite level.\nThis method do not lead to the most satisfactory results, but it is very fast since it\ndoes not require to choose a value of the parameter in particu lar and thus is better\nindicated for real-time applications.\n24"}
{"category": "abstract", "text": "In this paper, we are interested in the application to video segmenta tion of\nthe discrete shape optimization problem\n\u03bbJ(\u03b8)+/summationdisplay\ni(\u03b1\u2212fi)\u03b8i (1)\nincorporating a data f= (fi) and a total variation function J, and where the\nunknown\u03b8= (\u03b8i) with\u03b8i\u2208 {0,1}is a binary function representing the region to\nbe segmentedand \u03b1aparameter. Basedonthe recentworks[11], and Darbon and\nSigelle [14, 15], we justify the equivalence of the shape optimization pr oblem and\na weighted TV regularization in the case where Jis a \u201cweighted\u201d total variation.\nFor solving this problem, we adapt the projection algorithm propose d in [10] to\nthis case. Another way of solving (1) investigated here is to use gra ph cuts. Both\nmethods have the advantage to lead to a global minimum.\nSince we can distinguish moving objects from static elements of a sce ne by an-\nalyzing norm of the optical \ufb02ow vectors, we choose fas the optical \ufb02ow norm.\nIn order to have the contour as close as possible to an edge in the ima ge, we use\na classical edge detector function as the weight of the weighted to tal variation.\nThis model has been used in the former work [31]. We also apply the sam e meth-\nods to a video segmentation model used by Jehan-Besson, Barlaud and Aubert.\n\u2217CEREMADE, Universit\u00b4 e Paris Dauphine, Paris, France; ranc hin@ceremade.dauphine.fr\n\u2020CMAP, Ecole Polytechnique, Palaiseau, France; antonin.ch ambolle@polytechnique.fr\n\u2021LAGA & L2TI, Universit\u00b4 e Paris 13, Paris, France; dibos@mat h.univ-paris13.fr\n1In this case, it is a direct but interesting application of [11], as only st andard\nperimeter is incorporated in the shape functional. We also propose a nother way\nfor \ufb01nding moving objects by using an a contrario detection of objects on the im-\nage obtained by solving the Rudin-Osher-Fatemi Total Variation re gularization\nproblem.We can notice the segmentation can be associated to a level set in the\nformer methods.\nKeywords"}
{"category": "non-abstract", "text": "0809.3690v1  [cs.CV]  22 Sep 2008There are essentially two di\u000berent ways to generate models. The \frst\nmethod is to let a specialist build the model manually, using his technical\nexpertise. This is the most common method and unavoidable for systems that\ndo not yet exist. The second way is to generate models by observation and\ndescription. When the internals are not essential for the technical problem,\nthis method is an interesting option, especially for complicated systems.\nThis article is dedicated to the second way, which is called system identi\f-\ncation . While numerous papers deal with this topic, the framework presented\nhere is designed to have a much wider application range. In fact, it can be\nused for classi\fcation, recognition, regression, prediction, and reconstruction\nof disturbed patterns. This becomes possible due to a strict stochastic prob-\nlem de\fnition and a separation of data storing and data processing. For\ndata storing, we use an online kernel density estimation approach with com-\npression abilities and the possibility to forget unused knowledge. The data\nprocessing uses a modi\fed version of the Nadaraya-Watson regression. A\nspecial capability is the possibility to change arbitrary input and output of\nthe model and to mark some data as corrupt. This signi\fcantly increases\nthe \rexibility. A model and control strategy of a motorcar powertrain will\ndemonstrate these possibilities of the new framework.\n2 Basics\n2.1 Nadaraya-Watson Regression\nProbability densities are ideal for the modeling of uncertain knowledge. Com-\nmonly, technical relations are described by functions. For example, the func-\ntiony=f(x) maps a value x2Rdxto a valuey2Rdy. The inherent\nassumption is that the function fand the value xare known with in\fnite\ncertainty, which is mostly only approximately correct.\nThe second, more general option is to model the conditional probability\ndensitypX;Y(x;y). Because both parameters xandycan be interpreted as\nrealizations of random variables, it is possible to describe 'knowledge' and\n'uncertainty' in the same model. If a value x0is given, the related most\nprobable value y0can be computed by\ny0= argmax\n8y\b\npX;Y(x;y)jx=x0\t\n: (1)\nThis can also be applied for the opposite case, where y0is given and we\nwant to \fnd the most probable value for x. But this maximization is usually\ntoo costly. Although it seems to be very restrictive, for simple, unimodal\n2probability densities, the expression\ny0\u0019Z\n8yypYjX(yjx0)dy (2)\nwith\npYjX(yjx) =pX;Y(x;y)\npX(x)andpX(x) =Z\n8ypX;Y(x;y)dy; (3)\nwhich replaces the argmax-function with the expectation value, is often a\ngood approximation.\nUsually, probability densities have to be estimated based on sample data\nD=f(x1;y1);:::; (xn;yn)g. One way is the non-parametric kernel density\nestimation method, which turns every data point into a center of a kernel\nfunction. A compression algorithm can reduce the high memory consumption\nof this method. The resulting model structure is\n~pX;Y(x;y) =mX\nk=1ak\u001e(x\u0000xk;sxk)\u001e(y\u0000yk;syk); (4)\nwhich corresponds to a mixture model . Theakare weights with a sum of one.\nThexkandykare the centers of the kernels, while the sxkandsykdetermine\nthe smoothness of the estimation. An example for the kernel function \u001eis a\nGaussian with diagonal covariance matrix.\nInserting (4) into equation (2) and using the relation (3) results in an\nestimator for y0, givenx0:\ny0\u0019~y0=mP\nk=1akyk\u001e(x\u0000xk;sxk)\nmP\nk=1ak\u001e(x\u0000xk;sxk): (5)\nThis is known as a Nadaraya-Watson estimator, named after its inventors\nNadaraya (1964) and Watson (1964). Usually, this method is of little prac-\ntical value, because it interpolates poorly1. An improvement results from a\ncombination with the standard least-mean-square method (Cleveland, 1979;\nH\u007f ardle, 1990; Wang, 1990). For this, the error criterion\nEi=mX\nk=1ak\u001e(x\u0000xk;sxk)(\u000bi+\fT\nixk\u0000yki)2(6)\n1Equidistant sample points are an exception: The Shannon interpolation theorem is a\nNadaraya-Watson estimator and optimal, if the conditions of the sampling theorem are\nful\flled.\n3is de\fned for all i= 1;:::;d y. A minimization regarding \u000bi2Rand\fi2Rdx\ndelivers the regression function\n~yi= ^\u000bi+^\fT\nix: (7)\nNote that this is not a common linear function, because the optima ^ \u000biand\n^\fidepend onx. Only for homogenous akand very large values for the\nsmoothing parameters sxk, the formula (7) becomes a usual linear regres-\nsion, because the density estimation converges in this case against a uniform\ndistribution.\nNote also that expression (7) is identical to the Nadaraya-Watson regres-\nsion (5), if\fiis determined as zero by the minimization. This shows that\nthe Nadaraya-Watson regression is only an estimator of the order zero. The\n\frst order estimator (7) usually has a signi\fcantly better quality.\n2.2 The Associate Framework\nThe Nadaraya-Watson approximation, especially the linear localizing ver-\nsion, is an e\u000ecient and powerful method for extracting information that is\nrepresented by a probability density of the form (4). A description of the\ndensity estimation approach that we have used for our experiments is beyond\nthe scope of this work. We only want to mention here that we apply for the\ncomputation of the smoothing parameter an extended version of the method\ndescribed by Duin (1976) with online, real-time, and compression capabili-\nties. Furthermore, the approach is able to forget knowledge when it was not\nneeded for a long time.\nThe framework, which is composed of density estimation and the Nadaraya-\nWatson approach, can be used for various pattern recognition applications,\nsuch as classi\fcation, recognition, reconstruction, and regression. Figure 1\nshows the fundamental structure. In the center is the function Associate,\nwhich completes the missing elements in vector x. The information, which\nelements are missing, is stored in the binary valued vector bwith the same\ndimension. If a component is zero, the related component in vector xis miss-\ning and to be reconstructed. Furthermore, the function Associate assesses\nthe quality of the reconstructed vector ~x. This plausibility measurement can\nbe interpreted as the estimated probability for even more unlikely vectors\nthan ~x. A detailed description of this criterion is given by K\u007f uhn (2008).\nThe advantage of the framework is that data storing and data processing\nare entirely separated from each other. The density estimation works in\nparallel, independently, and fully automated. Its only purpose is to save\nthe observed data, without adding or deleting information. The evaluation\n4Associatex1?x3x4?x6x7\n1 0 1 1 0 1 1x1\u02dcx2x3x4\u02dcx5x6x7\nplausibility\n\u02dcpX(x)bx \u02dcxFigure 1: The function Associate may be used for regression, classi\fcation,\nand recognition. It estimates the missing components of the vector xand\ndelivers a plausibility measurement for the reconstructed vector.\nby the Associate function does not in\ruence this knowledge-saving process.\nHence, the framework is highly similar to a database: the probability density\ncontains the knowledge and the Associate function makes it available.\nWe will illustrate this with a short, theoretical example. Let us assume\nthat we have an arbitrary classi\fcation problem, which we want to solve.\nUsually, a training dataset is given\nD=f(m1;c1);:::; (mn;cn)g: (8)\nThemi, withi2[1;n], are pattern vectors with dimension dmand theci\nare class information vectors that encode the class to which mibelongs. The\ndimension of such a vector corresponds to the number of classes dc, with\nthejth component containing a measurement between zero and one for the\na\u000eliation of the pattern vector mito thejth class.\nIn order to use the associate framework, it is necessary to combine the\nvectorsmiandcito vectorsxi= (mi;ci)2Rdm+dc. After that, the proba-\nbility density pX(x) of the random variable Xcan be estimated based on the\ntraining dataset D=fx1;:::;xng. The Associate function can now classify\nnew pattern vectors m0by passing the vectors x= (m0\n1;:::;m0\ndm;?;:::; ?)\nandb= (1;:::; 1;0;:::; 0) to the function. The reconstructed elements in\nthe resulting vector ~xcorrespond to the class information vector. Further-\nmore, the function delivers the plausibility of the decision. This measurement\nis important if the class number is zero. In this case, there is no class infor-\n5mation vector to reconstruct and the decision by the plausibility becomes a\npure recognition. Note that it is possible to show that the classi\fcation by\nthis framework is identical to a Bayes-classi\fcation.\nFurthermore, the framework o\u000bers some additional possibilities. For ex-\nample, it is possible to mark an existing element of the pattern vector m0\nas unknown. This induces the Associate function to reconstruct this element\nas well. If this leads to a signi\fcant rising of the plausibility, this can be\nconsidered as an indication that the original value in the pattern vector was\ncorrupt. A careful utilization of this feature enables a classi\fcation even in\ncases were parts of the pattern vectors are corrupted.\nBut the application of the framework is not limited to classi\fcation or\nrecognition. The most interesting application is regression. The following\nsection will show this using a practical example.\n3 Modeling and Control of a Motorcar Pow-\nertrain\n3.1 Modeling\nFirst, we will describe the modeling of a motorcar powertrain with automatic\ngearbox in normal mode, before we show how the model can be used for\ncontrol engineering. For reasons of convenience and because there is no\nreason for the assumption that the modeling would not be successfully for\na real car, we will use here instead a complex ModelicaR\r/DymolaR\rmodel,\nwhich physically models engine, gearbox, and the other components.\nEssentially, it is possible to describe the speed v(t) of a car for the point in\ntimetas a function of the accelerator pedal position p(t)2[0:::1], the brake\npedal position b(t)2[0:::1], and the speed of the car for the point in time\nt\u0000dt, withdtdenoting a small time unit. Figure 2 shows the structure of the\nmodel. The feedback by the delay element is necessary, because the current\nspeedv(t) depends to a high degree on the speed a moment before. Note that\nthis model structure is only an approximation, which may be su\u000ecient or not.\nTechnical systems usually require further feedback loops with di\u000berent time\ndelays.\nAfter the model structure de\fnition, an estimation of the function ~fis\npossible. According to the associate framework, the inputs and outputs per\ntime step are combined to vectors\nx(t) = (v(t\u0000dt);v(t);p(t);b(t)): (9)\n6\u02dcf(v(t\u2212dt), p(t), b(t))\nv(t\u2212dt)b(t)v(t)\nz\u22121p(t)Figure 2: The model structure for the description of the dependencies be-\ntween accelerator pedal position p(t), brake pedal position b(t), and car speed\nv(t). The element z\u00001delays its input signal by a time unit dt.\nWith a set of such vectors, a modeling or estimation of the probability density\npX(x) is possible. There are two ways to get an appropriate sequence of\nvectorsx(t). The \frst, easier, option is to observe a driver. The second is to\ntry various inputs and to study the reactions of the systems\nFigure 3 shows the regression function that results from the online mod-\neling with our framework. It includes the behavior of the ModelicaR\rpower-\ntrain model for 500 seconds with a time resolution of one millisecond. The\nhigh time resolution was a requirement of the ModelicaR\rmodel and could be\nmost likely reduced for a real car. We have roughly approximated the driver\nby a random walk process.\nBecause of the simplifying model structure, it is necessary to consider\nlarge amounts of sample points in order to average out the dependencies of\ninternal states. A modeling with a spline based upon some pre-selected sam-\nple points would not lead to this result. Hence, \fgure 3 does not show a\nprecise relation, but describes only the most probable reaction of the power-\ntrain2.\nThe speeds v(t) andv(t+dt) di\u000ber only little from each other. For this\nreason, the graph of the estimated function ~f(v(t\u0000dt);p(t);b(t)) would result\n2However, the regression function itself could be used as a basis for a spline model.\n7in a simple slant, making the essential information di\u000ecult to recognize.\nHence, \fgure 3 shows, instead of ~f(v(t\u0000dt);p(t);b(t)), the di\u000berence ~f(v(t\u0000\ndt);p(t);b(t))\u0000v(t\u0000dt), which could be interpreted as acceleration. For\ninstance, a fully applied acceleration pedal leads to the highest acceleration\nat a low speed. On the other hand, the car hardly accelerates when the engine\nis idle and slows down noticeably for speeds over 60 km=h . The highest speed\nof the simulated car can be estimated to be approximately 180 km=h .\nv(t)\u2212v(t\u2212dt) [km/h ]\nv(t\u2212dt) [km/h ] p(t) [%/100]0.03\n-0.01\n-0.0200.010.020.040.05\n0.300.10.2\n0.40.50.60.70.80.91020406080100120140160180200\nFigure 3: The results of the modeling. The speed v(t) is a prognosis based\non the acceleration pedal position p(t) and the previous speed v(t\u0000dt). The\nbrake pedal position was disregarded and assumed as zero for this represen-\ntation.\n3.2 Feedback Control\nIt is only a small step from modeling to control. If it is known how a system\nwill react to an input, it is also possible to manipulate it systematically. For\nthis, however, it is necessary to invert the model, because in its original form\nit can only predict the reaction to a given input. For a precise manipulation,\non the contrary, it is necessary to \fnd an input that could result in the\ndesired output. This means that the output is given and we need to \fnd an\nappropriate input.\nThis is not a problem for our framework, because the Associate function\n(see \fgure 1) is able to use any component as input or output. The density\nestimation delivers predictions in both directions, because input and output\n8are modeled in the same way by a common distribution. In the following, we\nwill demonstrate this using the powertrain example.\nThe sample problem is to accelerate and decelerate the car to attain a\ngiven target speed. Because the target speed can be changed discontinuously\nand the car cannot reach arbitrary accelerations due to its inertia, not all\ntarget speed graphs can be realized. But it is desired that the car reaches\nthe target speed as fast as technically possible.\nAssociate\nplausibilityv(t\u2212dt)? b(t) p(t)\n1 0 1 1v(t\u2212dt)\u02dcv(t) b(t) p(t)\nAssociate\nplausibilityv(t\u2212dt)v(t) ? ?\n1 1 0 0v(t\u2212dt)v(t) \u02dcb(t) \u02dcp(t)forward model\nbackward model\nFigure 4: The associate function enables a forward and a backward predic-\ntion.\nThe estimated probability density ~ pX(x) and the Associate function allow\nus both to predict the behavior of the motorcar by a forward model and to\nguess theb(t) andp(t) ifv(t) is given and the current state of the car v(t\u0000dt)\nis known. Figure 4 shows the application of the Associate function for the\nimplementation of forward and backward prediction.\nThe backward model is also suitable as feedback controller, because a\nfeedback controller computes for reference and current values appropriate\ninputs. The well-known PID controller computes these inputs from the dif-\nference between setpoint and process variable using digital \flters or equiva-\nlent algorithms. This approach is fundamentally di\u000berent from the method\ndescribed here.\nThere are several important di\u000berences. One reason is that the models\n9used here are non-linear to a high degree, another one that the Associate\nfunction does not compute an error, that is, a di\u000berence between setpoint and\nprocess variable. Instead, it guesses which inputs could result in the desired\nsetpoints. These will then be applied to the controlled system, which leads\nto a reduction of the error, if the knowledge was su\u000ecient. Over the next\nsteps, the error becomes smaller and smaller, until the current value reaches\nmore or less the setpoint. At the same time, the controller can improve its\nknowledge about the controlled system, because the applied input leads to\na reaction that can be evaluated, regardless of whether it was the desired\nreaction or not. Both together, action and reaction, can be used as further\nexamples for the estimation of the probability density.\nSmall, random deviations of the controlled system from the 'normal be-\nhavior' cannot be foreseen by the controller, because it has no knowledge\nabout the in\ruences causing these disturbances. If it would have this knowl-\nedge, the behavior would be no longer stochastic, but deterministic and pre-\ndictable. In practice, there are always disturbances triggering reactions of\nthe system that do not correspond to the exact expected behavior. Because\nactions and reactions are modeled by a probability density, the controller\nessentially knows this degree of uncertainty. This allows it to express its\nexpectations in advance in the form of a plausibility criterion.\nSometimes, the normal behavior will change over time. Because of the\npermanently running update procedure of the estimated probability density,\nthe controller accommodates these changes automatically. This means that\nthe controller is able to learn and to compensate varying environmental con-\nditions. A special case of this auto-accommodation scenario is where the\ncontroller starts operation without having an estimated density. The only\nthing the controller can do at \frst is to guess. After setting the guessed\nvalues, the controller acquires some knowledge about the system from the\nreaction of the system it is controlling. This information can be used for the\nmodeling of the density estimation. In the next step, the controller has to\nguess again. But again, its knowledge improves. After many good or bad\ndecisions, the estimated probability density is good enough and the decisions\nof the controller are no longer blind guesses, but chosen and precise actions.\nThis self-contained way to learn by trial and error is risky of course,\nbecause it can result in damages to the controlled machine. But it is also\npossible to implement the initial learning by a 'teacher', who demonstrates a\nreasonable behavior. For our example, this teacher could be a human driver.\nThe controller only has to observe him until its predictions are good enough.\nAfter this, the backward model can replace the driver.\nWe will now complete these theoretical considerations with a discussion\nof the practical results of our powertrain example. The initial model was the\n100 5 10 15 20 25 30 35 40020406080100120140160180200\n-0.80.8\n0.6\n0.4\n0.2\n0.0\n-0.2\n-0.4\n-0.6\n00.10.20.30.40.50.60.70.80.91\n0 5 10 15 20 25 30 35 40\n00.10.20.30.40.50.60.70.80.91\n0 5 10 15 20 25 30 35 40desired speed in km/hdesired speederror\nerror in km/haccelerator pedal brake pedal\ntime in seconds\n0 5 10 15 20 25 30 35 40020406080100120140160180200desired speed in km/hdesired speederror\nerror in km/h\n00.10.20.30.40.50.60.70.80.91\n0 5 10 15 20 25 30 35 40051015\n-5\n-10\n-15\n00.10.20.30.40.50.60.70.80.91\n0 5 10 15 20 25 30 35 40accelerator pedal brake pedal\ntime in secondsFigure 5: The results of our controller experiment: The left plot shows that\nthe controller achieves the desired speed nearly perfectly. The error is every-\nwhere less than 1 km=h . The right side shows the results of increasing the\ncar mass from 1800 kgto 2800kg. The controller still works, the deviations\nare only the result of physical limitations.\nestimated probability density of the last section. What is most noticeable\nwhen using the backward model is that the density has only non-zero values\nforv(t\u0000dt)\u0019v(t). The reason is that within the short time period dtno large\nspeed changes occurred, something that is obviously technically impossible.\nAccordingly, the density was small if, for instance, the current speed v(t\u0000dt)\nwas zero, but a speed of v(t) = 100km=h was desired. Nevertheless, the\nAssociate function was immediately able to deliver good extrapolations. In\nthis case, the controller would have made the decision to open the accelerator\npedal far beyond the physical end-point. Although this is not possible, it is\ncorrect in the tendency. For this reason, all values for brake and accelerator\nwere limited to values in the valid range between zero and one. This would\nallow to immediately realize a functioning control.\nFor the simulation, the controller was tested with di\u000berent, arbitrarily\nchosen speed demands. An example is presented in \fgure 5. The left plot\n11shows that the speed attained corresponds almost exactly to the desired\nspeed. Also, a sudden increasing of the car mass from 1800 kgto 2800kgdoes\nnot lead to problems, despite that no longer every desired speed is realizable.\nFor instance, the engine is for speeds over 100 km=h simply not powerful\nenough, although the accelerator pedal is opened completely. Furthermore,\nthe car cannot brake fast enough sometimes. This control error cannot be\navoided.\n4 Conclusion\nA motorcar powertrain is a complex technical system and its physical mod-\neling is di\u000ecult. But a black box modeling with the associate framework is\nsimple. Already after less than 10 minutes observation, the associate frame-\nwork has a detailed model of the system and is able to control it. This seems\nto be not least due to the fact that the system can be described very well by\nonly four input and output values. Many internal details, which have to be\nconsidered by a physical modeling, can be neglected, because they are not\nvisible from the outside.\nThe associate framework obviously imitates a human driver. Even a pro-\nfessional driver does not have always a detailed physical idea of how the\nengine inside his car works. Nevertheless, he is able to control his car. He\nlearns and improves this ability by observation and permanent use. Just like\na human, the associate framework has no need for an initial set of param-\neters or external interventions by a supervisor. This becomes possible by\na strict separation of modeling and evaluation. The non-parametric density\nestimation, which runs permanently in the background, has the only purpose\nto save the observed knowledge as detailed and undistorted as possible. The\nevaluation of this information, for example, for the prediction, is based solely\non the laws of probability theory."}
{"category": "abstract", "text": "Black box models of technical systems are purely descriptive. They\ndo not explain why a system works the way it does. Thus, black box\nmodels are insu\u000ecient for some problems. But there are numerous\napplications, for example, in control engineering, for which a black\nbox model is absolutely su\u000ecient. In this article, we describe a general\nstochastic framework with which such models can be built easily and\nfully automated by observation. Furthermore, we give a practical\nexample and show how this framework can be used to model and\ncontrol a motorcar powertrain.\n1 Introduction\nThe modeling of technical systems is of eminent importance in engineering.\nOn the one hand, models are used to simulate systems in order to detect\nerrors during the design process. On the other hand models are developed\nbecause\n\u000fTests with existing systems are too expensive or too dangerous,\n\u000fSimulation results have to be reproducible or,\n\u000fTo detect a deviation of the normal behavior of a system.\nAnother aspect is the control of technical systems, as we will demonstrate in\nsection 3.2.\n1arXiv"}
{"category": "non-abstract", "text": "{joao, jleandro,\ncesar}@vision.ime.usp.br).\nH. Jelinek is with the School of Community Health, Charles St urt Univer-\nsity, Australia (e-mail: hjelinek@csu.edu.au).\nM. Cree is with the Department of Physics and Electronic Engi neering,\nUniversity of Waikato, Hamilton, New Zealand (e-mail: cree @waikato.ac.nz).\n(a) Inverted green channel of\nnon-mydriatic fundus image.\n(b) Pre-processed imagewith ex-\ntended border. The original im-\nage limit is indicated for illustra-\ntion.\nFig. 1. Fundus image pre-processing for removing undesired border effects.\nto the blood vessel patterns can prevent major vision loss as\nearly intervention becomes possible [5], [6].\nTo provide the opportunity for initial assessment to be\ncarried out by community health workers, computer based\nanalysis has been introduced, which includes assessment of\nthe presence of microaneurysms and changes in the blood\n\ufb02ow/vessel distribution due to either vessel narrowing, co m-\nplete occlusions or new vessel growth [7]\u2013[9].\nAn automatic assessment for blood vessel anomalies of the\noptic fundus initially requires the segmentation of the ves sels\nfrom the background, so that suitable feature extraction an d\nprocessing may be performed. Several methods have been\ndeveloped for vessel segmentation, but visual inspection a nd\nevaluation by receiver operating characteristic (ROC) ana lysis\nshows that there is still room for improvement [10], [11].\nIn addition, it is important to have segmentation algorithm s\nthat do not critically depend on con\ufb01guringseveral paramet ers\nso that untrained community health workers may utilize this\ntechnology.These limitations of the state-of-the-artalg orithms\nhave motivated the development of the framework described\nhere, which only depends on manually segmented images.\nMany different approaches for automated vessel segmenta-\ntion have been reported. The papers [12]\u2013[18] present vesse l\ntracking methods to obtain the vasculature structure, alon g\nwith vessel diameters and branching points. Tracking consi sts\nof following vessel center lines guided by local informatio n,\nusually trying to \ufb01nd the path which best matches a vessel\npro\ufb01le model. The use of deformable models also shows2\npromising results in [19]\u2013[22]. In [2], [23], [24], matched\n\ufb01lters are used to emphasize blood vessels. An improvement\nis obtained in [2] by a region-based threshold probing of\nthe matched \ufb01lter response. Multithreshold probing is dire ctly\napplied to the images in [25]. A non-linear \ufb01lter that enhanc es\nvessels by exploiting properties of the vessel pro\ufb01les is in -\ntroduced in [26]. Along this line is the use of mathematical\nmorphology \ufb01ltering in [27], [28], coupled with curvature\nevaluation. In [29], multi-scale curvature and border dete ction\nare used to drive a region growing algorithm.\nSupervisedmethodsforpixelclassi\ufb01cationhavebeenshown\nin [1], [30], [31]. In [30], feature vectors are formed by\ngray-scale values from a window centered on the pixel being\nclassi\ufb01ed. A window of values is also used in [31], but\nthe features used are a principal component transformation\nof RGB values and edge strength. In [1], ridge detection\nis used to form line elements and partition the image into\npatches belonging to each line element. Pixel features are\nthen generated based on this representation. Many features\nare presented and a feature selection scheme is used to selec t\nthose which provide the best class separability.\nPreviously, we have shown promising preliminary results\nusing the continuous wavelet transform (CWT) [32], [33]\nand integration of multi-scale information through superv ised\nclassi\ufb01cation [34]. Here we improve on those methods using\na Bayesian classi\ufb01er with Gaussian mixture models as class\nlikelihoods and evaluate performances with ROC analysis.\nROC analysis has been used for evaluation of segmentation\nmethods in [1], [2], [25] and comparison of some of the cited\nmethods in [10], [11].\nIn our approach, each pixel is represented by a feature\nvector including measurements at different scales taken fr om\nthecontinuoustwo-dimensionalMorletwavelettransform. The\nresulting feature space is used to classify each pixel as eit her\navesselornon-vessel pixel. We use a Bayesian classi\ufb01er with\nclass-conditional probability density functions (likeli hoods)\ndescribed as Gaussian mixtures, yielding a fast classi\ufb01cat ion,\nwhile being able to model complex decision surfaces and\ncompare its performance with the linear minimum squared\nerror classi\ufb01er.\nOriginally devised for suitably analyzingnon-stationary and\ninhomogeneous signals, the time-scale analysis took place to\naccomplish unsolvable problems within the Fourier frame-\nwork, based on the continuous wavelet transform (CWT).\nThe CWT is a powerful and versatile tool that has been\napplied to many different image processing problems, from\nimage coding [35] to shape analysis [36]. This success is\nlargely due to the fact that wavelets are especially suitabl e for\ndetecting singularities (e.g. edges and other visual featu res)\nin images [37], extracting instantaneous frequencies [38] , and\nperforming fractal and multi-fractal analysis. Furthermo re, the\nwavelettransformusingtheMorletwavelet,alsooftenrefe rred\nto as Gaborwavelet, hasplayeda centralrole in increasingo ur\nunderstanding of visual processing in different contexts f rom\nfeature detection to face tracking [39]. The Morlet wavelet\nis directional and capable of tuning to speci\ufb01c frequencies ,\nallowing it to be adjusted for vessel enhancement and noise\n\ufb01ltering in a single step. These nice characteristics motiv atetheadoptionofthe Morletwaveletinourproposedframework .\nThis work is organized as follows. The databases used\nfor tests are described in Subsection II-A. Subsection II-B\npresents our segmentation framework based on supervised\npixel classi\ufb01cation. In Subsection II-C the feature genera tion\nprocess is described, including the 2-D CWT and Morlet\nwavelet.Our use ofsupervisedclassi\ufb01cation andtheclassi \ufb01ers\ntested are presented in Subsection II-D. ROC analysis for\nperformance evaluation is described in Subsection II-E and\nresults are presented in Section III. Discussion and conclu sion\nare in Section IV.\nII. MATERIALS AND METHODS\nA. Materials\nThere are different ways of obtaining ocular fundus images,\nsuch as with non-mydriatic cameras, which do not require\nthe dilation of the eyes through drops, or through angiogram s\nusing \ufb02uorescein as a tracer [5]. We have tested our methods\non angiogram gray-level images and colored non-mydriatic\nimages [32], [34]. Here, our methods are tested and evaluate d\non two publicly available databases of non-mydriatic image s\nand corresponding manual segmentations: the DRIVE [1] and\nSTARE [2] databases.\nThe DRIVE database consists of 40 images (7 of which\npresent pathology), along with manual segmentations of the\nvessels. The images are captured in digital form from a Canon\nCR5 non-mydriatic 3CCD camera at 45\u25e6\ufb01eld of view (FOV).\nThe images are of size 768\u00d7584pixels, 8bits per color\nchannel and have a FOV of approximately 540pixels in\ndiameter. The images are in compressed JPEG-format, which\nis unfortunate for image processing but is commonly used in\nscreening practice.\nThe 40 images have been divided into a training and test\nset, each containing 20 images (the training set has 3 images\nwith pathology).Theyhavebeen manuallysegmentedby three\nobservers trained by an ophthalmologist. The images in the\ntraining set were segmented once, while images in the test se t\nweresegmentedtwice,resultinginsetsAandB.Theobserver s\nof sets A and B produced similar segmentations. In set A,\n12.7% of pixels where marked as vessel, against 12.3% vessel\nfor set B. Performance is measured on the test set using the\nsegmentations of set A as ground truth. The segmentations\nof set B are tested against those of A, serving as a human\nobserver reference for performance comparison.\nTheSTAREdatabaseconsistsof20digitizedslidescaptured\nby a TopCon TRV-50 fundus camera at 35\u25e6FOV. The slides\nwere digitized to 700\u00d7605pixels, 8bits per color channel.\nThe FOV in the images are approximately 650\u00d7550pixels in\ndiameter. Ten of the images contain pathology. Two observer s\nmanually segmented all images. The \ufb01rst observer segmented\n10.4%ofpixelsasvessel,against14.9%vesselsfortheseco nd\nobserver. The segmentations of the two observers are fairly\ndifferent in that the second observer segmented much more of\nthe thinner vessels than the \ufb01rst one.Performanceis comput ed\nwith the segmentations of the \ufb01rst observer as ground truth.3\nB. General framework\nThe image pixels of a fundus image are viewed as objects\nrepresentedby feature vectors, so that we may apply statist ical\nclassi\ufb01ers in order to segment the image. In this case, two\nclasses are considered, i.e. vessel \u00d7non-vessel pixels. The\ntraining set for the classi\ufb01er is derived by manual segmen-\ntations of training images, i.e. pixels segmented by hand ar e\nlabeledas vesselwhiletheremainingpixelsarelabeledas non-\nvessel. This approach allows us to integrate information from\nwavelet responses at multiple scales in order to distinguis h\npixels from each class.\nC. Pixel features\nWhen the RGB components of the non-mydriatic images\nare visualized separately, the green channel shows the best\nvessel/background contrast (Fig. 1(a)), whereas, the red a nd\nbluechannelsshowlowcontrastandareverynoisy.Therefor e,\nthe greenchannel was selected to be processed by the wavelet ,\nas well as to compose the feature vector itself, i.e. the gree n\nchannel intensity of each pixel is taken as one of its feature s.\nFor angiograms, the wavelet is applied directly to the gray-\nlevel values, which are also used to compose the feature\nvectors.\n1) Pre-processing: In order to reduce false detection of the\nborder of the camera\u2019s aperture by the wavelet transform, an\niterativealgorithmhasbeendeveloped.Ourintent isto rem ove\nthe strong contrast between the retinal fundus and the regio n\noutside the aperture (see Fig. 1).\nThe pre-processing algorithm consists of determining the\npixels outside the aperture that are neighbors to pixels ins ide\nthe aperture and replacing each of their values with the mean\nvalue of their neighbors inside the aperture. This process i s\nrepeated and can be seen as arti\ufb01cially increasing the area\ninside the aperture, as shown in Fig. 1(b).\nBefore the application of the wavelet transform to non-\nmydriatic images, we invert the green channel of the image,\nso that the vessels appear brighter than the background.\n2) Wavelet transform features: The notationand de\ufb01nitions\nin this section follow [40]. The real plane R\u00d7Ris denoted\nasR2, and the vectors are represented as bold letters, e.g.\nx,b\u2208R2. Letf\u2208L2be an image represented as a square\nintegrable (i.e. \ufb01nite energy) function de\ufb01ned over R2. The\ncontinuous wavelet transform T\u03c8(b,\u03b8,a)is de\ufb01ned as:\nT\u03c8(b,\u03b8,a) =C\u22121/2\n\u03c81\na/integraldisplay\n\u03c8\u2217(a\u22121r\u2212\u03b8(x\u2212b))f(x)d2x\nwhereC\u03c8,\u03c8,b,\u03b8andadenote the normalizing constant,\nanalyzing wavelet, the displacement vector, the rotation a ngle\nand the dilation parameter (also known as scale), respectiv ely.\n\u03c8\u2217denotes the complex conjugate of \u03c8.\nCombining the conditions for both the analyzing wavelet\nand its Fourier transform of being well localized in the time\nand frequency domain plus the requirement of having zero\nmean, one realizes that the wavelet transform provides a loc al\n\ufb01ltering at a constant rate\u2206\u03c9\n\u03c9, indicating its great ef\ufb01ciency\nas the frequency increases, i.e. as the scale decreases. Thi s\nproperty is what makes the wavelet effective for detection a ndanalysis of localized properties and singularities [38], s uch as\nthe blood vessels in the present case.\nAmong several available analyzing wavelets, for instance,\nthe 2-D Mexican hat and the optical wavelet, we chose the\n2-D Morlet wavelet for the purposes of this work, due to\nits directional selectiveness capability of detecting ori ented\nfeatures and \ufb01ne tuning to speci\ufb01c frequencies [38], [40].\nThis latter property is especially important in \ufb01ltering ou t\nthe background noise of the fundus images. The 2-D Morlet\nwavelet is de\ufb01ned as:\n\u03c8M(x) = exp(jk0x)exp/parenleftbigg\n\u22121\n2|Ax|2/parenrightbigg\nwherej=\u221a\u22121andA=diag[\u01eb\u22121/2,1],\u01eb\u22651is a2\u00d72\ndiagonal matrix that de\ufb01nes the anisotropy of the \ufb01lter, i.e . its\nelongation in any desired direction [38]. The Morlet wavele t\nis actually a complex exponential modulated Gaussian, wher e\nk0is a vector that de\ufb01nes the frequency of the complex\nexponential.\nWe haveset the \u01ebparameterto 8,makingthe \ufb01lter elongated\nandk0= [0,3], i.e. a low frequency complex exponential\nwith few signi\ufb01cant oscillations, as shown in Fig. 2. These\ntwo characteristics have been chosen in order to enable the\ntransform to present stronger responses for pixels associa ted\nwith the blood vessels.\nFor each considered scale value, we are interested in the re-\nsponse with maximum modulus over all possible orientations ,\ni.e.:\nM\u03c8(b,a) = max\n\u03b8|T\u03c8(b,\u03b8,a)| (1)\nThus, the Morlet wavelet transform is computed for \u03b8\nspanning from 0up to170degrees at steps of 10degrees and\nthe maximumis taken (thisis possible because |T\u03c8(b,\u03b8,a)|=\n|T\u03c8(b,\u03b8+ 180,a)|). The maximum modulus of the wavelet\ntransform over all angles for multiple scales are then taken as\npixel features. M\u03c8(b,a)is shown in Fig. 3 for a= 2and\na= 4pixels.\n3) Feature normalization: Given the dimensional nature of\nthe features forming the feature space, one must bear in mind\nthat this might give rise to errors in the classi\ufb01cation proc ess,\nas the units chosen might affect the distance in the feature\nspace.\nA strategy to obtain a new random variable with zero mean\nand unit standard deviations, yielding, in addition, dimen sion-\nless features, is to apply the normal transformation to the\nfeature space. The normal transformation is de\ufb01ned as [36]:\n\u02c6vi=vi\u2212\u00b5i\n\u03c3i\nwhereviis theithfeature assumed by each pixel, \u00b5iis\nthe average value of the ithfeature and \u03c3iis the associated\nstandard deviation.\nWe have applied the normal transformation separately to\neach image\u2019s feature space, i.e., every image\u2019s feature spa ce is\nnormalized by its own means and standard deviations, helpin g\nto compensate for intrinsic variation between images (e.g.\nillumination).4\nx y\u03c8M(x,y)\n(a) Surface representation of the real part.\n(b) Real part.\n (c) Imaginary part.\nFig. 2. Different representations for the 2-D Morlet wavele t (\u03c8M) with\nparameters\u01eb= 8andk0= [0,3].\nD. Supervised classi\ufb01cation for segmentation\nSupervised classi\ufb01cation has been applied to obtain the\n\ufb01nal segmentation, with the pixel classes de\ufb01ned as C1=\n{vessel pixels }andC2={non-vessel pixels }. In order to\nobtain the training set, several fundus image have been man-\nually segmented, allowing the creation of a labeled trainin g\nset into classes C1andC2(see Subsection II-A). Due to\nthe computational cost of training the classi\ufb01ers and the\nlarge number of samples, we randomly select a subset of the\navailable samples to use for actually training the classi\ufb01e rs.\nWe will present results for two different classi\ufb01ers, descr ibed\nbelow.\n1) Gaussian mixture model Bayesian classi\ufb01er: We have\nachieved very good results using a Bayesian classi\ufb01er in\nwhicheachclass-conditionalprobabilitydensityfunctio n(like-\nlihood) is described as a linear combination of Gaussian\nfunctions [41], [42]. We will call this the Gaussian mixture\nmodel(GMM) classi\ufb01er.\nTheBayes classi\ufb01cation rule for a feature vector vcan be\nstated in terms of posterior probabilities as\nDecideC1ifP(C1|v)>P(C2|v);\notherwise, decide C2(2)\n(a)M\u03c8(b,2).\n (b)M\u03c8(b,4).\nFig. 3. Maximum modulus of Morlet wavelet transform over ang les,\nM\u03c8(b,a)(Eq. 1), for scale values of a= 2anda= 4pixels. The remaining\nparameters are \ufb01xed at \u01eb= 8andk0= [0,3].\nWe recall Bayes rule :\nP(Ci|v) =p(v|Ci)P(Ci)\np(v)(3)\nwherep(v|Ci)is the class-conditional probability density\nfunction, also known as likelihood, P(Ci)is the prior proba-\nbility of class Ci, andp(v)is the probability density function\nofv(sometimes called evidence).\nTo obtain a decision rule based on estimates from our\ntraining set, we apply Bayes rule to Eq. 2, obtaining the\nequivalent decision rule:\nDecideC1ifp(v|C1)P(C1)>p(v|C2)p(C2);\notherwise, decide C2\nWe estimate P(Ci)asNi/N, the ratio of class isamples in\nthe training set. The class likelihoods are described as lin ear\ncombinations of Gaussian functions:\np(v|Ci) =ki/summationdisplay\nj=1p(v|j,Ci)Pj\nwherekiis the number of Gaussians modeling likelihood i,\nPjis the weight of Gaussian jand eachp(v|j,Ci)is ad-\ndimensional Gaussian distribution.\nFor each class i, we estimate the kiGaussian parameters\nand weights with the Expectation-Maximization (EM) algo-\nrithm [41]. The EM algorithm is an iterative scheme that\nguarantees a local maximum of the likelihood of the training\ndata.\nGMMs represent a halfway between purely nonparametric\nandparametricmodels,providingarelativelyfastclassi\ufb01 cation\nprocess at the cost of a more expensive training algorithm.\n2) Linear minimum squared error classi\ufb01er: We have also\ntested the linear minimum squared error classi\ufb01er [41], [42 ],\ndenoted LMSE. Linear classi\ufb01ers are de\ufb01ned by a linear\ndecision function gin thed-dimensional feature space:\ng(v) =wtv+w0 (4)5\nwherevis a feature vector, wis the weight vector and w0\nthe threshold.\nThe classi\ufb01cation rule is to decide C1ifg(v)>0andC2\notherwise. To simplify the formulation, the threshold w0is\naccommodated by de\ufb01ning the extended (d+ 1)-dimensional\nvectors v\u2032\u2261[vT,1]Tandw\u2032\u2261[wT,w0]T, so thatg(v) =\nw\u2032Tv\u2032.\nThe classi\ufb01er is determined by \ufb01nding w\u2032that minimizes\nthesum of error squares criterion:\nJ(w\u2032) =N/summationdisplay\ni=1(yi\u2212v\u2032T\niw\u2032)2\nwhereNis the total number of training samples, v\u2032\niis the\nextendedithtraining sample, and yiits desired output.\nThe criterion measures the sum of squared errors between\nthe true output of the classi\ufb01er ( v\u2032T\niw\u2032) and the desired output\n(yi). We have arbitrarily set yi= 1forvi\u2208C1andyi=\u22121\nforvi\u2208C2.\nLet us de\ufb01ne\nV=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0v\u2032T\n1\nv\u2032T\n2\n...\nv\u2032T\nN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,y=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\nMinimizing the criterion with respect to w\u2032results in:\n(VTV)\u02c6w\u2032=VTy\u21d2\u02c6w\u2032= (VTV)\u22121VTy\nIn comparison to the GMM classi\ufb01er, the LMSE classi-\n\ufb01er has a much faster training process, but is restricted in\nthe sense that it is linear, while GMMs allow for complex\ndecision boundaries. However, as we will show, the results\nobtained using LMSE are comparable to those using GMMs,\nrepresenting a reasonable trade-off.\nE. Experimental evaluation\nThe performances are measured using receiver operating\ncharacteristic (ROC) curves. ROC curves are plots of true\npositive fractions versus false positive fractions for var ying\nthresholds on the posterior probabilities. A pair formed by a\ntrue positive fraction and a false positive fraction is plot ted\non the graph for each threshold value (as explained below),\nproducing a curve as in Figs. 6 and 7. The true positive\nfraction is determinedby dividingthe numberof true positi ves\nby the total number of vessel pixels in the ground truth\nsegmentations, while the false positive fraction is the num ber\nof false positives divided by the total number of non-vessel\npixels in the ground truth. In our experiments, these fracti ons\nare calculated over all test images, considering only pixel s\ninside the FOV.\nFor the GMM classi\ufb01er, the ROC curve is produced by\nvarying the threshold on the posterior pixel probabilities (see\nEq. 3), while the LMSE ROC curve is produced varying the\nthresholdw0on the projection of the feature vectors on the\ndiscriminant vector (see Eq. 4).We have tested our methods on the DRIVE and STARE\ndatabases with the following settings. The pixel features u sed\nfor classi\ufb01cation were the inverted green channel and its\nmaximum Morlet transform response over angles M\u03c8(b,a)\n(Eq. 1) for scales a= 2,3,4,6pixels (see Subsection II-C).\nFor the DRIVE database, the training set was formed by\npixel samples from the 20 labeled training images. For the\nSTARE database, leave-one-out tests where performed, i.e. ,\nevery image is segmented using samples from the other 19\nimages for the training set. Due to the large number of pixels ,\nin all experiments, one million pixel samples where randoml y\nchosen to train the classi\ufb01ers. Tests were performed with th e\nLMSE and GMM classi\ufb01ers. For the GMM classi\ufb01er, we vary\nthe number k=k1=k2ofvesselandnon-vessel Gaussians\nmodeling each class likelihood.\nIII. RESULTS\nIllustrative segmentation results for a pair of images from\neach database (produced by the GMM classi\ufb01er with k= 20),\nalong with the manual segmentations, are shown in Figs. 4\nand 5.\nFor the DRIVE database, the manual segmentations from\nset A are used as ground truth and the human observer\nperformanceismeasuredusingthemanualsegmentationsfro m\nset B, which provide only one true/false positive fraction p air,\nappearingasa pointintheROCgraph(Fig.6).FortheSTARE\ndatabase, the \ufb01rst observer\u2019s manual segmentations are use d\nas ground truth, and the second observer\u2019s true/false posit ive\nfraction pair is plotted on the ROC graph (Fig. 7). The closer\nan ROC curve approaches the top left corner, the better the\nperformance of the method. A system that agreed completely\nwith the groundtruth segmentationswould yield an area unde r\nthe ROC curve Az= 1. However, note that the second sets\nof manual segmentations do not produce perfect true/false\npositive fractions, for the manual segmentations evaluate d\ndisagree on some of the pixels with the manual segmentations\nused as ground truth. Thus, the variance between observers\ncan be estimated, helping to set a goal for the method\u2019s\nperformance.\nThe areas under the ROC curves ( Az) are used as a single\nmeasure of the performance of each method and are shown in\nTable I for GMM classi\ufb01ers of varying kand for the LMSE\nclassi\ufb01er. For comparison with the manual segmentations, w e\nalso measure the accuracies (fraction of correctly classi\ufb01 ed\npixels) of the automatic and manual segmentations. Note tha t\nthe accuracy and Azvalues for the GMM classi\ufb01er increase\nwithk. The ROC curvesfor the DRIVE andSTARE databases\nproduced using the GMM classi\ufb01er with k= 20, as well as\nperformancesforhumanobservers,areshownin Figs. 6 and7.\nWe note that the EM training process for the GMMs is\ncomputationally more expensive as kincreases, while the\nclassi\ufb01cation phase is fast. On the other hand, LMSE is very\nfast for both training and classi\ufb01cation, but produces poor er\nresults, as seen in Table I.\nIV. DISCUSSION AND CONCLUSION\nThe Morlet transform shows itself ef\ufb01cient in enhancing\nvessel contrast, while \ufb01ltering out noise. Information fro m6\n(a) Posterior probabilities.\n (b) Segmentation.\n (c) Set A.\n (d) Set B.\n(e) Posterior probabilities.\n (f) Segmentation.\n (g) Set A.\n (h) Set B.\nFig. 4. Results produced by the GMM classi\ufb01er with k= 20and manual segmentations for two images from the DRIVE datab ase. The top row results are\nfor the image shown in Fig. 1(a).\n(a) Posterior probabilities.\n (b) Segmentation.\n (c) First observer.\n (d) Second observer.\n(e) Posterior probabilities.\n (f) Segmentation.\n (g) First observer.\n (h) Second observer.\nFig. 5. Results produced by the GMM classi\ufb01er with k= 20and manual segmentations for two images from the STARE datab ase. The top row images\noriginate from a pathological case, while the bottom ones or iginate from a normal case.7\n00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91\nfalse positive fractiontrue positive fraction\nGMM, k = 20\nSet B\nFig. 6. ROC curve for classi\ufb01cation on the DRIVE database usi ng the\nGMM classi\ufb01er with k= 20. The point marked as \u2014corresponds to set B,\nthe second set of manual segmentations. The method has Az= 0.9598.\n00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91\nfalse positive fractiontrue positive fraction\nGMM, k = 20\n2nd observer\nFig. 7. ROC curve for classi\ufb01cation on the STARE database usi ng the\nGMM classi\ufb01er with k= 20. The point marked as \u2014corresponds to the\nsecond observer\u2019s manual segmentations. The method has Az= 0.9651.\nTABLE I\nRESULTS FOR DIFFERENT CLASSIFICATION METHODS AND HUMAN\nOBSERVER .AzINDICATES THE AREA UNDER THE ROCCURVE,WHILE\nTHE ACCURACY IS THE FRACTION OF PIXELS CORRECTLY CLASSIFIED .\nDatabase\nClassi\ufb01cation Method DRIVE STARE\nAzAccuracy AzAccuracy\nLMSE 0.9520 0.9280 0.9584 0.9362\nGMM,k= 1 0.9250 0.9217 0.9394 0.9239\nGMM,k= 5 0.9537 0.9431 0.9609 0.9430\nGMM,k= 10 0.9570 0.9454 0.9627 0.9450\nGMM,k= 15 0.9588 0.9459 0.9648 0.9470\nGMM,k= 20 0.9598 0.9467 0.9651 0.9474\n2nd. observer 0.9473 0.9349Morlet transforms at different scales, which allows the seg -\nmentation of vessels of different diameters, are integrate d\nthrough the use of the statistical classi\ufb01ers presented. Th e\nLMSE classi\ufb01er shows a reasonable performance with a fast\nclassi\ufb01cation andtrainingphase,while the GMM classi\ufb01er h as\na computationally demanding training phase, but guarantee s a\nfast classi\ufb01cation phase and better performance.\nThe classi\ufb01cation framework demands the use of manual\nlabelings, but allows the methods to be trained for differen t\ntypes of images (provided the corresponding manual segmen-\ntations are available), possibly adjusted to speci\ufb01c camer a or\nlighting conditions and are otherwise automatic, i.e., adj ust-\nment of parametersor user interactionis not necessary.We a re\nstudying the use of training sets composed of a small portion\nof the image to be segmented. Using this approach, a semi-\nautomated fundus segmentation software may be developed,\nin which the operator only has to draw a small portion of the\nvessels over the input image or simply click on several pixel s\nassociated with the vessels. The remaining image would then\nbe segmented based on the partial training set. This approac h\nis interesting since it requires a small effort from the oper ator,\nwhich is compensated by the fact that image peculiarities ar e\ndirectly incorporated by the classi\ufb01er.\nIt is curious to note that, on the STARE database, the\naccuracy of the method is higher than that of the second\nobserver (Table I). The second observer\u2019s manual segmenta-\ntions contain much more of the thinnest vessels than the \ufb01rst\nobserver (lowering their accuracy), while the method, trai ned\nby the \ufb01rst observer, is able to segment the vessels at a simil ar\nrate. However, the ROC graph (Fig. 7) still re\ufb02ects the highe r\nprecisionofthesecondobserver,duetosomedif\ufb01cultiesfo und\nby the method, as discussed below.\nIt is possible to use only the skeleton of the segmentations\nfor the extraction of features from the vasculature. Depend ing\non the application, different evaluation methods become mo re\nappropriate [43]. For example, the evaluation of the skelet on\nwould not take into account the width of the vessels, but\ncould measure other qualities such as the presence of gaps\nand detection of branching points. Another interesting for m\nof evaluation would be directly through an application, suc h\nas in detection of neovascularizationby means of analysis a nd\nclassi\ufb01cation of the vessel structure [33]. A major dif\ufb01cul ty in\nevaluating the results is the establishment of a reliable gr ound\ntruth[44].Humanobserversare subjectiveandproneto erro rs,\nresulting in large variability between observations. Thus , it\nis desirable that multiple human-generated segmentations be\ncombined to establish a ground truth, which was not the case\nin the analysis presented.\nThoughverygoodROC results are presented,visual inspec-\ntion showssome typical dif\ufb01cultiesof the methodthat must b e\nsolved by future work. The major errors are in false detectio n\nof noise and other artifacts. False detection occurs in some\nimages for the border of the optic disc, haemorrhages and\nother types of pathologies that present strong contrast. Al so,\nthe method did not perform well for very large variations in\nlighting throughout an image, but this occurred for only one\nimage out of the 40 tested from both databases. This could\npossibly be solved by including intra-image normalization8\nin the pre-processing phase [45]. Another dif\ufb01culty is the\ninability to capture some of the thinnest vessels that are ba rely\nperceived by the human observers.\nAnother drawback of our approach is that it only takes\ninto account information local to each pixel through image\n\ufb01lters, ignoring useful information from shapes and struct ures\npresentintheimage.Weintendtoworkonmethodsaddressing\nthis drawback in the near future. The results can be slightly\nimproved through a post-processing of the segmentations fo r\nremoval of noise and inclusion of missing vessel pixels as\nin [34]. An intermediate result of our method is the intensit y\nimage of posterior probabilities, which could possibly ben e\ufb01t\nfrom a threshold probing as in [2] or region growing schemes.\nAutomated segmentation of non-mydriatic images provides\nthe basis for automated assessment by community health\nworkers. Skeletonized images of the vessel pattern of the\nocular funduscan be analyzed mathematically using nonline ar\nmethods such as global fractal [33] and local fractal [7] ana l-\nysis based on the wavelet transform thus providing a numeric\nindicator of the extent of neovascularization. Our ongoing\nwork aims at applying the shape analysis and classi\ufb01cation\nstrategies described in [33] to the segmented vessels produ ced\nby method described in this work.\nACKNOWLEDGMENTS\nThe authors thank J. J. Staal et al.[1] and A. Hoover et\nal.[2] for making their databases publicly available and Dr.\nAlan Luckie and Chris McQuellin from the Albury Eye Clinic\nfor providing \ufb02uorescein images used during our research.\nREFERENCES\n[1] J. J. Staal, M. D. Abr` amoff, M. Niemeijer, M. A. Viergeve r, and B. van\nGinneken, \u201cRidge based vessel segmentation in color images of the\nretina,\u201dIEEE Transactions on Medical Imaging , vol. 23, no. 4, pp. 501\u2013\n509, 2004.\n[2] A. Hoover, V. Kouznetsova, and M. Goldbaum, \u201cLocating bl ood vessels\nin retinal images by piece-wise threshold probing of a match ed \ufb01lter\nresponse,\u201d IEEE Transactions on Medical Imaging , vol. 19, pp. 203\u2013\n210, March 2000.\n[3] J. J. Kanski, Clinical Ophthalmology: A systematic approach . London:\nButterworth-Heinemann, 1989.\n[4] E. J. Sussman, W. G. Tsiaras, and K. A. Soper, \u201cDiagnosis o f diabetic\neye disease,\u201d Journal of the American Medical Association , vol. 247,\npp. 3231\u20133234, 1982.\n[5] S. J. Lee, C. A. McCarty, H. R. Taylor, and J. E. Keeffe, \u201cCo sts of\nmobile screening for diabetic retinopathy: A practical fra mework for\nrural populations,\u201d Aust J Rural Health , vol. 8, pp. 186\u2013192, 2001.\n[6] H. R. Taylor and J. E. Keeffe, \u201cWorld blindness: a 21st cen tury\nperspective,\u201d British Journal of Ophthalmology , vol. 85, pp. 261\u2013266,\n2001.\n[7] C. P. McQuellin, H. F. Jelinek, and G. Joss, \u201cCharacteris ation of\n\ufb02uorescein angiograms of retinal fundus using mathematica l morphol-\nogy: a pilot study,\u201d in 5th International Conference on Ophthalmic\nPhotography , Adelaide, 2002, p. 152.\n[8] L. Streeter and M. J. Cree, \u201cMicroaneurysm detection in c olour fundus\nimages,\u201d in Image and Vision Computing New Zealand , Palmerston\nNorth, New Zealand, November 2003, pp. 280\u2013284.\n[9] T. Y. Wong, W. Rosamond, P. P. Chang, D. J. Couper, A. R. Sha rrett,\nL. D. Hubbard, A. R. Folsom, and R. Klein, \u201cRetinopathy and ri sk of\ncongestive heart failure,\u201d Journal of the American Medical Association ,\nvol. 293, no. 1, pp. 63\u201369, 2005.\n[10] M. Niemeijer, J. J. Staal, B. van Ginneken, M. Loog, and M . D.\nAbr` amoff, \u201cComparative study of retinal vessel segmentat ion methods\non a new publicly available database,\u201d in SPIE Medical Imaging , J. M.\nFitzpatrick and M. Sonka, Eds., vol. 5370, 2004, pp. 648\u2013656 .[11] M. J. Cree, J. J. G. Leandro, J. V. B. Soares, R. M. Cesar-J r., G. Tang,\nH. F. Jelinek, and D. J. Cornforth, \u201cComparison of various me thods to\ndelineate blood vessels in retinal images,\u201d in Proc. of the 16th National\nCongress of the Australian Institute of Physics , Canberra, Australia,\n2005.\n[12] I. Liu and Y. Sun, \u201cRecursive tracking of vascular netwo rks in an-\ngiograms based on the detection-deletion scheme,\u201d IEEE Transactions\non Medical Imaging , vol. 12, no. 2, pp. 334\u2013341, 1993.\n[13] L. Zhou, M. S. Rzeszotarski, L. J. Singerman, and J. M. Ch okreff, \u201cThe\ndetection and quanti\ufb01cation of retinopathy using digital a ngiograms,\u201d\nIEEE Transactions on Medical Imaging , vol. 13, no. 4, pp. 619\u2013626,\n1994.\n[14] O. Chutatape, L. Zheng, and S. M. Krishnan, \u201cRetinal blo od vessel\ndetection and tracking by matched gaussian and kalman \ufb01lter s,\u201d inProc.\nof the 20th Annual International Conference of the IEEE Engi neering\nin Medicine and Biology Society, (EMBS\u201998) , vol. 20, 1998, pp. 3144\u2013\n3149.\n[15] Y. A. Tolias and S. M. Panas, \u201cA fuzzy vessel tracking alg orithm for\nretinal images based onfuzzy clustering,\u201d IEEETransactions onMedical\nImaging, vol. 17, pp. 263\u2013273, April 1998.\n[16] A.Can, H.Shen, J.N.Turner, H.L.Tanenbaum, and B. Roys am, \u201cRapid\nautomated tracing and feature extraction from retinal fund us images\nusing direct exploratory algorithms,\u201d IEEE Transactions on Information\nTechnology in Biomedicine , vol. 3, no. 2, pp. 125\u2013138, 1999.\n[17] M. Lalonde, L. Gagnon, and M.-C. Boucher, \u201cNon-recursi ve paired\ntracking for vessel extraction from retinal images,\u201d in Proc. of the\nConference Vision Interface 2000 , 2000, pp. 61\u201368.\n[18] X. Gao, A. Bharath, A. Stanton, A. Hughes, N. Chapman, an d S. Thom,\n\u201cA method of vessel tracking for vessel diameter measuremen t on retinal\nimages,\u201d in ICIP01, 2001, pp. II: 881\u2013884.\n[19] T. McInerney and D. Terzopoulos, \u201cT-snakes: Topology a daptive\nsnakes,\u201dMedical Image Analysis , vol. 4, pp. 73\u201391, 2000.\n[20] R. Toledo, X. Orriols, X. Binefa, P. Radeva, J. Vitria, a nd J. Villanueva,\n\u201cTracking of elongated structures using statistical snake s,\u201d inIEEECom-\nputer Society Conference on Computer Vision and Pattern Rec ognition\n(CVPR), vol. 1, 2000, p. 1157.\n[21] A.Vasilevskiy and K.Siddiqi, \u201cFlux maximizing geomet ric \ufb02ows,\u201d IEEE\nTransactions on Pattern Analysis and Machine Intelligence , pp. 1565\u2013\n1578, 2002.\n[22] D. Nain, A. Yezzi, and G. Turk, \u201cVessel segmentation usi ng a shape\ndriven \ufb02ow,\u201d in Medical Image Computing and Computer-assisted\nIntervention - MICCAI , 2004, pp. 51\u201359.\n[23] S. Chaudhuri, S. Chatterjee, N. Katz, M. Nelson, and M. G oldbaum,\n\u201cDetection of blood vessels in retinal images using two-dim ensional\nmatched \ufb01lters,\u201d IEEE Transactions on Medical Imaging , pp. 263\u2013269,\n1989.\n[24] L. Gang, O. Chutatape, and S. M. Krishnan, \u201cDetection an d measure-\nment of retinal vessels in fundus images using amplitude mod i\ufb01ed\nsecond-order gaussian \ufb01lter,\u201d IEEE Transactions on Biomedical Engi-\nneering, vol. 49, no. 2, pp. 168\u2013172, 2002.\n[25] X. Jiang and D. Mojon, \u201cAdaptive local thresholding by v eri\ufb01cation-\nbased multithreshold probing with application to vessel de tection in\nretinal images,\u201d IEEE Transactions on Pattern Analysis and Machine\nIntelligence , vol. 25, no. 1, pp. 131\u2013137, 2003.\n[26] J. Lowell, A. Hunter, D. Steel, A. Basu, R. Ryder, and R. K ennedy,\n\u201cMeasurement of retinal vessel widths from fundus images ba sed on 2-\nD modeling,\u201d IEEE Transactions on Medical Imaging , vol. 23, no. 10,\npp. 1196\u20131204, October 2004.\n[27] F. Zana and J.-C. Klein, \u201cSegmentation of vessel-like p atterns using\nmathematical morphology and curvature evaluation,\u201d IEEE Transactions\non Image Processing , vol. 10, pp. 1010\u20131019, 2001.\n[28] B. Fang, W. Hsu, and M. Lee, \u201cReconstruction of vascular structures in\nretinal images,\u201d in ICIP03, 2003, pp. II: 157\u2013160.\n[29] M. E. Mart\u00b4 \u0131nez-P\u00b4 erez, A. D. Hughes, A. V. Stanton, S. A . Thom,\nA. A. Bharath, and K. H. Parker, \u201cRetinal blood vessel segmen tation by\nmeans of scale-space analysis and region growing,\u201d in Medical Image\nComputing and Computer-assisted Intervention - MICCAI , 1999, pp.\n90\u201397.\n[30] R. Nekovei and Y. Sun,\u201cBack-propagation network and it s con\ufb01guration\nfor blood vessel detection in angiograms,\u201d IEEE Transactions on Neural\nNetworks, vol. 6, no. 1, pp. 64\u201372, 1995.\n[31] C. Sinthanayothin, J. Boyce, and C. T. Williamson, \u201cAut omated local-\nisation of the optic disc, fovea, and retinal blood vessels f rom digital\ncolour fundus images,\u201d British Journal of Ophthalmology , vol. 83, pp.\n902\u2013910, 1999.\n[32] J.J.G.Leandro, R. M.Cesar-Jr., and H. Jelinek, \u201cBlood vessels segmen-\ntation in retina: Preliminary assessment of the mathematic al morphology9\nand of the wavelet transform techniques,\u201d in Proc. of the 14th Brazilian\nSymposium on Computer Graphics and Image Processing . IEEE\nComputer Society, 2001, pp. 84\u201390.\n[33] H. F.Jelinek and R.M. Cesar-Jr., \u201cSegmentation of reti nal fundus vascu-\nlature in non-mydriatic camera images using wavelets,\u201d in Angiography\nand Plaque Imaging: Advanced Segmentation Techniques , J. Suri and\nT. Laxminarayan, Eds. CRC Press, 2003, pp. 193\u2013224.\n[34] J. J. G. Leandro, J. V. B. Soares, R. M. Cesar-Jr., and H. F . Jelinek,\n\u201cBlood vessels segmentation in non-mydriatic images using wavelets\nand statistical classi\ufb01ers,\u201d in Proc. of the 16th Brazilian Symposium on\nComputer Graphics and Image Processing . IEEE Computer Society\nPress, 2003, pp. 262\u2013269.\n[35] O. Rioul and M. Vetterli, \u201cWavelets and signal processi ng,\u201dIEEE Signal\nProcessing Magazine , pp. 14\u201338, Oct. 1991.\n[36] L. da F. Costa and R. M. Cesar-Jr., Shape analysis and classi\ufb01cation:\ntheory and practice . CRC Press, 2001.\n[37] A. Grossmann, \u201cWavelet transforms and edge detection, \u201d inStochastic\nProcesses in Physics and Engineering , S. A. et al., Ed. D. Reidel\nPublishing Company, 1988, pp. 149\u2013157.\n[38] J.-P. Antoine, P. Carette, R. Murenzi, and B. Piette, \u201cI mage analysis\nwith two-dimensional continuous wavelet transform,\u201d Signal Processing ,\nvol. 31, pp. 241\u2013272, 1993.\n[39] R.S.Feris,V.Krueger, and R.M.Cesar-Jr.,\u201cAwavelet s ubspace method\nfor real-time face tracking,\u201d Real-Time Imaging , vol. 10, pp. 339\u2013350,\n2004.\n[40] A. Arn\u00b4 eodo, N. Decoster, and S. G. Roux, \u201cA wavelet-bas ed method\nfor multifractal image analysis. i. methodology and test ap plications\non isotropic and anisotropic random rough surfaces,\u201d The European\nPhysical Journal B , vol. 15, pp. 567\u2013600, 2000.\n[41] S. Theodoridis and K. Koutroumbas, Pattern Recognition , 1st ed. USA:\nAcademic Press, 1999.\n[42] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classi\ufb01cation . John\nWiley and Sons, 2001.\n[43] K. W. Bowyer and P. J. Phillips, Eds., Empirical Evaluation Techniques\nin Computer Vision . IEEE Computer Society, 1998.\n[44] K. H. Fritzsche, A. Can, H. Shen, C.-L. Tsai, J. N. Turner , H. L.\nTanenbaum, C. V. Stewart, and B. Roysam, \u201cAutomated model-b ased\nsegmentation, tracing, and analysis of retinal vasculatur e fom digital\nfundus images,\u201d in Angiography and Plaque Imaging: Advanced Seg-\nmentation Techniques , J. Suri and S. Laxminarayan, Eds. CRC Press,\n2003, pp. 225\u2013297.\n[45] M. J. Cree, E. Gamble, and D. Cornforth, \u201cColour normali sation\nto reduce interpatient and intra-patient variability in mi croaneurysm\ndetection in colour retinal images,\u201d in WDIC2005 ARPS Workshop on\nDigital Image Computing , Brisbane, Australia, February 2005, pp. 163\u2013\n168."}
{"category": "abstract", "text": "\u2014We present a method for automated segmentation\nof the vasculature in retinal images. The method produces\nsegmentations by classifying each image pixel as vesselornon-\nvessel, based on the pixel\u2019s feature vector. Feature vectors are\ncomposed of the pixel\u2019s intensityand continuous two-dimen sional\nMorlet wavelet transform responses taken at multiple scale s. The\nMorlet wavelet is capable of tuning to speci\ufb01c frequencies, thus\nallowing noise \ufb01ltering and vessel enhancement in a single s tep.\nWe use a Bayesian classi\ufb01er with class-conditional probabi lity\ndensity functions (likelihoods) described as Gaussian mix tures,\nyielding a fast classi\ufb01cation, while being able to model com plex\ndecision surfaces and compare its performance with the line ar\nminimum squared error classi\ufb01er. The probability distribu tions\nare estimated based on a training set of labeled pixels ob-\ntained from manual segmentations. The method\u2019s performanc e\nis evaluated on publicly available DRIVE [1] and STARE [2]\ndatabases of manually labeled non-mydriatic images. On the\nDRIVE database, itachieves an areaunderthe receiver opera ting\ncharacteristic (ROC) curve of 0.9598, beingslightlysuper ior than\nthat presented by the method of Staal et al.[1].\nIndex Terms \u2014Fundus, Morlet, pattern classi\ufb01cation, retina,\nvessel segmentation, wavelet.\nI. INTRODUCTION\nOPTIC fundus (Fig. 1(a)) assessment has been widely\nused by the medical community for diagnosing vas-\ncular and non-vascular pathology. Inspection of the retina l\nvasculaturemayrevealhypertension,diabetes, arteriosc lerosis,\ncardiovascular disease and stroke [3]. Diabetic retinopat hy is\na major cause of adult blindness due to changes in blood\nvessel structure and distribution such as new vessel growth\n(proliferativediabeticretinopathy)andrequireslabori ousanal-\nysis from a specialist [4]. Endeavoring to reduce the effect\nof proliferative diabetic retinopathy includes obtaining and\nanalyzing images of the optic fundus at regular intervals su ch\nas every six months to a year. Early recognition of changes\nc/circlecopyrt2006 IEEE. Personal use of this material is permitted. Howev er, permis-\nsion to reprint/republish this material for advertising or promotional purposes\nor for creating new collective works for resale or redistrib ution to servers or\nlists, or to reuse any copyrighted component of this work in o ther works must\nbe obtained from the IEEE.\nThis work was supported by CNPq (131403/2004-4, 300722/98- 2 and\n474596/2004-4), FAPESP (99/12765-2), the Australian Diab etes Association\nand the CSU CoS.\nJ. Soares, J. Leandro, and R. Cesar-Jr. are with the Institut e of Mathematics\nand Statistics - University of S\u02dc ao Paulo - USP,Brazil (e-ma ils"}
{"category": "non-abstract", "text": "Face recognition, Principal Component Analysis, Log-Gabo r \ufb01lters,\nFERET database\n1 Introduction\nPrincipal Component Analysis (PCA) or Karhunen Loeve Transform (KLT)\n- based face recognition method was proposed in (Turk and Pentlan d, 1991)\nand became very popular because of its relatively simple implementatio n\nand high recognition accuracy. During past \ufb01fteen years face rec ognition was\na \ufb01eld of active research, and many other statistical methods (re lated to\nPCA)wereinvestigatedandproposedforfacerecognition:Linear Discriminant\nAnalysis (LDA), Independent Component Analysis (ICA) (Bartlett et al.,\nEmail address: vperlib@mmlab.ktu.lt (Vytautas Perlibakas).\nPreprint submitted to 11 March 20182002), Kernel PCA, Dual PCA (Moghaddam, 2002). Because KLT is data\ndependent and is not very fast, other transforms were also used for face\nrecognition: Discrete Cosine Transform (DCT) (Hafed and Levine, 2001),\nFast Fourier Transform (FFT) (Spies and Ricketts, 2000), Discre te Wavelet\nTransform(DWT) (Feng et al.,2000),Wavelet Packet Decompositio n(WPD)\n(Garcia et al., 2000). Wiskott et al. (1997) proposed Elastic Bunch G raph\nMatching (EBGM) and Gabor wavelets -based face recognition meth od that\nachievedveryhighrecognitionaccuracy. Escobar and Solar(2002 )usedEBGM\n-based recognition of faces in Log-Polar coordinates. Comprehen sive overview\nof various face recognition methods could be found in (Zhao et al., 2 000). As\nit was shown by numerous experiments, face recognition accuracy can be in-\ncreasedbycombiningseveralmethods,forexample,DCT+PCA (Ra masubramanian and Venkatesh,\n2001),DWT+PCA (Feng et al.,2000),WPD+PCA (Perlibakas,2004), PCA+LDA\n(Zhao et al., 1998), or by using various image pre-processing metho ds. Recent\nresults also showed that using Gabor or Log-Gabor features inste ad of tra-\nditional greyscale features and by combining these features with w ell known\nrecognition methods like PCA, ICA, LDA or SVM it is possible to achieve\nvery high recognition acuracy. Now we will overview various combined face\nrecognition methods that use Gabor or Log-Gabor features and t hat are re-\nlated with a method that we propose in this publication. Although many\nresearchers used the same Gabor \ufb01lters and well known feature c ompression\nand classi\ufb01cation methods, all proposed methods di\ufb00er from each o ther by\nfeature selection techniques, parameters of \ufb01lters, used classi\ufb01 cation method\nand its parameters, distance measure, and image normalization met hod. Af-\nter \ufb01ltering face image with Gabor \ufb01lters of multiple scales (usually 4-5 ) and\norientations (usually 6-8) we get very large number of features (2 4-40 images\nof the same dimensions as initial image). Perhaps one of the most impo rtant\nquestions is how to reduce the number of these features for furt her process-\ning. The most popular method is to extract Gabor features at a sma ll number\n(usually less than 100) of face points around face features (like ey es, lips, nose)\nthat were detected using EBGM (Wiskott et al., 1997) or similar metho d. At\neach detected point are extracted Gabor features from all scale s and orienta-\ntions. Lyons et al. (2000) combined EBGM Gabor features and LDA- based\nrecognition method. Smeraldi and Bigun (2002) detected face fea tures us-\ning saccadic search with a set of Log-Gabor \ufb01lters that were arran ged to\nconcentric circles (retinas). At detected points were extracted Log-Gabor fea-\ntures and passed to the SVM -based classi\ufb01er. EBGM -based featu re selection\nwas also used by Wang and Tang (2003) for Bayesian PCA -based rec ogni-\ntion. Because EBGM -based methods require training with manually lab elled\nfaces, other researchers use feature extraction methods tha t do not require\nsuch training. One possible approach is to combine Gabor magnitude im ages\nfrom all scales and orientations to a single feature vector (image) a nd use this\nvector for recognition. Such method was used by Zhang et al. (200 4) for Ga-\nbor+AdaBoost face recognition. Because such combined feature vectors may\nbe too large for further processing, we can reduce the number of features by\n2using smaller initial images. Liu and Wechsler (2002) decided to downsa mple\nfeature images of each scale and resolution and then combine these features to\na single vector for Gabor+Enhanced LDA (Liu and Wechsler, 2002) a nd Ga-\nbor+ICA (Liu and Wechsler, 2003) -based recognition using L1, Euclidean,\nand cosine -based distance measures. In order to reduce the num ber of Ga-\nbor features, Kepenekci et al. (2002) used sliding window based se arch at all\nscales and orientations. In each window were extracted features with maximal\nmagnitudes, stored their locations, and then both magnitudes and locations\nwere used for comparison. For recognition was used very similar dist ance mea-\nsure that was used by (Wiskott et al., 1997) with additional constra ints to\nfeature locations.\nIn this article we propose to \ufb01nd the locations of Log-Gabor (Field, 1 987)\nfeatures with maximal magnitudes at single scale and multiple orientat ions\nusing sliding window -based search and then use the same feature loc ations\nfor all other scales. For further feature compression we used Pr incipal Com-\nponent Analysis (PCA) because its simple implementation, fast trainin g and\nbecause using PCA with \u201dwhitened\u201d angle -based distance measure it is possi-\nble to achieve similar recognition accuracy like using EBGM and LDA -bas ed\nrecognition methods (CSU, 2003). We tested our method using 515 1 face\nimages of 1311 persons from the FERET and AR databases and the r esults\nshowed that the proposed recognition method can achieve higher r ecognition\naccuracy than many other existing methods. The results of exper iments also\nshowed that PCA using Log-Gabor features is less sensitive to face detec-\ntion errors and used image normalization method than PCA using grey scale\nfeatures.\n2 Feature extraction using Log-Gabor \ufb01lters\nThe Log-Gabor \ufb01lters were proposed by Field (1987) for coding of n atural\nimages. The experiments showed, that these \ufb01lters are consisten t with the\nmeasurements of the mammalian visual system and are more suitable for cod-\ning of natural images than Gabor (1946) \ufb01lters. The Log-Gabor \ufb01lt er in\nfrequency domain can be constructed in terms of two components , namely the\nradial \ufb01lter component G(f) and the angular \ufb01lter component G(\u03b8). In polar\ncoordinates the \ufb01lter transfer function could be written in the follo wing form\n(Field, 1987), (Bigun and du Buf, 1994), (Kovesi, 1996):\nG(f,\u03b8) =G(f)\u00b7G(\u03b8) = exp/parenleftBigg\n\u2212(log(f/f0))2\n2(log(k/f0))2/parenrightBigg\n\u00b7exp/parenleftBigg\n\u2212(\u03b8\u2212\u03b8o)2\n2\u03c32\n\u03b8/parenrightBigg\n,(1)\nheref0is the centre frequency of the \ufb01lter, kdetermines the bandwidth of\nthe \ufb01lter, \u03b8ois the orientation angle of the \ufb01lter, and \u03c3\u03b8=\u25b3\u03b8/s\u03b8wheres\u03b8-\n3scaling factor, \u25b3\u03b8-orientationspacing between \ufb01lters. Forfacerecognitionwe\ngenerated multiple Log-Gabor \ufb01lters of di\ufb00erent scales and orienta tions using\nthe following parameters: f0= 1/\u03bb,\u03bb=\u03bb0\u00b7s(ns\u22121)\n\u03bb,k/f0=\u03c3f,ns= 1,...,Ns;\n\u03b8o=\u03c0(no\u22121)/No,\u25b3\u03b8=\u03c0/No,no= 1,...,No;\u03bb0= 5,s\u03bb= 1.6,\u03c3f= 0.75,\nNs= 4,s\u03b8= 1.5,No= 6, here \u03bb0is the wavelength of the smallest scale \ufb01lter,\ns\u03bbis the scaling factor between successive \ufb01lter scales, Nsis the number of\nscales,Nois the number of orientations. Most of these parameters were cho sen\nfollowing the recommendations of (Kovesi, 2003).\nUsing Eq. 1 we calculate two-dimensional Log-Gabor \ufb01lter Gno,nsin Fourier\nspaceofachosen\ufb01lterscaleandorientation.Thesizeofthe\ufb01ltera rrayGno,nsis\nthesameasthesizeofthetwo-dimensional image Ithatwewishto\ufb01lter.Then\nwe perform \ufb01ltering (convolution in Fourier space), magnitude calcu lation and\nmasking using the following equation:\nVno,ns=abs(IFFT2(Gno,ns.\u2217FFT2(I))).\u2217mask, (2)\nhere\u201d.\u2217\u201d-array(notmatrix)multiplication, I-normalized(cropped,masked)\nface image, Gno,ns- Log-Gabor\ufb01lter of desired orientation and scale in Fourier\nspace,FFT2 - two-dimensional Fast Fourier Transform, IFFT2 - inverse\nFFT2,mask- binary mask for masking magnitude image (the same as is\nused for masking greyscale face image Iin order to leave only the internal\npart of the face), Vno,ns- masked Log-Gabor magnitude image.\nAfter image \ufb01ltering with multiple Log-Gabor \ufb01lters ( Nsscales and Noorien-\ntations) we get very large number of Log-Gabor features (magnit ude values\nin allNs\u00b7Nomagnitude images). In order to reduce the number of features\nand achieve partial face recognition invariance with respect to di\ufb00e rent facial\nexpressions and minor face detection errors, we use sliding window a lgorithm\nthat is illustrated in Fig. 1. Rectangular window of a chosen size (e.g., 8 x8\npixels) is slided over the magnitude image Vno,1using some sliding step (e.g.,\n6 pixels, overlapping of windows is 8-6=2 pixels). In each window we \ufb01nd one\nmaximal magnitude value and remember the location (coordinates in im age\nVno,1) of this value. If several equal values are found, we use the one t hat is\ncloser to the centre of the window. If magnitude image is masked, we perform\nsearch only in an unmasked image part.\nWe apply sliding window algorithm only for the \ufb01rst scale ( ns= 1) (obtained\nusing \ufb01lter with the smallest chosen wavelength) of each orientation and \ufb01nd\nthe locations of highest magnitudes. Then these locations are used for extract-\ning magnitudes from other scales, corresponding to the analysed o rientation,\nas it is illustrated in Fig. 1. In this \ufb01gure feature locations are marked with\nblack points. Using scales ns= 1 we decide at what locations (coordinates)\nwe will extract the features and then use the same coordinates fo r all mag-\nnitude images, corresponding to the same processed orientation no(the same\norientations no- the same locations, di\ufb00erent orientations - di\ufb00erent loca-\n4tions). All extracted Log-Gabor features (magnitude values) ar e stored in a\none-dimensional vector Xandusedasinput forPrincipal Component Analysis\n-based face recognition method.\nn\u25a1=1,on =1s V n\u25a1=1,\u25a1n =No s sV\nSliding\nwindow\nFound\nlocations\nof\u25a1maximal\nmagnitudesThe\u25a1same\nlocations\nn\u25a1=N\u25a1,o o n =1s V n\u25a1=N\u25a1,\u25a1n =No o\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1s sV\nFig. 1. Selection of Log-Gabor magnitude features using sli ding window algorithm\nExample Log-Gabor magnitude images are presented in Fig. 2 (images are\ninverted, dark points mean high magnitude values). In this example w e used\n\ufb01lters of No= 6 orientations and Ns= 4 scales ( No\u00b7Ns= 24 \ufb01lters), and\n\ufb01ltered a normalized (derotated, masked) facial image with these \ufb01 lters. Cal-\nculated Log-Gabor magnitude images were also masked. Left-most binary im-\nages in Fig. 2 show the locations (black points) of Log-Gabor featur es that\nwere found using sliding window algorithm. It must be noted, that for the\nselected image size the Log-Gabor \ufb01lters (of di\ufb00erent sizes and orie ntations)\ncan be calculated only once and stored. When we perform face reco gnition,\nthe Log-Gabor features (found using sliding window) for each image from the\ndatabase of faces are also calculated only once and stored.\n3 Face recognition using Principal Component Analysis of Lo g-\nGabor features\nIn this section we will describe Karhunen-Loeve transform (KLT) - based face\nrecognitionmethod,thatisoftencalledPrincipalComponent Analys is (PCA).\n5}\n}\n}\nFeature\u25a1locations 4\u25a1scalesn\u25a1=1on =1s n =2s n =3s n =4s\nn\u25a1=2o\nn\u25a1=3o\nn\u25a1=4o\nn\u25a1=5o\nn\u25a1=6o6\u25a1orientations\nFig. 2. Image magnitudes after usingLog-Gabor \ufb01lters, and t he locations of features\nat di\ufb00erent orientations that were found using sliding windo w algorithm\nWe will present only the main formulas of this method, which details cou ld\nbe found in (Gro\u00df, 1994).\nLetXjbeN-element one-dimensional image-column (vector) and suppose\nthat we have rsuch images ( j= 1,...,r). In traditional PCA -based face\nrecognition method, these images contain grey values of the two-d imensional\nfacial photographs. In our case these one-dimensional images Xj(data vec-\ntors) contain Log-Gabor features . We calculate the mean vector, centred data\nvectors and covariance matrix: m=1\nrr/summationtext\nj=1Xj, dj=Xj\u2212m, C=1\nrr/summationtext\nj=1djdT\nj,\nhereX= (x1,x2,...,xN)T, m= (m1,m2,...,m N)T, d= (d1,d2,...,dN)T.\nInordertoperformKLT,itisnecessary to\ufb01ndeigenvectors ukandeigenvalues\n\u03bbkofthecovariancematrix C(Cuk=\u03bbkuk).Becausethedimensionality( N2)\nof the matrix Cis usually large even for small images, and computation of the\neigenvectors using traditional methods is complicated, dimensionalit y of ma-\ntrixCis reduced using the decomposition described in (Kirby and Sirovich,\n1990) (if the number of training images is smaller than the length of th e vector\n6X).Foundeigenvectors u= (u1,u2,...,uN)Tarenormedandsortedindecreas-\ning order according to the corresponding eigenvalues. Then these vectors are\ntransposed and arranged to form the row-vectors of the trans formation ma-\ntrixT. Now any data Xcan be projected into the eigenspace and \u201dwhitened\u201d\n(Bishop, 1995) using the following formula:\nY= \u039b\u22121/2T(X\u2212m), (3)\nhereX= (x1,x2,...,xN)T, Y= (y1,y2,...,yr,0,...,0)T,\n\u039b\u22121/2=diag(/radicalBig\n1/\u03bb1,/radicalBig\n1/\u03bb2,...,/radicalBig\n1/\u03bbr).\nFor projection we can use not all found eigenvectors, but only a fe w of them,\ncorresponding to the largest eigenvalues. We can manually select th e desired\nnumber of eigenvectors or use the method described in (Swets et a l., 1998).\nFor each facial image (that we wish to use for recognition) we \ufb01nd Lo g-Gabor\nfeatures, projected these features into the eigenspace and ca lculate eigenfea-\nture vector Z= (z1,z2,...,zn)T= (y1,y2,...,yn)T, herenis the number of\nfeatures. Recognition of unknown face is performed by calculating its feature\nvectorZnewandcomparingitwiththefeaturevectors ofknownfaces. Forcom -\nparison we calculate the distances \u03b5i(Znew,Zi) between unknown faceand each\nknown face and say that the face with feature vector Znewbelongs to a person\ns= argmin\ni[\u03b5i]. For rejection of unknown faces a threshold \u03c4is chosen and\nit is said that the face with projection Znewis unknown if \u03b5s\u2265\u03c4. For recog-\nnition we used cosine-based distance measure \u03b5i(Znew,Zi) =\u2212cos(Znew,Zi),\nbecause using this distance measure we can achieve higher recognit ion ac-\ncuracy (Perlibakas, 2004) than using the Euclidean or Manhattan d istance\nmeasures.\n4 Normalization of face images\nFor recognition experiments we used two image normalization method s. One\nmethod uses manually selected centres of eyes and the tip of chin (3 -point\nnormalization method), and another method for normalization uses only the\ncentres of eyes (2-point normalization method). Image normalizat ion proce-\ndure of 3-point method is presented in Fig. 3. The last image (Fig. 3 (e ))\nalso presents the result of 2-point normalization. For illustration we used an\nimage from our personal archive.\nNow we will describe our implementation of 3-point normalization metho d.\nInitial images were denoised (using Gaussian \ufb01lter with \u03c3= 0.5 and window\nsize 5x5), derotated (in order to make the line connecting eye cent res hori-\n7(a) (b) (c) (d)d20.4d20.9d10.9d1\nd1\nFig. 3. Image normalization (a-d - 3-point method, e - 2-poin t method): a) ini-\ntial image with selected eyes and chin b) denoised, derotate d image, and cropping\nschema c) cropped, resized and masked image d) normalized im age after histogram\nequalization\nzontal), cropped, resized (to the size of 128x128 pixels), masked . For rotation\nand resizing we used bicubic interpolation. For masking we used an ellips e\nwith central point (64.5,45.5), horizontal axis of 120 pixels, and ver tical axis\nof 160 pixels. Then for unmasked part of the image we performed his togram\nequalization (256 levels). When the image is masked, are left 12646 un masked\npixels of 16384 (128x128).\nFor initial comparison of PCA and Log-Gabor PCA methods we used 3- point\nnormalization method in order to perform experiments with faces th at are not\novercropped and also contain no scene\u2019s background information. Because the\ntip of chin may be hard to locate, most recognition methods for norm alization\nuse only the centres of eyes. So we used 3-point normalization only f or initial\ncomparison of PCA and Log-Gabor PCA methods, and for the rest o f ex-\nperiments we used 2-point normalization method (for normalization a re used\nonly the centres of eyes). Because there is no agreement how imag es should\nbe normalized for face recognition experiments, we implemented 2-p oint nor-\nmalization that is very similar to the (CSU, 2003) normalization method .\nSimilar method was also used by some participants of the FERET (NIST ,\n2001) tests. At \ufb01rst images are derotated in order to make the line connecting\neye centres horizontal. Then images are resized in order to make th e distance\nbetween eyes equal to 70 pixels and cropped to the size of 130x150 pixels.\nDuring cropping the centres of eyes are vertically positioned on y=4 5 line (the\ncentre of coordinates (0,0) is in the left top corner). Then the imag e is masked\nusing an ellipse with its central point (65.5,50.5), horizontal axis of 12 8 pixels,\nand vertical axis of 236 pixels. After masking are left 17237 unmask ed pixels.\nFor an unmasked image part is performed histogram equalization. Th e main\ndi\ufb00erences between our 2-point normalization and (CSU, 2003) nor malization\nare as follows: initial images we \ufb01ltered using Gaussian \ufb01lter (CSU used no\n8\ufb01ltering), for image rotation and resizing we used bicubic interpolatio n (CSU\nused bilinear interpolation), cropped 130x150 images we resized to 1 28x128\npixels and then masked with resized (to 128x128) binary mask. Afte r such\nmasking were left 14454 unmasked pixels. Then for an unmasked imag e part\nwe performed histogram equalization. The result of this normalizatio n is pre-\nsented in Fig. 3 (e).\nWhen we performed face recognition using Log-Gabor PCA method, masked\nfaceimageswere \ufb01ltered withLog-Gabor\ufb01lters(24\ufb01lters of6orien tations and\n4 scales) and calculated magnitude images. We masked these Log-Ga bor mag-\nnitude images using the same masks that were used for image normaliz ation\nand performed sliding window search of Log-Gabor features. Sear ch window\nsize is 8x8 pixels, sliding step is 6 pixels (the same in horizontal and vert ical\ndirections), and window overlap is 8-6=2 pixels. After using sliding wind ow\nalgorithm with masked magnitude images, we select 9240 magnitude va lues\n(Log-Gabor features) for 3-point normalization method and 1000 8 magnitude\nvalues for 2-point normalization method, that are located in the unm asked\nparts of magnitude images. This is the total number of values in all 6 o rien-\ntations and 4 scales. Also we can use an unmasked magnitude images ( initial\ngreyscaleimagesarealwaysmasked), performslidingwindowsearch inawhole\nmagnitude image and select in total 10584 features for both 3-poin t and 2-\npoint methods (the size of images is the same). In all the experiment s we used\nthe same normalized image patterns and the same implementation of P CA.\nThe distances between \u201dwhitened\u201d feature vectors were measur ed using cosine\n-based distance measure.\n5 Used recognition performance measures\nFor comparison of face recognition methods we used Cumulative Mat ch Char-\nacteristic (CMC) and Receiver Operating Characteristic (ROC) - ba sed mea-\nsures described in (Bromba, 2003): the area above Cumulative Mat ch Charac-\nteristic (CMCA) (smaller CMCA means better overall recognition acc uracy);\nhow many images (in percents) must be extracted from the databa se in order\nto achieve some cumulative recognition rate (e.g., not smaller than 95 -100%)\n(smaller values mean that we need to extract fewer images in order t o achieve\nsome cumulative recognition rate); Equal Error Rate (EER) and th e area be-\nlow Receiver Operating Characteristic (ROCA) (smaller values mean b etter\nresults); \ufb01rst one recognition rate (First 1) that is achieved if only the \ufb01rst one\n(most similar) imagefromthedatabaseis extracted (largervalues m eanbetter\nresult). Percent (rank) of images that we need to extract from t he database in\norder to achieve 100% cumulative recognition rate in the future we w ill denote\nas Cum100. Graphical representation of the used characteristic s is shown in\nFig. 4 - 5.\n910 20 30 40 50 60 70 80 90102030405060708090100\n100\nRank,\u25a1%100%\u25a1cumulative\u25a1rec.,\u25a1rank\u25a1=\u25a140%\nCMCA\nRecognition\u25a1rate,\u25a1%First\u25a11\u25a1recognition\u25a1=\u25a170%90%\u25a1cum.\u25a1rec.,\u25a1rank\u25a1=\u25a115%\nFig. 4. Cumulative Match Characteristic\n(CMC)10 20 30 40 50 60 70 80 90102030405060708090100\n100\nFAR,\u25a1%EER\u25a1=\u25a120%\u25a1\u25a1(\u25a1FAR\u25a1=\u25a1FRR\u25a1)\nROCAFRR,\u25a1%\nFig. 5. Receiver Operating Characteristic\n(ROC)\n6 Experiments and results\nFor recognition experiments we used the FERET database (Phillips et al., a,\n1998) containing greyscale photographs of 1196 persons. This da tabase was\ncollected in 1993-1996 at George Mason University during the FERET (FacE\nREcognition Technology) program. As far as we know, this is one of t he\nlargest databases of face photographs (of di\ufb00erent persons) in the world that\nis publicly available for face recognition research purposes (Gro\u00df, 2 005). This\ndatabase is widely used for evaluating identi\ufb01cation (Phillips et al., a, 19 98)\nand veri\ufb01cation (Rizvi et. al., 1998) performance of face recognit ion methods.\nFor recognition experiments we used 3541 facial images from this da tabase.\nThe size of each image is 256x384 pixels, for each image this database contains\nmanually selected eye coordinates. For training we used 1196 greys cale images\nfrom the faset of this database. The same 1196 faimages were used as a\ngallery (known persons), and images from other sets (1195 fbimages, 722\ndup1 images, 234 dup2 images, 194 fcimages) were used as probes (unknown\npersons that we wish to recognize). fbset contain face images with di\ufb00erent\nfacial expressions, dup1 anddup2 sets contain images that were taken after\nsometimeinterval(upto1.5years)from faimages,and fcsetcontainsimages\nwith di\ufb00erent image input conditions (camera position and illumination).\nAt \ufb01rst we performed recognition experiments with faandfbsets using dif-\nferent number (100-1000) of PCA features (di\ufb00erent number of used eigen-\nvectors, corresponding to the largest eigenvalues) and compare d the proposed\nmasked Log-Gabor PCA with traditional PCA (using cosine -based dis tance\n10measure between \u201dwhitened\u201d feature vectors). Images were no rmalized using\n3-point normalization method. The results are presented in Table 1. The\nresults showed, that \ufb01rst one recognition rate of masked Log-Ga bor PCA\n(89.29-98.41%) is always higher than of traditional PCA (80.42-88.03 %), and\nEER values of masked Log-Gabor PCA (0.33-1.59 %) are always lower t han\nEER values of traditional PCA (1.92-4.26 %). Other characteristics (CMCA,\nROCA, cumulative recognition) of masked Log-Gabor PCA are also be tter\nthan of traditional PCA. The results showed, that masked Log-Ga bor PCA\nachieves larger \ufb01rst one recognition accuracy when we use larger n umber of\nfeatures (e.g., 100 PCA features - 89.29% recognition accuracy, 1 000 PCA\nfeatures - 98.41% recognition accuracy). It must be noted, that masked Log-\nGabor PCA uses shorter vectors (9240 Log-Gabor features) th an traditional\nPCA (12646 greyscale features) for PCA training. Also we investiga ted an-\nother version of Log-Gabor PCA when Log-Gabor magnitude images are not\nmasked (only magnitudes, initial images are always masked). In this c ase the\nsliding window search selects 10584 Log-Gabor features. The resu lts showed,\nthat in some cases unmasked Log-Gabor PCA can achieve higher \ufb01rs t one\nrecognition accuracy, but because these di\ufb00erences are not ver y large (<0.2%\nwith>200 features) we prefer to use masked version of Log-Gabor PCA .\nAlso we compared our face recognition results with the results of ot her re-\nsearchers. For comparison we used the best results of the FERET program\nparticipants that took o\ufb03cial FERET 1996-1997 tests (Phillips et al., 2000),\n(NIST, 2001). Also we present the best results of some other res earchers\nwho tested their recognition methods using all images (not subsets ) from the\nFERET fa(images of 1196 persons) and fb(images of 1195 persons) sets\nthat contain faces with di\ufb00erent facial expressions. Most part of the compared\nmethods for face normalization used manually located coordinates o f eye cen-\ntres. These coordinates were marked by the creators of the FER ET database\nand are distributed with this database. The results are summarized in Table\n2. Fully automatical methods are denoted by \u201dauto\u201d, our di\ufb00erent normal-\nization methods are denoted by \u201d3 pt.\u201d (3-point normalization) and \u201d 2 pt.\u201d\n(2-point normalization). In Figures 6 - 7 we also present CMC and ROC\ncharacteristics of our Log-Gabor PCA method (900 PCA features , 2-point\nnormalization).\nNow we will brie\ufb02y describe face recognition methods of other resea rchers and\nwill compare achieved results. MIT 1996 (Massachusetts Institut e of Technol-\nogy, MIT Media Laboratory) method was developed by (Moghaddam et al.,\n1996). For recognition they used dual (intrapersonal and extra personal) PCA\nand Bayesian MAP (maximum a posteriori) similarity measure. For learn ing\nwere used image pairs of the same and di\ufb00erent persons. UMD 1996, UMD\n1997(University ofMaryland)facerecognitionmethodsarebased onPCAand\nLDA(LinearDiscriminantAnalysis)andweredevelopedby (Etemad an d Chellapa,\n1997; Zhao et al., 1998). For training were used several images per person, for\n11Table 1\nComparison of 3 face recognition methods: 1) Masked Log-Gab or PCA (MskLG),\n2) UnMasked Log-Gabor PCA (UnMskLG), and 3) Traditional PCA (Trad).\nMethod Feat. Rank (%) in order to achieve desired CMCA First 1 EER, ROCA\nnum. cumulative recognition accuracy rec., % %\n95 96 97 98 99 100\nMskLG 100 0.33 0.42 0.50 0.84 2.34 18.14 17.67 89.29 1.59 9.49\nUnMskLG 100 0.42 0.50 0.75 1.17 2.17 11.87 17.20 89.87 1.42 9. 03\nTrad 100 0.84 1.17 1.59 2.93 6.44 36.87 37.27 83.85 2.26 28.77\nMskLG 200 0.17 0.25 0.33 0.42 0.84 6.69 11.34 93.56 0.75 2.48\nUnMskLG 200 0.17 0.17 0.25 0.42 0.75 6.94 11.44 93.56 0.75 2.6 1\nTrad 200 0.59 0.84 1.17 2.34 7.19 84.20 40.21 86.44 2.01 31.59\nMskLG 300 0.08 0.17 0.17 0.25 0.42 7.27 10.36 95.31 0.59 1.65\nUnMskLG 300 0.08 0.17 0.17 0.25 0.59 8.61 10.41 95.40 0.50 1.7 3\nTrad 300 0.59 0.84 1.34 2.84 5.52 93.65 39.36 87.87 1.92 29.91\nMskLG 400 0.08 0.08 0.17 0.17 0.33 3.76 9.46 96.99 0.42 0.93\nUnMskLG 400 0.08 0.08 0.08 0.17 0.33 6.44 9.58 97.15 0.42 1.04\nTrad 400 0.59 0.92 1.67 3.09 12.12 92.73 52.94 88.03 2.09 42.7 9\nMskLG 500 0.08 0.08 0.08 0.17 0.33 2.84 9.39 97.24 0.42 0.69\nUnMskLG 500 0.08 0.08 0.08 0.17 0.33 3.68 9.26 97.15 0.42 0.63\nTrad 500 0.92 1.25 2.01 4.10 16.64 74.50 58.73 87.53 2.34 48.2 6\nMskLG 600 0.08 0.08 0.08 0.17 0.25 2.26 9.07 97.74 0.33 0.48\nUnMskLG 600 0.08 0.08 0.08 0.17 0.25 1.42 8.91 97.74 0.33 0.39\nTrad 600 1.34 1.67 2.59 5.94 13.55 82.86 67.05 86.03 2.59 56.2 1\nMskLG 700 0.08 0.08 0.08 0.17 0.17 2.76 8.89 97.91 0.42 0.36\nUnMskLG 700 0.08 0.08 0.08 0.08 0.17 2.42 8.86 98.08 0.33 0.32\nTrad 700 1.42 2.26 4.35 8.86 22.41 95.07 88.53 85.02 3.18 77.0 3\nMskLG 800 0.08 0.08 0.08 0.17 0.17 3.51 8.98 97.99 0.33 0.38\nUnMskLG 800 0.08 0.08 0.08 0.08 0.17 3.18 9.02 98.08 0.33 0.38\nTrad 800 2.09 3.34 5.35 10.28 28.51 93.14 109.44 83.09 3.51 97 .12\nMskLG 900 0.08 0.08 0.08 0.08 0.25 1.84 8.92 98.16 0.33 0.33\nUnMskLG 900 0.08 0.08 0.08 0.17 0.17 1.76 8.85 98.08 0.33 0.28\nTrad 900 2.76 4.35 7.02 12.63 38.13 92.89 123.40 82.43 3.93 11 0.99\nMskLG 1000 0.08 0.08 0.08 0.08 0.17 6.35 9.44 98.41 0.33 0.63\nUnMskLG 1000 0.08 0.08 0.08 0.08 0.17 2.51 9.01 98.24 0.33 0.3 5\nTrad 1000 4.01 5.52 9.28 16.22 45.48 90.80 143.82 80.42 4.26 1 30.80\n12Table 2. Recognition of expression -variant faces from the F ERET database (gallery contains 1196 faimages, and probe contains 1195\nfbimages).\nMethod and its Rank (%) in order to achieve CMCA, First 1 EER, ROCA,\nauthors desired cumulative recognition, (0 ,100%] [0 ,104] recognition, [0 ,100%] [0 ,104]\n95 96 97 98 99 100 [0 ,100%]\nMIT 1996 (Moghaddam et al., 1996) 0.17 0.25 0.33 1.09 23.33 99.83 84.14 9 4.81 4.77 203.25\nMIT 1996 auto (Phillips et al., b, 1998) - - - - - - - \u223c88.00 - -\nUMD 1996 (Etemad and Chellapa, 1997) - - - - - - - \u223c83.50 \u223c7.00 -\nUMD 1997 (Zhao et al., 1998) 0.08 0.08 0.17 0.33 0.84 75.92 18.91 96.23 1.09 14.37\nUSC 1997 (Okada et al., 1998) 0.17 0.17 0.25 0.33 3.09 50.67 27.94 94.98 2.5 1 57.52\nUSC 1997 auto (Okada et al., 1998) - - - - - - - 94.00 - -\nMSU 1996 (Swets and Weng, 1996) - - - - - - - \u223c88.50 \u223c3.00 -\nBayesian MAP (Teixeira, 2003) 1.92 2.51 4.18 6.52 13.8 70.15 67.11 81.92 - -\nEBGM Standard (Bolme, 2003) 0.59 0.92 1.34 2.42 9.2 37.54 34.26 88.37 - -\nEBGM Optimised (Bolme, 2003) - - - - - - - 89.80 - -\nPCA MahCosine (CSU, 2003) 0.84 1.17 2.26 4.43 10.28 60.45 48.90 85.27 - -\nGabor features (Kepenekci et al., 2002) - - - - - - - 96.30 - -\nHaar+AdaBoost (Jones and Viola, 2003) - - - \u223c0.42\u223c1.17 - - \u223c94.00 \u223c1.00 -\nGabor+AdaBoost (Yang et al., 2004) - - - - - - - \u223c95.20 - -\nSOM (Tan et. al., 2005) - - - - - - - \u223c91.00 - -\nOur Log-Gabor PCA, 900 PCA feat., 3 pt. 0.08 0.08 0.08 0.08 0.25 1.84 8.9 2 98.16 0.33 0.33\nOur Log-Gabor PCA, 900 PCA feat., 2 pt. 0.08 0.08 0.08 0.17 0.33 68.81 2 4.02 97.99 0.33 15.78\nOur trad. PCA, 900 PCA feat., 3 pt. 2.76 4.35 7.02 12.63 38.13 92.89 123 .40 82.43 3.93 110.99\nOur trad. PCA, 900 PCA feat., 2 pt. 6.94 8.61 13.21 16.64 31.44 99.58 14 9.75 76.90 5.27 136.24\nOur Log-Gabor PCA 4x4, 900 PCA feat., 3 pt. 0.08 0.08 0.08 0.08 0.08 0.1 7 0.59 98.49 0.17 0.14\n13recognition were used 300 features. USC 1997 (University of Sout hern Califor-\nnia) method was developed by (Wiskott et al., 1997; Okada et al., 1998 ). For\nrecognitiontheyusedGaborJetsandElastic BunchGraphMaching ( EBGM).\nFaces were resized to 128x128 pixels and normalized using histogram equal-\nization For recognition were used about 1920 features that corre spond to 40\nGabor\ufb01lters(5scalesand8orientations)at48graphnodes.MSU1 996(Michi-\ngan State University) method was developed by Swets and Weng (19 96). For\nrecognition they used PCA and LDA. CSU (2003) Bayesian MAP (Teixe ira,\n2003), EBGM Standard, and EBGM Optimised (Bolme, 2003) face rec ogni-\ntion methods were developed by the researchers at Colorado Stat e University.\nThese methods are similar to the corresponding methods developed at MIT\nand USC. The CSU (2003) PCA MahCosine method is a traditional PCA\nwithcosine-baseddistancemeasurebetween\u201dwhitened\u201d feature vectors. CSU\n(2003) for recognition used 130x150 images, faces were masked u sing ellipti-\ncal mask, unmasked image part was normalized using histogram equa lization.\nCSU EBGM method for recognition extracts more than 6000 featur es (80\nGabor features x 80 graph points). Kepenekci et al. (2002) for r ecognition\nused magnitudes of Gabor \ufb01lters and similar distance measures as we re used\nby (Wiskott et al., 1997). For recognition were extracted 40 Gabor features\n(5 scales and 8 orientations) and 2 coordinates of these features at varying\nnumber of face points. Jones and Viola (2003) used Haar -like featu res and\nAdaBoost training. For recognition were used 45x36 images without masking.\nThe use small images may be related with the fact that AdaBoost tra ining re-\nquires huge computational resources. Yang et al. (2004) used Ga bor features\nand AdaBoost training -based recognition method. Tan et. al. (200 5) for face\nrecognition used Self-Organizing Map (SOM) andsoft k nearest neig hbor (soft\nk-NN) ensemble method.\nAs we can see from the Table 2, the highest \ufb01rst one recognition acc uracy\nwas achieved by the following methods: our Log-Gabor PCA (98.16% u s-\ning 2-point normalization and 98.49% using 3-point normalization), Gab or\nfeatures (Kepenekci et al., 2002) -based method (96.30%), and U MD 1997\n(Zhao et al.,1998)PCA+LDA-basedmethod(96.23%).ThebestEER results\nwereachievedbyourLog-GaborPCA(0.33%),Haar+AdaBoost (Jo nes and Viola,\n2003) method ( \u223c1.00), and UMD 1997 (Zhao et al., 1998) PCA+LDA -based\nmethod (1.09%). It is interesting to note that our traditional PCA w ith 2-\npoint normalization achieves lower recognition accuracy than tradit ional PCA\nof (CSU, 2003). But when we combine our traditional PCA with Log-G abor\nfeatures, our method achieves higher recognition accuracy that many other\nmethods. Also we can note that PCA with grayscale features is much more\nsensitive to the chosen image normalization method (76.90% \ufb01rst one recog-\nnition using 2-point normalization and 82.43% using 3-point normalizatio n)\nthan our Log-Gabor PCA (98.16% using 2-point normalization and 98.4 9%\nusing 3-point normalization). Using 3-point normalization faces are m asked\nand cropped more accurately than using 2-point normalization, and recogni-\n14Fig. 6. CMC characteristic of Log-Gabor\nPCA method\nFig. 7. ROC characteristic of Log-Gabor\nPCA method\ntion results using Log-Gabor PCA and 3-point normalization are also b etter.\nThese di\ufb00erences are especially visible when we compare Cum100, CMC A and\nROCA values of 2-point ant 3-point methods. It is interesting to not e, that\nEBGM -based methods perform positioninig of graph nodes around t he face\nalso enough accurately and this may be one of the reasons why EBGM -based\nmethods and Log-Gabor PCA method using 3-point normalization ach ieve\nbetter Cum100 results than other methods that use 2-point norm alization.\nUsing our method with 3-point normalization in order to achieve 100% c u-\nmulative recognition rate we need to extract from the database on ly 1.84% of\nimages (that is 1196*1.84/100 = 22 images), and using CSU EBGM meth od\nwe need to extract 37.54% of images (that is 449 images). In the last line\nof the Table 2 we present the results of our method if we use 3-point nor-\nmalization, 4x4 sliding window (without overlapping), masking, and 197 04\nLog-Gabor magnitude features (the number of PCA features rem ains 900).\nAs we can see from these results, using larger number of Log-Gabo r features\n(smaller sliding window) we can achieve even better cumulative recogn ition\nresults than using 8x8 window with 2 pixels overlapping. That is in order to\nachieve 99% cumulative recognition rate we need to extract from th e database\nonly 2 images (0.17%), and in order to achieve 100% cumulative recogn ition\nrate we need to extract 7 images (0.59%).\nBecause in real life situations face recognition methods are usually u sed with\nautomatically detected faces and facial features. So it is desirable to know\nhow detection errors a\ufb00ect recognition accuracy, what recognit iom method is\nless sensitive to feature detection errors. Using this information w e can de-\ncide how accurately faces and facial features should be detected in order to\nachieve desirable recognition accuracy. In the Table 2 we presente d some re-\n15sults of other researchers that used automatical detection of f aces and facial\nfeatures (notation \u201dauto\u201d). USC 1997 automatic method (Okada et al., 1998)\nfor loacation of face and facial features (eyes, nose, lips, face c ontour) used\nEBGM with small number of graph nodes (16 nodes). As it is stated by the\nauthors, their method locates facial features very accurately, so the di\ufb00erence\nbetween recognition results using manually and automatically detect ed fea-\ntures is\u223c1%. MIT 1996 (Moghaddam et al., 1996) automatical method for\ndetection of eyes and lips used PCA-based detector and probabilist ic veri\ufb01ca-\ntion of detected feature locations. Automatical method achieved \u223c7% lower\nrecognition accuracy than the same method that used manual fea ture detec-\ntion. The results showed that fully automatical USC 1997 method ca n achieve\nmuch higher recognition accuracy than MIT 1996 method. But beca use the\nauthors used di\ufb00erent methods for detecting faces and facial fe atures and did\nnot present any quantitative information about feature detectio n accuracy,\nwe cannot say for sure what recognition method (that was tested without\nautomatical detection) it is better to use with automatical featur e detection\nmethod. It is possible that one recognition method is less sensitive to feature\ndetection errors than another, but also it is possible that the main d i\ufb00erences\nare only in feature detection methods and their feature detection accuracy.\nIn order to \ufb01nd out how sensitive is our face recognitiom method to f eature\ndetection errorsand not to bind to concrete featuredetection m ethod we man-\nually shifted the markers of eye centres using pre-de\ufb01ned shift dir ections and\ndistances. For this experiment we used all facial images from the FE RETfa\nandfbsets. We shifted only eye markers of fbimages using 4 shift directions\n(0,\u03c0/2,\u03c0, 3\u03c0/2) and the following shift distances: 0%, 2%, 4%, 6%, 8%, 10%,\n12%. Recognition accuracy using each shift distance was calculated as anaver-\nage of 4 results that correspond to 4 directions. Shift distance is c alculated as\na percentage of the distance between manually selected eye centr es. For exam-\nple, if the distance between manually selected eyes is 100 pixels and we wish to\nuse 4% shift, then these 4% will correspond to 4 pixels. The results t hat were\nachieved using 0% shift (it means that no shift is performed and we sim ply\nuse manually selected eye coordinates) were used as a baseline for c omparison\nwith the results that were achieved using other shift distances. Th e results\nare presented in Table 3, where notations First1d and EERd mean ab solute\ndi\ufb00erences between achieved recognition result using some shift an d the result\nwithout any shift (baseline). For experiments was used 2-point nor malization\nmethod.\nThe results (Table 3) showed that our Log-Gabor PCA is less sensitiv e to fea-\nturedetectionerrorsthantraditionalPCAandcanachieve 89-90 %recognition\naccuracy even if one eye is shifted by 10%. In order to create fully a utomat-\nical face recognition method and achieve similar recognition accurac y that\nwas achieved by USC 1997 (Okada et al., 1998) fully automatical meth od\nwe should combine our recognition method with automatical eye dete ction\n16Table 3\nFace recognition accuracy using shifted markers of eye cent res (simulated eye loca-\ntion errors).\nShift Method Shifted left eye marker Shifted right eye marke r\nsize, % name First1 First1d EER EERd First1 First1d EER EERd\n0% PCA 76.90 - 5.27 - 76.90 - 5.27 -\nLog-Gabor PCA 97.99 - 0.33 - 97.99 - 0.33 -\n2% PCA 73.10 3.80 6.05 0.78 72.95 3.95 6.05 0.78\nLog-Gabor PCA 97.45 0.54 0.40 0.07 97.41 0.58 0.40 0.07\n4% PCA 62.45 14.45 8.39 3.12 61.86 15.04 8.56 3.29\nLog-Gabor PCA 96.76 1.23 0.44 0.11 96.88 1.11 0.50 0.17\n6% PCA 47.78 29.12 12.32 7.05 45.04 31.86 12.59 7.32\nLog-Gabor PCA 95.65 2.34 0.65 0.32 95.31 2.68 0.63 0.30\n8% PCA 31.17 45.73 16.38 11.11 31.30 45.60 17.22 11.95\nLog-Gabor PCA 93.41 4.58 0.96 0.63 92.95 5.04 0.96 0.63\n10% PCA 19.21 57.69 21.51 16.24 19.29 57.61 22.53 17.26\nLog-Gabor PCA 90.02 7.97 1.46 1.13 89.02 8.97 1.55 1.22\n12% PCA 11.40 65.50 26.46 21.19 11.92 64.98 27.13 21.86\nLog-Gabor PCA 84.29 13.70 2.24 1.91 83.24 14.75 2.36 2.03\nmethod that detects the centres of eyes with smaller than 6% shift s (total\nshift for both images) when compared to manually selected eye cent res. Those\nreaders who are interested in automatical face and facial featur es detection\nmethods can \ufb01nd an overview of such methods in (Yang et al., 2002) a nd\n(Perlibakas, 2003).\nIn real life situations the accuracy of face recognition is also a\ufb00ect ed by many\nother factors like aging and manual change of appearance (hairst yle, makeup),\nimage input (camera position) and illumination conditions. It is natural that\nwecannot have thesame lookingfacesandthesameimagingcondition s aftera\nlonger time period. Inorder to \ufb01nd out how recognitionaccuracy is a \ufb00ected by\nthese factors we performed recognition experiments using the fo llowing probe\nsets of images from the FERET database: dup1 - 722 images of 243 persons,\nat least 2 images with di\ufb00erent expressions per person, time interva l from\nfaimages is 0-34 months, photographs of 166 persons were taken af ter some\nperiod of time (not the same day than faimages); dup2 - 234 images of 75\npersons, at least 2 images with di\ufb00erent expressions per person, t ime interval\nfromfaimages is more than 18 months; and fc- 194 images of 194 persons\nthat were acquired on the same day, but with di\ufb00erent camera posit ion and\nillumination.\nThe results (Table 4) showed that our Log-Gabor PCA method achie ves\n17Table 4\nThe in\ufb02uence of aging and illumination to face recognition a ccuracy using 1196 fa\ngallery images (FERET database) and the following probe set s:dup1,dup2,fc.\nMethod and its authors First one rec., [0 ,100%] EER, [0 ,100%]\ndup1 dup2 fc dup1 dup2 fc\nMIT 1996 (NIST, 2001) 57.60 34.20 32.00 17.70 21.20 18.00\nMIT 1996 auto (Phillips et al., b, 1998) \u223c50.00 - - - - -\nUMD 1996 (Phillips et al., b, 1998) \u223c32.00\u223c9.00\u223c30.00 - - -\nUMD 1997 (NIST, 2001) 47.20 20.90 58.80 12.60 13.40 10.00\nMSU 1996 (Phillips et al., b, 1998) \u223c33.00\u223c17.00\u223c32.00 - - -\nUSC 1997 (NIST, 2001) 59.10 52.10 82.00 13.30 14.20 5.10\nUSC 1997 (Okada et al., 1998) 62.00 52.00 82.00 - - -\nUSC 1997 auto (Okada et al., 1998) 61.00 52.00 80.00 - - -\nGabor feat. (Kepenekci et al., 2002) 58.30 47.40 69.60 - - -\nOur trad. PCA, 900 PCA feat., 2 pt. 44.74 35.04 62.89 13.99 19. 03 9.79\nOur Log-Gabor PCA, 900 PCA feat., 2 pt. 72.44 65.81 90.21 3.60 4.70 1.03\n8-10% higher recognition accuracy and at least 4% lower EER than ot her\ncompared methods. Our recognition results showed that even usin g single\ntraining image per person we can improve recognition accuracy of fa ce images\nthat were took after longer time period. But also it is obvious that di\ufb00 erent\nimagingconditionsafterlongertimeperiodsigni\ufb01canly reducefacere cognition\naccuracy of all compared methods and that for such di\ufb03cult tasks we need\nbetter image normalization and feature extraction techniques.\nWealsoperformedseveral facerecognitionexperiments using the ARdatabase\n(Martinez, 1998). This databasewas created by A. MartinezandR . Benavente\nat Computer Vision Center, Purdue University in 1998. It contains f acial pho-\ntographs of 126 persons with strictly controlled facial expression s and lighting.\nThe size of images is 768x576 pixels. Images of each person were cap tured\nin two sessions (s1, s2) that were separated by two weeks time. Fr om this\ndatabase we used 1610 images of 115 persons (14 images per perso n = 2 ses-\nsions x 7 images per session). We used the following images: neutral ( ne),\nhappy (ha), angry (an), and screaming (sc) expressions; neutr al expression\nwith left illumination source (lis) turned on, right illumination source (ris )\nturned on, and both illumination sources (bis) turned on. For trainin g and\nas a galery set we used 115 images with neutral expression from the \ufb01rst\nsession (s1ne). For recognition we used the following probe sets th at corre-\nspond to di\ufb00erent type of transformation (neutral, expression, illumination)\n(Wang and Tang, 2003): s1expr (s1ha, s1an, s1sc images), s1illum (s1lis, s1ris,\ns1bis images), s2neutral (s2ne images), s2expr (s2ha, s2an, s2 sc images), and\ns2illum(s2lis, s2ris,s2bisimages). Firstonerecognitionresults using thesesets\n18are presented in Table 5. The last lines of this table also present EER r esults\nof our methods. For experiments was used 2-point normalization me thod.\nTable 5\nFace recognition results using AR database.\nMethod and its authors Firs one rec. results using di\ufb00erent pr obe sets\ns1expr s1illum s2neutral s2expr s2illum\nPCA (Wang and Tang, 2003) - - 84.4 56.7 24.4\nEBGM Gabor features (Wang and Tang, 2003) - - 86.7 66.7 52.2\nEBGM Gabor features +\nBayes matching (Wang and Tang, 2003) - - 93.3 86.0 86.7\nPCA (Martinez, 2003,a) 72.00 - - - -\nCorrelation (Martinez, 2003,a) 74.33 - - - -\nPCA + optical \ufb02ow (Martinez, 2003,a) 83.00 - - - -\nMotion estimation (Martinez, 2003,b) 84.67 - - - -\nOur traditional PCA, 100 feat., 2 pt. 70.43 62.90 92.17 58.52 46.67\nOur Log-Gabor PCA, 100 PCA feat., 2 pt. 85.51 82.90 99.13 77.3 9 63.48\nEER results of our methods\nOur traditional PCA, 100 feat., 2 pt. 6.96 5.51 2.61 11.02 10. 72\nOur Log-Gabor PCA, 100 PCA feat., 2 pt. 3.48 3.19 0.87 6.67 6.6 7\nThe results (Table 5) showed that our method achieves slightly highe r recog-\nnition accuracy than other compared methods that used single tra ining image\n(with neutral expression) per person. It is important to note tha t the compar-\nison of di\ufb00erent methods in the Table 5 is not very exact, because fo r exper-\niments di\ufb00erent authors used di\ufb00erent number of images: (Wang an d Tang,\n2003) used images of 90 persons, (Martinez, 2003,a), (Martinez, 2003,b) used\nimages of 100 persons, and we used images of 115 persons. The res ults showed\nthat EBGM Gabor features (features are extracted at graph no des) and Bayes\nmatching -based algorithm (Wang and Tang, 2003) can achieve much higher\nrecognition accuracy than our method and all other methods (our method\nachieved better results only when recognizing faces with neutral e xpressions).\nBut for training of this method we need multiple images per person, an d this\nmethodwastrainedusing7imagesperpersonfromthe\ufb01rstsession withdi\ufb00er-\nent expressions and illumination conditions. All other compared meth ods for\ntraining used single image per person (image with neutral expression from the\n\ufb01rst session). It is interesting to note PCA + optical \ufb02ow (Martinez , 2003,a)\nand Motion estimation (Martinez, 2003,b) -based recognition metho ds were\nspecially designed for recognizing expression -variant faces, and t hese methods\nuse weigting of facial features in order to reduce the in\ufb02uence of c hanged ex-\npression to the accuracy of face recognition. Perhaps these weig ting methods\ncould improve recognition accuracy of our face recognition method , and in\n19the future we are going to investigate di\ufb00erent feature weighting a nd masking\nmethods in order to improve recognition accuracy of expression -v ariant faces.\n7 Conclusions and future work\nIn this article we proposed a novel face recognition method based o n Principal\nComponent Analysis (PCA) and Log-Gabor \ufb01lters. The experiments showed\nthat using the proposed combination of Log-Gabor features and s liding win-\ndow -based feature selection method, Principal Component Analys is, \u201dwhiten-\ning\u201d, and cosine -based distance measure we can achieve very high r ecognition\naccuracy (97-98%) and low error rates (0.3-0.4% Equal Error Rat e) using the\nFERET database that contains photographs of more than 1000 pe rsons. The\nresults of our algorithm are among the best results that were ever achieved\nusing this database. In the future we are going to investigate the p ossibilities\nof using decomposed Log-Gabor feature vectors and multiple PCA s paces in\norder to have the possibility of using this method with an unlimited numb er\nof training images. Because the results of all compared methods sh owed that\nthe accuracy of face recognition is very a\ufb00ected by the lighting con ditions, in\nthe future we are going to investigate di\ufb00erent lighting normalization methods\nand test them with the Log-Gabor PCA face recognition method.\n8 Acknowledgements\nPortionsoftheresearch inthispaper usetheFERETdatabaseoff acialimages\ncollected under the FERET program."}
{"category": "abstract", "text": "In this article we propose a novel face recognition method ba sed on Principal\nComponent Analysis (PCA) and Log-Gabor \ufb01lters. The main adv antages of the\nproposed method are its simple implementation, training, a nd very high recognition\naccuracy. For recognition experiments we used 5151 face ima ges of 1311 persons\nfrom di\ufb00erent sets of the FERET and AR databases that allow to a nalyze how\nrecognition accuracy isa\ufb00ected bythechangeoffacial expre ssions,illumination, and\naging. Recognition experiments with the FERET database (co ntaining photographs\nof 1196 persons) showed that our method can achieve maximal 9 7-98% \ufb01rst one\nrecognition rate and 0.3-0.4% Equal Error Rate. The experim ents also showed that\nthe accuracy of our method is less a\ufb00ected by eye location erro rs and used image\nnormalization method than of traditional PCA -based recogn ition method.\nKey words"}
{"category": "non-abstract", "text": "Learning View Generalization Functions\nThomas M. Breuel\nPARC, Palo Alto, USA\ntmb@parc.com\nNovember 2003\u0003\nAbstract\nLearning object models from views in 3D visual ob-\nject recognition is usually formulated either as a func-\ntion approximation problem of a function describing\nthe view-manifold of an object, or as that of learn-\ning a class-conditional density. This paper describes\nan alternative framework for learning in visual object\nrecognition, that of learning the view-generalization\nfunction. Using the view-generalization function, an\nobserver can perform Bayes-optimal 3D object recog-\nnition given one or more 2D training views directly,\nwithout the need for a separate model acquisition\nstep. The paper shows that view generalization func-\ntions can be computationally practical by restating\ntwo widely-used methods, the eigenspace and linear\ncombination of views approaches, in a view general-\nization framework. The paper relates the approach to\nrecent methods for object recognition based on non-\nuniform blurring. The paper presents results both on\nsimulated 3D \\paperclip\" objects and real-world im-\nages from the COIL-100 database showing that useful\nview-generalization functions can be realistically be\nlearned from a comparatively small number of train-\ning examples.\n1 Introduction\nLearning view-based or appearance-based models of\nobjects has been a major area of research in visual\n\u0003This paper was originally written in November 2003, but\nhas been submitted to Arxiv in 2007."}
{"category": "abstract", "text": "Learning object models from views in 3D visual ob-\nject recognition is usually formulated either as a func-\ntion approximation problem of a function describing\nthe view-manifold of an object, or as that of learn-\ning a class-conditional density. This paper describes\nan alternative framework for learning in visual object\nrecognition, that of learning the view-generalization\nfunction. Using the view-generalization function, an\nobserver can perform Bayes-optimal 3D object recog-\nnition given one or more 2D training views directly,\nwithout the need for a separate model acquisition\nstep. The paper shows that view generalization func-\ntions can be computationally practical by restating\ntwo widely-used methods, the eigenspace and linear\ncombination of views approaches, in a view general-\nization framework. The paper relates the approach to\nrecent methods for object recognition based on non-\nuniform blurring. The paper presents results both on\nsimulated 3D \\paperclip\" objects and real-world im-\nages from the COIL-100 database showing that useful\nview-generalization functions can be realistically be\nlearned from a comparatively small number of train-\ning examples.\n1 Introduction\nLearning view-based or appearance-based models of\nobjects has been a major area of research in visual\n\u0003This paper was originally written in November 2003, but\nhas been submitted to Arxiv in 2007."}
{"category": "non-abstract", "text": "ni.com/italy   Email: ni.italy@ni.com  \nNational Instruments Italy \u2013 via A. Kuliscioff 22, 20152 Milano \u2013 Tel. +39 02 413091 \u2013 Fax +39 02 41309215  \nFiliale di Roma  \u2013 via Mar della Cina 276, 00144 Roma \u2013 Tel. +39 06 520871 \u2013 Fax +39 06 52087309  The most relevant parameters are the FRR ( False Rejection Rate) and the FAR ( False Acceptance Rate): \nthe first one is linked with the safety while the second one with the security . Here we present a system \nbased on the fingerprint sensor  by Authentec  and Labview Environment by NI , which has a scaleable \naccuracy respect to different sensors  and so respect to the request and cost. However we deal with low \ncost system. In our system the FRR parameter is 1/100, while the FAR is 1/100.000 that in the present \nscenario is a good result and it allows us to consider a possible use for car security and safety system.  \n \nThe Fingerprint System  \nThe hardware consists of : \n\u2022 a x86 based ha rdware  with the Microsoft Window as OS (we are waiting the release 7 of Labview to \nproduce an embedded system based on Intel Strong Arm processor  and Windows CE ),  \n\u2022 an Entr\u00e8Pad AES 3500  sensor by Authentec .  \nThe system architecture is structured as shown i n Fig.1.  \n \n  \nFigure 1: The system architecture.  \n \n \n The software is realized with Labview and Labview RT. The image analysis is realized with IMAQ and \nthe data analysis use the SIGNAL PROCESSING TOOLSET by NI. A private library was developed for \nthe recogni tion based on wavelet analysis. It uses SQL language, and  is linked to the system by SQL \nTOOLKIT by NI.  \nEach raw image  (see Fig 2)  is given to the Process and Analysis Unit (P&A) for the recognition. This unit \nis formed by four subunits: 1) the first tes ts the morphology of the finger  in the real space;  2) the second \nsubunit transforms the image in a 3 -d image, where the third dimension corresponds to a different weight \nof the finger  respect to a fixed coordinate frame and respect to some characteristic parameters (see \nFig.3); 3) the third subunit transforms the image in the frequencies domain where a wavelets analysis is \ncarried out (here the best coefficients are fixed thanks to a neural network based on a multilayer  \nWeb: ni.com/italy   Email: ni.italy@ni.com  \nNational Instruments Italy \u2013 via A. Kuliscioff 22, 20152 Milano \u2013 Tel. +39 02 413091 \u2013 Fax +39 02 41309215  \nFiliale di Roma  \u2013 via Mar della Cina 276, 00144 Roma \u2013 Tel. +39 06 520871 \u2013 Fax +39 06 52087309  perceptron); 4) the last subunit tra nsforms the image in a multidimensional objects, where the dimensions \nof the space are linked with some principal parameters of the finger: curves, lines and minutiae ; then the \nalgorithm   executes the last analysis to recognize a person by using a genetic pipeline.  \n \n \n \nFigure 2:  A typical fingerprint.  \n \n \n \nAfter the analysis the data come back the DB . In particular, statistics, plots of data and events are \nproduced and stored by this module. In the occurrence of a special events, like as an alert, this unit can \nautomatically reach supervisors and the police with e -mail service and SMS (Short Message System). \nThis module is put into practice by using the INTERNET TOOLKIT  and the SPC tools by NI.  \n \n  \nFigure 3:  A 3-d fingerprint in a transformed space.  \n \n \n \n \nResu lts and Benefits  \nBy using a fingerprint authentication system, the consumer may enter and start the vehicle without the \nuse of keys. Automotive manufacturers will also be able to provide consumers a unique combination of \nsecurity, convenience and personali zation. Keys, as well as insecure PIN codes or passwords, will \neventually be rendered obsolete because vehicles may be activated by biometric information that cannot \nbe lost, stolen or forgotten. Personalized levels of access may also be specified for diff erent users of a \nvehicle, including:  \n1. Access to storage compartments  \n2. Access to e -mail and radio settings   \nWeb: ni.com/italy   Email: ni.italy@ni.com  \nNational Instruments Italy \u2013 via A. Kuliscioff 22, 20152 Milano \u2013 Tel. +39 02 413091 \u2013 Fax +39 02 41309215  \nFiliale di Roma  \u2013 via Mar della Cina 276, 00144 Roma \u2013 Tel. +39 06 520871 \u2013 Fax +39 06 52087309  3. Seat adjustments  \n4. Mirror adjustments  \n5. Climate control settings  \nThe Benefits of a Fingerprint Validation System  are: \n1. Virtually eliminates unauthorized  access  \n2. Reduces possibility of auto theft  \n3. Allows personalization at the touch of a finger  \n4. Appeals to security -conscious consumers  \n5. Appeals to tech -savvy consumers  \n \nConclusion  \nAn high challenge for safety and security is reach and an interesting solution  is fixed. The present system \nget high level results in fingerprint  recognition. The accuracy and the precision are appropriate for the \nautomotive segment . Fingerprint  System is scalable respect to the needed level of  security and the \nbudget. It is low co st system respect to the standard cost per year  in public structure or enterprise.  \n \nAcknowledgements  \nThe author s wish to thank I.Piacentini and the NI imaging group in Italy for relevant software suggestions \nand comments."}
{"category": "abstract", "text": "In the paper will be presented a safety and security  system based on fingerprint technology. The results \nsuggest a new scenario where the new cars can use a fingerprint sensor integrated in car handle to allow \naccess and in the dashboard as starter button .  \n \nIntroduction  \nOne of the most relevant question in the security world is how identify and recognize people not only  in \nsome protected areas with a supervisor, but also in  public area like as in airports, banks, etc.  \nThe automotive segment is another example of how keys, PIN pads, and tokens have been used to \nprovide the security mechanism for the auto. Even though many \"high end\" autos use a transponder key \nto initiate the engine ignition process, if this key is stolen or given to an unauthorized pers on, it will still \nprovide access to unauthorized person, just as if he were authorized.   \nWeb"}
{"category": "non-abstract", "text": "least squares \ufb01t, curve \ufb01tting, circle \ufb01tting, algebraic \ufb01t, Rao-Cramer\nbound, e\ufb03ciency, functional model.\n1 Introduction\nIn many applications one \ufb01ts a parametrized curve described by an im plicit equation\nP(x,y;\u0398) = 0 to experimental data ( xi,yi),i= 1,...,n. Here \u0398 denotes the vector of\nunknown parameters to be estimated. Typically, Pis a polynomial in xandy, and its\ncoe\ufb03cients areunknown parameters (or functions of unknown pa rameters). Forexample,\na number of recent publications [2, 10, 11, 16, 19] are devoted to t he problem of \ufb01tting\nquadrics Ax2+Bxy+Cy2+Dx+Ey+F= 0, in which case \u0398 = ( A,B,C,D,E,F )is the\nparametervector. Theproblemof\ufb01ttingcircles, givenbyequation (x\u2212a)2+(y\u2212b)2\u2212R2=\n0 with three parameters a,b,R, also attracted attention [8, 14, 15, 18].\nWe consider here the problem of \ufb01tting general curves given by implic it equations\nP(x,y;\u0398) = 0 with \u0398 = ( \u03b81,...,\u03b8 k) being the parameter vector. Our goal is to in-\nvestigate statistical properties of various \ufb01tting algorithms. We a re interested in their\nbiasedness, covariance matrices, and the Cramer-Rao lower boun d.\n1First, we specify our model. We denote by \u00af\u0398 the true value of \u0398. Let (\u00af xi,\u00afyi),\ni= 1,...,n, be some points lying on the true curve P(x,y;\u00af\u0398) = 0. Experimentally\nobserved data points ( xi,yi),i= 1,...,n, are perceived as random perturbations of the\ntrue points (\u00af xi,\u00afyi). We use notation xi= (xi,yi)Tand\u00afxi= (\u00afxi,\u00afyi)T, for brevity. The\nrandom vectors ei=xi\u2212\u00afxiare assumed to be independent and have zero mean. Two\nspeci\ufb01c assumptions on their probability distribution can be made, se e [4]:\nCartesianmodel : Eacheiisatwo-dimensionalnormalvectorwithcovariancematrix\n\u03c32\niI, whereIis the identity matrix.\nRadial model :ei=\u03beiniwhere\u03beiis a normal random variable N(0,\u03c32\ni), andniis a\nunit normal vector to the curve P(x,y;\u00af\u0398) = 0 at the point xi.\nOur analysis covers both models, Cartesian and radial. For simplicity, we assume that\n\u03c32\ni=\u03c32for alli, but note that our results can be easily generalized to arbitrary \u03c32\ni>0.\nConcerning the true points \u00afxi,i= 1,...,n, two assumptions are possible. Many\nresearchers [6, 13, 14] consider them as \ufb01xed, but unknown, poin ts on the true curve.\nIn this case their coordinates (\u00af xi,\u00afyi) can be treated as additional parameters of the\nmodel (nuisance parameters). Chan [6] and others [3, 4] call this a ssumption a functional\nmodel. Alternatively, one can assume that the true points \u00afxiare sampled from the curve\nP(x,y;\u00af\u0398) = 0 according to some probability distribution on it. This assumption is\nreferred to as a structural model [3, 4]. We only consider the functional model here.\nIt is easy to verify that maximum likelihood estimation of the paramete r \u0398 for the\nfunctional model is given by the orthogonal least squares \ufb01t (OLS F), which is based on\nminimization of the function\nF1(\u0398) =n/summationdisplay\ni=1[di(\u0398)]2(1.1)\nwheredi(\u0398) denotes the distance from the point xito the curve P(x,y;\u0398) = 0. The\nOLSF is the method of choice in practice, especially when one \ufb01ts simple curves such\nas lines and circles. However, for more general curves the OLSF be comes intractable,\nbecause the precise distance diis hard to compute. For example, when Pis a generic\nquadric (ellipse or hyperbola), the computation of diis equivalent to solving a polynomial\nequation of degree four, and its direct solution is known to be numer ically unstable, see\n[2, 11] for more detail. Then one resorts to various approximations . It is often convenient\nto minimize\nF2(\u0398) =n/summationdisplay\ni=1[P(xi,yi;\u0398)]2(1.2)\ninstead of (1.1). This method is referred to as a (simple) algebraic \ufb01t (AF), in this case\none calls |P(xi,yi;\u0398)|thealgebraic distance [2, 10, 11] from the point ( xi,yi) to the\ncurve. The AF is computationally cheaper than the OLSF, but its acc uracy is often\nunacceptable, see below.\n2The simple AF (1.2) can be generalized to a weighted algebraic \ufb01t , which is based on\nminimization of\nF3(\u0398) =n/summationdisplay\ni=1wi[P(xi,yi;\u0398)]2(1.3)\nwherewi=w(xi,yi;\u0398) are some weights, which may balance (1.2) and improve its\nperformance. One way to de\ufb01ne weights wiresults from a linear approximation to di:\ndi\u2248|P(xi,yi;\u0398)|\n/ba\u2207dbl\u2207xP(xi,yi;\u0398)/ba\u2207dbl\nwhere\u2207xP= (\u2202P/\u2202x,\u2202P/\u2202y ) is the gradient vector, see [20]. Then one minimizes the\nfunction\nF4(\u0398) =n/summationdisplay\ni=1[P(xi,yi;\u0398)]2\n/ba\u2207dbl\u2207xP(xi,yi;\u0398)/ba\u2207dbl2(1.4)\nThis method is called the gradient weighted algebraic \ufb01t (GRAF). It is a particular case\nof (1.3) with wi= 1//ba\u2207dbl\u2207xP(xi,yi;\u0398)/ba\u2207dbl2.\nThe GRAF isknown since atleast 1974[21] and recently became stan dard forpolyno-\nmial curve \ufb01tting [20, 16, 10]. The computational cost of GRAF depe nds on the function\nP(x,y;\u0398), but, generally, the GRAF is much faster than the OLSF. It is als o known\nfrom practice that the accuracy of GRAF is almost as good as that o f the OLSF, and\nour analysis below con\ufb01rms this fact. The GRAF is often claimed to be a statistically\noptimalweighted algebraic \ufb01t, and we will prove this fact as well.\nNot much has been published on statistical properties of the OLSF a nd algebraic\n\ufb01ts, apart from the simplest case of \ufb01tting lines and hyperplanes [1 2]. Chan [6], Berman\nand Culpin [4] investigated circle \ufb01tting by the OLSF and the simple algeb raic \ufb01t (1.2)\nassuming the structural model. Kanatani [13, 14] used the Cart esian functional model\nand considered a general curve \ufb01tting problem. He established an a nalogue of the Rao-\nCramer lower bound for unbiased estimates of \u0398, which we call here K anatani-Cramer-\nRao (KCR) lower bound. He also showed that the covariance matrice s of the OLSF and\nthe GRAF attain, to the leading order in \u03c3, his lower bound. We note, however, that in\nmost cases the OLSF and algebraic \ufb01ts are biased[4, 5], hence the KCR lower bound, as\nit is derived in [13, 14], does not immediately apply to these methods.\nIn this paper we extend the KCR lower bound to biased estimates, wh ich include the\nOLSF and all weighted algebraic \ufb01ts. We prove the KCR bound for est imates satisfying\nthe following mild assumption:\nPrecision assumption . For precise observations (when xi=\u00afxifor all 1\u2264i\u2264n), the\nestimate \u02c6\u0398 is precise, i.e.\n\u02c6\u0398(\u00afx1,...,\u00afxn) =\u00af\u0398 (1.5)\nIt is easy to check that the OLSF and algebraic \ufb01ts (1.3) satisfy this assumption. We\nwill also show that all unbiased estimates of \u02c6\u0398 satisfy (1.5).\nWe then prove that the GRAF is, indeed, a statistically e\ufb03cient \ufb01t, in t he sense that\nits covariance matrix attains, to the leading order in \u03c3, the KCR lower bound. On the\n3other hand, rather surprisingly, we \ufb01nd that GRAF is not the only st atistically e\ufb03cient\nalgebraic \ufb01t, and we describe all statistically e\ufb03cient algebraic \ufb01ts. F inally, we show that\nKanatani\u2019s theory and our extension to it remain valid for the radial f unctional model.\nOur conclusions are illustrated by numerical experiments on circle \ufb01t ting algorithms.\n2 Kanatani-Cramer-Rao lower bound\nRecall that we have adopted the functional model, in which the true points\u00afxi, 1\u2264i\u2264\nn, are \ufb01xed. This automatically makes the sample size n\ufb01xed, hence, many classical\nconcepts of statistics, such as consistency and asymptotic e\ufb03cie ncy (which require taking\nthe limit n\u2192 \u221e) lose their meaning. It is customary, in the studies of the functiona l\nmodel of the curve \ufb01tting problem, to take the limit \u03c3\u21920 instead of n\u2192 \u221e, cf.\n[13, 14]. This is, by the way, not unreasonable from the practical po int of view: in many\nexperiments, nis rather small and cannot be (easily) increased, so the limit n\u2192 \u221eis\nof little interest. On the other hand, when the accuracy of experim ental observations is\nhigh (thus, \u03c3is small), the limit \u03c3\u21920 is quite appropriate.\nNow, let \u02c6\u0398(x1,...,xn) be an arbitrary estimate of \u0398 satisfying the precision assump-\ntion (1.5). In our analysis we will always assume that all the underlying functions are\nregular (continuous, have \ufb01nite derivatives, etc.), which isa stand ardassumption [13, 14].\nThe mean value of the estimate \u02c6\u0398 is\nE(\u02c6\u0398) =/integraldisplay\n\u00b7\u00b7\u00b7/integraldisplay\n\u02c6\u0398(x1,...,xn)n/productdisplay\ni=1f(xi)dx1\u00b7\u00b7\u00b7dxn (2.1)\nwheref(xi) is the probability density function for the random point xi, as speci\ufb01ed by\na particular model (Cartesian or radial).\nWe now expand the estimate \u02c6\u0398(x1,...,xn) into a Taylor series about the true point\n(\u00afx1,...,\u00afxn) remembering (1.5):\n\u02c6\u0398(x1,...,xn) =\u00af\u0398+n/summationdisplay\ni=1\u0398i\u00d7(xi\u2212\u00afxi)+O(\u03c32) (2.2)\nwhere\n\u0398i=\u2207xi\u02c6\u0398(\u00afx1,...,\u00afxn), i= 1,...,n (2.3)\nand\u2207xistands for the gradient with respect to the variables xi,yi. In other words, \u0398 iis\nak\u00d72 matrix of partial derivatives of the kcomponents of the function \u02c6\u0398 with respect\nto the two variables xiandyi, and this derivative is taken at the point ( \u00afx1,...,\u00afxn),\nSubstituting the expansion (2.2) into (2.1) gives\nE(\u02c6\u0398) =\u00af\u0398+O(\u03c32) (2.4)\nsinceE(xi\u2212\u00afxi) = 0. Hence, the bias of the estimate \u02c6\u0398 is of order \u03c32.\n4It easily follows from the expansion (2.2) that the covariance matrix of the estimate\n\u02c6\u0398 is given by\nC\u02c6\u0398=n/summationdisplay\ni=1\u0398iE[(xi\u2212\u00afxi)(xi\u2212\u00afxi)T]\u0398T\ni+O(\u03c34)\n(it is not hard to see that the cubical terms O(\u03c33) vanish because the normal random\nvariables with zero mean also have zero third moment, see also [13]). N ow, for the\nCartesian model\nE[(xi\u2212\u00afxi)(xi\u2212\u00afxi)T] =\u03c32I\nand for the radial model\nE[(xi\u2212\u00afxi)(xi\u2212\u00afxi)T] =\u03c32ninT\ni\nwhereniis a unit normal vector to the curve P(x,y;\u00af\u0398) = 0 at the point \u00afxi. Then we\nobtain\nC\u02c6\u0398=\u03c32n/summationdisplay\ni=1\u0398i\u039bi\u0398T\ni+O(\u03c34) (2.5)\nwhere \u039b i=Ifor the Cartesian model and \u039b i=ninT\nifor the radial model.\nLemma.We have \u0398ininT\ni\u0398T\ni= \u0398i\u0398T\nifor each i= 1,...,n. Hence, for both models,\nCartesian and radial, the matrix C\u02c6\u0398is given by the same expression:\nC\u02c6\u0398=\u03c32n/summationdisplay\ni=1\u0398i\u0398T\ni+O(\u03c34) (2.6)\nThis lemma is proved in Appendix.\nOur next goal is now to \ufb01nd a lower bound for the matrix\nD1:=n/summationdisplay\ni=1\u0398i\u0398T\ni (2.7)\nFollowing [13, 14], we consider perturbations of the parameter vect or\u00af\u0398 +\u03b4\u0398 and the\ntrue points \u00afxi+\u03b4\u00afxisatisfying two constraints. First, since the true points must belon g\nto the true curve, P(\u00afxi;\u00af\u0398) = 0, we obtain, by the chain rule,\n/angb\u2207acketleft\u2207xP(\u00afxi;\u00af\u0398),\u03b4\u00afxi/angb\u2207acket\u2207ight+/angb\u2207acketleft\u2207\u0398P(\u00afxi;\u00af\u0398),\u03b4\u0398/angb\u2207acket\u2207ight= 0 (2.8)\nwhere/angb\u2207acketleft\u00b7,\u00b7/angb\u2207acket\u2207ightstands for the scalar product of vectors. Second, since the iden tity (1.5) holds\nfor all \u0398, we get\nn/summationdisplay\ni=1\u0398i\u03b4\u00afxi=\u03b4\u0398 (2.9)\nby using the notation (2.3).\n5Now we need to \ufb01nd a lower bound for the matrix (2.7) subject to the constraints\n(2.8) and (2.9). That bound follows from a general theorem in linear a lgebra:\nTheorem (Linear Algebra) .Letn\u2265k\u22651andm\u22651. Suppose nnonzero vectors\nui\u2208IRmandnnonzero vectors vi\u2208IRkare given, 1\u2264i\u2264n. Consider k\u00d7mmatrices\nXi=viuT\ni\nuT\niui\nfor1\u2264i\u2264n, andk\u00d7kmatrix\nB=n/summationdisplay\ni=1XiXT\ni=n/summationdisplay\ni=1vivT\ni\nuT\niui\nAssume that the vectors v1,...,v nspanIRk(henceBis nonsingular). We say that a set\nofnmatrices A1,...,A n(each of size k\u00d7m) isproperif\nn/summationdisplay\ni=1Aiwi=r (2.10)\nfor any vectors wi\u2208IRmandr\u2208IRksuch that\nuT\niwi+vT\nir= 0 (2.11)\nfor all1\u2264i\u2264n. Then for any proper set of matrices A1,...,A nthek\u00d7kmatrix\nD=/summationtextn\ni=1AiAT\niis bounded from below by B\u22121in the sense that D\u2212B\u22121is a positive\nsemide\ufb01nite matrix. The equality D=B\u22121holds if and only if Ai=\u2212B\u22121Xifor all\ni= 1,...,n.\nThis theorem is, probably, known, but we provide a full proof in Appe ndix, for the\nsake of completeness.\nAs a direct consequence of the above theorem we obtain the lower b ound for our\nmatrixD1:\nTheorem (Kanatani-Cramer-Rao lower bound) .We have D1\u2265 Dmin, in the sense\nthatD1\u2212Dminis a positive semide\ufb01nite matrix, where\nD\u22121\nmin=n/summationdisplay\ni=1(\u2207\u0398P(\u00afxi;\u0398))(\u2207\u0398P(\u00afxi;\u0398))T\n/ba\u2207dbl\u2207xP(\u00afxi;\u0398)/ba\u2207dbl2(2.12)\nIn view of (2.6) and (2.7), the above theorem says that the lower bo und for the\ncovariance matrix C\u02c6\u0398is, to the leading order,\nC\u02c6\u0398\u2265 Cmin=\u03c32Dmin (2.13)\n6The standard deviations of the components of the estimate \u02c6\u0398 are of order \u03c3\u02c6\u0398=O(\u03c3).\nTherefore, the bias of \u02c6\u0398, which is at most of order \u03c32by (2.4), is in\ufb01nitesimally small, as\n\u03c3\u21920, compared to the standard deviations. This means that the estim ates satisfying\n(1.5) are practically unbiased.\nThe bound (2.13) was \ufb01rst derived by Kanatani [13, 14] for the Car tesian functional\nmodel and strictly unbiased estimates of \u0398, i.e. satisfying E(\u02c6\u0398) =\u00af\u0398. One can easily\nderive (1.5) from E(\u02c6\u0398) =\u00af\u0398 by taking the limit \u03c3\u21920, hence our results generalize those\nof Kanatani.\n3 Statistical e\ufb03ciency of algebraic \ufb01ts\nHere we derive an explicit formula for the covariance matrix of the we ighted algebraic \ufb01t\n(1.3) and describe the weights wifor which the \ufb01t is statistically e\ufb03cient. For brevity,\nwe write Pi=P(xi,yi;\u0398). We assume that the weight function w(x,y,;\u0398) is regular, in\nparticular has bounded derivatives with respect to \u0398, the next sec tion will demonstrate\nthe importance of this condition. The solution of the minimization prob lem (1.3) satis\ufb01es\n/summationdisplay\nP2\ni\u2207\u0398wi+2/summationdisplay\nwiPi\u2207\u0398Pi= 0 (3.1)\nObserve that Pi=O(\u03c3), so that the \ufb01rst sum in (3.1) is O(\u03c32) and the second sum is\nO(\u03c3). Hence, to the leading order, the solution of (3.1) can be found by discarding the\n\ufb01rst sum and solving the reduced equation\n/summationdisplay\nwiPi\u2207\u0398Pi= 0 (3.2)\nMore precisely, if \u02c6\u03981and\u02c6\u03982are solutions of (3.1) and (3.2), respectively, then \u02c6\u03981\u2212\u00af\u0398 =\nO(\u03c3),\u02c6\u03982\u2212\u00af\u0398 =O(\u03c3), and/ba\u2207dbl\u02c6\u03981\u2212\u02c6\u03982/ba\u2207dbl=O(\u03c32). Furthermore, the covariance matrices of\n\u02c6\u03981and\u02c6\u03982coincide, to the leading order, i.e. C\u02c6\u03981C\u22121\n\u02c6\u03982\u2192Ias\u03c3\u21920. Therefore, in what\nfollows, we only deal with the solution of equation (3.2).\nTo\ufb01ndthecovariancematrixof \u02c6\u0398satisfying(3.2)weput \u02c6\u0398 =\u00af\u0398+\u03b4\u0398andxi=\u00afxi+\u03b4xi\nand obtain, working to the leading order,\n/summationdisplay\nwi(\u2207\u0398Pi)(\u2207\u0398Pi)T(\u03b4\u0398) =\u2212/summationdisplay\nwi(\u2207xPi)T(\u03b4xi)(\u2207\u0398Pi)+O(\u03c32)\nhence\n\u03b4\u0398 =\u2212/bracketleft\uf8ecig/summationdisplay\nwi(\u2207\u0398Pi)(\u2207\u0398Pi)T/bracketright\uf8ecig\u22121/bracketleft\uf8ecig/summationdisplay\nwi(\u2207xPi)T(\u03b4xi)(\u2207\u0398Pi)/bracketright\uf8ecig\n+O(\u03c32)\nThe covariance matrix is then\nC\u02c6\u0398=E/bracketleft\uf8ecig\n(\u03b4\u0398)(\u03b4\u0398)T/bracketright\uf8ecig\n=\u03c32/bracketleft\uf8ecig/summationdisplay\nwi(\u2207\u0398Pi)(\u2207\u0398Pi)T/bracketright\uf8ecig\u22121/bracketleft\uf8ecig/summationdisplay\nw2\ni/ba\u2207dbl\u2207xPi/ba\u2207dbl2(\u2207\u0398Pi)(\u2207\u0398Pi)T/bracketright\uf8ecig\n\u00d7/bracketleft\uf8ecig/summationdisplay\nwi(\u2207\u0398Pi)(\u2207\u0398Pi)T/bracketright\uf8ecig\u22121+O(\u03c33)\n7Denote by D2the principal factor here, i.e.\nD2=/bracketleft\uf8ecig/summationdisplay\nwi(\u2207\u0398Pi)(\u2207\u0398Pi)T/bracketright\uf8ecig\u22121/bracketleft\uf8ecig/summationdisplay\nw2\ni/ba\u2207dbl\u2207xPi/ba\u2207dbl2(\u2207\u0398Pi)(\u2207\u0398Pi)T/bracketright\uf8ecig /bracketleft\uf8ecig/summationdisplay\nwi(\u2207\u0398Pi)(\u2207\u0398Pi)T/bracketright\uf8ecig\u22121\nThe following theorem establishes a lower bound for D2:\nTheorem .We have D2\u2265 Dmin, in the sense that D2\u2212Dminis a positive semide\ufb01nite\nmatrix, where Dminis given by (2.12). The equality D2=Dminholds if and only if wi=\nconst//ba\u2207dbl\u2207xPi/ba\u2207dbl2for alli= 1,...,n. In other words, an algebraic \ufb01t (1.3) is statistically\ne\ufb03cient if and only if the weight function w(x,y;\u0398)satis\ufb01es\nw(x,y;\u0398) =c(\u0398)\n/ba\u2207dbl\u2207xP(x,y;\u0398)/ba\u2207dbl2(3.3)\nfor all triples x,y,\u0398such that P(x,y;\u0398) = 0. Herec(\u0398)may be an arbitrary function\nof\u0398.\nThe bound D2\u2265 Dminhere is a particular case of the previous theorem. It also can\nbe obtained directly from the linear algebra theorem if one sets ui=\u2207xPi,vi=\u2207\u0398Pi,\nand\nAi=\u2212wi\uf8ee\n\uf8f0n/summationdisplay\nj=1wj(\u2207\u0398Pj)(\u2207\u0398Pj)T\uf8f9\n\uf8fb\u22121\n(\u2207\u0398Pi)(\u2207xPi)T\nfor 1\u2264i\u2264n.\nThe expression (3.3) characterizing the e\ufb03ciency, follows from the last claim in the\nlinear algebra theorem.\n4 Circle \ufb01t\nHere we illustrate our conclusions by the relatively simple problem of \ufb01t ting circles. The\ncanonical equation of a circle is\n(x\u2212a)2+(y\u2212b)2\u2212R2= 0 (4.1)\nand we need to estimate three parameters a,b,R. The simple algebraic \ufb01t (1.2) takes\nform\nF2(a,b,R) =n/summationdisplay\ni=1[(xi\u2212a)2+(yi\u2212b)2\u2212R2]2\u2192min (4.2)\nand the weighted algebraic \ufb01t (1.3) takes form\nF3(a,b,R) =n/summationdisplay\ni=1wi[(xi\u2212a)2+(yi\u2212b)2\u2212R2]2\u2192min (4.3)\n8In particular, the GRAF becomes\nF4(a,b,R) =n/summationdisplay\ni=1[(xi\u2212a)2+(yi\u2212b)2\u2212R2]2\n(xi\u2212a)2+(yi\u2212b)2\u2192min (4.4)\n(where the irrelevant constant factor of 4 in the denominator is dr opped).\nIn terms of (2.12), we have\n\u2207\u0398P(\u00afxi;\u0398) =\u22122(\u00afxi\u2212a,\u00afyi\u2212b,R)T\nand\u2207xP(\u00afxi;\u0398) = 2(\u00af xi\u2212a,\u00afyi\u2212b)T, hence\n/ba\u2207dbl\u2207xP(\u00afxi;\u0398)/ba\u2207dbl2= 4[(\u00afxi\u2212a)2+(\u00afyi\u2212b)2] = 4R2\nTherefore,\nDmin=\uf8eb\n\uf8ec\uf8ed/summationtextu2\ni/summationtextuivi/summationtextui/summationtextuivi/summationtextv2\ni/summationtextvi/summationtextui/summationtextvin\uf8f6\n\uf8f7\uf8f8\u22121\n(4.5)\nwhere we denote, for brevity,\nui=\u00afxi\u2212a\nR, vi=\u00afyi\u2212b\nR\nThe above expression for Dminwas derived earlier in [7, 14].\nNow, our Theorem in Section 3 shows that the weighted algebraic \ufb01t ( 4.3) is statisti-\ncally e\ufb03cient if and only if the weight function satis\ufb01es w(x,y;a,b,R) =c(a,b,R)/(4R2).\nSincec(a,b,R) maybeanarbitraryfunction, thenthedenominator 4 R2hereisirrelevant.\nHence, statistically e\ufb03ciency is achieved whenever w(x,y;a,b,R) is simply independent\nofxandyfor all (x,y) lying on the circle. In particular, the GRAF (4.4) is statistically\ne\ufb03cient because w(x,y;a,b,R) = [(x\u2212a)2+(y\u2212b)2]\u22121=R\u22122. The simple AF (4.2) is\nalso statistically e\ufb03cient since w(x,y;a,b,R) = 1.\nWe note that the GRAF (4.4) is a highly nonlinear problem, and in its exac t form\n(4.4) is not used in practice. Instead, there are two modi\ufb01cations o f GRAF popular\namong experimenters. One is due to Chernov and Ososkov [8] and Pr att [17]:\nF\u2032\n4(a,b,R) =R\u22122n/summationdisplay\ni=1[(xi\u2212a)2+(yi\u2212b)2\u2212R2]2\u2192min (4.6)\n(it is based on the approximation ( xi\u2212a)2+(yi\u2212b)2\u2248R2), and the other due to Agin\n[1] and Taubin [20]:\nF\u2032\u2032\n4(a,b,R) =1\n/summationtext(xi\u2212a)2+(yi\u2212b)2n/summationdisplay\ni=1[(xi\u2212a)2+(yi\u2212b)2\u2212R2]2\u2192min (4.7)\n9(here one simply averages the denominator of (4.4) over 1 \u2264i\u2264n). We refer the reader\nto [9] for a detailed analysis of these and other circle \ufb01tting algorithm s, including their\nnumerical implementations.\nWe have tested experimentally the e\ufb03ciency of four circle \ufb01tting algo rithms: the\nOLSF (1.1), the simple AF (4.2), the Pratt method (4.6), and the Tau bin method (4.7).\nWe have generated n= 20 points equally spaced on a circle, added an isotropic Gaussian\nnoise with variance \u03c32(according to the Cartesian model), and estimated the e\ufb03ciency\nof the estimate of the center by\nE=\u03c32(D11+D22)\n/angb\u2207acketleft(\u02c6a\u2212a)2+(\u02c6b\u2212b)2/angb\u2207acket\u2207ight(4.8)\nHere (a,b) is the true center, (\u02c6 a,\u02c6b) is its estimate, /angb\u2207acketleft\u00b7\u00b7\u00b7/angb\u2207acket\u2207ightdenotes averaging over many\nrandom samples, and D11,D22are the \ufb01rst two diagonal entries of the matrix (4.5).\nTable 1 shows the e\ufb03ciency of the above mentioned four algorithms f or various values\nof\u03c3/R. We see that they all perform very well, and indeed are e\ufb03cient as \u03c3\u21920. One\nmight notice that the OLSF slightly outperforms the other methods , and the AF is the\nsecond best.\n\u03c3/ROLSF AFPrattTaubin\n<0.01\u223c1\u223c1\u223c1\u223c1\n0.010.9990.9990.9990.999\n0.020.9990.9980.9970.997\n0.030.9980.9960.9950.995\n0.050.9960.9920.9870.987\n0.100.9850.9700.9530.953\n0.200.9350.9000.8370.835\n0.300.8250.8240.7010.692\nTable 1. E\ufb03ciency of circle \ufb01tting algorithms. Data are sampled along a full circle.\nTable 2 shows the e\ufb03ciency of the same algorithms as the data points are sampled\nalong half a circle, rather than a full circle. Again, the e\ufb03ciency as \u03c3\u21920 is clear, but we\nalso make another observation. The AF now consistently falls behind the other methods\nfor all\u03c3/R\u22640.2, but for \u03c3/R= 0.3 the others suddenly break down, while the AF keeps\na\ufb02oat.\n10\u03c3/ROLSF AFPrattTaubin\n<0.01\u223c1\u223c1\u223c1\u223c1\n0.010.9990.9960.9990.999\n0.020.9970.9830.9970.997\n0.030.9940.9610.9920.992\n0.050.9840.9020.9780.978\n0.100.9350.7200.9160.916\n0.200.7200.4930.7030.691\n0.300.1220.4370.1860.141\nTable 2. E\ufb03ciency of circle \ufb01tting algorithms with data sampled along ha lf a circle.\nThe reason of the above turnaround is that at large noise the data points may occa-\nsionally line up along a circular arc of a very large radius. Then the OLSF , Pratt and\nTaubin dutifully return a large circle whose center lies far away, and s uch \ufb01ts blow up\nthe denominator of (4.8), a typical e\ufb00ect of large outliers. On the c ontrary, the AF is\nnotoriously known for its systematic bias toward smaller circles [8, 1 1, 17], hence while\nit is less accurate than other \ufb01ts for typical random samples, its bia s safeguards it from\nlarge outliers.\nThis behavior is even more pronounced when the data are sampled alo ng quarter1of\na circle (Table 3). We see that the AF is now far worse than the other \ufb01ts for\u03c3/R <0.1\nbut the others characteristically break down at some point ( \u03c3/R= 0.1).\n\u03c3/ROLSF AFPrattTaubin\n0.010.9970.9110.9970.997\n0.020.9770.7220.9780.978\n0.030.9440.5550.9460.946\n0.050.8370.3650.8430.842\n0.100.1550.2750.1630.158\nTable 3. Data are sampled along a quarter of a circle.\nItisinteresting totestsmallercirculararcs, too. Figure1showsac olor-codeddiagram\nof the e\ufb03ciency of the OLSF and the AF for arcs from 0oto 50oand variable \u03c3(we set\n\u03c3=ch, wherehis the height of the circular arc, see Fig. 2, and cvaries from 0 to 0.5).\nThe e\ufb03ciency of the Pratt and Taubin is virtually identical to that of t he OLSF, so it is\nnot shown here. We see that the OLSF and AF are e\ufb03cient as \u03c3\u21920 (both squares in\nthe diagram get white at the bottom), but the AF loses its e\ufb03ciency a t moderate levels\n1All our algorithms are invariant under simple geometric transformat ions such as translations, rota-\ntions and similarities, hence our experimental results do not depend on the choice of the circle, its size,\nand the part of the circle the data are sampled from.\n11of noise ( c >0.1), while the OLSF remains accurate up to c= 0.3 after which it rather\nsharply breaks down.\n50403020100.5\n0.4\n0.3\n0.2\n0.1\n0c\nArc in degrees50403020100.5\n0.4\n0.3\n0.2\n0.1\n0c\nArc in degrees1.0\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\nFigure 1: The e\ufb03ciency of the simple OLSF (left) and the AF (center) . The bar on the\nright explains color codes.\nThefollowing analysissheds morelightonthebehaviorofthecircle\ufb01tt ing algorithms.\nWhen the curvature of the arc decreases, the center coordinat esa,band the radius R\ngrow to in\ufb01nity and their estimates become highly unreliable. In that c ase the circle\nequation (4.1) can be converted to a more convenient algebraic for m\nA(x2+y2)+Bx+Cy+D= 0 (4.9)\nwithanadditionalconstrainontheparameters: B2+C2\u22124AD= 1. Thisparametrization\nwas used in [17, 11], and analyzed in detail in [9]. We note that the origina l parameters\ncan be recovered via a=\u2212B/2A,b=\u2212C/2A, andR= (2|A|)\u22121. The new parametriza-\ntion (4.9) is safe to use for arcs with arbitrary small curvature: th e parameters A,B,C,D\nremain bounded and never develop singularities, see [9]. Even as the curvature vanishes,\nwe simply get A= 0, and the equation (4.9) represents a line Bx+Cy+D= 0.\nhch\u03c3=\nFigure 2: The height of an arc, h, and our formula for \u03c3.\nIn terms of the new parameters A,B,C,D , the weighted algebraic \ufb01t (1.3) takes form\nF3(A,B,C,D ) =n/summationdisplay\ni=1wi[A(x2+y2)+Bx+Cy+D]2\u2192min (4.10)\n12(undertheconstraint B2+C2\u22124AD= 1). ConvertingtheAF(4.2)tothenewparameters\ngives\nF2(A,B,C,D ) =n/summationdisplay\ni=1A\u22122[A(x2+y2)+Bx+Cy+D]2\u2192min (4.11)\nwhich corresponds to the weight function w= 1/A2. The Pratt method (4.6) turns to\nF4(A,B,C,D ) =n/summationdisplay\ni=1[A(x2+y2)+Bx+Cy+D]2\u2192min (4.12)\nWe now see why the AF is unstable and inaccurate for arcs with small c urvature: its\nweight function w= 1/A2develops a singularity (it explodes) in the limit A\u21920. Recall\nthat, in our derivation of the statistical e\ufb03ciency theorem (Sectio n 3), we assumed that\nthe weight function was regular (had bounded derivatives). This as sumption is clearly\nviolated by the AF (4.11). On the contrary, the Pratt \ufb01t (4.12) use s a safe choice w= 1\nand thus behaves decently on arcs with small curvature, see next .\n50403020100.5\n0.4\n0.3\n0.2\n0.1\n0c\nArc in degrees50403020100.5\n0.4\n0.3\n0.2\n0.1\n0c\nArc in degrees1.0\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\nFigure 3: The e\ufb03ciency of the simple AF (left) and the Pratt method ( center). The bar\non the right explains color codes.\nFigure 3 shows a color-coded diagram of the e\ufb03ciency of the estimat e of the param-\neter2Aby the AF (4.11) versus Pratt (4.12) for arcs from 0oto 50oand the noise level\n\u03c3=ch, wherehis the height of the circular arc and cvaries from 0 to 0.5. The e\ufb03ciency\nof the OLSF and the Taubin method is visually indistinguishable from tha t of Pratt (the\ncentral square in Fig. 3), so we did not include it here.\nWe see that the AF performs signi\ufb01cantly worse than the Pratt met hod for all arcs\nand most of the values of c(i.e.,\u03c3). The Pratt\u2019s e\ufb03ciency is close 100%, its lowest point\nis 89% for 50oarcs and c= 0.5 (the top right corner of the central square barely gets\ngrey). The AF\u2019s e\ufb03ciency is below 10% for all c >0.2 and almost zero for c >0.4. Still,\n2Note that |A|= 1/2R, hence the estimation of Ais equivalent to that of the curvature, an important\ngeometric parameter of the arc.\n13the AF remains e\ufb03cient as \u03c3\u21920 (as the tiny white strip at the bottom of the left square\nproves), but its e\ufb03ciency can be only counted on when \u03c3is extremely small.\nOur analysis demonstrates that the choice of the weights wiin the weighted algebraic\n\ufb01t (1.3) should be made according to our theorem in Section 3, and, in addition, one\nshould avoid singularities in the domain of parameters.\nAppendix\nHere we prove the theorem of linear algebra stated in Section 2. For the sake of clarity,\nwe divide our proof into small lemmas:\nLemma 1 .The matrix Bis indeed nonsingular .\nProof. IfBz= 0forsomenonzerovector z\u2208IRk,then0 = zTBz=/summationtextn\ni=1(vT\niz)2//ba\u2207dblui/ba\u2207dbl2,\nhencevT\niz= 0 for all 1 \u2264i\u2264k, a contradiction.\nLemma 2 .If a set of nmatrices A1,...,A nis proper, then rank (Ai)\u22641. Furthermore,\neachAiis given by Ai=ziuT\nifor some vector zi\u2208IRk, and the vectors z1,...,z nsatisfy/summationtextn\ni=1zivT\ni=\u2212IwhereIis thek\u00d7kidentity matrix. The converse is also true.\nProof. Let vectors w1,...,w nandrsatisfy the requirements (2.10) and (2.11) of the\ntheorem. Consider the orthogonal decomposition wi=ciui+w\u22a5\niwherew\u22a5\niis perpendic-\nular toui, i.e.uT\niw\u22a5\ni= 0. Then the constraint (2.11) can be rewritten as\nci=\u2212vT\nir\nuT\niui(A.1)\nfor alli= 1,...,nand (2.10) takes form\nn/summationdisplay\ni=1ciAiui+n/summationdisplay\ni=1Aiw\u22a5\ni=r (A.2)\nWe conclude that Aiw\u22a5\ni= 0 for every vector w\u22a5\niorthogonal to ui, henceAihas a (k\u22121)-\ndimensional kernel, so indeed its rank is zero or one. If we denote zi=Aiui//ba\u2207dblui/ba\u2207dbl2, we\nobtainAi=ziuT\ni. Combining this with (A.1)-(A.2) gives\nr=\u2212n/summationdisplay\ni=1(vT\nir)zi=\u2212/parenleft\uf8eciggn/summationdisplay\ni=1zivT\ni/parenright\uf8ecigg\nr\nSince this identity holds for any vector r\u2208IRk, the expression within parentheses is \u2212I.\nThe converse is obtained by straightforward calculations. Lemma is proved.\nCorollary .Letni=ui//ba\u2207dblui/ba\u2207dbl. ThenAininT\niAi=AiAT\nifor each i.\nThis corollary implies our lemma stated in Section 2. We now continue the proof of\nthe theorem.\n14Lemma 3 .The sets of proper matrices make a linear variety, in the foll owing sense. Let\nA\u2032\n1,...,A\u2032\nnandA\u2032\u2032\n1,...,A\u2032\u2032\nnbe two proper sets of matrices, then the set A1,...,A nde\ufb01ned\nbyAi=A\u2032\ni+c(A\u2032\u2032\ni\u2212A\u2032\ni)is proper for every c\u2208IR.\nProof. According to the previous lemma, A\u2032\ni=z\u2032\niuT\niandA\u2032\u2032\ni=z\u2032\u2032\niuT\nifor some vectors\nz\u2032\ni,z\u2032\u2032\ni, 1\u2264i\u2264n. Therefore, Ai=ziuT\niforzi=z\u2032\ni+c(z\u2032\u2032\ni\u2212z\u2032\ni). Lastly,\nn/summationdisplay\ni=1zivT\ni=n/summationdisplay\ni=1z\u2032\nivT\ni+cn/summationdisplay\ni=1z\u2032\u2032\nivT\ni\u2212cn/summationdisplay\ni=1z\u2032\nivT\ni=\u2212I\nLemma is proved.\nLemma 4 .If a set of nmatrices A1,...,A nis proper, then/summationtextn\ni=1AiXT\ni=\u2212I, whereI\nis thek\u00d7kidentity matrix.\nProof. By using Lemma 2/summationtextn\ni=1AiXT\ni=/summationtextn\ni=1zivT\ni=\u2212I. Lemma is proved.\nLemma 5 .We have indeed D\u2265B\u22121.\nProof. For each i= 1,...,nconsider the 2 k\u00d7mmatrixYi=/parenleft\uf8ecigg\nAi\nXi/parenright\uf8ecigg\n. Using the\nprevious lemma gives\nn/summationdisplay\ni=1YiYT\ni=/parenleft\uf8ecigg\nD\u2212I\n\u2212I B/parenright\uf8ecigg\nBy construction, this matrix is positive semide\ufb01nite. Hence, the follo wing matrix is also\npositive semide\ufb01nite:\n/parenleft\uf8ecigg\nI B\u22121\n0B\u22121/parenright\uf8ecigg/parenleft\uf8ecigg\nD\u2212I\n\u2212I B/parenright\uf8ecigg/parenleft\uf8ecigg\nI0\nB\u22121B\u22121/parenright\uf8ecigg\n=/parenleft\uf8ecigg\nD\u2212B\u221210\n0B\u22121/parenright\uf8ecigg\nBy Sylvester\u2019s theorem, the matrix D\u2212B\u22121is positive semide\ufb01nite.\nLemma 6 .The set of matrices Ao\ni=\u2212B\u22121Xiis proper, and for this set we have\nD=B\u22121.\nProof. Straightforward calculation.\nLemma 7 .IfD=B\u22121for some proper set of matrices A1,...,A n, thenAi=Ao\nifor all\n1\u2264i\u2264n.\nProof. Assume that there is a proper set of matrices A\u2032\n1,...,A\u2032\nn, di\ufb00erent from\nAo\n1,...,Ao\nn, for which D=B\u22121. Denote \u03b4Ai=A\u2032\ni\u2212Ao\ni. By Lemma 3, the set of\nmatrices Ai(\u03b3) =Ao\ni+\u03b3(\u03b4Ai) is proper for every real \u03b3. Consider the variable matrix\nD(\u03b3) =n/summationdisplay\ni=1[Ai(\u03b3)][Ai(\u03b3)]T\n=n/summationdisplay\ni=1Ao\ni(Ao\ni)T+\u03b3/parenleft\uf8eciggn/summationdisplay\ni=1Ao\ni(\u03b4Ai)T+n/summationdisplay\ni=1(\u03b4Ai)(Ao\ni)T/parenright\uf8ecigg\n+\u03b32n/summationdisplay\ni=1(\u03b4Ai)(\u03b4Ai)T\nNote that the matrix R=/summationtextn\ni=1Ao\ni(\u03b4Ai)T+/summationtextn\ni=1(\u03b4Ai)(Ao\ni)Tis symmetric. By Lemma 5\nwe have D(\u03b3)\u2265B\u22121for all\u03b3, and by Lemma 6 we have D(0) =B\u22121. It is then easy\n15to derive that R= 0. Next, the matrix S=/summationtextn\ni=1(\u03b4Ai)(\u03b4Ai)Tis symmetric positive\nsemide\ufb01nite. Since we assumed that D(1) =D(0) =B\u22121, it is easy to derive that S= 0\nas well. Therefore, \u03b4Ai= 0 for every i= 1,...,n. The theorem is proved."}
{"category": "abstract", "text": "We study the problem of \ufb01tting parametrized curves to noisy d ata. Under\ncertain assumptions (known as Cartesian and radial functio nal models), we derive\nasymptotic expressions for the bias and the covariance matr ix of the parameter\nestimates. We also extend Kanatani\u2019s version of the Cramer- Rao lower bound,\nwhich he proved for unbiased estimates only, to more general estimates that include\nmany popular algorithms (most notably, the orthogonal leas t squares and algebraic\n\ufb01ts). We then show that the gradient-weighted algebraic \ufb01t i s statistically e\ufb03cient\nand describe all other statistically e\ufb03cient algebraic \ufb01ts .\nKeywords"}
{"category": "non-abstract", "text": "We propose a new algorithm to the problem of poly gonal curve approximation based \non a multiresolution approach. This algorithm is su boptimal but still maintains some optimality \nbetween successive levels of resolution using dynam ic programming. We show theoretically and \nexperimentally that this algorithm has a linear com plexity in time and space. We experimentally \ncompare the outcomes of our algorithm to the optima l \u201cfull search\u201d dynamic programming \nsolution and finally to classical merge and split a pproaches. The experimental evaluations \nconfirm the theoretical derivations and show that t he proposed approach evaluated on 2D  coastal \nmaps either show a lower time complexity or provide  polygonal approximations closer to the \ninput discrete curves. \n \nKeywords : Polygonal curves; Polygonal approximation; Dynami c \nprogramming; Data reduction; Compression; Multireso lution;  \n \n1. Originality and Contribution \nWe provide a semi optimal efficient solution to the  problem of approximating \nmultidimensional discrete curves using a small numb er of linear segments. This \nsolution when compared with previous existing appro aches (Douglas-Peucker in \nO(N) , MergeL2 in O(Nlog(N) ), Kolesnikov-Franti in O(N 2/K) , optimal dynamic \nprogramming solution in O(K.N 2), where N is the size of the discrete input \ncurve, and K the number of polygonal segments of the required a pproximation) \neither shows a lower time complexity or provides be tter polygonal \napproximations. We theoretically prove that the tim e complexity of our \nalgorithm is ( O(N) ), as it is upper bounded by a linear function of N that is \nindependent from the number of segments K of the final approximation . To our 2 knowledge, the proposed algorithm is the best so fa r having a linear time \ncomplexity. Furthermore, this algorithm provides a set of nested polygonal \napproximations that realises a multiresolution repr esentation of the input curve \nallowing post-processing at various resolution leve ls. The applications that are \nmore and more resource demanding such as computer v ision, shape analysis, \ndata mining, etc, greatly benefit from low complexi ty algorithms able to simplify \na complex curve into a simple shape characterized w ith few polygonal segments. \nWe analyse in details the sensibility of the parame ters that conditioned the \nbehaviour of the proposed algorithm and provide exp erimentations on 2D \ngeographic maps. \n \n2. Introduction \nApproximation of multi dimensional discrete curves has been widely studied \nessentially to speed up data processing required by  resource demanding \napplications such as Computer Vision, Computer Grap hics, Geographic \nInformation Systems and Digital Cartography, Data C ompression or Time Series \nData Mining. For polygonal approximation of discret e curves, the problem can \nbe informally stated as follows: given a digitized curve X of N \u2265 2 ordered \nsamples, find K dominant samples among them that define a sequence of \nconnected segments which most closely approximate t he original curve. This \nproblem is known as the min -\u03b5 problem. Numerous algorithms have been \nproposed for more than thirty years to solve effici ently this optimisation \nproblem. Most of them belong either to graph-theore tic, dynamic programming \nor to heuristic approaches.  \nGraph-theoretical applied to the min -\u03b5 problem produce a weighted directed \nacyclic graph ( DAG ) from the vertices (discrete points) of X, and then find the \nshortest path in this graph (Imai and Iri, 1986, 19 88; Melkman and O\u2019Rourke, \n1988; Chan and Chin, 1996; Zhu, and Seneviratne, 19 97; Chen and Daescu, \n1998; Katsaggelos et al., 1998). For min -\u03b5 problem, finding a minimum path in \nthe corresponding DAG can be solved in )) log( .(2N NO  time (Chan and Chin, \n1996) and in )(NO space (Chen and Daescu, 1998).  \nAmong dynamic programming  solutions, Perez and Vidal (Perez and Vidal, \n1994) were the first (to our knowledge) to propose an algorithm that exploits the 3 sum of the squared Euclidean distance as the global  error criterion. Their \nalgorithm requires ) .(2NKO time and ) .(NKO  space. Some improvements have \nbeen proposed by Salotti (Salotti, 2001) to reduce the time complexity of this \nalgorithm down to ) (2NO . Salotti\u2019s improvements consist of inserting a low er \nbound to limit the search space and employing the A*  search algorithm instead \nof the dynamic programming one. Keeping with the id eas of Perez and Vidal, \nKolesnikov et al. (Kolesnikov et al., 2004) introdu ced a \u2018bounding corridor\u2019, to \nlimit the search space, and used an iterated dynami c programming within it to \nfind an almost optimal solution. The time complexit y is reduced to ) /.(2KNWO  \nwhere W is the size of the bounded corridor. \nWhile dynamic programming and graph theoretic appro aches target relatively \noptimal results, many algorithms try to relax optim ality in order to lower the \nalgorithmic complexity. Relying on the Diophantine definition of discrete \nstraight line and its arithmetical characteristics,  Debled-Rennesson and Reveill\u00e8s \n(Debled-Rennesson and Reveill\u00e8s, 2003 ) gave a line ar method for segmentation \nof curves into exact discrete lines. Their idea is to extend a segment \nincrementally as much as possible so that the verte x that cannot be added to the \nsegment becomes the lower extremity of the followin g segment. Similarly, \nCharbonier & al., (Charbonnier  al., 2004) proposed  an algorithm that splits a \nmonitored signal into line segments\u2014continuous or d iscontinuous\u2014of various \nlengths and determines on-line when a new segment m ust be calculated: they \nused a cumulative sum (CUSUM) error criteria as the  basis for their splitting \nheuristic. Pratt and Fink (Fink & Pratt, 2002) desc ribed a heuristic procedure for \nidentifying major minima and maxima of a time serie s, and for their procedure \nproposed compression and time series information re trieval applications that \ncould be used to extract line segments in linear ti me complexity. \nThis paper focuses on polygonal approximation of mu ltidimensional curves \nusing multiresolution for a given set of segment nu mber for the crudest \napproximation level. Our main contribution is the d evelopment of an algorithm \nthat, starting from the finest resolution level, fi nds min-epsilon polygonal \napproximation for more coarse resolution level usin g the approximating nodes \nobtained for the previous (more fine) resolution le vel. The number of line \nsegments for the next, more coarse resolution level  is reduced using a fixed \nfactor \u03c1 in ]0;1[. Such multiresolution approximation can e xploit any polygonal 4 simplification methods between two successive level s of resolution, in particular \nheuristic algorithms (Douglas-Peucker, Merge-based algorithms, etc.). We \naddress in this paper the use of algorithms based o n constrained dynamic \nprogramming approach to ensure that the provided po lygonal approximations are \nmaintained close to optimal.  \n \nThe first part of the paper describes the multireso lution algorithm we propose as \nan altenative solution to the min -\u03b5 problem . We show theoretically that the \ncomplexity of the algorithm is linear, both in time  and space. The second part of \nthe paper demonstrates experimentally the behaviour  and the efficiency of our \nmulti resolution procedure on a set of 2D  maps representing parts of the \u2018fractal\u2019 \nBrittany coast line (Mandelbrot, 1967). Finally, fo llowing previous works \n(Perrez and Vidal, 1994, Kolesnikov and al. 2004, K eogh and Pazzani, 2000) we \npresent in the appendix the way we have specificall y addressed the question of \nhow to manage the polygonal approximation of curves  using dynamic \nprogramming solutions for a single resolution level .  \n \n3. Multi resolution simplification of Multivariate \nTimes series using polygonal curves \napproximation \n \nAs briefly explained in the introduction, several a uthors have already proposed \nto approximate polygonal curves using dynamic progr amming solutions for \nsingle resolution level.  \nBased on these earlier works, we present hereinafte r a multiresolution algorithm \nthat uses iteratively a constrained dynamic program ming algorithm to find \nefficiently and sequentially polygonal approximatio ns with minimal errors \nbetween each successive resolution levels. \n \n 5 3.1 Parameters and notation: \n \n\u2022 X(m) : a discrete time series \n\u2022 p: the dimension of the space that embeds X; pR mXm \u2208 \u2200 )(,  \n\u2022 N: the number of samples or length of a multivariate  time series \n\u2022 K: the number of segments of a polygonal approximati on  \n\u2022 \u03c1: the ratio K/N : \u03c1 \u2208]0;1[ \n\u2022 band : half size of the bounded corridor used to reduce the search \nspace of the PyCA  algorithm (see FIG. 12). band  is a positive integer.  \n\u2022 \u03b1: a parameter used for formal and experimental evalu ations of time \nand space complexities. \u03b1is related to band  and \u03c1: \n\u03c1\u03b1\u03b1 \u03b1 = \u2264 =KN\nKNRound band . ).(  . We take in practice \u03b1 in {1,2, \u2026}.  \n\u2022 Lb(i) = band\u2013i ; Lower bound used to limit the search space of the  \nPyCA  algorithm \n\u2022 Cinf (j):  Corridor lower bound for the jth  segment \n\u2022 Csup (j): Corridor upper bound for the jth  segment \n\u2022 CN: Complexity of the PyCA  algorithm \n\u2022 MR \nNC: Complexity of the multi resolution PyCA  algorithm \n \n \n3.2 A Multi Resolution approach to Polygonal Curve Approximation \n(MR-PyCA algorithm ) \n \nBasically, the idea behind the multi resolution app roach to polygonal curve \nsimplification is to successively approximate previ ous approximations obtained 6 by using some given simplification algorithm, this process being initiated from \nan original discrete time series. Following (Kolesn ikov & al. 2004), we take a \nsequence of polygonal curves {  X 0, X1, X2,\u2026, Xr} as a multiresolution \n(multiscale) approximation of a N-vertex input curve X, if the set of curves \n{Xi}satisfies the following conditions:  \ni) A polygonal curve X i is an approximation of the curve X for a given \nnumber of segments Ki (min -\u03b5 problem ) or error tolerance \u03b5i ( min-# \nproblem ), where i is a resolution level (i=0,1,2,\u2026, r). \nii) The set of vertices of curve X i for resolution level i is a subset of vertices \nof curve X i-1 for the previous (higher) resolution level ( i-1). The lowest \nresolution level r is represented by the coarsest approximation of X. The \nhighest resolution level i=0 is represented by the most detailed \napproximation (namely the original curve X0=X ) with the largest number of \nsegments K0 =N. (N=K 0 > K1 >  K 2 >\u2026>  K r) or smallest error tolerance \u03b50=0 \nfor some distance measure (e.g. L2) ( \u03b50<\u03b51<\u03b52<\u2026< \u03b5r). \nThus, an approximation curve Xi is either obtained by inserting new points into \nthe approximation curve Xr+1 , or, conversely, Xi+1 is obtained by deleting points \nfrom the approximation curve Xi. These two approaches have led to the \ndevelopment of two very popular heuristic approache s: respectively Split and \nMerge methods. In the Split approach, an iterative mechan ism splits the input \ncurve into smaller and smaller segments until the m aximum deviation is smaller \nthan a given error tolerance \u03b5 (min -# problem ), or the number of linear segments \nequals to the given Ki (min-\u03b5 problem ) for the current resolution level i. A \nfamous split method is the Douglas-Peucker algorith m (Douglas and Peucker, \n1973); this algorithm is known to have a O(K.N)  complexity; it has been used for 7 multiresolution approximation in (Le Buhan Jordan &  al., 1998, Buttenfield, \n2002).  \nIn the Merge approach (Pikaz and Dinstein, 1995, Vi svalingam and Whyatt \n1993), the polygonal approximation is performed by using a cost function that \ndetermines sequential elimination of the vertices w ith the smallest cost value, \nwhile the two adjacent segments of the eliminated v ertex are merged into one \nsegment. The approximation curve Xi is obtained by discarding vertices from the \ncurve Xi-1 until the desired number of vertices Ki (min-\u03b5 problem ) is reached. \nThis merge approach is known to have a O(N.log(N))  complexity. \n \nThere are two sources for error increasing in multi resolution approximation in \ncomparison with individual polygonal approximation:   \n1.  In multiresolution approximation, vertices for the next level of resolution \nshould be selected among the vertices available at the current level of \nresolution. In individual polygonal approximation f or the levels we do not \nhave this constraint. \n \n2. Non-optimality of algorithm used for min-\u03b5 polygonal approximation.  \n \nIn multiresolution approximation, we cannot reduce errors related to the first \nreason, but with better algorithm for min-\u03b5 problems between successive levels \nof resolution one can expect to approach near to op timal solutions. This \nobservation leads to the basis of the MR-PyCA  algorithm we proposed. MR-\nPyCA  algorithm relates to the Merge  approach: we initiate the simplification \nprocess from the finest resolution level and iterat e to obtain the crudest one, \nwhile discarding some vertices during each iteratio n using a constrained based 8 dynamic programming approach. We present in the app endix how we have \nspecifically addressed this \u201cone step\u201d simplificati on procedure to ensure the \npaper is self content.  Basically, the PyCA  algorithm we use during each \nsimplification iteration can be seen as a special c ase of the so-called \u201cReduced \nSearch Dynamic Programming\u201d ( RSDP ) algorithm proposed in (Kolesnikov and \nFr\u00e4nti 2003), for which the reduced search is confi ned inside a fixed sized \ncorridor. The main slight difference between PyCA  and RSDP  lies in the way \ncost functions are evaluated: processing time is re duced for PyCA  algorithm by \ncost of increasing distortion. PyCA  is also adapted so that the simplification \nprocedure can be called iteratively from the origin al time series down to the \ncoarsest approximation level.  \n \nThe pseudo-code for the multiresolution algorithm MR-PyCA is given in FIG.1 . \n \n**** FIG.1 around here **** \n \nThe inputs of MR-PyCA  algorithm are: \n/square4 K, the number of segments in the polygonal approxima tion,  \n/square4 band , the corridor width that reduces the search space,   \n/square4 \u03c1\u2208]0;1[, the decimation factor,  that determines the fixed r atio of \nsegments between two successive resolution levels, \n/square4  the original multidimensional time series X=X 0.  \n \nAs K is an input, the number of resolution levels r (the number of iterations) is \ncalculated given K and \u03c1. Given K and \u03c1,  r is chosen such that: 9 r rNK N \u03c1 \u03c1 . .1\u2264<+ \nAs potentially rNK \u03c1.< , a residual iteration is required to simplify the rth  \napproximation (corresponding to resolution level r) that has rN\u03c1. segments in \norder to get an approximation having exactly K segments. This last iteration \ndiscards ( K Nr\u2212\u03c1. ) segments. \n \nThe multiresolution is the sequence of nested appro ximations provided in output. \nBy construction this algorithm maintains partial op timality between two \nsuccessive resolution levels, since a constrained d ynamic algorithm (cf. \nAppendix) is used to search inside a fixed size \u2018co rridor \u2019 for which segment \nextremities should be discarded and which should be  kept. The approximation \ncorresponding to the last resolution level is the K-segments  polygonal \napproximation provided by MR-PyCA . \n \n3.3 Complexity of MultiResolution MR-PyCA  \n \nFor all N,  K<N and  \u03c1 in ]0;1[  there exists a natural number r such that \nr rNK N \u03c1 \u03c1 . .1\u2264<+, and defining  N Kj\nj .\u03c1= , then, using the PyCA algorithm \nwhile setting \u03c1\u03b1\u03b1 \u03b1 = \u2264 =\n1 1. ).(KN\nKNround band , where \u03b1>1 is a constant,  we \nobtain from an original curve ( X) of size N a polygonal curve approximation ( X1) \nhaving N K .1\u03c1=  segments with time complexity: \n \n\u03c1\u03b1 \u03b1 N\nKNK band MR \nN.. 2 .. 2. . 22\n12 2\n12\n1 , = \u2264 =C  10 If we consider as a second step the simplification of the X1 curve still using the \nPyCA  algorithm while setting \u03c1\u03b1\u03b1 \u03b1 = \u2264 =\n21\n21. ).(KK\nKKround band then we get a \npolygonal curve approximation X2 having N K .2\n2\u03c1=  segments from the X1 \ncurve with time complexity: \n \nNKNK band MR \nN .. 2.. 2. . 22\n22\n22\n2 , \u03b1\u03b1= \u2264 =C  \nIterating the process with \u03c1\u03b1\u03b1 \u03b1 = \u2264 =\n+ + 1 1. ) .(\njj\njj\nKK\nKKround band remaining \nconstant, we get successively: \n \nN K band MR \nN .. 2.. 2. . 22\n22\n32\n3 , \u03b1\u03c1\u03c1\u03b1= \u2264 =C  \n... ... 2 ... 2. 22 2\n24 2\n42\n4 , N N K band MR \nN \u03c1\u03b1\u03c1\u03c1\u03b1= \u2264 =C \n \nBy induction, it is easy to show that for all j in {1,..,r}  we have : \nN K band jj\njMR \njN . .. 2.. 2. 22 2\n22\n2\n,\u2212= \u2264 = \u03c1\u03b1\u03c1\u03c1\u03b1C  \nFor the final iteration required to ensure that the  last approximation has exactly \nK segments we use the PyCA algorithm setting KK\nKKround band r r. ).( \u03b1 \u03b1 \u2264 = . \nSince K Nr<+1.\u03c1 , we have:  K K Nrr< =. )..( \u03c1 \u03c1\u03c1  and then \u03c11<KKr. The time \ncomplexity for the last iteration is: 11 NN\nKKKK band rr\nr MR \nrN ... 2... 2... 2. 21 22\n22 2\n2\n1,\u2212\n+ = \u2264 \u2264 = \u03c1\u03b1\u03c1\u03c1\u03b1 \u03b1C  \n \nFinally, from the original time series of size N, we get after r iterations of the \nprevious process a polygonal approximation having K segments with time \ncomplexity: \n \n\u03c1\u03c1\n\u03c1\u03b1\u03c1 \u03c1\u03c1 \u03b1\u2212\u2212= ++ ++ \u2264 =\u2212\n=\u221111... 2) ... 1.( .. 22\n1 2\n12\n,r\nrr\njMR \njNMR \nNNN CC   (1) \n \nas \u03c1 \u2208]0;1[, we get the following upper bound that shows that the time \ncomplexity of MR-PyCA  when producing a K segments polygonal \napproximation from the original time series X:   \n) 1.( .. 22\n\u03c1 \u03c1\u03b1\n\u2212\u2264N MR \nNC   (2) \nWe note that this upper bound is independent from K, showing that the time \ncomplexity of MR-PyCA   is O(N) . This lower bound is furthermore minimized \nfor 2 / 1 =\u03c1 . \n \nThe size of the search space required by the PyCA  algorithm used during the jth  \niteration of MR-PyCA  is included into a ( ) band Kj. 2.  matrix. For   \u03c1\u03b1\u2264 band  \nand N Kj\nj .\u03c1= , the size of this matrix is N Nj.. 2 .. 21\u03b1 \u03c1\u03b1 \u2264\u2212. For the first \niteration ( j=1 ), the space requirement for the matrix encoding is  maximized \nupper bounded by N.. 2\u03b1 . So, the search space required at any resolution l evel \ncan fit into a N.. 2\u03b1 matrix allocated for the first resolution level. Th e polygonal 12 approximation provided by the ( j-1) th  level of resolution is also required to \ncompute the jth  resolution level : the space requirement is Nj1\u2212\u03c1 . If we want to \nkeep the approximations at all resolution levels we  need to allocate a memory \nspace that is upper bounded by N NNr r\njj.11\n11.1 1\n01\n\u03c1 \u03c1\u03c1\u03c1\u2212\u2264\u2212\u2212=+ +\n=\u2212\u2211 . The overall \nspace requirement is thus upper bounded by N.11. 2\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n\u2212+\u03c1\u03b1 .  This shows that \nthe space complexity required for MR-PyCA  is O(N) . \n \n \n4. Experimentations and discussion \n \nTo evaluate the quality of suboptimal algorithms, R osin (1997) introduced a \nmeasure known as fidelity ( F). It measures how good (or how bad) a given \nsuboptimal approximation is in respect to the optim al approximation in terms of \nthe approximation error:  \nEEFmin .100 = , \nwhere Emin  is the approximation error of the optimal solution , and E is the \napproximation error of the given suboptimal solutio n. In practice, we will \nidentify Emin  to the error (the Euclidian distance between the o riginal time series \nand the polygonal approximation) obtained using the  \u2018Full Search\u2019 dynamic \nprogramming ( FSDP ) solution, namely the algorithm of Perez and Vidal , or \nalternatively our PyCA  implementation with the band  parameter set to N. \nThe evaluation consists essentially in measuring th e fidelity and runtime elapsed \ntime for various parameter values of the MR-PyCA  algorithm. 13  \nWe have tested the MR-PyCA  algorithm on 2D coastal maps extracted from \nthe National Geophysical Data Center (NGDC, 2006) d ataset. We have used \nessentially two maps: the Morbihan Gulf coastal map  and the Brittany coastal \nmap ( FIG 2 ). \n \n \n****  FIG.2  AROUND HERE **** \n \n \nFIG. 3, FIG. 4  and FIG. 5  compare respectively on the Morbihan Gulf 2D map \n(with N=1478 ) the optimal approximation solution with K=33  segments in black \ncontinuous lines with the Douglas Peucker solution ( FIG. 3 ), le Merge_L2  \nsolution ( FIG. 4 ) and the MR-PyCA  solution ( FIG. 5) . The MR-PyCa  solution \n(with \u03c1=.85 , and \u03b1=4 ) is clearly the closest to the optimal solution ( F=92,7% ) \nwhile the Douglas-Peucker  algorithm provides an approximation with a \nsignificantly lower quality ( F=53% ) and the Merge_L2 algorithm that provides \nan in between approximation ( F=77% ). \n \n**** FIG.3, 4, 5 around here **** \n \n \nAll the evaluation tests presented hereinafter have  been performed on the \nBrittany coastal map ( FIG. 2) . This 2D  map contains 17476  points given in \nlongitude and latitude coordinates. \n \n \n \nFIG.6  shows the evolution of the error rate evaluated as  the euclidian distance \nbetween the original map and the approximation map given by the MR-PyCA 14 (with \u03b1=4 and  \u03c1=.7, .75, .8 and.85), Merge_L2, Douglas Peucker and  FSDP  \nalgorithms as the resolution level decreases, e.g. K decreases. As expected, this \nerror is a decreasing function of  K. The MR-PyCA  error curve is the closest to \nthe optimal solution provided by the FSDP  solution.  \n \n**** FIG. 6 around here **** \n \n \nFIG.7  shows the evolution of the F measure as a function of K evaluated for \nthe crudest approximation map given by the MR-PyCA ( with \u03b1=8 and  \u03c1=.1, .2, \n.3, .4, .5, .6,.7,  .8 and .9), Merge_L2  and  Douglas Peucker . MR-PyCA  has a \nbetter F for all \u03c1 values.  Furthermore, the higher \u03c1, the better F in general. \n \n**** FIG. 7 around here **** \n \nFIG. 8  shows the sensibility of the quality measure F while varying \nparameter \u03b1 that defines the size of the fixed \u2018corridor \u2019. This experiment shows \nthat for a value which is too small for \u03b1 ( \u03b1 < 4 ) the quality of the \napproximations is poor for all K. The \u2018 plateau\u2019  of the curves indicates that it is \nnot worth increasing the \u2018corridor \u2019 size too much: for \u03b1 >8  the quality curves \nsaturate for all K and no significant improvement is expectable. \n \n**** FIG. 8  around here **** \n \n \n 15 FIG.9  compares the experimental time complexity of the o ptimal solution \n(FSDP ), of the full search dynamic programming solution ( FSDP ), of the \nmultiresolution algorithm ( MR-PyCA ), of the Douglas-Peucker algorithm ( DP ) \nand of the Merge_L2  algorithm. The time complexity is measured as \u2018use r clock \nticks\u2019 spent as N increases while K remains fixed on a pentium 4 processor \nrunning Linux. The scale used in FIG. 9  is logarithmic, so that all curves are \nlinear with different slopes. The figure shows that  the Douglas-Peucker  \ncomplexity curve has the same slope as the MR-PyCA  complexity curve. As the \nDouglas-Peucker algorithm is known to be O(N) , these two algorithms have the \nsame linear complexity even though MR-PyCA is more expensive since the MR-\nPyCA  curve is above the DP  curve. DPFS  curve has a higher slope, and as such \nexhibit a polynomial complexity as expected ( FSDP is O(N 2)). The Merge_L2  \ncomplexity curve has a slope in between FSDP and Do uglas-Peucker curve as \nexpected ( Merge_L2  is known to be O(N.log(N) ). \n \n \n**** FIG.9 around here **** \n \n**** FIG.10 around here **** \n \nFIG. 10  shows the variations of the elapsed time for the MR-PyCA  algorithm \nwhile varying parameter \u03b1. Clearly the complexity is polynomial with \u03b1 meaning \nthat enlarging the size of the \u2018corridor \u2019 implies an important time cost increase. \nFor \u03b1<32 , the elapsed time remains small, and given that fo r \u03b1 > 16  (as shown \nin FIG. 9 ), we do not have real improvement on the F measure , an optimal value \nfor \u03b1 is located around 8. 16  \n \n \n \n**** FIG.11  around here **** \n \n \n \nIn FIG. 11  we evaluate the time complexity of the MR-PyCA  algorithm in \nfunction of \u03c1, for various values of N.  The experimentations shows that for all N \nvalues, the curves exhibit a minimum between \u03c1=.30  to \u03c1=.55 . This roughly \nconfirms the theoretical expectation (see eq. 11). The observed fluctuations are \ndue to the fact that for the last iteration we use a varying value of \u03c1 to reach the \nrequired end value for K.   \n \n \n5. Conclusion \n \nTo our knowledge the proposed multiresolution solut ion applied to the problem \nof simplifying a curve using polygonal approximatio ns is original. It consists in \niteratively applying a constrained dynamic programm ing search algorithm on \nsuccessive approximations of a polygonal curve. We have shown both \ntheoretically and practically that this algorithm h as a linear time complexity \n(O(N) ), whatever the chosen number of resolution levels.  This algorithm does \nnot provide a single approximation, but a family of  nested approximations from \nthe finest to the crudest approximating levels with  increasing distance between \nthe original curve and the successive approximation s. This algorithm is \nsuboptimal but maintains partial optimality between  each resolution levels. It \noffers good approximating solutions when real time and storage space are issues, 17 namely each time the optimal solution cannot be cal culated due to the size of N. \nFor all tests we have performed, the quality of the  resulting approximation is \nlargely better than the quality of well known heuri stic approaches (the Douglas-\nPeucker splitting approach or the merge approach): the gain on the quality \nmeasure F varies from 30%  to 50 %  according to the choice of the tuning \nparameters. The lowest quality measure that we have  obtained is above 79%  (for \nsmall \u03c1, \u03b1 and K values) while the best ones reach 100% (for large \u03c1, \u03b1 and K \nvalues). The experimental results give highlights f or the configuration of the \ntuning parameters of the algorithm, i.e. \u03c1, \u03b1 that could vary according to the \ntask. Furthermore, the multiresolution aspect of th e method allows managing \nsimultaneously various resolution levels, a functio nality that could be very \nuseful in time series information retrieval tasks.  \n 18 APPENDIX: Polygonal Curve Approximation using \nconstrained Dynamic Programming ( PyCA ) \n \n \n1.1 Problem formulation and \u201cfull search\u201d dynamic p rogramming \nsolution \n \nWe consider time series as a multivariate process X(t) =[x 1(t), x 2(t),\u2026, x p(t)]  \nwhere X(t)  is a time-stamped spatial vector in pR. In practice, we will deal with \na discrete sampled time series X(m)  where m is the time-stamp index \n({ }N m ,..., 1\u2208 ). Adopting a data modelling approach to handle the  adaptive \napproximation of the time series, we are basically trying to find an \napproximation \u03b8\u02c6X of X(m)  such as: \n( )\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\u2212 = = \u2211\nmmX mX ArgMin XXE ArgMin 2)( )( ),( \u02c6\n\u03b8\n\u03b8\u03b8\n\u03b8\u03b8    (3) \n \nwhere E is the RMS  error or Euclidian distance between X and the model \u03b8X. \n \nIn the case of polygonal curve approximation, we se lect the \nfamily { }{ } N mmX,..., 1)(\u2208 \u03b8 as the set of piecewise linear and continuous funct ions \n(successive segments have to be contiguous, so that  the end of a segment is the \nbeginning of the next one). Numerous methods have b een proposed for the \nproblem of approximating multidimensional curves us ing piecewise linear \nsimplification and dynamic programming in O(k N.N 2) time complexity (Perez et \nal. 1994). Some efficient algorithms (Goodrich, 199 4) (Agarwal, 2002)  with \ncomplexity O(Nlog(N))   have been proposed for planar curves, but none fo r the 19 general case in Rd. Here, we have constrained the search of the segme nts by \nimposing that the extremities of the piecewise line ar segments are vertices of \ntime series X(t) . Thus, \u03b8  is nothing but the set of discrete time location { mi} of \nthe segments\u2019 endpoints. Since the end of a segment  is the beginning of the \nfollowing one, two successive segments share a comm on mi at their interface. \nThe selection of the optimal set of parameters {}im\u02c6\u02c6=\u03b8  is performed using a \ndynamic programming algorithm (Bellman  1957, Perez  and Vidal 1994) as \nfollows:  \n \nGiven a value for \u03c1  and the size of the trajectory window to sample \nN={ }{ }n nmX,.., 1)(\u2208, the number K= {}im-1 of piecewise linear segments is known.  \n \nLet us define \u03b8(j) as the parameters of a piecewise approximation cont aining j \nsegments, and \u03b4(j,i)  as the minimal error between the best piecewise li near \napproximation containing j segments and covering the discrete time window \n{1,.., i}: \n \n\uf8fe\uf8fd\uf8fc\n\uf8f3\uf8f2\uf8f1\u2212 =\u2211\n=i\nmjjmX m X Min i j\n12\n)()()( )( ),(\u03b8\u03b8\u03b4  (5) \n \nAccording to the Bellman optimality principle (Bell man, 1957) Perez and Vidal \n(Perez-Vidal, 1997) decomposed \u03b4(j,i)  as follows: \n \n{ }), 1 ( ),( ),( m j imd Min i j\nim\u2212 + =\n\u2264\u03b4 \u03b4    \n\u2211\n=\u2212 =i\nmlim lXl X imd2\n, )( )(~),( where )( )). ( ) (()(~and , mXmimlmXiX l Xim +\u2212\u2212\u2212 =     (6) 20  \nis the linear segment between X(i)  and X(m) . \n \nThe initialization of the recursion is obtained giv en that: 0) 1 , 1 (= \u03b4 . \nThe end of the recursion gives the optimal piecewis e linear approximation, e.g. \nthe set of discrete time locations of the extremity  of the linear segments: \n \n\uf8f4\uf8fe\uf8f4\uf8fd\uf8fc\n\uf8f4\uf8f3\uf8f4\uf8f2\uf8f1\n\u2212 =\u2211\n=N\nmK\nKmX X ArgMin K\n12\n)(\n)()( )(\u02c6\n\u03b8\n\u03b8\u03b8    (7) \nwith the minimal error : \n\u2211\n=\u2212 =N\nmKmX m X nK\n12\n)(\u02c6 )( )( ),(\u03b8\u03b4  \n \nIt is shown in (Vidal and Perez, 1994) that the com plexity of the previous \nalgorithm that implements a \u201cFull Search\u201d (FS) is i n O(K.N 2), a complexity that \nprevents the use of such an algorithm for large N.  \n \n1.2  \u201cConstrained search\u201d dynamic programming solut ion : the \nPyCA  algorithm \n \nIn the scope of dynamic search algorithm the only w ay to reduce the time \ncomplexity is to reduce the search space itself. Sa koe and Shiba (Sakoe & Shiba, \n1978) have managed to reduce the complexity of the Dynamic time Warping \nalgorithm down to O(N)  while defining fixed constraints that define a \u2018co rridor \u2019 \ninside the search space. In the same mind-set, Kole snikov and Fr\u00e4nti \n(Kolesnikov & Fr\u00e4nti, 2003) have developed locally adaptive constraints 21 defining a varying width \u2018corridor\u2019 inside the sear ch space to solve the problem \nof approximating polygonal curves using dynamic pro gramming approach. Their \nalgorithm shows to have complexity O(W 2.N 2/K) , where W is the size of their \ncorridor.  Following Sakoe and Shiba works, we have  developed here a dynamic \nprogramming solution that implements a fixed size \u2018 corridor \u2019: the search space is \nthus reduced using two fixed constraints, as shown in FIG 12 . The first \nconstraint limits the search of the jth  segment upper extremity i around the mean \nvalue j.N/K  namely the limit of the jth  segment as to be chosen inside the \ninterval:  \n{ }\n{ } band KNjN Min j cband KNj Max jcj cjc\n+ =\u2212 =\n/., )(         ; /., 1 )(         :  where )[, ( ); ([\nsup inf sup inf \n (8) \nThe second constraint limits the search of the of t he jth  segment lower extremity \nm in the interval { } ). () (    where [, ; ) ( , 1 [ band i iLb iiLb Max \u2212=  \n \n \n**** FIG12 Around here**** \n \nThus, the first constraint defines search bounds fo r the upper extremity of the jth  \nsegment (the i index) while the second constraint defines a searc h bound for the \nlower limit of the jth  segment { }[ ; , 1 [max i band i m \u2212 \u2208 , where band  is fixed by the \nuser. The recursive equations for the PyCA  algorithm are then: \n{ }\n{ }\n{ } band KNjN Min j cband KNj Max jcj cjcim j jmd Min i j\nimilb \n+ =\u2212 =\u2208\u2212 + =\n\u2264\u2264\n/., )(         ; /., 1 )(          )[, ( ); ([ with ), 1 ( ),( ) ,(\nsup inf sup inf )(\u03b4 \u03b4\n  (9) 22 (10) )) (()( ) ()())). (( )) ( (()(~ where  )( )(~),( :  with and \n)(), ()(\n)(2\n)(), (\nmTY XmTY iTY mTY lmTY X iTY X l XlXl X imd\niTY mTY iTY \nmTY liTY mTY \n+\u2212\u2212\u2212 =\u2212 =\u2211\n=  \nwith )(jTY giving the time stamps correspondence of the jth  segment \nextremity of the nested approximation Yof X in the original time series \nX such that: )) (~( )( jXTXjY = . TY  is required for the multiresolution \nalgorithm MR-PyCA  that iteratively merges segments from the \nprevious polygonal approximation to provide the nex t approximation. \nIndeed, for a direct use of PyCA, TY  should be set to the identity \nrelation such that TY(j)=j . \n \nThe initialization of the recursion is still obtain ed observing that: 0) 1 , 1 (= \u03b4  and \nthe end of the recursion gives the suboptimal  (it is optimal on the constrained \nsearch space) piecewise linear approximation, e.g. the set of discrete time \nlocations of the extremities of the linear segments : \n{ }\n{ } [; , 1 [ where ), 1 ( ),( ) ,(\n)(\nN band N Max ij K Kmd Min iK\nimilb \n\u2212 \u2208\u2212 + =\n\u2264\u2264\u03b4 \u03b4\n (11) \n \nThe pseudo code of the PyCa  algorithm is presented in FIG. 13. \n \n \n \n \n**** FIG 13. Around here **** \n \n \n 23 1.3 Complexity of PyCA   \n \nAccording to the previous notations, the time compl exity for the PyCA  algorithm \nevaluates to: \n \nK band N . . 22=C  (12) \n \nIf we choose KN\nKNround band \u03b1 \u03b1 \u2264 = ) (  where \u03b1 is a constant then \nKN\nKKN\nN2 2 2 2.. 2 ) 2 / .( . 2 \u03b1 \u03b1\u2264 \u2264\u2212C  showing the time complexity of PyCA  \nalgorithm is O(N 2). \n \nThe size of the search space for the PyCA  algorithm is included into a \nK.(2.band)  matrix. For ) (KNround band \u03b1 = , the size of this matrix is upper \nbounded by N.. 2\u03b1showing the space complexity of PyCA  is O(N) . 24"}
{"category": "abstract", "text": ""}
{"category": "non-abstract", "text": "()()()()gPfPg|f P f|g P =  (1) \nwhere ( ) \u220f\uf8f7\uf8f8\uf8f6\uf8ec\uf8ed\uf8eb\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n\u2212 \u2297\n\u2212 =\njijigjihf\ng|f P\n,222\n, ,) (\nexp \n21\n\u03c3 \u03c3\u03c0 (2) \nand ( ) \u220f\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\u039b\u2212\n=\nji Tji\nTfP\n,22,2\nexp 1exp  (3) \nWhere T is referred to as the temperature and \u039b is a gra- \ndient operator. The quadratic variation operator (f or more \ninformation see Wang et al. [1]) is used as the gra dient \noperator in this particular algorithm but it is lik ely that more \nsuitable Gradient Operators are available for vario us types \nof NM images. The aim is to seek an estimate of f referred \nto as f*  which will maximize the posterior conditional prob - \nability [4].  \nThe objective function for minimization is derived by \nsubstituting Equations 2 & 3 into 1, taking the nat ural loga- \nrithm, changing the sign and ignoring the constant term. \nThe objective function is the Hamiltonian and becom es a \nfunction of the image estimate f* : \n ( ) ( ) \u2211\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb\n\uf8f7\uf8f8\uf8f6\uf8ec\uf8ed\uf8eb\u2211\u039b\u2212\n\u2212 + \u2212 \u2297 =\nji Tji\nT jigjih f\njifH\n,22,2\nexp 12\n, ,*\n,221* \u03b2\n\u03c3 (4) \nEquation 4 ensures that the restored image has the maxi- \nmum probability of being in the form of f while still mini- \nmizing the noise in f*  by penalizing and hence smoothing it. \nThe coefficient \u03b2 in front the prior Hamiltonian term is the \nratio between penalizing noise and maintaining form . The \nprior term, also referred to as the penalty functio n, takes the form of an inverted Gaussian function. As the tempe rature T \ndecreases, penalty increases as can be in seen Fig.  1. \n \n \nFig. 1 Depiction of the inverted Gaussian as the pen alty function. \nAfter the objective function has been derived, grad ient \ndescent is utilized for the minimization process.  \n \njik\nT\njik\njiff Hf f\n,,1\n,)(\n\u2202\u2202\u2212 =+\u03b1 (5) \nDetailed discussion for the determination of value of \u03b1 \nand the partial derivative required in gradient des cent may \nbe found in Bilbro et al. [5]. An outline of the MF A iterative \ntechnique can be seen in Fig. 2. \n \n \nFig. 2 Flow chart depicting overview of MFA algorith m. \nB.  Discussion \nA vital aspect of MFA is defining when the annealin g \nprocess is complete. Excessive annealing will add s ignifi- \ncant blur to the image. Normally when dealing with phan- \ntom images error metrics such as PSNR or RMSE may b e used since the algorithm may constantly compare the  re- \nstored image with the real image. However, when dea ling \nwith NM images this comparison is can not be made. The \neasiest solution would be to provide NM physicians with a \n\u201cmovie\u201d of the restoration process and allow the NM  physi- \ncian to view the iterated image of choice. A more m athe- \nmatical approach at achieving the correct stopping criteria is \nsuggested by using the noise and prior Hamiltonians  as \nenhancement indicators.  \n \n \nFig. 3 Comparison between MFA and Wiener phantom res ults. \nPhantom images were used extensively in the develop - \nment of a MFA algorithm and MFA parameters. Experi-\nmental empirical methods were used on numerous phan tom \nimages such as Fig. 3 to determine optimal paramete rs.  \nIt is evident from Fig. 3A & C that the MFA algorit hm \nwith the correct parameters can reduce noise substa ntially \nwithout damaging edge integrity. Fig. 3B shows a Wi ener \nfilter restored image. Comparative noise reduction and edge classification is evident from Fig. 3E & F, which d isplays \nthe Sobel edges.  \nLooking carefully at Fig. 3A, B & C, it is noticeab le that \nedges appear sharper in the original image and Wien er im- \nage in certain regions compared to the MFA restored  image. \nThis implies that MFA has blurred the image slightl y. How- \never since MFA has extensively reduced the noise it  is now \npossible to apply filters to further enhance image edges \nwithout out amplifying the noise. A standard sharpe ning \nfilter h is used and the result is visible in Fig. 3D. \n \n\uf8fa\uf8fa\uf8fa\n\uf8fb\uf8f9\n\uf8ef\uf8ef\uf8ef\n\uf8f0\uf8ee\n\u2212 \u2212 \u2212\u2212 \u2212\u2212 \u2212 \u2212\n=\n167 . 0 67 . 0 167 . 067 . 0 33 . 4 67 . 0167 . 0 67 . 0 167 . 0\nh (6) \nThis post-sharpening affect is more evident in the clinical \nresults (see Section III) and highlights the streng ths of \nMFA, which can be categorized as a supplementary pr e-\nfilter image enhancing technique.  \nOne of the disadvantages in NM as shown Fig. 4 is t hat \nblurring increases with increasing distance from th e colli- \nmator i.e. it is not depth invariant. This implies that the PSF \nwill only be correctly specified for a single plane  in the \nimage. The other planes will experience a blurring affect \ndue to the MFA process and an incorrectly specified  PSF. \nNM physicians can therefore select planes of interest  by \nmodifying the Gaussian distribution to define the P SF used \nin MFA. The planes of interest  will experience image en- \nhancement while the other planes may experience inc reased \nblur.  \n \n \nFig. 4 Point and line sources at 10, 15, 20, 25cm. \nTo determine the PSF, point sources were placed at vari- \nous distances away from the collimator. A discrete Gaussian \ndistribution was then fitted to the acquired point source. \nVertical and horizontal line sources were imaged us ing \ncapillary tubes to verify the point sources\u2019 distri butions and \nto verify the radial symmetry of the blur. Fig. 5 s hows how \nthe point source is convolved with a line and then compared to the acquired line source. Ignoring ends, the two  lines \nsuffered only small differences with an RMSE of 5.5 % that \nmay be attributed to the noise. The process is repe ated with \nthe vertical line resulting in a RMSE of 4.8% which  implies \napproximate radial symmetry. Radial symmetry and th e \nfitted Gaussian PSF were verified at numerous dista nces. \nFig. 6 shows the Standard Deviation of the resultin g fitted \nPSFs, in which the PSFs display regional linearity.  A linear \ntrend line may be fitted and used to predict approx imate \nPSFs at different distances from the collimator. \n \n \nFig. 5 Comparison of convolved line and acquired lin e source. \ny = 0.0583x + 1.075 \nR2 = 0.9941 \n1.3 1.5 1.7 1.9 2.1 2.3 2.5 2.7 \n5 10 15 20 25 30 \nDistance from Collimator Standard Deviation of Gaussian Fitted \nPSF \n \nFig. 6 Standard deviation of PSF measurements. \nTo simplify this work, PSF shift invariance is assu med, \neven though this is not strictly the case. A more c omplex \nalgorithm may be derived to include a shift variant  PSF.  \nIII . R ESULTS  \nIt was determined experimentally that MFA parameter s \nfor NM images are similar to those used in Fig. 3C although \nthey are not necessarily the optimal parameters). I n this \nstudy the distance of the subject from the collimat or is un- \nknown, so a PSF of a standard deviation of 2 is cho sen \n(corresponding to \u00b117cm from the collimator see Fig . 6). \nThe noise variance is determined using a flood sour ce and \ndoes not change with changing collimator distance. Note that noise variance does change with image intensit y scaling \nand this change must be factored in when applying M FA.  \n \nFig. 7 Example of clinical results. \nFig. 7A shows an image acquired on General Electric  In- \nfinia gamma camera. Fig. 7C shows the restored imag e after \n20 MFA iterations, the image appears to be slightly  blurred \nwith substantial reduction in noise. Fig. 7E shows a Wiener \nrestored image which looks very similar to the MFA re- \nstored image. The images are all optimally sharpene d using \nEquation 6 as the sharpening filter. The sharpening  filter \namplifies the noise in the original image as seen i n Fig. 7B. \nThe sharpening filter is run three times on the MFA  restored \nimage with image enhancement before noise amplifica tion \nbecomes apparent (Fig. 7D). The result is a clearer  and \nsharper image which may improve diagnosis. In contr ast the \nsharpening filter can only be run twice before nois e amplifi- \ncation becomes visually obstructive in the Wiener r estored image seen in Fig. 7F. Fig. 7D clearly contains mor e view- \nable detail then Fig. 7F.  \nIV. C ONCLUSIONS  \nWith current processing technology the computationa l \ntime required to run this image enhancing MFA algor ithm is \nno longer significant. Although not all the criteri a for image \nenhancement are present in NM images, enhancement o f \nindividual or multiple planes of interest  is possible. Sharp- \nening filters are utilized as a post-MFA enhancemen t tech- \nnique and provide good results. We thus conlude tha t MFA \nholds promise as a supplementary pre-filter diagnos tic tool \nfor the enhancement of NM images. \nACKNOWLEDGMENT \nThe authors would like to thank Prof. Vangu of the De- \npartment of Nuclear Medicine at Wits University for  pro- \nviding the research facilities required in this stu dy. In par- \nticular, the authors would like to thank Mr. Sibusi so Jozela \nof the Medical Physics Department, for all his time  spent \nacquiring the experimental data, and Mr. Nico van d er \nMerwe, also of the Medical Physics department for a ll his \ninput.  We look forward to working with these depar tments \nto further develop this study.  \nREFERENCES  \n1.  L. Small C. Wang W. Snyder R. Williams. Edge detect ion in gated \ncardiac nuclear medicine images, 7th annual IEEE sy mposium on \ncomputer-based medical systems. IEEE Transactions o n Image Proc- \nessing, 1994. \n2.  G. Wang, N. Ansari. Searching for optimal frame pat terns in an \nintegrated TDMA communication system using mean fie ld annealing. \nIEEE Trans. Neural Networks, 9(6):1292\u20131300, 1998. \n3.  A. C. Bovik S. T. Acton. Anisotropic edge detection  using mean field \nannealing. IEEE International Conference on Acousti cs, Speech, and \nSignal Processing, 2:393\u2013396, 1992. \n4.  W. Snyder Y. Han G. Bilbro R. Whitaker S. Pizer. Im age relaxation: \nRestoration and feature extraction, IEEE Trans. on Patt. Anal. Ma- \nchine Intel., 17(6), 1995. \n5.  G. L. Bilbro W. E. Snyder S. J. Garnier J. W. Gault . Mean field \nannealing: a formalism for constructing gnc-like al gorithms. IEEE \nTrans. Neural Networks, 3(1):131\u2013138, 1992. \nAddress of the corresponding author: \nAuthor: Daniyel Falk \nInstitute: University of the Witwatersrand, Johanne sburg \nStreet:  \nCity: Johannesburg \nCountry: South Africa \nEmail: daniyelfalk@gmail.com"}
{"category": "abstract", "text": "\u2014 Nuclear medicine (NM) images inherently suf- \nfer from large amounts of noise and blur. The purpo se of this \nresearch is to reduce the noise and blur while main taining \nimage integrity for improved diagnosis. The propose d solution \nis to increase image quality after the standard pre - and post-\nprocessing undertaken by a gamma camera system. Mea n \nField Annealing (MFA) is the image processing techn ique used \nin this research. It is a computational iterative t echnique that \nmakes use of the Point Spread Function (PSF) and th e noise \nassociated with the NM image. MFA is applied to NM images \nwith the objective of reducing noise while not comp romising \nedge integrity. Using a sharpening filter as a post -processing \ntechnique (after MFA) yields image enhancement of p lanar \nNM images.  \nKeywords \u2014 Mean Field Annealing, Nuclear Medicine, \nImage Restoration \nI.  INTRODUCTION  \nThe MFA algorithm makes use of two techniques to \nachieve image restoration. Gradient descent is used  as the \nminimization technique, while optimization is achie ved by a \ndeterministic approximation to Simulated Annealing (SA) \n[1]. The algorithm anisotropically  diffuses  an image, itera- \ntively smoothing regions that are considered non-ed ges \nwhile still preserving edge integrity until a globa l minimum \nis obtained. A known advantage of MFA is that it is  able to \nminimize to this global minimum skipping over local  min- \nima while providing comparable results to SA with s ignifi- \ncantly less computational effort [2]. \nImage blur is measured using either a point or line  \nsource. Both allow for the derivation of a PSF whic h is used \nin the image de-blurring process. The noise varianc e can be \nmeasured using a flood source. Noisy blurred NM ima ges \ncan be difficult to diagnose particularly at edges,  for this \nreason MFA is suitable for planar NM image restorat ion. \nPlanar NM images are assumed to be piece-wise con- \ntinuous images and thereby produce Markovian neighb or- \nhoods and allows the representation of images as Ma rkov \nRandom Fields [3].  \n II.  MFA APPLIED TO NUCLEAR MEDICINE IMAGES \nA.  Theory \nA Bayesian approach is used to define the objective  func- \ntion. Consider an ideal image f and a measured image g; the \na-posteriori conditional probability is given by"}
{"category": "non-abstract", "text": "image registration, registration criteria, information theory, ent ropy, mutual\ninformation, piecewise rigid, prior knowledge, dentistry, cephalome try, implants, digital\nsubtraction radiography.\n12 W. JACQUET & P. DE GROEN\n1.Introduction\nIn a wide variety of clinical applications subtraction of well-aligned (or well-registered)\nimages is a standard tool to monitor an evolution [11]. We study in this p aper the popular\nregistration criterion Mutual Information (MI) [20], which is based o n the joint entropy of\nimages. Shannon [18] introduced this type of entropy in the contex t of a communication\nchannel and Collignon [3] applied Shannon\u2019s model to MI-registration . On the one hand,\nmodeling image registration using Mutual Information (MI) as a func tional on a \u201ccommu-\nnication channel\u201d imposes unwanted restrictions in the context of im age registration, and\non the other hand, the basic mutual information is not rich enough: spatial information is\nlost, and it does not incorporate prior knowledge. In traditional Mu tual Information [20]\nthe gray value combinations of all pixels in the overlap of reference im age and \ufb02oating test\nimage are equally weighed. In a probabilistic context this means that e qual probability is\nattributed to each pixel in the overlap. In [8] we studied weighing and introduced Gauss\nFocussed Mutual Information (GFMI). Here we place this in a proba bilistic context, and in-\ntroduce mutual information with respect to a more general non-u niform distribution. Prior\nknowledge related to a speci\ufb01c registration problem is translated int o a sampling distribu-\ntion emphasizing the contribution in the neighborhood of structure s the practitioner wants\nto align or structures that might contribute to the alignment, and r educe the contribution\nof regions unimportant to the practitioner or unimportant to the a lignment. When moni-\ntoring implants or dental restorations, an obvious element of prior knowledge is the radio\nopacity of these implants and restorations. When 2D/3D images of a hollow longitudinal\nbone structure are to be aligned it is natural to use edge detection to model the geometry\nof the bone. This geometry can be brought into the alignment proce ss as prior knowledge\nthrough the sampling distribution. In dental bitewing images used as a means to assess\nthe evolution of local phenomena, such as monitoring (small) dental lesions [8], it is the\npractitioner who has to provide the information about which struct ure (tooth) that has to\nbe aligned. In Jacquet et al. [8] Gaussian Focussed Mutual Inform ation (GFMI) is com-\npared to a Region Of Interest (ROI) approach. It was shown that the former approach in\ncombination with registration based on a\ufb03ne transformations is par ticularly well suited to\nalign rigid parts when the context of the underlying structure is rele vant to the alignment.\nWhen ROI is used without prior segmentation the resulting MI (and th erefore also the\nquality of the registration) is highly dependent on the amount of bac kground contained in\nthe ROI.\nIn Section 2 image registration, the alignment of images, is formally de \ufb01ned. Intrinsic\nregistration methods are introduced in Section 3, joint entropy of images in Section 4. In-\nformation theory [18] is brie\ufb02y presented in Section 5. In Section 6 m utual information\nbased registration is placed in this information theoretical context , and extended to incor-\nporate prior knowledge. In Section 7 we use this extension to develo p new methodologies\nto successfully address speci\ufb01c registration problems, the follow- up of dental restorations,\ncephalometry, and the monitoring of mandibular implants. The same id eas can be used\nfor registration of 3D images; currently we are developing softwar e and test strategies for\nhip-, knee-, and shoulder implants. We do not address issues of med ical interpretation and\ndiagnosis.IMAGE REGISTRATION 3\n2.Image registration\nIn practice, a gray scale image is a rectangular array of pixels and a f unctionuassigning\none ofKgray value bins {1,\u00b7\u00b7\u00b7, K}to each pixel. It can be considered as the discretiza-\ntion of a continuous function on a subset Aof IR2, with values in the interval [0 ,1]. Letu\nandvbe two (continuous) images with domains AandB\u2282IR2respectively. Let Abe a\nclass of smooth bijective mappings from IR2to IR2. The idea of image registration is to \ufb01nd\na transformation T\u2208 Asuch that image uand the transformed image vT:=v\u25e6T\u22121are as\nsimilar as possible. Given a measure Iof similarity between images, optimal registration\ncan be de\ufb01ned as the problem to \ufb01nd\narg max\nT\u2208AI(u,vT),\nthe domain-transformations maximizing this measure. In this settin g we will refer to uas\nthe reference image, vas the test image, and vTthe \ufb02oating image. Di\ufb00erent classes of\ntransformations are required depending on the registration applic ations.\n3.Intrinsic registration methods\nIn Maintz and Viergever [12] a classi\ufb01cation of registration method s is introduced. They\ncall a method \u201cintrinsic\u201d when it relies only on patient generated image content, and \u201cex-\ntrinsic\u201d when objects foreign to the patient are introduced into th e scene of which an\nimage is taken to serve as reference to the alignment process. The intrinsic methods are\nsplit into landmark based, segmentation based, and voxel/pixel pro perty based registration\nmethods. In landmark based and segmentation based registration corresponding structures\nare indicated or extracted from reference and test image, to be u sed pairwise as input for\nthe alignment procedure. A voxel/pixel property based registrat ion criterion is a criterion\ndirectly linked to the discrete two-dimensional gray value maps (3.1) .\nTo be precise, in the process of registration we consider two discre te images, denoted by\nuandv, and transformations T, and the two-dimensional gray value maps:\n/tildewidegT(m,n) := (/hatwideu(T(m,n)),v(m,n)) , (3.1)\nwhere/hatwideuis the gray value bin of an interpolation of the known values of uatT(m,n)\nwithinA(Fig. 1). Several interpolating techniques can be applied. We will ado pt bilinear\ninterpolation.\n4.Joint entropy of image pairs\nConsider the function that counts the number of times a gray value combination occurs\nin the intersection of the reference image with the transformed te st image:\nCT(k,\u2113) :=\u266f{(m,n)|T(m,n)\u2208A\u2227(k,\u2113) = (/hatwideu(T(m,n)),v(m,n))}(4.1)\nIf test and reference image are equal and well aligned with respect to each other, the\nfrequency of unequal pairs of pixel values or o\ufb00-diagonal element s is zero. If the well-\naligned images slightly di\ufb00er, o\ufb00-diagonal elements may become non-z ero but are expected\nto be small. This suggests that a measure based on the function CTcan be the basis for\naligning images. We divide CTby the total number of pixels in the intersection of reference\nand transformed test image to eliminate the dependence on the imag e size. The result is\na two-dimensional probability distribution, called the joint probability of the images. Let4 W. JACQUET & P. DE GROEN\nT(m,n)\u2208AA\nFigure 1. Mapping of the test image pixels on the reference image through\nstransform Tcf. [8].\npk\u2113denote the probability linked to the combination of gray value classes kandl, and let\npk\u2022andp\u2022\u2113denote its marginal distributions,\npk\u2113:=CT(k,\u2113)P\nijCT(i,j)joint probability,\npk\u2022:=/summationtext\nlpk\u2113marginal linked to the reference image,\np\u2022\u2113:=/summationtext\nkpk\u2113 marginal linked to the test image.(4.2)\nAlthough pk\u2113has all formal aspects of a probability, it is merely a relative frequen cy not\nlinked to a stochastic experiment at present.\nShannon Entropy. A \ufb01nite probability model is characterized by a set of nelements\n(outcomes) and their probabilities p1,p2,...,p n. Shannon de\ufb01nes a measure of uncertainty\nHon the class of all \ufb01nite probability models, {(p1,p2,...,p n)|n\u2208Nand/summationtextpi= 1},\nsatisfying:\nH(p1,p2,\u00b7\u00b7\u00b7,pn) is a non-negative continuous function of the pi,\nHn:=H(1/n,1/n,\u00b7\u00b7\u00b7,1/n) is an increasing function of n,\nH(p1,...,p n\u22121,pn) =H(p1,\u00b7\u00b7\u00b7,pn\u22121+pn)\n+(pn\u22121+pn)H(pn\u22121\npn\u22121+pn,pn\npn\u22121+pn) .(4.3)\nImplicitly, but clear from his application:\nH(p1,p2,...,p n) =H(p\u03c3(1),p\u03c3(2),...,p \u03c3(n)) , (4.4)\nwith\u03c3a permutation of the indices 1 \u00b7\u00b7\u00b7n.\nTheorem 1. IfHsatis\ufb01es the above requirements then:\nH(p1,p2,...,p n) =\u2212Kn/summationdisplay\ni=1pilnpi, (4.5)\nwhereKis a positive constant.\nThe proof of this theorem can be found in [18]. For K= 1 isHreferred to as the\nShannon entropy. The Shannon entropy measures uncertainty a bout a precise outcome of\nan experiment linked to a probability distribution.IMAGE REGISTRATION 5\nInformation\nsource Transmitter Receiver Destination\nMessageSignal Received\nsignal\nMessage\nNoise\nsource\nFigure 2. Schematic diagram of a general communication system [18]\nJoint entropy. The joint entropy H(u,vT) of two images uandvTis the Shannon entropy\nof the joint probability {pk\u2113}of the images. The entropy H(u) of the reference image is\nthe Shannon entropy of {pk\u2022}, the entropy H(vT) is the Shannon entropy of {p\u2022\u2113}.\nWhen two images are well aligned but intensities of the same structur e di\ufb00er, the proba-\nbilitieswillstillconcentrateandtheShannonentropywillnotbea\ufb00ec tedsinceitisinvariant\nunder permutation of elements \u2013 see (4.4). Therefore a similarity me asure based on the\nentropy can be applied when di\ufb00erent image acquisition modalities are u sed. This is in\naddition to the absence of preprocessing, a reason why Entropy b ased similarity measures\nare popular in multi-modal registration applications at present.\n5.Information theory applied to image registration\nThe mathematical theory of communication developed by Shannon [ 18] is based on a\ngeneral communication system described by Fig. 2. Shannon attem pts to build an indica-\ntor for the quality of the combination transmitter, channel and re ceiver. A communication\nsystem should be designed to enable passing all acceptable message s generated by the in-\nformation source. When a language such as English is used by the info rmation source as\ncodingsystem, notallchoices ofacceptablemessages areequally lik ely tobegenerated. E.g.\nthe occurrence of the sequence of symbols \u201cMy name is\u201d is more likely than the sequence\n\u201cPrescience is\u201d. Shannon introduces an arti\ufb01cial information sour ce consisting of a Markov\nprocess as a model for the English language. The introduction of th e surrogate source\nallows a study of the quality and limitations of a communication system w ith probabilistic\nmethods. The English language is not the most e\ufb03cient coding system . To study a com-\nmunication system in combination with an English information source, it is important to\nde\ufb01ne a measure of information content of a phrase or of its comple ment \u2013 the redundancy.\nThe Shannon Entropy His build to measure how uncertain we are about a speci\ufb01c phrase\n(outcome), or the complement, how probable the speci\ufb01c phrase is when we suppose it has\nbeen generated by a Markov process modeling the English language.\nLet us try to understand the requirements that de\ufb01ne H. The \ufb01rst requirement is conti-\nnuity: there is no clear reason to introduce \u201cjumps\u201d. The continuit y requirement does not\nseem to be too restrictive. The second requirement states that if the number of possible\noutcomes increases, and if all outcomes are equally probable, the u ncertainty about the6 W. JACQUET & P. DE GROEN\nrealization of one particular outcome increases. The last requireme nt can be understood\nfrom modeling of a language at di\ufb00erent levels. E.g. modeling the langua ge at the level of\ncharacters should be compatible with, but less distinctive than, mod eling the language at\nthe level of words. To be precise: when a model is re\ufb01ned, the entr opy should grow. If a\nsystem is split into two sub-systems, the increment of the entropy is equal to the probability\nof the systems multiplied by the entropy within the system.\nCollignon [3] applied the model of a communication channel to image reg istration under\nthe following assumptions:\n\u2022Consider the test image to be the transmitted signal.\n\u2022Take the reference image to be the received signal.\n\u2022The communication channel is determined by the registration param eters.\n\u2022Optimizing the mutual information between the signals is equivalent to the design\nof an optimal communication channel.\n\u2022Both images are assumed to represent the same scene, and their m ulti-modal dif-\nferences are considered a noise generated by the communication c hannel.\nInspecting the construction of the joint probability of two images in more detail, one can\nsee that it is induced by the gray value mapping /tildewidegTtransforming the uniform distribution\non the pixels T(m,n)\u2208Afrom the test image into an induced probability on [1 ,\u00b7\u00b7\u00b7, K]\u00d7\n[1,\u00b7\u00b7\u00b7, K]. Therefore a process generating the joint probability consists of the sequential\nuniform random pick of nodes T(m,n) located within the domain of the reference image\nAand the evaluation of /hatwideu(T(m,n)) andv(m,n). The signal becomes the evaluations\nut:=/hatwideu(T(mt,nt)), the channel is de\ufb01ned through the conditional probabilities pkut/p\u2022ut\nwherek= 1,\u00b7\u00b7\u00b7,K. The origin of the dispersion of these conditional probabilities is\ndetermined by the modalities used to acquire the images, the instrum ent settings, and the\nquality of the alignment. The joint probability is only a \ufb01rst order mode l in the sense that\nall information concerning the location of the gray value couples in bo th images is ignored.\nThis is comparable to the generation of a sequence of letters and sp aces with the frequency\nof occurrence in the English language and its use as a model for an En glish phrase. Spatial\nrelation of pixels corresponds to the sequential order of the char acters in a sentence. This\nspatial relation is discarted altogether in MI.\nAlthough the e\ufb00ort to translate the image registration problem into a problem about\nmodeling a communication channel is appealing from a theoretical valid ation viewpoint it\nlimits the search of new criteria, and it directs the search towards t he statistical estimation\nof probability densities naturally emerging from an unknown \u201ccommun ication system\u201d.\nThe third condition of (4.3) does not seem mandatory in a image modelin g context.\nTherefore, the substitution of e.g. the logarithm by a square root or any other convex func-\ntion in (4.5) is an interesting line of thought, in\ufb02uencing the robustne ss of the criterion.\nHughes and Daubechies [4] propose simpler alternative metrics base d on the joint proba-\nbility of overlapping images. The introduction of the spatial relation c an be achieved by\ntaking into account neighboring gray valuecouples. This approach h asbeen explored a.o. in\n[17]; in [14] and [5] gradient information is calculated and incorporate d into the functional,\nand in [15] gray value di\ufb00erences of neighboring pixels/voxels are use d. The introduction\nof spatial information in MI can also be achieved through blurring of t he images before\nregistration. However, this blurring may cause loss of a signi\ufb01cant a mount of information.\nAnother line of thought, apart from spatial considerations and ro bustness, is to incor-\nporate prior knowledge about scene and application by replacing the uniform sampling byIMAGE REGISTRATION 7\na sampling according to a suitable distribution. A higher probability can be attributed to\nregions required to alignparticularly well, or to structures relevant to the alignment. Inthis\npaper we shall consider non-uniform distributions, and introduce d i\ufb00erent methodologies to\nincorporate prior knowledge into the sampling distribution tailored to speci\ufb01c applications.\nAnother issue in registration based on pixel/voxel based criteria is t he image overlap\n[19]. Size and content of the overlap may change considerably during the registration.\nWhen dealing with images with very high aspect ratio even \u201dsmall\u201d trans formations can\nhave an important in\ufb02uence on the overlap. To reduce the sensitivit y of MI to these\nchanges in overlap statistics, and to minimize the resulting misalignmen ts, the Normalized\nMutual Information Y[19] and the Entropy Correlation Coe\ufb03cient (ECC) [10] have been\nintroduced. These criteria have shown a better robustness to ch anges in overlap statistics\nthan MI does. Therefore, we will adapt Yto non-uniform distributions and use it in our\napplications. We will not use an analogous adaptation of the ECC, sinc e ECC is directly\nrelated to Y.\n6.(Normalized) Focussed Mutual Information with respect to a d ensity\nShannon [18] extends the entropy of a \ufb01nite probability distribution to the entropy of a\nprobability distribution with density fon a domain \u2126:\nH(f) =\u2212/integraldisplay\n\u2126f log(f) . (6.6)\nLetuandwbe continuous images with intersecting domains AandB,fa probability\ndensity function on A\u2229Bandgthe function assigning to each point \u03c9in the intersection\nA\u2229Bthe couple of gray values g(\u03c9) := (u(\u03c9),w(\u03c9)). Denote by fgthe probability density\nfunction on [0 ,1]\u00d7[0,1], generated through g, and denote by fuandfwthe probability\ndensity functions on [0 ,1] generated by uandw, respectively. Note that fuandfware the\nmarginal distributions of fgwith respect to yandxrespectively. We de\ufb01ne\n\u2022Focussed Mutual Information of the images uandwwith respect to a density fon\nthe overlap of the images A\u2229Bas follows:\nMIf(u,w) :=H(fu) +H(fw)\u2212H(fg), (6.7)\n\u2022Normalized Focussed Mutual Information of the images uandwwith respect to a\ndensityfon the overlap of the images A\u2229Bas follows:\nYf(u,w) :=H(fu)+H(fw)\nH(fg), (6.8)\n\u2022Focussed Entropy Correlation Coe\ufb03cient of the images uandwwith respect to a\ndensityfon the overlap of the images A\u2229Bas follows:\nECCf(u,w) :=2MIf(u,w)\nH(fu)+H(fw). (6.9)\nThese generalizations return to the original concepts if fis chosen as the uniform distri-\nbution, i.e. if there is no focussing. So Focussed Mutual Informatio n see Eq. (6.7) returns\nto MI introduced by Collignon [3] and Maes [10], Normalized Focussed Mu tual Information\nsee Eq. (6.8) returns to Yintroduced by Studholme [19], and and the Focussed Entropy\nCorrelation Coe\ufb03cient see Eq. (6.9) returns to ECC of Maes [10].\nThe Focussed Mutual Information with respect to a density funct ionfis an extension of\nthe Gauss Focussed Mutual Information introduced by Jacquet e t al. [8]. More precisely,8 W. JACQUET & P. DE GROEN\ngiven two continuous images uandwwith domains AandBrespectively, and f=/summationtextaifi\nthe convex combination of a \ufb01nite number of normal density functio nsfi, normalized to\nbe a probability density function on A\u2229B, thenMIf(v,w) is the Gauss Focussed Mutual\nInformation.\nApproximation of MIf,Yf, andECCf.We consider two discrete images, denoted by\nuandv. Let (k,\u2113) be a gray value combination. Denote by D(k,\u2113) the set of all pixel pairs\nin the intersection that have (approximately) the gray value combin ation (k,\u2113),\nD(k,\u2113) :={(m,n)|T(m,n)\u2208A\u2227(k,\u2113) = (/hatwideu(T(m,n)),v(m,n))}.\nAs before in (4.1), we de\ufb01ne CT(k,\u2113) as the number of elements of D(k,\u2113) in which each\nelement contributes equally (or is equally weighed). We introduce WT(k,\u2113) as a weighed\nsum in which the contribution of each pixel pair is proportional to the focus probability\nassigned to its location, and we normalize those quantities into proba bilities:\nCT(k,\u2113) :=/summationdisplay\n(m,n)\u2208D(k,\u2113)1 ,WT(k,\u2113) :=/summationdisplay\n(m,n)\u2208D(k,\u2113)f(m,n) ,\npk\u2113:=CT(k,\u2113)/summationtext\nijCT(i,j),\u03c0k\u2113:=WT(k,\u2113)/summationtext\nijWT(i,j),\npk\u2022:=/summationdisplay\nlpk\u2113,\u03c0k\u2022:=/summationdisplay\nl\u03c0k\u2113,\np\u2022\u2113:=/summationdisplay\nkpk\u2113,\u03c0\u2022\u2113:=/summationdisplay\nk\u03c0k\u2113.(6.10)\nHerepk\u2113is the joint probability of the images, see Eq. (4.2), \u03c0k\u2113the focussed joint proba-\nbility of the images, \u03c0k\u2022the focussed marginal linked to the reference image, and \u03c0\u2022\u2113the\nfocussed marginal linked to the test image. The corresponding (\u201cf ocussed\u201d) entropies are:\n/hatwideHf(u,vT) =\u2212/summationtext\nk\u2113\u03c0k\u2113log(\u03c0k\u2113),\n/hatwideHf(u) = \u2212/summationtext\nk\u03c0k\u2022log(\u03c0k\u2022),\n/hatwideHf(vT) = \u2212/summationtext\n\u2113\u03c0\u2022\u2113log(\u03c0\u2022\u2113).(6.11)\nWe introduce approximations of MIf(u,vT),Yf(u,vT) andECCf(u,vT):\n/hatwidestMIf(u,vT) = /hatwideHf(u)+/hatwideHf(vT)\u2212/hatwideHf(u,vT),\n/hatwideYf(u,vT) =/hatwideHf(u)+/hatwideHf(vT)\n/hatwideHf(u,vT),\n/hatwideECCf(u,vT) =2/hatwidestMIf(u,vT)\n/hatwideHf(u)+/hatwideHf(vT).(6.12)\n7.Applications\nIn this section, we will introduce methodologies involving FMI and Digita l Subtraction\nRadiography (DSR), tailored to speci\ufb01c clinical applications. Each of the proposed regis-\ntration methods will be a hybrid form between a landmark/segmenta tion and a pixel/voxel\nbased method. Anatomical structures, present in reference an d test image, will be used to\nde\ufb01ne a probability distribution fon the reference image incorporating the prior knowl-\nedge of the problem. The trace distributions fTof the probability distribution fon theIMAGE REGISTRATION 9\nintersection of the domains of reference image and \ufb02oating test ima ge constitute the basis\nfor a pixel/voxel based registration. The registration criterion is t he Normalized Focussed\nMutual Information Yfsee Eq. (6.8).\nInastandardfeaturebasedregistration, landmarks have tobeid enti\ufb01ed bothinreference\nimage and test image. In contrast, the focus distribution has to be de\ufb01ned for the reference\nimage only and it eliminates the need for accurate landmark detection and for pairwise\nlandmark matching. The locations of the landmarks are used only to d e\ufb01ne the regions\nof high probability; extreme accuracy is not needed. Nevertheless it may be handy to\nadd some landmarks to the test image (not necessarily accurately lo cated), to obtain an\ninitial guess T\u2032. The search space of parameters of the transformation Tis limited to a\nregion located symmetrically around the parameter values of T\u2032. In our experiments we\nwill restrict to a\ufb03ne transformations as \ufb01rst order approximation s.\n7.1.Dentistry and orthodontics. The detection and evolution of periodontal diseases\nand of alveolar bone changes can be facilitated through intra-oral radiography in combina-\ntion with Digital Subtraction Radiography (DSR) [13]. Non interventio n therapy, obser-\nvation in combination with adequate preventive measures [9], and the rapies such as pulpa\ncapping require the possibility to assess evolution. DSR is a promising t echnique for the\nfollow-up of small lesions [6], restorations, and pulpa capping. The mo st common imaging\ntechniques in dentistry and orthodontics are 2D. The variation in ge ometry between image\nacquisitions most often results in irreversible distortions from one im age to the other [21].\nSatisfactory alignment of the whole image can only be obtained if the v ariation in geometry\nis su\ufb03ciently small. The variation can only be small enough if the X-raye d scene can be\nconsidered rigid. In dental applications this is most often not the ca se, due to natural\ntooth mobility, orthodontic treatment or movement of the lower ja w with respect to the\nupper jaw. Focussing the structures around the local phenomen on under study will allow\nfor a better local alignment. Such a structure can be indicated man ually or by means of\nan automatic or semi-automatic procedure. For the follow-up of sm all lesions Jacquet et\nal. [8] explore the use of FMI based on a convex combination of Gauss ian distributions in\ncombination with DSR. In this study the marking of the tooth to be mo nitored is done\nin the reference image, manually with a digital tool. In what follows we w ill give a case\nstudy of (semi-) automatic generation of a focus distribution for m onitoring the quality of\na dental restoration over time. In Fig. 3 we see two images of the te eth of one person,\ntaken two years apart. We want to register the restoration on th e smaller upper molar. To\nthis aim we construct a focus distribution around the restoration in the reference image\nexploiting prior knowledge, that restorations (and implants) are mo re radio-opaque than\nthe surrounding natural material.\nAlgorithm Automatic generation of a focus distribution aiming at the correct r egistration\nof consecutive images of a tooth restoration:\n(1). Find (all) edges in the reference image by:\n\u2022median \ufb01ltering to eliminate \u201cpepper and salt\u201d noise from the referen ce image.\n\u2022computation of the modulus of the gradient.\n\u2022convolution with a Gaussian kernel.\nThis results in Fig. 4 left.\n(2). Find a patch that contains the whole restoration:\n\u2022segmentation using a threshold to select the restoration.\n\u2022morphological closing and dilation.10 W. JACQUET & P. DE GROEN\nFigure 3. Reference image (left) and Test image (right).\nFigure 4. Edge image (left) and closed and dilated restoration (right ).\nThe indicator function of the patch is shown in Fig. 4 right.\n(3). Create the focus distribution:\n\u2022multiply the patch from step (2) and edge distribution produced in st ep (1).\nFMI registration using this focus distribution results in Fig. 5 right, s howing a well\naligned restoration. One can think of \ufb01rst creating the patch selec ting a part of the image\ncontaining the restoration, followed by edge detection and convolu tion. Working in this\norder we may easily create spurious edges due to the border of the indicator of the patch.\n7.2.Cephalometry. Cephalometry is used as a diagnostic tool and as a basis for treat-\nment planning, but also to monitor and evaluate treatment results [ 1]. In clinical practice\ntheevolutionisassessedbysuperimposingconsecutive lateralrad iographsbasedonanatom-\nical landmarks.\nAs a case study we applied FMI registration to an example of false max illary prog-\nnathism. A lack of growth of the mandible is corrected by means of a c ombined surgical\nand orthodontic treatment, where the mandibular has been advan ced. A lateral radiograph\nis taken before treatment (Fig. 6 left), and a follow up lateral radio graph is taken two\nyears after treatment (Fig. 6 right). The purpose of the images is the evaluation of skeletal\nstability, and orthodontic treatment.IMAGE REGISTRATION 11\nFigure 5. Focus distribution f(left) and subtraction image (right).\nFigure 6. Reference image (left) and Test image (right) with indicate d features.\nFigure 7. Focus distribution f(left) and subtraction image (right).\nThe practitioner is asked to indicate each structure to beused in th e alignment procedure\nwith a limited number of points (15) in the reference image (Fig. 6 left) and the test image\n(Fig. 6 right). The points in the reference image are transformed in to a model of the skull\nusing B-splines. The resulting image is convolved with a Gaussian kerne l, and used as\ndistribution in an FMI registration \u2013 see Fig. 7 left. The subtraction im age shows clearly\nthe e\ufb00ect of the orthodontic treatment, and the extent of the s urgical correction \u2013 see\nFig. 7 right.12 W. JACQUET & P. DE GROEN\nIn the aligning process of the lateral radiographs of the skull the in put of the practitioner\ncan easily be reduced or removed. The detection of the edges deline ating the front and back\nof the skull can be fully automated and used as the input for the FMI registration of the\nlateral radiographs. Another line of thought is to use automatically detected landmarks in\nthe reference image as prior knowledge to construct a focus distr ibution. The automatic\ndetection of cephalometric anatomical landmarks is promising e.g. [2] and [16]. In combi-\nnation with the reduced need for accuracy of the localization of land marks in a FMI they\ncan provide the basis for a successful automated FMI registratio n algorithm.\nAn even more challenging application is the use of registration of later al images of the\nskull in treatment planning. Crucial in the decision to start the orth odontic and/or oper-\native treatment of an adolescent is the detection of the end-of-p uberty growth sprint. For\ncharacterizing the growth curve we plan to study the evolution of t he registration parame-\nters, more precise, the scaling needed to adjust consecutive imag es of the skull.\n7.3.Follow up of implants. Digital subtraction of scans taken at time intervals can be\nused to monitor the evolution of a prosthesis with respect to its wea r and anchoring in the\nskeleton, more particular to assess the relative movement of a pro sthesis with respect to the\ncavity in which it resides. The use of digital subtraction could be relev ant to the detection\nand follow up of aseptic loosening of implants. Focussing on the bone s tructure we can\nmodel the bone in its related surrounding soft tissue, moreover we can eliminate the e\ufb00ect\nof the implant from the registration procedure, using the hollow str ucture of the bones and\nthe radio opacity of the implants as prior knowledge.\nAlgorithm Automatic generation of a focus distribution aiming at the correct r egistration\nof the bone structure surrounding an implant:\n(1). Find (all) edges in the reference image by:\n\u2022median \ufb01ltering to eliminate \u201cpepper and salt\u201d noise from the referen ce image.\n\u2022computation of the modulus of the gradient.\n\u2022convolution with a Gaussian kernel.\nThis results in an edge distribution focussing all the edges.\n(2). Find the complement of a patch covering the implant:\n\u2022segmentation using a threshold to select the implant.\n\u2022morphological closing and dilation.\n\u2022creation of an indicator of the complement of the patch covering th e implant.\n(3). Create the focus distribution:\n\u2022multiply the patch from step (2) and edge distribution produced in st ep (1).\nOnly edges corresponding to structures not related to the implant will contribute to the\nFMI registration. The reason to focus on the bone structure is th at it becomes easy to\nmeasure the movement of the implants when the bone structure is w ell aligned. In the case\nof dental implants the opposite procedure is more appropriate. It is better to register the\nimplant and evaluate the evolution of the surrounding bone tissue. 3 D-2D projections will\nmake displacement measurements unreliable.\nAlgorithm Automatic generation of a focus distribution aiming at the correct r egistration\nof a dental implant:\n(1). Find (all) edges in the reference image by:\n\u2022median \ufb01ltering to eliminate \u201cpepper and salt\u201d noise from the referen ce image.\n\u2022computation of the modulus of the gradient (Fig. 9 left).IMAGE REGISTRATION 13\nFigure 8. Reference image (left) and Test image (right).\nFigure 9. Modulus of the gradient (left) and modulus convolved with a G aussian\nkernel (right).\n\u2022convolution with a Gaussian kernel.\nThis results in an edge distribution focussing all the edges (Fig. 9 righ t).\n(2). Find a patch covering the implant:\n\u2022segmentation using a threshold (Fig. 10 left).\n\u2022morphological closing and dilation (Fig. 10 right).\n(3). Create the focus distribution:\n\u2022multiply the patch from step (2) and edge distribution produced in st ep (1).\nFMI using this focus distribution results in Fig. 11 right, demonstrat ing that both images\nare well aligned with respect to the dental implant.\n8.Discussion\nIn this paper we have explored Mutual Information as registration criterion from its\ninformation theoretical origin. The parallelism put forward by Colligno n [3] between im-\nage registration and the model of a communication channel remains unsatisfactory. The\nvalidity of MI cannot be explained from information theory. Hughes a nd Daubechies [4]\nidentify fundamental properties of MI in the framework of multi-mo dal image registration,\nto introduce simpler alternative similarity measures (distance metric between equivalence14 W. JACQUET & P. DE GROEN\n01002003004005000\n100\n200\n300\n400\n500\n600\n700\nFigure 10. Segmentation using a threshold (left) and patch covering th e implants\n(right).\nFigure 11. Focus distribution f(left) and subtraction image (right).\nclasses of images). Traditional MI neglects spatial information, it is dependent on overlap,\nand does not allow for the introduction of prior knowledge. Image re gistration based on\n(Normalized) Focussed Mutual Information with respect to a dens ity function is a means to\nintroduce this prior knowledge. In Jacquet et al. [7] Gauss Focuss ed Mutual Information is\nused to eliminate the dependence on overlap. In Jacquet et al. [8] G auss Focussed Mutual\nInformation is applied to the follow-up of small dental lesions. The ce nters of the Gaussian\ndistributions are placed manually by the practitioner on the tooth un der study by means\nof a digital tool. In the present paper, several methodologies tailo red to speci\ufb01c registra-\ntion applications are proposed. In Subsection 7.1, a dental restor ation is detected through\nsegmentation, and transformed into a regional focus distribution through convolution with\na Gaussian kernel. In Subsection 7.2, the purpose of the approach is to create elongated\nfoci along line structures used as references for the registratio n. These lines structures are\nindicated with a series of points, manually placed by the practitioner, using a digital tool.\nB-spline curves are generated using these points as control point s. The B-splines modeling\nthe skull are transformed into the above mentioned focus distribu tion by convolution with\na Gaussian kernel. In Subsection 7.3, two methodologies are presen ted. In both cases, the\ninteraction between implants and the surrounding bone tissue is stu died. The fact that theIMAGE REGISTRATION 15\nimplants are simply connected objects in the scene with a maximal rad io-opacity consti-\ntute the prior knowledge. Both applications are handled in a fully auto mated procedure in\nwhich the focus is derived from the image representing the modulus o f the gradient. In the\n\ufb01rst case the object of the study is the movement of the implant du e to aseptic loosening,\nwhich requires focussing on the bone, and therefore, removing th e implant from the focus.\nIn the second case the object of the study is the evolution of the b one tissue surrounding\nan implant and therefore, focus is put on the implant.\nFurther study will combine the e\ufb00orts to incorporate spatial infor mation with the intro-\nduction of prior knowledge through the sample distribution and exte nd it to registration\nof 3D images, such as hip-, knee-, and shoulder implants. We will elabo rate the automatic\ncreation of models to be used as basis for the sampling distribution. A lternative functional\nforms will be studied in order to increase robustness. Determinatio n of the optimal number\nof gray value bins is traditionally an aspect of optimal estimation. We w ill explore optimal\nrecombination of gray values into gray value bins from a pure registr ation point of view.\nFurthermore, there is no fundamental objection to the use of ela stic transformations in\ncombination with FMI registration. The segmentation technique use d in the examples is\nextremely crude (threshold). When migrating to e.g. elastic transf ormations of images of\nsoft tissue we will incorporate more subtle image segmentation meth ods.\n9.Acknowledgements\nData lateral radiographies: courtesy of Prof. Guy De Pauw, UGen t.\nData bitewing: courtesy of Prof. Peter Bottenberg, UZ Brussel.\nData dental implants: courtesy of Prof. Jan Cosyn, UZ Brussel."}
{"category": "abstract", "text": ". Subtraction of aligned images is a means to assesschanges in a wide va riety of\nclinical applications. In this paper we explore the information theore tical origin of Mutual\nInformation (MI), which is based on Shannon\u2019s entropy. However, the interpretation of\nstandard MI registration as a communication channel suggests th at MI is too restrictive\na criterion. In this paper the concept of Mutual Information (MI) is extended to (Nor-\nmalized) Focussed Mutual Information (FMI) to incorporate prior knowledge to overcome\nsomeshortcomingsofMI. We use this to developnew methodologiest o successfully address\nspeci\ufb01c registration problems, the follow-up of dental restoratio ns, cephalometry, and the\nmonitoring of implants.\nKeywords"}
{"category": "non-abstract", "text": "1.  A prototype template image T0 describing the \noverall architecture of the shape in terms of its \nrepresentative contours and edges. \n2.  A set of control points (CPs) on the template, \nused to define a parametric mapping governing \nthe deformation of the prototype. \nThe prototype template is based on average or expec ted \nevidence and is designed to capture expert, prior k nowledge \nof the shape, size and orientation of the anatomica l structure \nto be localized. \nThe CPs are defined on the image before the localiz ation \nprocess commences and are used to facilitate warpin g of the \nprototype to create shape variation. These points c an be \nplaced to incorporate expert knowledge of where sha pe \nvariation is more likely to occur. \nThis type of model is particularly appropriate in m edical \nimage segmentation, where inexact knowledge about t he \nshape of an object is available and where it is nec essary to \naccommodate the often significant variability of bi ological \nstructures over time and across different individua ls [3].   \nFigure 1 shows a typical template and base image. \n \n \n(a)  \n(b) \n \nFig. 1. Deformable Template Matching:  (a) A protot ype template of a \ntypical Corpus Callosum shape with control points.  (b)  MRI base image \nwhere the Corpus Callosum must be localized and seg mented.  \nIII.  THE MULTI-STAGE ALGORITHM \nIn typical applications, an object has to be locata ble re- \ngardless of translation, rotation and size. Given t hat the \nlocalization also needs to be invariant to partial shape \nchanges, this introduces a large number of variable s to be \ndetermined during optimization. Objective functions  are \ntypically non-convex, and so a multi-stage algorith m is \nemployed to reduce the search space [1]. Using the algo- \nrithm described in this section, objects localizati on and \nsegmentation is accomplished in 10s-30s. \nStage 1 \u2013 Regions of Interest: This stage is designed to \nreduce the search space by identifying the correct regions of \nthe image to search for an object.  \n The template T with dimensions X and Y is convolved \nwith the edges of the base image B using a 2D convolution \nfilter in the spatial domain [4].   \n\u2211\u2211\n= =\u2212 \u2212 =)(\n1)(\n1) , (),( ),(Y size \nyX size \nxyYxXByxT YXC   (1) \nThe result of the convolution is given by Equation 1 and \nrepresents the image as an intensity map with high intensity \nwhere the convolution integral is large. \nTemplate-sized regions of high intensity are search ed for \nobject matches in the next stage of the algorithm.  Using \nonly these areas, the visual search space can be re duced by \nup to 80%. \nStage 2 \u2013 Multi-Resolution Approximate Matches: Dur- \ning this stage, possible object matches are determi ned using \napproximate matching.   \nThe template is windowed at discrete positions and orien- \ntations over the base image and a match between the  two is \nevaluated.  The discrete window locations do not co ver the \nentire base image, but are obtained from Stage 1. \nThe template is attracted and aligned to the salien t edges \nin the base image via directional Edge Potential Fi elds \n(EPFs), determined by the positions and directions of edges \nin the base image [1].  The EP for each pixel in th e base \nimage is defined by: \n( )) ( exp ),(2 2\ny x yx \u03b4 \u03b4+ \u2212 \u2212= \u03a6  (2) \nWhere ( \u03b4x, \u03b4y) is the displacement from the pixel to the \nnearest edge point in the image. A directional comp onent is \nalso included for each pixel ( x,y ) in edge I, determined by: \n\uf8f7\uf8f7\n\uf8f8\uf8f6\n\uf8ec\uf8ec\n\uf8ed\uf8eb= \u0398dx yxdI \ndy yxdI yx),( ),(arctan ),(  (3) This modified EPF induces an energy function betwee n \nthe template image and the base image given by [1]:  \n( )\u2211\n=\u0398 \u03a6+ =TN\nii\nTd s yx yxNB TE\n1,,, )) ,( cos( ),( 11), ( \u03b2\u03be   (4) \nWhere the summation is over the NT pixels on the tem- \nplate and \u03b2(x, y) is the angle between the tangent of the edge \npixel nearest ( x, y) and the tangent direction of the template \nat ( x, y).  This energy measure, expressed as a correlation  \nbetween the base image and the template deformed in  terms \nof scale s, rotation \u0398, displacement d and warp \u03b6, is based \non the Chamfer matching function but requires that the \ntemplate agree with the image edges not only in pos ition, \nbut also in direction [1].  This requirement provid es signifi- \ncantly improved robustness in noisy images.  The lo cations \nof low energy matches (below an application-specifi c \nthreshold) are taken as possible matches. \nThe resolution of the EPF image is controlled by va rying \nthe standard deviation \u03c3, of the Gaussian filter used to find \nthe image edges [5].   Once the set of possible mat ches is \nfound at a coarse resolution Stage 2 is repeated at  progres- \nsively finer resolutions to determine final matches . The \nlocations of possible matches at previous resolutio ns are \nused as starting locations for finer resolution mat ching.  \nStage 3 \u2013 Template Deformation: At this stage, the tem- \nplate is deformed to fit the image edges accurately .  This \ndeformation can be thought of as a registration bet ween the \ntemplate image and the base. \n The approximate match locations from the finest re solu- \ntion EPFs are used to initialize the template place ment on \nthe base edge map. Control points on the template a re trans- \nferred to corresponding locations on the edge map a nd are \nthen repositioned iteratively using Particle Swarm optimiza- \ntion (PSO) [6].  At each iteration of the PSO algor ithm, a \nLocal Weighted Mean (LWM) warp [7] is used to warp the \ntemplate to fit these new CP positions.  The PSO al gorithm \ncan be run for a specified number of iterations, or  until a \nterminating criterion, such as a minimum energy, is  met. \nGiven N corresponding CPs ( Xi, Yi) on the template and \n(x i, y i) on the base image, LWM warping requires two func- \ntions, \nXi \u2248 f(x i, y i) \nYi \u2248 g(x i, y i) \n \nthat approximate a mapping between these points, as  closely \nas possible. The transformation functions can be ob tained \ndirectly from the given control points and do not r equire the \nsolution of systems of equations. f (and similarly g) are \nobtained from Equation 5 [7]. \u2211\u2211\n==\n\u2212+ \u2212\u2212+ \u2212\n=\nN\nin i iN\nii n i i\nR yy xx WyxPR yy xx W\nyxf\n121\n2 2121\n2 2\n}/]) () {[( ),(}/]) () {[( \n),( (5) \nWhere the N polynomials Pi are constructed using the \nGram-Schmidt orthogonalization process, and a set o f line- \narly independent functions hi, to have the form [7]: \n),( ),( ),( ),(),( ),( ),( ),(),( ),( ),(),( ),(\n11 002 22 121 0 20 2111 0 10 10 00 0\nyxha yxPayxPa yxPyxhayxPayxPa yxPyxhayxPa yxPyxha yxP\nT TT T T T ++ + =+ + =+ ==\nKM (6) \nWhere  (x, y) is any arbitrary point in the image. The N \nweight functions Wi are given by: \n0)(2 31)(3 2\n=+ \u2212=\nRWR R RW\nii                  \n11 0\n>\u2264\u2264\nRR   (7) \nWhere n i i R yy xx R /]) () [( 21\n2 2\u2212+ \u2212 =  and nRis the \ndistance of point (x i, y i) from its (n-1) th  nearest control point \nin the base image. \nApart from not requiring the solution of a system o f \nequations at each iteration, this LWM warp implemen tation \nhas a number of advantages over other warp methods[ 8]: \n\u2022 Corresponding control points are not mapped exactly  to \neach other so digital errors in the correspondences  as \nwell as small mismatch errors are smoothed. \n\u2022 The rational weight functions adapt to the density and \norganization of the points and automatically extend  to \nlarge gaps between control points. \n\u2022 Because varied placement of control points is accep t- \nable, expert knowledge of deformation can be incorp o- \nrated. \n \nThe use of PSO to determine the optimal placement o f \nCPs is effective because the regions of the global minima \nare known from Stage 2.  To prevent the PSO from ex plor- \ning outside of these regions, and to limit warp dim ensional- \nity, a penalty function is introduced that measures  the sum \nof the squared differences between CPs on the origi nal and \ndeformed templates. The penalty function is added t o the \nenergy cost given by Equation 4 and penalizes extre me \nwarps that would leave the region of global minimum . It is \ngiven by Equation 8, where \u03b1controls rigidity of the warp. \u2211\n=\u2212 + \u2212 =N\nxi i i i y Y x X yxP\n12 2) () ( ),( \u03b1  (8) \nIV.  EXPERIMENTAL RESULTS \n The deformable template model presented has been a p- \nplied to different biological structures in a numbe r of func- \ntional medical images. \nThe first test experiment presented involves the se gmen- \ntation of the Corpus Callosum in four different MRI  images.  \nThe prototype template used is the first Corpus Cal losum \nshape.  This experiment is designed to illustrate t he warp \ncapabilities of the algorithm, and the template ima ge is \ninitialized at the center of the base images. Figur e 2 shows \nthe initial and final base images. As can be seen, all four \nCorpus Callosums are localized and segmented, even \nthough there is considerable shape variation betwee n the \nimages. \n \n \n(a)  \n(b) \n \nFig. 2. Corpus Callosum MRI images. (a) Original im age. (b) Segmented \nimage.  \n \nThe second experiment involves the detection of ane u- \nrysms in ultrasound images.  The search algorithm i s illus- \ntrated in this experiment, where an aneurysm is det ected in \ntwo images regardless of template rotation and slig ht scale \nchange.  The two segmented ultrasound images are sh own \nin Figure 3.  \nThe third experiment involved the segmentation of c ar- \ndiac MRI images.  The two images of the heart were seg- \nmented using the same template, illustrating the wa rping \ncapability as well as rotation invariance. This is shown in \nFigure 4. \n \n \n  \n \n \nFig. 3. Segmentation of Aneurysms in ultrasound ima ges.  \n \n  \n \nFig. 4.  Cardiac MRI segmentation. \n \nThe final experiment involves the detection of Carp al \nbones in x-ray images. This experiment shows how th e \nalgorithm can be adapted to object tracking tasks.  X-ray \nimages were taken of the hand and wrist moving in a n ark. \nIn each consecutive image, the final template from the pre- \nvious image is used as the initial template for the  current \nimage.  In this way, full localization is not requi red, result- \ning in speed and computational efficiency. Figure 5  shows \nthe x-ray images, clockwise in consecutive order. \nV.  CONLUSION \nThis paper presents a systematic approach to segmen ta- \ntion of medical images using deformable template.  Proto- \ntype templates capture typical object structure, an d are then \nused to localize similar structures within an image . The \nmethod utilizes a multi-stage, multi-resolution alg orithm to achieve computation efficiency.  The algorithm begi ns by \nidentifying regions of interest in the image, and p roceeds to \nsearch these regions at progressively finer resolut ions. Once \nan object is located, the template is deformed to f it it using a \nParticle Swarm optimized, LWM warp routine.  Experi men- \ntal results have been presented showing invariant l ocaliza- \ntion of objects in MRI, x-ray and ultrasound images . \n \n \n \nFig. 5. Carpal Bone Segmentation from X-ray \nREFERENCES  \n1.  Jain A K, Zhong Y, Lakshmanan S. (1996) Object matc hing using \ndeformable templates. IEEE Transactions on Pattern Analysis and \nMachine Intelligence, 18(3):267\u2013277 \n2.  Grenander U. (1976) Pattern synthesis: Lectures in pattern theory. \nApplied Mathematical Sciences (18). Springer-Verlag  \n3.  McInerney T, Terzopoulos D. (1996) Deformable Model s in Medical \nImage Analysis: A Survey. Medical Image Analysis, 1 (2):91\u2013108 \n4.  Matlab Function Reference: Conv2. Matlab Signal Pro cessing Tool- \nbox, at: www.mathworks.com/access/helpdesk/ help/te chdoc /ref/con \nv2.html Last date of access: 05-11-2005 \n5.  Canny J. (1986) A Computational Approach to Edge De tection. IEEE \nTransactions on Pattern Analysis and Machine Intell igence, 8(6): \n679\u2013698 \n6.  Kennedy J, Eberhart R C. (1995) Particle swarm opti mization. IEEE \nInternational Conference on Neural Networks, 1(1):1 942\u20131948 \n7.  Goshtasby A. (1988) Image registration by local app roximation \nmethods. Image and Vision Computing, 6(4):255\u2013261 \n8.  Zagorchev C, Goshtasby A. (2006) A Comparative Stud y of Trans- \nformation Functions for Nonrigid Image Registration . IEEE Transac- \ntions on Image Processing, 15(3):529\u2013538 \nAddress of the corresponding author: \nJ.M. Spiller \nSchool of Electrical & Information Engineering \nPrivate Bag 3, University of the Witwatersrand \nJohannesburg, Wits 2050 \nSouth Africa \nj.spiller@ee.wits.ac.za"}
{"category": "abstract", "text": "\u2014 This paper presents deformable templates as a \ntool for segmentation and localization of biologica l structures \nin medical images. Structures are represented by a prototype \ntemplate, combined with a parametric warp mapping u sed to \ndeform the original shape. The localization procedu re is \nachieved using a multi-stage, multi-resolution algo rithm de- \nsigned to reduce computational complexity and time.  The \nalgorithm initially identifies regions in the image  most likely to \ncontain the desired objects and then examines these  regions at \nprogressively increasing resolutions.  The final st age of the \nalgorithm involves warping the prototype template t o match \nthe localized objects.  The algorithm is presented along with \nthe results of four example applications using MRI,  x-ray and \nultrasound images. \nKeywords \u2014 Deformable template, Localization, Segmen- \ntation, Multi-resolution algorithm, Medical imaging . \nI.  INTRODUCTION  \nImage segmentation and localization plays an import ant \nrole in many medical imaging applications by automa ting or \nfacilitating the delineation of anatomical structur es. Since \nall subsequent interpretation tasks, such as featur e extrac- \ntion, object recognition, and classification depend  largely on \nthe quality of the segmentated output, effective se gmenta- \ntion has become a critical step for automated analy sis in \nmedical imaging. Segmentation and localization of a na- \ntomical structures is difficult in practice, especi ally when \ndealing with inherently noisy, low spatial resoluti on images \nsuch as those produced using functional imaging. \nDeformable Templates provide a powerful tool for im age \nsegmentation, by exploiting constraints derived fro m the \nimage data together with a priori knowledge about the loca- \ntion, size, and shape of the required structures [1 ].   \nThe algorithm presented here is used to find a matc h be- \ntween a deformed template and objects in the image,  by \nminimizing a cost function between the template and  object \nboundary.  The algorithm achieves computational eff iciency \nby searching the image in a number of stages and re solu- \ntions, refining the search at each stage.  Object L ocalization \nrequires that the template be matched regardless of  the ob- \nject\u2019s displacement, rotation, scale and deformatio n.  Invari- \nance to these characteristics is incorporated at ea ch stage of \nthe algorithm using discrete template orientations.   \n Localization of an object typically takes between 1 0s and \n30s.  \nII.  A MODEL OF DEFORMATION \nThe model of deformation presented is based on the pat- \ntern theoretic model of Grenander [2]. It consists of two \nparts"}
{"category": "non-abstract", "text": "one that generates a seq uence of hypothe-\nses on object identities and poses, the other that evaluates them based on the\nobject models. Viewed as an optimization problem, the former is conc erned with\nthe search sequence, the latter with the objective function. Usu ally the evalua-\ntion of the objective function is computationally expensive. A reaso nable search\nalgorithm will thus arrive at an acceptable hypothesis within a small nu mber of\nsuch evaluations.\nIn this article, we will analyze the search for a scene interpretation from a\nprobabilistic perspective. The object models will be formulated as ge nerative\nmodels for range data. For visual analysis of natural scenes, tha t is, scenes that\nare cluttered with multiple, non-completely visible objects in an uncon trolled\ncontext, it is a highly non-trivial task to optimize the match of a gene rative\nmodel to the data. Local optimization techniques will usually get stu ck in mean-\ningless, local optima, while techniques akin to exhaustive search are precludedby time constraints. The critical aspect of many object recognitio n problems\nhence concerns the generation of a clever search sequence.\nIn the probabilistic framework explored here, the problem of optimiz ing the\nmatch of generative object models to data is alleviated by the introd uction of\nanother statistical match criterion that is more easily optimized, alb eit less re-\nliable in its estimates of object parameters. The new criterion is used to de\ufb01ne\na sequence of hypotheses for object parameters of decreasing probability, start-\ning with the most probable, while the generative model remains the me asure for\ntheir evaluation. For an e\ufb03cient generation of the search sequenc e, it is desirable\nto make the new criterion as simple as possible. On the other hand, fo r obtaining\na short search sequence for acceptable object parameters, it is necessary to make\nthe criterion as informative as is feasible. As a key quantity for sequ ence opti-\nmization, an estimate of features\u2019 posterior probability enters th e new criterion.\nThe method is an alternative to other, often more heuristic strate gies aimed at\nproducing a short search sequence, such as checking for model f eatures in the\norder of the features\u2019 prior probability [7, 12], in a coarse-to-\ufb01ne hierarchy [6],\nor as predicted from the current scene interpretation [4, 8].\nClassichypothesize-and-testparadigmshavebeendemonstrate dinRANSAC-\nlike [5, 19] and alignment techniques [9, 10]. The method proposed her e is more\nsimilar to the latter in that testing of hypotheses is done with respec t to the full\ndata rather than on a sparse feature representation. It signi\ufb01c antly di\ufb00ers from\nboth, however, in that it recommends a certain orderof hypotheses to be tested,\nwhich is indeed the main point of the present study. Other classic con cepts\nthat are recovered naturally from the probabilistic perspective ar e feature-based\nindexing and geometric hashing [13, 20], and feature grouping and pe rceptual\norganization [14, 15, 11].\nTo keep the notational load in this article to a minimum and to support e ase\nof reading, we will denote all probability densities by the letter pand indicate\neach type of density by its arguments. Moreover, we will not introd uce symbols\nfor random variables but only for the values they take. It is unders tood that\nprobability densities become probabilities whenever these values are discrete.\n2 Generative Object Models for Range Data\nConsider the task of recognizing and localizing a number of objects in a scene.\nMore precisely, we want to estimate object parameters ( c,p) from data, where\nc\u2208IN is the object\u2019s discrete class label and p\u2208IR6are its pose parameters\n(rotation and translation). Suppose we use a sensor like a set of st ereo cameras\nor a laser scanner to obtain range-data points d\u2208IR3of one view of the scene,\nthat is, 2 + 1 /2 dimensional (2 + 1 /2D) range data. A reasonable generative\nmodel of the range-data points within the volume V(c,p) of the object cwith\nposepis given by the conditional probability density\np(d|c,p) =/braceleftbigg\nN(c)f[\u03c6(d;c,p)] ford\u2208 S(c,p),\nN(c)b ford\u2208 V(c,p)\\S(c,p).(1)\n2The condition is on the object parameters ( c,p). The set S(c,p)\u2282 V(c,p) is the\nregionclosetovisiblesurfacesofthe object cwithpose p,b >0isaconstantthat\ndescribes the background of spurious data points, and N(c) is a normalization\nconstant. The surface density of points is described by a function fof the angle\n\u03c6(d;c,p) between the direction of gaze of the sensor and the object\u2019s inwa rd\nsurface normal at the (close) surface point d\u2208 S(c,p). The function ftakes its\nmaximal value, that we may set to 1, at \u03c6= 0, i.e., on surfaces orthogonal to\nthe sensor\u2019s gazing direction.\nFourcomments are in order.First, the normalizationconstant N(c) generally\ndepends upon both candp. However, we here neglect its dependence on the\nobject pose p. The consequence is that objects will be harder to detect, if they\nexpose less surface area to the sensor. Second, the extension o f the set S(c,p)\nhas to be adapted to the level of noise in the range data. Third, poin tsd\u2208\nS(c,p) that are close to but not on the surface have assigned surface n ormals of\nclose surface points. Fourth, given the object parameters ( c,p), the data points\nd\u2208 V(c,p) can be assumed statistically independent with reasonable accurac y.\nOur task is to estimate the parameters candpby optimizing the match\nbetween the generative model (1) to the range data D. Because of the condi-\ntional independence of the data points in V(c,p), this means to maximize the\nlogarithmic likelihood\nL(c,p;D) :=/summationdisplay\nd\u2208D\u2229V(c,p)L(c,p;d) :=/summationdisplay\nd\u2208D\u2229V(c,p)lnp(d|c,p) (2)\nwith respect to ( c,p)\u2208\u2126\u2282IN\u00d7IR6. A computationally e\ufb03cient version is\nobtained by assuming that \u03c6\u226a1, which is true for patches of surface that are\napproximately orthogonal to the direction of gaze. Fortunately, such parts of the\nsurface contribute most data points; cf. (1). Observing that\nlnf(\u03c6) =\u2212a\u03c62+O/parenleftbig\n\u03c64/parenrightbig\n= 2a(cos\u03c6\u22121)+O/parenleftbig\n\u03c64/parenrightbig\n, (3)\nwith some constant a >0, we may neglect terms of order O(\u03c64) to obtain\nL(c,p;d)\u2248/braceleftbigg\n2a[n(d;c,p)\u00b7g\u22121]+lnN(c) ford\u2208 S(c,p),\nlnb+lnN(c) for d\u2208 V(c,p)\\S(c,p),(4)\nwheren(d;c,p)istheobject\u2019sinwardsurface-normalvectoratthe(close)sur face\npointd\u2208 S(c,p) andgis the sensor\u2019s direction of gaze; both are unit vectors.\nThe constant ln b <0 is e\ufb00ectively a penalty term for data points that come\nto lie in the object\u2019s interior under the hypothesis ( c,p). Again, discarding the\ntermsO(\u03c64) in (3) is acceptable as data points producing a large error will be\nrare; cf. (1).\n3 Probabilistic Search for Model Matches\nIn this section, we derive the probabilistic search algorithm. We \ufb01rst intro-\nduce another statistical criterion function of object parameter s (c,p). The new\n3criterion, to be called truncated probability (TP), de\ufb01nes a sequence of ( c,p)-\nhypotheses. The hypotheses are evaluated by the likelihood (2), s tarting with\nthe most probable ( c,p)-value and proceeding to gradually less probable values,\nas estimated by the TP. The search terminates as soon as L(c,p;D) is large\nenough.\nWe will obtain the TP from a series of steps that simplify from the ideal\ncriterion, the posterior probability of object parameters ( c,p). Although this\nderivation cannot be regarded as a thoroughly controlled approxim ation, it will\nmake explicit the underlying assumptions and give some feeling for how far we\nhave to depart from the ideal to arrive at a feasible criterion.\nThe TP makes use of the probability of the presence of feature valu es consis-\ntentwithobjectparameters( c,p).Itisthusanessentialpropertyoftheapproach\nto treat features as random variables.\n3.1 The Search Sequence\nLet us de\ufb01ne point features as surface points centered on certa in local, isolated\nsurfaceshapes. Examplesofsuchpoint featuresarecorners,s addle points, points\nof locally-extreme surface curvature etc. They are characteriz ed by a pair ( s,f),\nwheres\u2208INisthefeature\u2019sshape-classlabeland f\u2208IR3isitslocation.Consider\nnow a random variable that takes values ( s,f) of point features that are related\nto the objects sought in the data. Its values are statistically depe ndent upon the\nrangedata D.Letusrestrictthepossiblefeaturevalues( s,f)tos\u2208 {1,2,... ,m}\nandf\u2208D, such that only datapoints can be feature locations.This is equivale nt\nto setting the feature-value probability to 0 for values ( s,f) withf/\u2208D. The\nrestriction is not really correct as we will loose true feature location s between\ndata points. However, it will greatly facilitate the search by limiting fe atures\nto discrete values that lie close to the true object surfaces and, h ence, include\nhighly probable candidates.\nLet us introduce the concept of groupsof point features. A feature group is\na setGg={(s1,f1),(s2,f2),... ,(sg,fg)}of feature values with fi\u221d\\e}atio\\slash=fjfori\u221d\\e}atio\\slash=j.\nA group Gghence contains simultaneously possible values of gfeatures.\nThe best knowledgewecould haveabout true values ofthe object p arameters\n(c,p) is encapsulated in their posterior-probability density given the dat aD,\np(c,p|D). However, we usually do not know this density, and if we knew it, its\nmaximization would pose a problem similar to our initial one of maximizing th e\ngenerative model (2). We can nonetheless expand it using feature groups,\np(c,p|D) =/summationdisplay\nGgp(c,p|D,Gg)p(Gg|D),/summationdisplay\nGgp(Gg|D) = 1.(5)\nThesummationsareoverallpossiblegroups Ggof\ufb01xedsize g.Theirenumeration\nis straightforward but tedious to explicate, so we omit this here. No te that the\nexpansion (5) is only valid, if we can be sure to \ufb01nd gtrue feature locations\namong the data points.\n4Let us now simplify the density (5) to the density\npg(c,p;D) :=/summationdisplay\nGgp(c,p|Gg)p(Gg|D). (6)\nUnlike the posterior density (5), pgdepends upon the feature-group size g. For-\nmally,pgis a Bayesian belief network with a g-feature probability distribution\nat the intermediate node. Maximization of pgwith respect to ( c,p) is still too\ndi\ufb03cult a task, as all possible feature-group values contribute to a ll values of\n(c,p)-density.\nA radically simplifying step thus takes us naturally to the density\nqg(c,p;D) := max\nGgp(c,p|Gg)p(Gg|D), (7)\nwhere for each ( c,p) only the largest term in the sum of (6) contributes. Note\nthatqg(c,p;D)\u2264pg(c,p;D) for all ( c,p), such that qgis not normalized on the\nset\u2126of object parameters ( c,p). This density will nonetheless be useful for our\npurpose of guiding a search through \u2126, as there only relativedensities matter.\nAs pointed out above, for all this to make sense, we need gtrue feature\nlocations among the data points. To be safe, one could be tempted t o setg= 1.\nHowever, the simplifying step from density pgto density qgsuggests that the\nlatter will be more informative as to the true value of ( c,p), if the sum in\n(6) is dominated for high-density values of ( c,p) by only fewterms. Now, less\nthan three point features do not generally de\ufb01ne a \ufb01niteset of consistent object\nparameters ( c,p). The density p(c,p|Gg) will hence not exhibit a pronounced\nmaximum for g <3. High-density points of pgarise then from accumulation over\nmanyg-feature values, i.e., terms in (6), and are necessarily lost in qg. Groups\nof sizeg\u22653, on the other hand, do de\ufb01ne \ufb01nite sets of consistent ( c,p)-values,\nandp(c,p|Gg) has in\ufb01nite density there. Altogether, feature groups of size g= 3\nseem to be a good choice for our search procedure; see, however , the discussion\nin Sect. 5.\nLet us now introduce the logarithmic truncated probability (TP),\nTP(c,p;D) := lim\n\u01eb\u21920ln/integraldisplay\nS\u01eb(p)dp\u2032q3(c,p\u2032;D), (8)\nwhereS\u01eb(p) is a sphere in pose space centered on pwith radius \u01eb >0. The\nintegral and limit are needed to pick up in\ufb01nities in the density q3(c,p;D); see\nbelow. The TP is truncated in a double sense: \ufb01nite contributions fro mq3are\ntruncated and q3itself is obtained from truncating the sum in (6).\nAccording to the discussion above, it is expected that ( c,p)-values of high\nlikelihood (2) will mostly yield a high TP (8). Our search thus proceeds b y\nevaluating, in that order,\nL(c1,p1;D),L(c2,p2;D),... with\nTP(c1,p1;D)\u2265TP(c2,p2;D)\u2265... ,(c1,p1) = arg max\n(c,p)\u2208\u2126TP(c,p;D).(9)\n5The search stops, as soon as L(ck,pk;D)> \u0398or all (c,p)-candidates have\nbeen evaluated. In the former case, the object identity ckand object pose pk\nare returned as the estimates. In the latter case, it is inferred th at none of the\nobjects sought is present in the scene. Alternatively, if we know a p riori that one\nof the objects mustbe present, we may pick the object parameters that have\nscored highest under Lin the whole sequence. The algorithm will be formulated\nin more detail in Sect. 3.3.\nIn the density (7) that guides the search, one of the factors is th e density of\nthe object parameters conditioned on the values G3of a triple of point features.\nThis is explicitly\np(c,p|G3) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3/summationtexth(G3)\ni=1\u03b3i(G3)\u03b4c,Ci(G3)\u03b4[p\u2212Pi(G3)]\n+/bracketleft\uf8ecig\n1\u2212/summationtexth(G3)\ni=1\u03b3i(G3)/bracketright\uf8ecig\n\u03c11(c,p;G3) forG3\u2208 C,\n\u03c12(c,p;G3) for G3/\u2208 C.(10)\nHere\u03b4c,c\u2032is the Kronecker delta (elements of the unit matrix) and \u03b4(p\u2212p\u2032) is\nthe Dirac-delta distribution on pose space; Cis the set of feature-triple values\nconsistent with any object hypotheses; h(G3) is the number of object hypotheses\nconsistent with the feature triple G3; (Ci(G3),Pi(G3)) are the consistent ( c,p)-\nhypotheses; \u03b3i(G3)\u2208(0,1) are probability-weighting factors for the hypotheses.\nGenerally we have\nh(G3)/summationdisplay\ni=1\u03b3i(G3)<1, (11)\nwhich leaves a non-vanishing probability 1 \u2212/summationtext\ni\u03b3i>0 that three consistent\nfeatures do not all belong to one of the objects sought. Accordin gly, the density\np(c,p|G3)on\u2126containssome\ufb01nite, spreadcontribution \u221d\u03c11(c,p;G3), a\u201cback-\nground\u201ddensity,in theconsistent-featurescase.Inthe incons istent-featurescase,\nthe density is similarly spread on \u2126and given by some \ufb01nite term \u03c12(c,p;G3).\nClearly, only the values ( Ci,Pi) of object parameters where the density (10) is\nin\ufb01nite will be visited during the search (9); cf. (8).\nThe functions \u03b3i(G3),Ci(G3),Pi(G3) arecomputed by generalized geometric\nhashing. LetG3={(s1,f1),(s2,f2),(s3,f3)}. As key into the hash table we use\nthe pose invariants\n(s1,s2,s3,||f1\u2212f2||,||f2\u2212f3||,||f3\u2212f1||), (12)\nappropriately shifted, scaled, and quantized. Here || \u00b7 ||denotes the Euclidean\nvectornorm.Theposes Pi(G3),i= 1,2,... ,h(G3),arecomputedfrommatching\nthe triple G3toh(G3) model triples from the hash table. Small model distortions\nthat result from an error-tolerantmatch are removed by orthog onalizationof the\nmatch-related pose transforms. The weight \u03b3i(G3) and the object class Ci(G3)\nare drawn along with each matched model triple.\nThe weights \u03b3i(G3) can be adapted during a training period and during op-\neration of the system in an unsupervised manner. One simply needs t o count\n6the number of instances where a feature triple G3draws a correct set of object\nparameters ( Ci,Pi), i.e., one with L(Ci,Pi;D)> \u0398. The adapted \u03b3i(G3) repre-\nsent empirical grouping laws for the point features. Since it will often be the case\nthat\u03b3i(G3)\u221d1/h(G3) for all i= 1,2,... ,h(G3), it turns out that hypothe-\nses drawn by feature triples less common among the sought objects tend to be\nvisitedbeforethe ones drawn by more common triples during the search (9).\nIn order to fully specify the TP (8) and formulate our search algorit hm, it\nremains to establish a model for the feature-value probabilities p(G3|D) given\nthe data D; cf. (7). This is the subject of the next section.\n3.2 Inferring Local Surface Shape from Point-Relation Dens ities\nIn principle, a generative model of range data on local surface sha pesscan be\nbuilt analogous to (1). Thus, we may construct a conditional proba bility density\np(D|s,f,r), where r\u2208IR3are the rotational parameters of the local surface\npatch represented by the point feature ( s,f). During recognition, however, it\nwould then be necessary to optimize the parameters rfor eachs\u2208 {1,2,... ,m}\nandf\u2208D, in order to obtain the likelihood of each feature value ( s,f). Besides\nthe huge overhead of doing so without being interested in an estimat e forr,\nthis procedure would confront us with a problem similar to our original one of\noptimizing p(D|c,p). On the other hand, the density p(D|s,f) is an infeasible\nmodel because of high-order correlations between the data point sD. Neglecting\nthese correlations would throw out all information on surface shap es.\nThe strategy we pursue here is to capture some of the informative cor-\nrelations between data points Din a family of new representations T(c) =\n{\u22061,\u22062,... ,\u2206 N},c\u2208IR3. EachT(c) represents the geometry of the data D\nrelative to the point c, and each \u2206irepresents geometric relations between mul-\ntiple data points. A reasonable statistical model is then obtained by neglecting\ncorrelations between the \u2206i.\nLearning Point-Relation Densities. The point relations we are going to\nexploit here are between four data points, that is, tetrahedron g eometries, where\nthree of the points are selected from a spherical neighborhood of the fourth. For\nfour points x1,x2,x3,c\u2208IR3we de\ufb01ne the map\n\uf8eb\n\uf8edx1\nx2\nx3\uf8f6\n\uf8f8\u221dma\u221asto\u2192\u2206(c;x1,x2,x3) :=\uf8eb\n\uf8ec\uf8edr/R\nd/slash\uf8ecig/radical\uf8ecig\nr2\u22124a\n3\u221a\n3\n4a/3\u221a\n3(r2\u2212d2)\uf8f6\n\uf8f7\uf8f8\u2208[0,1]\u00d7[\u22121,1]\u00d7[0,1],\n(13)\nwhereris the mean distance of the center ctox1,x2,x3,Ris the radius of the\nspherical neighborhood of c,dis the (signed) distance of cto the plane de\ufb01ned\n7byx1,x2,x3, andais the area of the triangle they de\ufb01ne1. Explicitly,\nr=1\n33/summationdisplay\ni=1||xi\u2212c||, (14)\nd= sgn[g\u00b7(x1\u2212c)]||(x1\u2212c)\u00b7[(x2\u2212x1)\u00d7(x3\u2212x1)]||\n||(x2\u2212x1)\u00d7(x3\u2212x1)||,(15)\na=1\n2||(x2\u2212x1)\u00d7(x3\u2212x1)||. (16)\nAs seen in (15), the sign of dis determined by the direction of gaze g. We can\nnow de\ufb01ne the family of tetrahedron representations T(c) by\nT(c) :=/braceleft\uf8ecig\n\u2206(c;d1,d2,d3)/vextendsingle/vextendsingle/vextendsingle{d1,d2,d3} \u2286D\u2227max\ni=1,2,3||di\u2212c||< R\n\u2227max\ni=1,2,3||di\u2212c||\u2212min\ni=1,2,3||di\u2212c||< \u01eb/braceright\uf8ecig\n.(17)\nThe parameter \u01eb >0 sets a small tolerance for di\ufb00erences in distance to camong\neach data-point triple d1,d2,d3, that is, ideally all three points have the same\ndistance. In practice, the tolerance \u01ebhas to be adapted to the density of range-\ndata points Dto obtain enough \u2206-samples in each T(c). Figure 1 depicts the\ngeometry of the tetrahedron representations.\nPSfrag replacements\nd1d2d3c\nrrr\nd\naFig.1.Transformation of range data points to a tetrahedron\nrepresentation. For an inspection point c, the 3-tuples ( r,d,a)\nare collected and normalized [cf. (13)] for each triple of data\npointsd1,d2,d3\u2208Dwith (approximately) equal distance r\u2208\n(0,R) fromc.\nOne of the nice properties of the tetrahedron representation (1 7) is that for\nany surface of revolution we get at its symmetry point csamples\u2206\u2208T(c) that\nare con\ufb01ned to a surface in \u2206-space characteristic of the original surface shape.\nFor surfaces of revolution, the distinctiveness and low entropy of the distribution\nof rangedata Dsampled from a surface in 3Dis thus completely preservedin the\ntetrahedron representation. For more general surfaces, it is t herefore expected\nthat a high mutual information between \u2206-samples and shape classes can be\nachieved.\nOur goal is to estimate point-relation densities for each shape class s=\n1,2,... ,m. Samples are taken from tetrahedron representations centere d on\nfeature locations ffrom a training set Fs, that is, from \u222af\u2208FsT(f). Moreover,\nwe add a non-feature class s= 0 that lumps together all the shapes we are\n1The quantity introduced in (13) will be denoted by \u2206(c;x1,x2,x3),\u2206(c), or simply\n\u2206to indicate its dependence on points as needed.\n8not interested in. The result are the shape-conditioned densities p(\u2206|s) for\u2206\u2208\n[0,1]\u00d7[\u22121,1]\u00d7[0,1]ands= 0,1,2,... ,m. In particular, p(\u2206|s) =p(\u2206|s,f) for\n\u2206\u2208T(f).\nEstimation of p(\u2206|s) is made simple by the fact that we get O(n3), i.e., a lot\nof samples of \u2206for each feature location fwithndata points within a distance\nR. For our application, it is su\ufb03cient to count \u2206-samples in bins to estimate\nprobabilities p(\u2206|s) for discrete \u2206-events as normalized frequencies.\nInferring Local Surface Shape. Let the features\u2019 prior probabilities be p(s)\nfors= 1,2,... ,m, that is, p(s)\u2208(0,1) is the probability that any given data\npoint from Dis a feature location of type s. The feature priors are virtually\nimpossible to know for general scenes. We do know, however, that p(s)\u226a1 for\ns= 1,2,... ,m, i.e., the overwhelming majority of data points are not feature\nlocations. This must be so for a useful dictionary of point features . It thus makes\nsense to expand the logarithm of the shapes\u2019 posterior probabilitie s, given l\nsamples\u22061(f),\u22062(f),... ,\u2206 l(f)\u2208T(f),f\u2208D,\nlnp[s|\u22061(f),\u22062(f),... ,\u2206 l(f)] = lnp(s)+l/summationdisplay\ni=1{lnp[\u2206i(f)|s]\u2212lnp[\u2206i(f)|0]}\n+O[p(1),p(2),... ,p(m)],(18)\nand neglect the terms O[p(1),p(2),... ,p(m)]. Remember that p(\u2206|0) is the \u2206-\ndensity of the non-feature class. The expression (18) neglects c orrelations be-\ntween the \u2206i(f)\u2208T(f).\nThe sample length lfor each representation T(f) will usually be l\u226a |T(f)|,\nthe cardinalityofthe set T(f).It isin fact crucialthat wedonot havetogenerate\nthe complete representations T(f) for recognition, as this would require O(n3)\ntime forndata points within a distance Rfromf. Instead, it is possible to draw\na random, unbiased sub-sample of T(f) inO(n) andO(l) time.\nThe \ufb01rst term in (18) can be split into\nlnp(s) = lnq(s)+lnm/summationdisplay\ns\u2032=1p(s\u2032), (19)\nwhereq(s)\u2208(0,1) are the relative frequencies of shape classes s= 1,2,... ,m.\nNow, these frequencies do not di\ufb00er between features over many orders of mag-\nnitude for a useful dictionary of point features2. The dependence of (19) on the\nshape class sis thus negligible compared to the sum in (18) for reasonable sam-\nple lengths l\u226b1 (several tens to hundreds). The \ufb01rst term in (18) may hence\n2A feature that is more than a hundred times less frequent than the others should\nusually be dropped for computational e\ufb03ciency.\n9be disregarded, and we are left with the feature\u2019s log-probability\n\u03a6(s,f;D) := lnp[s|\u22061(f),\u22062(f),... ,\u2206 l(f)]+const .\n\u2248l/summationdisplay\ni=1{lnp[\u2206i(f)|s]\u2212lnp[\u2206i(f)|0]},(20)\nthat is, a log-likelihood ratio.\n3.3 The Search Algorithm\nWe still need to determine the TP (8) for guiding the search (9). Let againG3=\n{(s1,f1),(s2,f2),(s3,f3)}. It is reasonable to assume statistical independence of\nthe point features in G3, as long as there are many di\ufb00erent objects in the\nworld that have these features3. We hence model the feature-triples\u2019 posterior\nprobability p(G3|D) in (7) as\np(G3|D) =3/productdisplay\ni=1p[si|\u22061(fi),\u22062(fi),... ,\u2206 l(fi)]\u221dexp/bracketleft\uf8ecigg3/summationdisplay\ni=1\u03a6(si,fi;D)/bracketright\uf8ecigg\n.(21)\nThe TP can then be expressed as\nTP(c,p;D) = max\nG3\u2208C\uf8ee\n\uf8f0lnh(G3)/summationdisplay\ni=1\u03b3i(G3)\u03b4c,Ci(G3)\u03b4p,Pi(G3)+3/summationdisplay\ni=1\u03a6(si,fi;D)\uf8f9\n\uf8fb,\n(22)\nup to additive constants. The \ufb01rst term under the max-operation is the contri-\nbution from feature grouping4, the second from single features. In particular, we\nget\nTP(c,p;D) =\u2212\u221efor\n(c,p)/\u2208/braceleftbig\n(Ci(G3),Pi(G3))/vextendsingle/vextendsingleG3\u2208 C, i\u2208 {1,2,... ,h(G3)}/bracerightbig\n,(23)\nthat is, for ( c,p)-values not suggested by any feature triple G3.\nWe are now prepared to formulate the search algorithm. The followin g is not\nmeant to specify a \ufb02ow of processing, but rather a logic sequence o f operations.\n1. From all possible feature values ( s,f)\u2208 {1,2,... ,m}\u00d7D, select as feature\ncandidates the values with log-probability \u03a6(s,f;D)> \u039e.\n2. From the feature candidates, generate all triples G3. Use their keys (12) into\na hash table to \ufb01nd the associated object hypotheses ( Ci(G3),Pi(G3)) and\ntheir weights \u03b3i(G3) fori= 1,2,... ,h(G3).\n3These objects may or may not be in our modeled set.\n4The factor \u03b4p,Pi(G3)is an idealization. Since the pose parameters pare continuous,\nthe hashing with the key (12) is necessarily error tolerant. The factor is then to be\nreplaced by an integral/integraltext\nPi(G3)dp\u2032\u03b4(p\u2212p\u2032) for a set Pi(G3) of pose parameters.\n103. For the object hypotheses obtained, compute the TPs.\n4. Evaluate the object hypotheses by the likelihood Lin the order of decreasing\nTP, starting with the hypothesis scoring highest under TP. Skip dup licate\nhypotheses. Stop when L(c,p;D)> \u0398for a hypothesis ( c,p).\n5. Returnthelastevaluatedobjectparameters( c,p)astheresult,if L(c,p;D)>\n\u0398. Else conclude that there is none of the sought objects in the data .\nSome comments are in order.\n\u2013Comment on step 1: The restriction to the most probable feature v alues by\nintroduction of the threshold \u039eis optional. It is a powerful means, however,\nto radically speed up the algorithm, if there are many range data poin ts\nD. If the probability measure on the features is reliable and \u039eis adjusted\nconservatively, the risk of missing a good set of object parameter s is very\nsmall; see Sect. 4.1 below.\n\u2013Comment on step 2: Feature triples G3/\u2208 C, that is, inconsistent triples, are\ndiscarded by the hashing process.\n\u2013Comment on step 3: Computation of TP is cheap as the hashed weight s\u03b3i\nfrom step 2 and the feature\u2019s log-probabilities \u03a6from step 1 can be used; cf.\n(22).\n\u2013Comment on step 4: Duplicate hypotheses may be very unlikely, but t his\ndepends on when two hypotheses are considered equal. In any cas e, skipping\nthem realizes the max G3\u2208C-operation in the TP (22), ensuring that each\nhypothesis is drawn only by the feature triple which makes it most pro bable.\n\u2013Comment on step 5: If the likelihood Ldoes not reach the threshold \u0398\nbut we know a priori that at least one of the objects must be prese nt in\nthe scene, the algorithm may return the object parameters ( c,p) that have\nscored highest under L.\nA complete scene interpretation often involves more than one reco gnized set\nof object parameters ( c,p). A simultaneous search for several objects is possible\nwithin the current framework, although it will be computationally ver y costly.\nIt requires evaluation of TPs and likelihoods\nTP(c1,p1,c2,p2,...;D), L(c1,p1,c2,p2,...;D) (24)\nfor a combinatorially enlarged set of object parameters ( c1,p1,c2,p2,...). A\nmuch cheaper, although less reliable, alternative is sequential sear ch, where the\ndata belonging to a recognized object ( c,p) are removed prior to the algorithm\u2019s\nrestart.\n4 Experiments\nWehaveperformedtwotypesofexperiment.Oneteststhe quality ofthemeasure\n(20) of the features\u2019 posterior probability. The other probes th e capabilities of\nthe complete algorithmforobject segmentationandrecognition.A ll experiments\nwererunonstereodataobtainedfromathree-cameradevice(th e\u2018TriclopsStereo\n11Vision System\u2019, Point Grey Research Inc.). The stereo data were ca lculated\nfrom three 120 \u00d7160-pixel images by a standard least-sum-of-pixel-di\ufb00erences\nalgorithm for correspondence search. The data were accordingly of a rather low\nresolution. They were moreover noisy, contained a lot of artifacts and outliers,\nand even visible surfaces lacked large regions of data points, as is no t uncommon\nfor stereo data; see Figs. 4 and 5.\n4.1 Local Shape Recognition\nOnly correct feature values are able to draw correct object hypo theses (c,p),\nexcept from accidental, unprobable events. For an acceptable se arch length it is,\ntherefore, crucial that correct feature values yield a high log-pr obability \u03a6; cf.\n(20) and (22). This is even more crucial, if feature candidates are p reselected by\nimposing a threshold \u03a6(s,f;D)> \u039e; cf. step 1 of the algorithm in Sect. 3.3.\nAs point features, we chose convex and concave rectangular cor ners. The\ntraining set contained 221 feature locations that produced 466,48 5\u2206-samples\nfor the densities p(\u2206|s),s\u2208 {\u201cconvex corner\u201d ,\u201cconcave corner\u201d }. For the non-\nfeature class s= 0 we obtained 9,623,073 \u2206-samples from several thousand\nnon-feature locations (mostly on planes, also some edges). The pa rameters for\n\u2206sampling were R= 15 mm and \u01eb= 1 mm; cf. Sect. 3.2. The \u2206-samples were\ncounted in 15 \u00d720\u00d710 bins, which corresponds to the discretization used for\nthe\u2206-space [0 ,1]\u00d7[\u22121,1]\u00d7[0,1].\nFeature-recognition performance was evaluated on 10 scenes th at contained\n(i) an object with planar surface segments, edges, saddle points, and altogether\n65 convex and concave corner points (feature locations); (ii) a pie ce of loosely\nfolded cloth that contributed a lot of potential distractor shapes . The sample\nlength was l= 50\u2206-samples; cf. Sect. 1. Since drawing \u2206-samples is a stochastic\nprocess, we let the program run 10 times on each scene with di\ufb00eren t seeds for\nthe random-number generator to obtain a more signi\ufb01cant statist ics.\nCornerpoints areparticularlyinterestingas test features, since they allow for\ncomparisonofthe proposedlog-probabilitymeasure(20) with the c lassicmethod\nofcurvatureestimation.Let c1(f;D) andc2(f;D) bethe principalcurvaturesofa\nsecond-order polynomial that is least-squares \ufb01tted to the data points from Din\ntheR-sphere around the point f. Because the corners are the maximally curved\nparabolic shapes in the scenes, it makes sense to use the curvatur e measures\nC(s= \u201cconvex corner\u201d ,f;D) := min[ c1(f;D),c2(f;D)], (25)\nC(s= \u201cconcave corner\u201d ,f;D) := min[ \u2212c1(f;D),\u2212c2(f;D)],(26)\nas alternative measures of \u201ccornerness\u201d of the point ffor convex and concave\nshapes, respectively. We thus compare \u03a6(s,f;D) as de\ufb01ned in (20) to C(s,f;D)\non true and false feature values\n(s,f)\u2208 {\u201cconvex corner\u201d ,\u201cconcave corner\u201d }\u00d7D . (27)\nIn Fig. 2 we show the distribution of \u03a6- andC-scores for the 65 true feature\nvalues in relation to their distribution for all possible feature values, which are\n12more than 100,000. As can be seen, the \u03a6- andC-distributions for the popu-\nlation of true values are both distinct from, but overlapping with the \u03a6- and\nC-distributions for the entire population of feature values. A qualita tive di\ufb00er-\nence between \u03a6- andC-scoring of feature values is more obvious in the ranking\nthey produce. In Fig. 3 we show the distribution of \u03a6- andC-ranks of the true\nfeature values among all feature values, with 1 being the \ufb01rst rank , i.e., highest\n\u03a6/C-score, and 0 the last. Both \u03a6- andC-ranks of true values are most fre-\nquently found close to 1 and gradually less frequently at lower ranks . There are\nalmost no true features \u03a6-ranked below 0.6. The C-ranks, in contrast, seem to\nscatteruniformly in [0 ,1]for partof the true features. These featuresscoreunder\nCas poorly as the non-features. If using the C-score, they would in practice not\nbe available for object recognition, as they would only be selected to draw an\nobject hypothesis after hundreds to thousands of false featur e values.\n-300 -200 -100 0 100 2000.10.20.30.4\n-150 -100 -50 0 500.10.20.30.4\nPSfrag replacements\u03a6-score C-score\nFig.2.Score distribution of true (transparent bars, thick lines) and all p ossible\n(\ufb01lled bars, thin lines) feature values as produced by the \u03a6-score (left) and\ntheC-score (right). Horizontally in each plot extends the score, vertic ally the\nnormalized frequency of feature values.\n0.2 0.4 0.6 0.8 10.10.20.30.40.50.6\n0.2 0.4 0.6 0.8 10.10.20.30.40.50.6\nPSfrag replacements\u03a6-rank C-rank\nFig.3.Rank distribution of true feature values among all possible feature values\nas produced by the \u03a6-score (left) and the C-score (right); rank 1 is the highest\nscore in each data set, rank 0 the lowest score. Horizontally in each plot extends\nthe rank, vertically the normalized frequency of true feature valu es.\nThe result of the comparison comes not as a complete surprise. Som e kind of\nsurface-\ufb01tting procedure is necessary for estimation of the prin cipal curvatures\n[2, 3, 18]. If not a lot of care is invested, \ufb01tting to data will always be v ery\n13sensitive to outliers and artifacts, of which there are many in typica l stereo\ndata. A robust \ufb01tting procedure, however, has to incorporate in some way a\nstatistical model of the data that accounts for its outliers and ar tifacts. We here\npropose to go all the way to a statistical model, the point-relation d ensities, and\nto do without any \ufb01tting for local shape recognition.\n4.2 Object Segmentation and Recognition\nWe have tested segmentation and recognition performance of the proposed al-\ngorithm on two objects. One is a cube from which 8 smaller cubes are r emoved\nat its corners; the other is a subset of this object. These two obj ects can be\nassembled to create challenging segmentation and recognition scen arios.\nThe test data were taken from 50 scenes that contained the two o bjects to\nbe recognized, and additional distractor objects in some of them. A single data\nset consisted of roughly 5,000 to 10,000 3D points. As an acceptable accuracy\nfor recognition of an object\u2019s pose we considered what is needed fo r grasping\nwith a hand (robot or human), i.e., less than 3 degrees of angular and less than\n5 mm of translational misalignment. For a \ufb01xed set of parameters of the search\nalgorithm, in all but 3 cases the result was acceptable in this sense. P rocessing\ntimes ranged roughly between 1 and 50 seconds, staying mostly well below 5\nseconds, on a Sun UltraSPARC-III workstation (750 MHz).\nIn Figs. 4 and 5 we show examples of scenes, camera images, range d ata, and\nrecognition results. Edges drawn in the data outline the object pos e recognized\n\ufb01rst. The other object can be recognized after deletion of data p oints belonging\nto the \ufb01rst object.\n5 Conclusion\nWe have derived from a probabilistic perspective a new variant of hyp othesize-\nand-test algorithm for object recognition. A critical element of an y such algo-\nrithm is the ordered sequence of hypotheses that is tested. The o rdered sequence\nthat is generated by our algorithm is inferred from a statistical crit erion, here\ncalled the truncated probability (TP) of object parameters. As a key component\nof the truncated probability, we have introduced point-relation densities , from\nwhich we obtain an estimate of posterior probability for surface sha pe given\nrange data.\nOne of the strengths of the algorithm, as demonstrated in experim ents, is its\nveryhigh degreeofrobustnessto noise and artifactsin the data. Raw stereo-data\npoints of low quality are su\ufb03cient for many recognition tasks. Such d ata are fast\nand cheap to obtain and are intrinsically invariant to changes in illuminat ion.\nWe have argued in Sect. 3.1 that feature groups of size g= 3, i.e., minimal\ngroups, are a good choice for guiding the search for model matche s. However,\nif we can be sure that more than three true object features are r epresented in\nthe data, it may be worth considering larger groups. For g >3, the density\n(10) that enters the TP (8) looks more complicated. In particular, hash tables of\n14Top View\nFig.4.Example scene with the two objects we used for evaluation of segme n-\ntation and recognition performance. The smaller object is stacked on the larger\nas drawn near the center of the \ufb01gure. Shown are one of the came ra images and\nthree orthogonal views of the stereo-data set. The object pos e recognized \ufb01rst\nis outlined in the data. The length of both objects\u2019 longest edges is 1 3 cm. The\nrecognition time was 1.2 seconds.\n15Top View\nFig.5.Example scene with the two objects we used for evaluation of segme n-\ntation and recognition performance; cf. Fig. 4. The recognition tim e was 1.5\nseconds.\n16higher dimension are needed to accommodatethe groupinglaws,i.e., t he weights\n\u03b3(Gg). If we do without the grouping laws, using larger groups is equivalen t to\nhaving more than just the largest term of the density (6) contribu ting to the\nTP; cf. (7). In the limit of large feature groups, the approach the n resembles\ngeneralized-Hough-transform and pose-clustering techniques [1 , 17, 16].\nOne avenue of research that is suggested here is for generalizatio ns of point-\nrelation densities. Alternative representations of range data hav e to be explored\nfor learning posterior-probability estimates for various surface s hapes."}
{"category": "abstract", "text": ". The problem of searching for a model-based scene interpre-\ntation is analyzed within a probabilistic framework. Objec t models are\nformulated as generative models for range dataofthescene. Anewstatis-\ntical criterion, the truncated object probability, is intr oduced to infer an\noptimal sequence of object hypotheses to be evaluated for th eir match to\nthe data. The truncated probability is partly determined by prior knowl-\nedge of the objects and partly learned from data. Some experi ments on\nsequence quality and object segmentation and recognition f rom stereo\ndata are presented. The article recovers classic concepts f rom object\nrecognition (grouping, geometric hashing, alignment) fro m the proba-\nbilistic perspective and adds insight into the optimal orde ring of object\nhypotheses for evaluation. Moreover, it introduces point- relation densi-\nties, a key component of the truncated probability, as stati stical models\nof local surface shape.\nPublished in Proceedings European Conference on Computer Vision 2002 ,\nLecture Notes in Computer Science Vol. 2352, Springer, pp. 791\u20138 06.\n1 Introduction\nModel-based object recognition or, more generally, scene interpr etation can be\nconceptualized as a two-part process"}
{"category": "non-abstract", "text": "Camera calibration, Radial distortion, Geometric distort ion, Geometric undistortion.\nI. Introduction\nFor many computer vision applications, such as robot visual inspection and industrial metrology, where a\ncamera is used as a sensor in the system, the camera is usually assumed to be fully calibrated beforehand.\nCamera calibration is the estimation of a set of parameters t hat describes the camera\u2019s imaging process. With\nthis set of parameters, a perspective projection matrix can directly link a point in the 3-D world reference\nframe to its projection (undistorted) on the image plane. Th is is given by:\n\u03bb\uf8ee\n\uf8f0u\nv\n1\uf8f9\n\uf8fb=A[R|t]\uf8ee\n\uf8ef\uf8ef\uf8f0Xw\nYw\nZw\n1\uf8f9\n\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8f0\u03b1 \u03b3 u 0\n0\u03b2 v 0\n0 0 1\uf8f9\n\uf8fb\uf8ee\n\uf8f0Xc\nYc\nZc\uf8f9\n\uf8fb, (1)\nwhere ( u,v) is the distortion-free image point on the image plane. The m atrixAfully depends on the\ncamera\u2019s 5 intrinsic parameters ( \u03b1,\u03b3,\u03b2,u 0,v0), with ( \u03b1,\u03b2) being two scalars in the two image axes, ( u0,v0)\nthe coordinates of the principal point, and \u03b3describing the skewness of the two image axes. [ Xc,Yc,Zc]T\ndenotes a point in the camera frame that is related to the corr esponding point [ Xw,Yw,Zw]Tin the world\nreference frame by Pc=RPw+t, with ( R,t) being the rotation matrix and the translation vector.\nIn camera calibration, lens distortion is very important fo r accurate 3-D measurement [1]. The lens distortion\nintroduces certain amount of nonlinear distortions, denot ed by a function Fin Fig. 1, to the true image. The\nobserved distorted image thus needs to go through the invers e function F\u22121to output the corrected image.\nThat is, the goal of lens undistortion, or image correction, is to achieve an overall one-to-one mapping.\nAmong various nonlinear distortions, the radial distortio n, which is along the radial direction from the\ncenter of distortion, is the most severe part [2], [3]. The re moval or alleviation of radial distortion is commonly\nperformed by \ufb01rst applying a parametric radial distortion m odel, estimating the distortion coe\ufb03cients, and\nthen correcting the distortion. Most of the existing works o n the radial distortion models can be traced back\nto an early study in photogrammetry [4], where the radial dis tortion is governed by the following polynomial\nequation [4], [5], [6], [7]:\nrd=r+\u03b4r=r f(r,k) =r(1 +k1r2+k2r4+k3r6+\u00b7 \u00b7 \u00b7), (2)\nAll correspondence should be addressed to Dr. YangQuan Chen . Tel.: 1(435)797-0148, Fax: 1(435)797-3054, Email:\nyqchen@.ece.usu .edu . CSOIS URL: http://www.csois.usu.edu/2\n \n \n \n True Image \nCamera Distortion F \n Undistortion F -1 Observed Image  Corrected Image  \nFig. 1\nLens distortion and undistortion.\nwhich is equivalent to\nxd=xf(r,k), yd=y f(r,k), (3)\nwhere k= [k1,k2,k3,...] is a vector of distortion coe\ufb03cients.\nFor cameras whose distortion is not perfectly radially symm etric around the center of distortion (which\nis assumed to be at the principal point in our discussion), ra dial distortion modeling will not be accurate\nenough for applications such as precise visual metrology. I n this case, a more general distortion model, i.e.,\ngeometric distortion, needs to be considered. In this work, a simpli\ufb01ed geometric distortion modeling method\nis proposed, where two di\ufb00erent functions in the form of a var iety of polynomial and rational functions are\nused to model the distortions along the two image axes. The pr oposed simpli\ufb01ed geometric distortion models\nare simpler in structure than that considered in [8] where th e total geometric distortion consists of the radial\ndistortion and the decentering distortion. The geometric d istortion modeling method proposed is a lumped\ndistortion model that includes all the nonlinear distortio n e\ufb00ects.\nFor real time image processing applications, the property o f having analytical undistortion formulae is\na desirable feature for both the radial and the geometric dis tortion models. Though there are ways to\napproximate the undistortion without numerical iteration s, having analytical inverse formulae is advantageous\nby giving the exact inverse without introducing extra error sources. The key contribution of this paper is\nthe proposition of a family of simpli\ufb01ed geometric distorti on models that can achieve comparable calibration\naccuracy to that in [8] and better performance than their rad ial distortion modeling counterparts. For fairness,\nthese comparisons are based on the same (or reasonable) numb ers of distortion coe\ufb03cients. To preserve the\nproperty of having analytical inverse formulae with satisf actory calibration accuracy, a piecewise \ufb01tting idea\nis applied to the simpli\ufb01ed geometric modeling for two parti cular rational distortion functions presented in\nSec. II.\nThe rest of the paper is organized as follows. Sec. II summari zes some existing polynomial and rational\nradial distortion models that can also be applied to model th e geometric distortion. The simpli\ufb01ed geometric\ndistortion modeling method is proposed in Sec. III. Experim ental results and comparisons between the\nsimpli\ufb01ed geometric and the radial distortion models are il lustrated and discussed in Sec. IV. Finally, some\nconcluding remarks are given in Sec. V. The variables used th roughout this paper are listed in Table I.\nII. Polynomial and Rational Distortion Functions\nThe commonly used polynomial radial distortion model is giv en in the form of (2). In this paper, we\nconsider both the polynomial (functions # 1 ,2,3 in Table II) and rational radial distortion functions (fun ctions\n# 5,6,7,8,9,10 in Table II) [9], [10]. Clearly, all these functions in Tab le II, except the function #4, are special\ncases of the following radial distortion function having an alytical inverse formulae:\nf(r,\u03ba) =1 +\u03ba1r+\u03ba2r2\n1 +\u03ba3r+\u03ba4r2+\u03ba5r3. (4)3\nTABLE I\nList of Variables\nVariable Description\n(ud, vd)Distorted image point in pixel\n(u, v)Distortion-free image point in pixel\n(xd, yd)[xd, yd,1]T=A\u22121[ud, vd,1]T\n(x, y)[x, y,1]T=A\u22121[u, v,1]T\nrd r2\nd=x2\nd+y2\nd\nr r2=x2+y2\nk Distortion coe\ufb03cients (radial or geometric)\nFor example, when \u03ba1= 0,\u03ba5= 0, equation (4) becomes the function #10 in Table II with k1=\u03ba2,\nk2=\u03ba3, and k3=\u03ba4. Notionwise, k1,k2, and k3here correspond to their speci\ufb01c distortion function,\ni.e.,k1,k2, and k3do not have a global meaning. The function #4 in Table II is in t he form of (2) with\n2 distortion coe\ufb03cients, which is the most commonly used con ventional radial distortion function in the\npolynomial approximation category. The other 9 functions i n Table II are studied speci\ufb01cally with the goal\nto achieve comparable performance with the function #4 usin g the least amount of model complexity and as\nfew distortion coe\ufb03cients as possible. Since the functions #9 and #10 in Table II begin to show comparable\ncalibration performance to the function #4 (as can be seen la ter in Table III) [10], more complex distortion\nfunctions are not studied in this work.\nTABLE II\nPolynomial and Rational Distortion Functions\n#f(r,k) #f(r,k)\n11 +k1r 61/(1 +k1r2)\n21 +k1r27(1 +k1r)/(1 +k2r2)\n31 +k1r+k2r281/(1 +k1r+k2r2)\n41 +k1r2+k2r49(1 +k1r)/(1 +k2r+k3r2)\n51/(1 +k1r) 10(1 +k1r2)/(1 +k2r+k3r2)\nNotice that all the functions in Table II satisfy the followi ng properties:\n1)The function is radially symmetric around the center of dist ortion and is expressed in terms of the radius\nronly;\n2)The function is continuous and rd= 0 i\ufb00 r= 0;\n3)The approximation of xdis an odd function of x.\nThe above three properties act as the criteria to be a candida te for the radial distortion function. However,\nfor the general geometric distortion functions, which are n ot necessarily the same along the two image axes,\nthe \ufb01rst property does not need to be satis\ufb01ed, though the fun ctions need to be continuous such that there\nwill be no distortion only at the center of distortion.\nThe well-known radial distortion model (2) that describes t he laws governing the radial distortion does not\ninvolve a quadratic term. Thus, it might be unexpected to add one. However, when interpreting from the\nrelationship between ( xd,yd) and ( x,y) in the camera frame, the purpose of radial distortion funct ion is to\napproximate the xd\u2194xrelationship, which is intuitively an odd function. Adding a quadratic term to \u03b4r\ndoes not alter this fact as shown in [11]. As demonstrated in [ 11], it is reasonable to introduce a quadratic\nterm to \u03b4rto broaden the choice of radial distortion functions with a b etter calibration \ufb01t. Therefore, as\nlong as the above listed three properties are satis\ufb01ed, ther e should be no restriction in the form of \u03b4r. With\nthis argument in mind, we also proposed the rational radial d istortion models with analytical undistortion\nformulae as shown in Table II, with details presented in [10] .4\nTo compare the performance of the simpli\ufb01ed geometric disto rtion models with their radial distortion\ncounterparts, the calibration procedures presented in [5] are applied. In [5], the estimation of radial distortion\nis done after having estimated the intrinsic and the extrins ic parameters, just before the nonlinear optimization\nstep. So, for di\ufb00erent distortion models (radial or geometr ic), we can reuse the estimated intrinsic and extrinsic\nparameters. To compare the performance of di\ufb00erent distort ion models, the \ufb01nal value of optimization function\nJ, which is de\ufb01ned to be [5]:\nJ=N/summationdisplay\ni=1n/summationdisplay\nj=1/bardblmij\u2212\u02c6m(A,k,Ri,ti,Mj)/bardbl2, (5)\nis used, where \u02c6 m(A,k,Ri,ti,Mj) is the projection of point Mjin the ithimage using the estimated parameters,\nkdenotes the distortion coe\ufb03cients (radial or geometric), Mjis the jth3-D point in the world frame with\nZw= 0,nis the number of feature points in the coplanar calibration o bject, and Nis the number of images\ntaken for calibration.\nIII. Simplified Geometric Distortion Models\nA. Model\nA family of simpli\ufb01ed geometric distortion models is propos ed as\nxd=xf(r,k1), yd=y f(r,k2), (6)\nwhere the distortion function f(r,k) in (6) can be chosen to be, though not restricted to, any of th e functions\nin Table II. When k1=k2=k, the geometric distortion reduces to the radial distortion in equation (3).\nFrom (6), the relationship between ( ud,vd) and ( u,v) becomes\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3ud\u2212u0= (u\u2212u0)f(r,k1)\n+\u03b3/\u03b2(v\u2212v0)[f(r,k2)\u2212f(r,k1)]\nvd\u2212v0= (v\u2212v0)f(r,k2). (7)\nIf we de\ufb01ne /braceleftBigg\nud\u2212u0= (u\u2212u0)f(r,k1)\nvd\u2212v0= (v\u2212v0)f(r,k2), (8)\nthe relationship between ( xd,yd) and ( x,y) becomes\n/braceleftBigg\nxd=xf(r,k1) +\u03b3/\u03b1 y [f(r,k1)\u2212f(r,k2)]\nyd=y f(r,k2). (9)\nAfter nonlinear optimization, the \ufb01nal values of J, the intrinsic and extrinsic parameters, and the distortio n\ncoe\ufb03cients using the pair (6), (7) and (8), (9) are extremely close. Thus, in this paper, we only focus on the\npair (6), (7), while being aware that similar results can be a chieved using (8), (9).\nRemark III.1: The distortion models discussed in this paper belong to the c ategory of U ndistorted-D istorted\nmodel, while the D istorted-U ndistorted model also exists in the literature to correct di stortion [12]. The idea\nof simpli\ufb01ed geometric distortion modeling can be applied t o the D-U formulation simply by de\ufb01ning\nx=xdf(rd,\u02dck1), y=ydf(rd,\u02dck2). (10)\nConsistent improvement can be achieved in the above D-U form ulation.\nIn [8], the geometric distortion modeling when written in th e U-D formulation is presented as:\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3ud= \u00afu(1 +k1r2+k2r4+k3r6+\u00b7 \u00b7 \u00b7) +u0\n+(2p1\u00afu\u00afv+p2(r2+ 2\u00afu2))(1 + p3r2+\u00b7 \u00b7 \u00b7)\nvd= \u00afv(1 +k1r2+k2r4+k3r6+\u00b7 \u00b7 \u00b7) +v0\n+(p1(r2+ 2\u00afv2) + 2p2\u00afu\u00afv)(1 + p3r2+\u00b7 \u00b7 \u00b7), (11)5\nwhere \u00af u=u\u2212u0,\u00afv=v\u2212v0. The parameters ( k1,k2,k3) are the coe\ufb03cients for the radial distortion and\nthe parameters ( p1,p2,p3) are for the decentering distortion. Compared with (11), th e proposed simpli\ufb01ed\ngeometric distortion modeling (6), (7) is simpler in the str ucture and it is a lumped distortion model that\nincludes all the nonlinear distortion e\ufb00ects.\nRemark III.2: The two functions that model the distortion in the two image a xes are not necessarily of the\nsame form or structure. That is, equation (6) can be extended to have the following more general form\nxd=xfx(r,k1), yd=y fy(r,k2). (12)\nHowever, since we have no prior information as to how the dist ortions proceed along the two image axes,\nmodel (12) is not investigated in this work for lack of motiva tion. Of course, by choosing fx(r,k1) and\nfy(r,k2) di\ufb00erently, there is a chance to get an even better result at the expense of making more e\ufb00orts in\n\ufb01guring out what the best combination should be.\nB. Geometric Undistortion\nFor the simpli\ufb01ed geometric distortion model (6), the prope rty of having analytical undistortion formulae\nis not preserved for most of the functions in Table II as for th e radial distortion. However, when using the\nfunction #5 and #6 in Table II, the geometric undistortion ca n be performed analytically. For example, when\nf(r,k1) =1\n1 +k1r, f(r,k2) =1\n1 +k2r, (13)\nfrom (6), we have\nxd=x1\n1 +k1r, yd=y1\n1 +k2r. (14)\nThe geometric undistortion problem is to calculate ( x,y) from ( xd,yd) given the distortion coe\ufb03cients ( k1,k2)\nthat are determined through the nonlinear optimization pro cess. From equation (14), we have the following\nquadratic function of r\nx2\nd(1 +k1r)2+y2\nd(1 +k2r)2=r2, (15)\nwhose analytical solutions exist. The above quadratic func tion in rhas two analytical solutions, where one\nsolution can be discarded because it deviates from rddramatically. After ris derived, ( x,y) can be calculated\nfrom ( xd,yd) uniquely. In this way, the geometric undistortion using th e function #5 in Table II can be achieved\nnon-iteratively. For the function #6, a similar quadratic f unction in the form of x2\nd(1+k1\u00afr)2+y2\nd(1+k2\u00afr)2= \u00afr\ncan be derived with \u00af r=r2.\nC. Piecewise Geometric Distortion Models Using Functions #5and#6in Table II\nFor real time image processing applications, geometric dis tortion models with analytical undistortion for-\nmulae are very desirable for the exact inverse. When there is no analytical undistortion formula and to avoid\nperforming the undistortion via numerical iterations, the re are ways to approximate the undistortion, such as\nthe model described in [8] for the radial undistortion, wher ercan be calculated from rdby\nr=rdf(rd,\u2212k). (16)\nThe \ufb01tting results given by the above model can be satisfacto ry when the distortion coe\ufb03cients are small\nvalues. However, equation (16) itself introduces addition al error that will inevitably degrade the overall\ncalibration accuracy.\nThe appealing feature of having analytical geometric undis tortion formulae when using the functions #5\nand #6 in Table II may come with a price. The simple model struc ture may limit the \ufb01tting \ufb02exibility and\nhence the \ufb01tting accuracy. In this case, a piecewise \ufb01tting i dea can be applied to enhance accuracy of the\nsimpli\ufb01ed geometric distortion modeling, which is illustr ated in Fig. 2 with two segments.6\n \nFig. 2\nA piecewise continuous function (two-segment).\nWhen using the function #5 in Table II for each segment of f(r,k1) orf(r,k2), the two segments are of\nthe form \uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3g1(r) =1\n1 +\u00afk1r,forr\u2208[0,r1]\ng2(r) =1\na+\u00afk2r,forr\u2208(r1,r2], (17)\nwithr1=r2/2. To ensure that the overall function (17) is continuous acr oss the interior knot, the following\n3 constraints can be applied\n1\n1 +\u00afk1r1=g1,1\na+\u00afk2r1=g1,1\na+\u00afk2r2=g2, (18)\nwhere g1=g1(r1) =g2(r1) and g2=g2(r2). Since the coe\ufb03cients ( \u00afk1,a,\u00afk2) can be calculated from (18)\nuniquely by\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\u00afk1= (1/g1\u22121)/r1\n\u00afk2= (1/g2\u22121/g1)/(r2\u2212r1)\na= 1/g1\u2212\u00afk2r1, (19)\nthe geometric distortion coe\ufb03cients that are used in the non linear optimization can be chosen to be ( g1,g2)\nwith the initial values (1 ,1). During the nonlinear optimization process, ( \u00afk1,a,\u00afk2) are calculated from ( g1,g2)\nin each iteration. When using the function #6 in Table II, sim ilar functions to (18) and (19) can be derived by\nsubstituting ( r1,r2) with ( r2\n1,r2\n2). Furthermore, the piecewise idea can be easily extended to more segments.\nWhen applying the piecewise idea using the functions #5 and # 6 in Table II, better calibration accuracy\ncan be achieved yet the property of having analytical geomet ric undistortion formulae can be retained. The\nthe above feature is clearly at the expense of more segments, i.e., more distortion coe\ufb03cients to be searched\nin the optimization process.\nIV. Experimental Results and Discussions\nA. Comparison Between the Simpli\ufb01ed Geometric and the Radia l Distortion Models\nUsing three groups of test images (the public domain test ima ges [13], the desktop camera images [14]\n(a color camera), and the ODIS camera images [14], [15] (the c amera on the ODIS robot built at Utah\nState University [16])), the \ufb01nal values of Jof the simpli\ufb01ed geometric distortion model (6) after nonli near\noptimization by the Matlab function fminunc using the 10 functions in Table II are shown in Table III, wher e\nthe values of Jusing the same function but under the assumption of the radia l distortion are also listed for\ncomparison. In Table III, the numbers 1-10 in the \ufb01rst column denote the 10 functions in Table II in the same\norder. The extracted corners for the model plane of the deskt op and the ODIS cameras are shown in Figs. 37\nFig. 3\nFive images of the model plane with the extracted corners (indic ated by cross) for the desktop\ncamera.\nFig. 4\nFive images of the model plane with the extracted corners (indic ated by cross) for the ODIS\ncamera.\nand 4, where the plotted dots in the center of each square are o nly used for judging the correspondence with\nthe world reference points.\nFrom Table III, the values of Jof the simpli\ufb01ed geometric distortion models are generally smaller than those\nof the radial distortion models. The improvement for the pub lic and desktop cameras are not signi\ufb01cant, while\nit is signi\ufb01cant for the ODIS camera. However, the above comp arison between the simpli\ufb01ed geometric and the\nradial distortion models might not be fair since the geometr ic models have more coe\ufb03cients and it is evident\nthat each additional coe\ufb03cient in the model tends to decreas e the \ufb01tting residual. Due to this concern, the\nobjective function Jof the radial distortion modeling using 6 coe\ufb03cients in equa tion (2) (2 \u00d7the maximal\nnumber of coe\ufb03cients used in the geometric modeling) for the three groups of test images are also shown\nin the last row of Table III. The reason for choosing the radia l distortion model (2) with 6 coe\ufb03cients is\nthat this model is conventionally used and it is always among the best models in Table II giving the top\nperformance. Again, for the ODIS images, it is observed that the values of Jof the geometric modeling are\nall smaller than that of the radial distortion modeling usin g 6 coe\ufb03cients in (2), where the 6 coe\ufb03cients are\n(\u22120.3601,0.1801,\u22120.5149,3.1911,\u22126.4699,4.1625), except for the functions #1 ,2,5,6, which are relatively\nsimple in complexity and have fewer distortion coe\ufb03cients. It can thus be concluded that the distortion of\nthe ODIS camera is not as perfectly radially symmetric as the other two cameras and the geometric modeling\nis more appropriate for the ODIS camera.\nFigure 5 shows the undistorted image points for the third ima ge in Fig. 4 using the simpli\ufb01ed geometric\ndistortion model #4 and the radial distortion model (2) with 6 coe\ufb03cients. The di\ufb00erence between the\nundistorted image points using the above two models can be ob served at the image boundary (the enlarged\nplots of region 1and region 2are shown in Fig. 6), which is quite signi\ufb01cant for applicati ons that require\nsub-pixel accuracy.\nThe detailed estimated parameters using the 10 functions in Table II for the simpli\ufb01ed geometric and the\nradial distortion models are shown in Table IV using the ODIS images, where the 5 intrinsic parameters are\nalso listed for showing the consistency. Furthermore, the v alues of Jof the radial and geometric distortion\nmodels are plotted in Fig. 7 for the ODIS images, where the xaxis denotes the sorted model numbers in8\nTABLE III\nObjective Function Jof The Simplified Geometric and The Radial Distortions using the 10\nFunctions in Table II\nPublic Images Desktop Images ODIS Images#Geometric Radial Geometric Radial Geometric Radial\n1 180.4617 180.5713 999.6644 1016.7437 928.9073 944.4418\n2 148.2608 148.2788 904.0705 904.6796 913.1676 933.0981\n3 145.5766 145.6592 801.3148 803.3074 836.9277 851.2619\n4 144.8226 144.8802 777.3812 778.9767 825.8771 840.2649\n5 184.9429 185.0628 1175.7494 1201.8001 1019.8750 1036.6208\n6 146.9811 146.9999 797.9312 798.5720 851.6244 867.6192\n7 145.3864 145.4682 786.2204 787.6185 830.6345 845.0206\n8 145.3688 145.4504 784.8960 786.3590 829.4675 843.7991\n9 144.7560 144.8328 779.0693 780.9060 823.0736 837.9181\n10 144.7500 144.8256 777.9869 780.0391 823.2726 838.3245\n144.8179 776.7103 837.7749\n \n \n \nRegion 1 \nUndistorted image points for the third image in pixel \nDot: geometric, circle: radial \nRegion 2 \nFig. 5\nUndistorted image points for the third image in Fig. 4 using th e simplified geometric distortion\nmodel #4and the radial distortion model (2) with 6 coefficients.\nTable II in an order with Jdecreasing monotonously.\nFrom Table IV, it is observed that the distortion coe\ufb03cients kfor the radial distortion models always lie\nbetween the corresponding values of k1andk2for the simpli\ufb01ed geometric distortion models. Due to this\nreason, the resultant f(r,k) curves also lie between the f(r,k1) and f(r,k2) curves, which can be seen from\nFig. 8, where the f(r,k),f(r,k1), and f(r,k2) curves for the ODIS images using the third function in Table II\n(referred to as Model 3hereafter) are plotted as an example. It can thus be conclude d that when using one\nf(r,k), it tries to \ufb01nd a compromise between f(r,k1) and f(r,k2).\nFrom Fig. 8, the di\ufb00erence between f(r,k1) and f(r,k2) increases as rincreases, which is barely noticeable\natr= 0.1 but begins to be observable at r= 0.3. This information can also be seen from Fig. 9, where\nthe relationship between ( x,y) and ( xd,yd) is plotted for the ODIS images using Model 3. When using two\ndi\ufb00erent functions to model the distortion along the two ima ge axes, the distortion shown in Fig. 9 is not9\nTABLE IV\nComparisons Between The Simplified Geometric and The Radial D istortion Models for the ODIS\nImages\nDistortion # J k1 k2 \u03b1 \u03b3 u 0 \u03b2 v 0\n1928.9073 -0.2232 - - -0.2413 - - 272.5073 -0.0784 140.7238 268.7688 115.5717\n2913.1676 -0.2624 - - -0.2890 - - 256.7545 -0.4848 137.3176 252.4421 117.6516\n3836.9277 -0.1150 -0.1305 - -0.1206 -0.1454 - 264.5867 -0.3322 140.4929 260.2474 115.0102\nSimpli\ufb01ed 4825.8771 -0.3386 0.1512 - -0.3718 0.1756 - 259.4480 -0.2434 140.5699 255.2091 114.8777\nGeometric 51019.8750 0.2679 - - 0.2968 - - 275.9477 -0.0049 139.6337 272.7017 117.0080\nDistortion 6851.6244 0.3039 - - 0.3348 - - 258.0766 -0.3970 139.5523 253.7985 115.7800\n7830.6345 -0.0826 0.1964 - -0.0768 0.2320 - 263.1308 -0.3143 140.6762 258.3793 114.9656\n8829.4675 0.0736 0.2259 - 0.0685 0.2608 - 262.8587 -0.3068 140.7462 258.1623 114.9220\n9823.0736 0.9087 0.8695 0.5494 1.6571 1.4811 0.8974 259.5748 -0.2509 140.9331 251.9627 114.7501\n10823.2726 0.2719 0.0232 0.5950 0.6543 -0.0563 1.1524 260.8910 -0.2444 140.8209 253.8259 114.8106\n1944.4418 -0.2327 - - - - - 274.2660 -0.1153 140.3620 268.3070 114.3916\n2933.0981 -0.2752 - - - - - 258.3193 -0.5165 137.2150 252.6856 115.9302\n3851.2619 -0.1192 -0.1365 - - - - 266.0850 -0.3677 139.9198 260.3133 113.2412\n4840.2649 -0.3554 0.1633 - - - - 260.7658 -0.2741 140.0581 255.1489 113.1727\nRadial 51036.6208 0.2828 - - - - - 278.0218 -0.0289 139.5948 271.9274 116.2992\nDistortion 6867.6192 0.3190 - - - - - 259.4947 -0.4301 139.1252 253.8698 113.9611\n7845.0206 -0.0815 0.2119 - - - - 264.4038 -0.3505 140.0528 258.6809 113.1445\n8843.7991 0.0725 0.2419 - - - - 264.1341 -0.3429 140.1092 258.4206 113.1129\n9837.9181 1.2859 1.1839 0.7187 - - - 259.2880 -0.2824 140.2936 253.7043 113.0078\n10838.3245 0.4494 -0.0124 0.8540 - - - 260.9370 -0.2804 140.2437 255.3178 113.0561\n \nFig. 6\nEnlarged plot of the two regions in Fig. 5.\nexactly a smaller circle (since k1<0andk2<0), but a wide ellipse that is slightly shorter in the ydirection.\nRemark IV.1: Classical criteria that are used in the computer vision to as sess the accuracy of calibration\nincludes the radial distortion [17]. However, to our best kn owledge, there is not a systematically quantitative\nand universally accepted criterion in the literature for pe rformance comparisons among di\ufb00erent distortion\nmodels. Due to this lack of criterion, in our work, the compar ison is based on, but not restricted to, the \ufb01tting\nresidual of the full-scale nonlinear optimization in (5).\nB. Comparison Between the Simpli\ufb01ed Geometric and the Piece wise Geometric Distortion Models using Func-\ntions#5and#6in Table II\nUsing the ODIS images, the \ufb01nal values of Jfor the 1-segment, 2-segment, and 3-segment piecewise rati onal\ngeometric distortion models using the functions #5 and #6 in Table II are shown in Table V, where f(r,k1)10\n (5) \n(1) \n(2) \n(6) \n(3) (7) (8) (4) (10) (9) \nFig. 7\nObjective function Jof the simplified geometric and the radial distortion models f or the ODIS\nimages using the 10 functions in Table II (corresponding model numbers are shown in the text).\nandf(r,k2) have 1, 2, or 3 components depending on the number of segment s used. The maximal range of\nris listed in the last column for each case. From Table V, it is o bserved that the values of Jafter applying\nthe piecewise idea are always smaller than those using fewer segments. A careful comparison between the\nvalues of Jof the 3-segment piecewise geometric distortion modeling u sing the function #6 in Table V and\nthe simpli\ufb01ed geometric distortion models in Table III show s that the 3-segment piecewise geometric modeling\nusing the function #6 can have fairly good results. The piece wise idea is thus more suitable for applications\nthat require real time image undistortion.\nThe resulting estimated f(r,k1) and f(r,k2) curves of the 2-segment and 3-segment geometric distortio n\nmodels using the rational function #6 in Table II for the ODIS images are plotted in Figs. 10 and 11.\nOne issue in the implementation of the piecewise idea is how t o decide rmax, which is related to the estimated\nextrinsic parameters that are changing from iteration to it eration during the nonlinear optimization process.\nIn our implementation, for each camera, 5 images are taken. rmaxis chosen to be the maximum rof all the\nextracted feature points on the 5 images for each iteration.\nC. Comparison Between the Geometric Modeling Methods (6) an d (11)\nBoth using 6 coe\ufb03cients, the values of Jof the simpli\ufb01ed geometric distortion modeling method (6) u sing\nfunction f(r) = 1 + k1r2+k2r4+k3r6are shown in Table VI for the three groups of test images, wher eJof\nthe geometric modeling method (11) in [8] with the distortio n coe\ufb03cients ( k1,k2,k3,p1,p2,p3) is also listed\nfor comparison. From Table VI, it is observed that the simpli \ufb01ed geometric modeling method, though simpler\nin structure, does not necessarily give a less accurate cali bration performance.\nRemark IV.2: To make the results in this paper reproducible by other resea rchers for further investigation,\nwe present the options we use for the nonlinear optimization :options = optimset(\u2018Display\u2019, \u2018iter\u2019,\n\u2018LargeScale\u2019, \u2018off\u2019, \u2018MaxFunEvals\u2019, 8000, \u2018TolX\u2019, 10\u22125, \u2018TolFun\u2019, 10\u22125, \u2018MaxIter\u2019, 120) . The\nraw data of the extracted feature locations in the image plan e are also available [14].\nV. Concluding Remarks\nIn this paper, a family of simpli\ufb01ed geometric distortion mo dels are proposed that apply di\ufb00erent polynomial\nand rational functions along the two image axes. Experiment al results are presented to show that the proposed\nsimpli\ufb01ed geometric distortion modeling method can be more appropriate for cameras whose distortion is not\nperfectly radially symmetric around the center of distorti on. Analytical geometric undistortion is possible11\n f1(r) of Geometric Model  \nf2(r) of Geometric Model  \nf(r) of Radial Model \nFig. 8\nf(r,k1)andf(r,k2)vs.f(r,k)for the ODIS images using Model 3.\nusing two of the distortion functions discussed in this pape r and their performance can be improved by\napplying a piecewise idea.\nThe proposed simpli\ufb01ed geometric distortion modeling meth od is simpler than that in [8], where the nonlin-\near geometric distortion is further classi\ufb01ed into the radi al distortion and the decentering distortion and the\ntotal distortion is a sum of these distortion e\ufb00ects. Though simple in the structure, the simpli\ufb01ed geometric\ndistortion modeling gives comparable performance to that i n [8]. Furthermore, for some cameras, like the\nODIS camera studied here, the simpli\ufb01ed geometric distorti on modeling can even perform better.\nIn this paper, we are restricting the maximal number of disto rtion coe\ufb03cients considered to be 3 in all the\ndistortion functions in Table II, because it has also been fo und that too high an order may cause numerical\ninstability [3], [5], [18]. However, the appropriate numbe r of distortion coe\ufb03cients should not be determined\nonly by a numerical issue. A stronger argument should come fr om the relationship between Jand the number\nof distortion coe\ufb03cients. The appropriate number of distor tion coe\ufb03cients is chosen when the calibration\naccuracy does not show to have much improvement as the number of distortion coe\ufb03cients increases beyond\nthis value.\nThe comparison between the piecewise and the simpli\ufb01ed geom etric distortion models in Sec. IV-B brings\nup the question of preference between \u201cmore segments with lo w-complexity function\u201d or \u201cmore distortion\ncoe\ufb03cients with more complex function\u201d. The above question is not answered in this work and is a direction\nof future investigation."}
{"category": "abstract", "text": "The commonly used radial distortion model for camera calibr ation is in fact an assumption or a restriction. In\npractice, camera distortion could happen in a general geome trical manner that is not limited to the radial sense. This\npaper proposes a simpli\ufb01ed geometrical distortion modelin g method by using two di\ufb00erent radial distortion functions i n\nthe two image axes. A family of simpli\ufb01ed geometric distorti on models is proposed, which are either simple polynomials\nor the rational functions of polynomials. Analytical geome tric undistortion is possible using two of the distortion\nfunctions discussed in this paper and their performance can be improved by applying a piecewise \ufb01tting idea. Our\nexperimental results show that the geometrical distortion models always perform better than their radial distortion\ncounterparts. Furthermore, the proposed geometric modeli ng method is more appropriate for cameras whose distortion\nis not perfectly radially symmetric around the center of dis tortion.\nKey Words"}
{"category": "non-abstract", "text": "LULU, connection, separator, discrete pulse trans form, total vari-\nation.\n1 Introduction\nThe LULU operators and the associated Discrete Pulse Transf orm developed\nduring the last two decades or so are an important contributi on to the theory\nof the nonlinear multi-resolution analysis of sequences. T he basics of the\ntheory as well as the most signi\ufb01cant results until 2005 are p ublished in the\nmonograph [13]. For more recent developments and applicati ons see [1], [4],\n[7], [8], [14]. Central to the theory is the concept of separator . This concept is\nde\ufb01ned in [13] only for operators on sequences due to the cont ext of the book.\nHowever, it is meaningful in more general settings. Infact, some of the axioms\nhave been used earlier, e.g. see [18], for functions on arbit rary domains. We\nwill give the de\ufb01nition of separator for operators on real fu nctions de\ufb01ned on\na domain with a group structure.\nLet a \u2126 be an abelian group. Denote by A(\u2126) the vector lattice of all real\nfunctions de\ufb01ned on \u2126 with respect to the usual point-wise de \ufb01ned addition,\nscalar multiplication and partial order. For every a\u2208\u2126 the operator Ea:\nA(\u2126)\u2192 A(\u2126) given by\nEa(f)(x) =f(x+a), x\u2208\u2126,\nis called a shift operator.\n1De\ufb01nition 1 An operator P:A(\u2126)\u2192 A(\u2126)is called a separator if\n(i)P\u25e6Ea=Ea\u25e6P, a\u2208\u2126;\n(ii)P(f+c) =P(f)+c, f,c\u2208 A(\u2126), c- constant function ;\n(iii)P(\u03b1f) =\u03b1P(f), \u03b1\u2208R, \u03b1\u22650, f\u2208 A(\u2126);\n(iv)P\u25e6P=P; (Idempotence)\n(v) (id\u2212P)\u25e6(id\u2212P) =id\u2212P. (Co-idempotence)\nHereiddenotes the identity operator and the operator id\u2212Pis de\ufb01ned in\nterms of the point-wise linear operations for the operators onA(\u2126), that is,\n(id\u2212P)(f) =f\u2212P(f). The \ufb01rst two axioms in De\ufb01nition 1 and partially the\nthird one were \ufb01rst introduced as required properties of non linear smoothers\nby Mallows, [9]. Rohwer further made the concept of a smoothe r more precise\nby using the properties (i)\u2013(iii) as a de\ufb01nition of this conc ept. The axiom\n(iv) is an essential requirement for what is called a morphological \ufb01lter , [18],\n[19], [21]. In fact, a morphological \ufb01lter is exactly a synto ne operator which\nsatis\ufb01es (iv). Let us recall that an operator Pis called syntoneif\nf\u2264g=\u21d2P(f)\u2264P(g), f,g\u2208 A(\u2126).\nThe co-idempotence axiom (v) in De\ufb01nition 1 was introduced b y Rohwer in\n[13], where it is also shown that it is an essential requireme nt for operators\nextracting signal from a sequence.\nThe LULU theory was developed for sequences, that is, the cas e \u2126 =Z.\nGiven a bi-in\ufb01nite sequence \u03be= (\u03bei)i\u2208Zandn\u2208Nthe basic LULU operators\nLnandUnare de\ufb01ned as follows\n(Ln\u03be)i= max{min{\u03bei\u2212n,...,\u03bei},...,min{\u03bei,...,\u03bei+n}}, i\u2208Z.(1)\n(Un\u03be)i= min{max{\u03bei\u2212n,...,\u03bei},...,max{\u03bei,...,\u03bei+n}}, i\u2208Z.(2)\nIt is shown in [13] that for every n\u2208Nthe operators LnandUnas well\nas their compositions are syntone separators. Hence they ar e an appropriate\ntool for signal extraction. Furthermore, these operators f orm the so called\nstrong LULU semi-group. This a four element semi-group with respect to\ncomposition, see Table 1, which is fully ordered with respec t to the usual\npoint-wise de\ufb01ned order\nP\u2264Q\u21d0\u21d2P(f)\u2264Q(f), f\u2208 A(\u2126). (3)\nWe have\nLn\u2264Un\u25e6Ln\u2264Ln\u25e6Un\u2264Un. (4)\nLet us recall that, according to the well known theorem of Mat heron [10],\nin general, two ordered morphological operators generate a six element semi-\ngroup which is only partially ordered.\nThe power of the LULU operators as separators is further demo nstrated\nby their Total Variation Preservation property. Let BV(Z) be the set of\nsequences with bounded variation, that is,\nBV(Z) ={\u03be\u2208 A(Z) :/summationdisplay\ni\u2208Z|\u03bei\u2212\u03bei+1|<\u221e}.\nTotal Variation of a sequence \u03be\u2208BV(Z) is given by TV(\u03be) =/summationtext\ni\u2208Z|\u03bei\u2212\u03bei+1|.\n2LnUnUn\u25e6LnLn\u25e6Un\nLnLnLn\u25e6UnUn\u25e6LnLn\u25e6Un\nUnUn\u25e6LnUnUn\u25e6LnLn\u25e6Un\nUn\u25e6LnUn\u25e6LnLn\u25e6UnUn\u25e6LnLn\u25e6Un\nLn\u25e6UnUn\u25e6LnLn\u25e6UnUn\u25e6LnLn\u25e6Un\nTable 1: LULU semi-group\nDe\ufb01nition 2 An operator P:BV(Z)\u2192BV(Z)is called total variation\npreserving if\nTV(\u03be) =TV(P(\u03be))+TV((id\u2212P)(\u03be)), \u03be\u2208BV(Z). (5)\nWe should note that since TVis a semi-norm on BV(Z) we always have\nTV(\u03be)\u2264TV(P(\u03be))+TV((id\u2212P)(\u03be)).\nHence, the signi\ufb01cance of the equality (5) is that the decomp ositionf=\nA(f) +(id\u2212A)(f) does not create additional total variation. In particular ,\nthis property is very important for the application of the LU LU operators to\ndiscrete pulse decompositions of sequences.\nThe aim of this paper is to generalize the LULU operators to fu nctions on\nZdin such a way that their essential properties are preserved. In Section 2 the\nde\ufb01nitions of the basic operators LnandUnonA(Zd) are derived by using a\nstrengthened form of the morphological concept of connecti on. Then we show\nthat indeed these operators replicate the properties of the LULU operators\nfor sequence. More precisely, we prove that: (i) they are sep arators (Section\n2); (ii) their smoothing e\ufb00ect can be described in a similar wa y to the n-\nmonotonicity of sequences (Section 3); (iii) they generate a four element fully\nordered semi-group (Section 4). Thedeveloped theory can be applied to many\nproblems of Image Analysis and it is the intention of the auth ors to research\nsuch applications in thefuture. However, as an illustratio n anddemonstration\nof thepower of this approach we apply the newly de\ufb01nedoperat ors to deriving\na total variation preserving discrete pulse decomposition of images. Noise\nremoval and partial reconstructions are discussed in Secti on 6.\n2 The basic operators LnandUn.\nThe de\ufb01nition of the operators LnandUnfor sequences involves maxima and\nminima over sets of consecutive terms, thus, making an essen tial use of the\nfact that Zis totally ordered. Since Zd,d >1, is only partially ordered the\nconcept of \u2018consecutive\u2019 does not make sense in this setting . Instead, we use\nthe morphological concept of set connection, [19].\nDe\ufb01nition 3 LetBbe an arbitrary non-empty set. A family Cof subsets of\nBis called a connected class or a connection onBif\n(i)\u2205 \u2208 C\n3(ii){x} \u2208 Cfor allx\u2208B\n(iii) for any family {Ci:i\u2208 C} \u2286 C\n/intersectiondisplay\ni\u2208ICi/\\e}atio\\slash=\u2205=\u21d2/uniondisplay\ni\u2208ICi\u2208 C\nThis de\ufb01nition generalizes the topological concept of conn ectivity to arbitrary\nsets including discrete sets like Zd. If a set Cbelongs to a connection Cthen\nCis called connected .\nIt is clear from De\ufb01nition 3 that a connection on Zddoes not necessarily\ncontain sets of every size. For example, {\u2205}\u222a{{x}:x\u2208Zd}and\n{\u2205}\u222a{{x}:x\u2208Zd}\u222a{Zd}are connections on Zdbut neither of them con-\ntain sets of \ufb01nite size other than 0 and 1. In the de\ufb01nition of t he operators\nLnandUnwe need sets of every size. We assume that the set Zdis equipped\nwith a connection Cwhich satis\ufb01es the following three conditions\n\u2022Zd\u2208 C (6)\n\u2022For anya\u2208Zd,Ea(C)\u2208 Cwhenever C\u2208 C (7)\n\u2022IfV/subsetnoteqlW, V,W \u2208 C,then there exists x\u2208W\\V\nsuch that V\u222a{x} \u2208 C (8)\nThe aim of the conditions (6)\u2013(8) is to de\ufb01ne a connection whi ch is su\ufb03ciently\nrich in connected sets. This is demonstrated by the followin g property, which\nis obtained via iterative application of the property (8):\n\u2022LetV/subsetnoteqlW, V,W \u2208 C.For every k\u2208Nsuch that\ncard(V)< k <card(W) there exists S\u2208 C (9)\nsuch that V\u2286S\u2286Wand card( S) =k.\nAs usual, card( V) is the number of the elements in the set V, that is, the\nsize ofV. ForV\u2286Zdwe have card( V)\u2208N\u222a {0} \u222a {\u221e}. Given a point\nx\u2208Zdandn\u2208Nwe denote by Nn(x) the set of all connected sets of size\nn+1, which contain point x, that is,\nNn(x) ={V\u2208 C:x\u2208V,card(V) =n+1}. (10)\nNow the operators LnandUnare de\ufb01ned on A(Zd) as follows.\nDe\ufb01nition 4 Letf\u2208 A(Zd)andn\u2208N. Then\nLn(f)(x) = max\nV\u2208Nn(x)min\ny\u2208Vf(y), x\u2208Zd, (11)\nUn(f)(x) = min\nV\u2208Nn(x)max\ny\u2208Vf(y), x\u2208Zd. (12)\nLet us \ufb01rst see that De\ufb01nition 4 generalizes the de\ufb01nition of LnandUn\nfor sequences. Suppose d= 1 and let Cbe the connection on Zgenerated by\nthe pairs of consecutive numbers. Then all connected sets on Zare sequences\nof consecutive integers and for any i\u2208Zwe have\nNn(i) ={{i\u2212n,i\u2212n+1,...,i},{i\u2212n+1,i\u2212n+2,...,i+1},...,{i,i+1,...,i+n}}\nHence for an arbitrary sequence \u03beconsidered as a function on Zthe formulas\n(11) and (12) are reduced to (1) and (2), respectively.\n4Theorem 5 (Order Properties)\na)Ln\u2264id\u2264Un\nb)f\u2264g=\u21d2(Ln(f)\u2264Ln(g), Un(f)\u2264Un(g) )\nc)n1< n2=\u21d2(Ln1\u2265Ln2, Un1\u2264Un2)\nProof.We will only prove the inequalities involving Lnsince those involving\nUnare proved similarly.\na) Letf\u2208 A(Zd). For every x\u2208ZdandV\u2208 Nn(x) we have\nmin\ny\u2208Vf(y)\u2264f(x).\nHence\nLn(f)(x) = max\nV\u2208Nn(x)min\ny\u2208Vf(y)\u2264f(x), x\u2208Zd.\nTherefore, Ln(f)\u2264f,f\u2208 A(Zd), which implies Ln\u2264id.\nb) Letf\u2264g. For any x\u2208ZdandV\u2208 Nn(x), we have min\ny\u2208Vf(y)\u2264min\ny\u2208Vg(y).\nTherefore\nLn(f)(x) = max\nV\u2208Nn(p)min\ny\u2208Vf(y)\u2264max\nV\u2208Nn(p)min\ny\u2208Vg(y) =Ln(g)(x), x\u2208Zd.\nc) Letf\u2208 A(Zd). It follows from (10) that for every x\u2208ZdandV\u2208 Nn2(x)\nthere exists a set W\u2208 Nn1(x) such that W\u2286V. Therefore\nmin\ny\u2208Vf(y)\u2264min\ny\u2208Wf(y)\u2264max\nS\u2208Nn1(x)min\ny\u2208Sf(y) =Ln1(f)(x).\nHence\nLn2(f)(x) = max\nV\u2208Nn2(x)min\ny\u2208Vf(y)\u2264Ln1(f)(x),x\u2208Zd.\nTheorem 6 For any n\u2208Nthe operators LnandUnare separators.\nProof.We will only verify the conditions (i)\u2013(v) in De\ufb01nition 1 for Lnsince\nUnis dealt with in a similar manner.\n(i) Leta\u2208Zdandf\u2208 A(Zd). Using the property (7), for every x\u2208Zdwe\nhave\nNn(x+a) =a+Nn(x) ={a+V:V\u2208 Nn(x)}\nTherefore,\nEa(Ln(f))(x) =Ln(f)(x+a) = max\nV\u2208Nn(x+a)min\ny\u2208Vf(y)\n= max\nV\u2208Nn(x)min\ny\u2208a+Vf(y) = max\nV\u2208Nn(x)min\ny\u2208Vf(y+a)\n= max\nV\u2208Nn(x)min\ny\u2208VEa(f)(y), x\u2208Zd\n(ii) Letf,c\u2208 A(Zd), where cis a constant function with a value of \u03b8. Then\nfor every x\u2208Zdwe have\nLn(f+c)(x) = max\nV\u2208Nn(x)min\ny\u2208V(f+c)(y) = max\nV\u2208Nn(x)min\ny\u2208V(f(y)+\u03b8)\n=/parenleftbigg\nmax\nV\u2208Nn(x)min\ny\u2208Vf(y)/parenrightbigg\n+\u03b8=Ln(f)(x)+c(x)\n5(iii) Letf\u2208 A(Zd) and\u03b1\u2208R,\u03b1\u22650. For every x\u2208Zdwe have\nLn(\u03b1f)(x) = max\nV\u2208Nn(x)min\nq\u2208V(\u03b1f)(y) =\u03b1/parenleftbigg\nmax\nV\u2208Nn(x)min\nq\u2208Vf(y)/parenrightbigg\n=\u03b1Ln(f)(x).\n(iv) The inequality\nLn\u25e6Ln\u2264Ln\nis an immediate consequence of Theorem 5. Then it is su\ufb03cient to prove the\ninverse inequality. Let f\u2208 A(Zd) andx\u2208Zd. We have\nLn(Ln(f))(x) = max\nW\u2208Nn(x)min\ny\u2208Wmax\nV\u2208Nn(y)min\nz\u2208Vf(z). (13)\nButy\u2208W\u2208 Nn(x) implies W\u2208 Nn(y). Therefore for every W\u2208 Nn(x) and\ny\u2208Wwe have\nmax\nV\u2208Nn(y)min\nz\u2208Vf(z)\u2265min\nz\u2208Wf(z).\nUsing that the right hand side is independent of ywe further obtain\nmin\ny\u2208Wmax\nV\u2208Nn(y)min\nz\u2208V\u2265min\nz\u2208Wf(z), W\u2208 Nn(x).\nThen it follows from the representation (13) that\nLn(Ln(f))(x)\u2265max\nW\u2208Nn(x)min\nz\u2208Wf(z) =Ln(f)(x).\n(v) The co-idempotence of the operator Lnis equivalent to Ln\u25e6(id\u2212Ln) = 0.\nThe inequality Ln\u25e6(id\u2212Ln)\u22650 is an easy consequence of Theorem 5. Hence,\nfor the co-idempotence of Lnit remains to show that Ln\u25e6(id\u2212Ln)\u22640.\nAssume the opposite. Namely, there exists a function f\u2208 A(Zd) andx\u2208Zd\nsuch that ( Ln\u25e6(id\u2212Ln))(f)(x)>0. Using the de\ufb01nition of Lnthis inequality\nimplies that there exists V\u2208 Nn(x) such that for every y\u2208Vwe have\n(id\u2212Ln)(f)(z)>0, or equivalently\nf(y)> Ln(f)(y), y\u2208V. (14)\nLetz\u2208Vbe such that f(z) = min\nt\u2208Vf(t). Then for every y\u2208Vwe have\nLn(f)(y) = max\nW\u2208Nn(y)min\nt\u2208Wf(t)\u2265min\nt\u2208Vf(t)\u2265f(z). (15)\nTakingy=zin (14) and (15) we obtain a contradiction which completes th e\nproof.\n3 The operators LnandUnas smoothers\nSimilar to their counterparts for sequences the operators LnandUnde\ufb01ned in\nSection 2 smooth the inputfunction by removing sharp peaks ( the application\nofLn) and deep pits (the application of Un). The smoothing e\ufb00ect of these\noperations is made more precise by using the concepts of a loc al maximum\nset and a local minimum set given below.\n6De\ufb01nition 7 LetV\u2208 C. A point xis called adjacent toVifV\u222a{x} \u2208 C.\nThe set of all points adjacent to Vis denoted by adj(V), that is,\nadj(V) ={x\u2208Zd:V\u222a{x} \u2208 C}.\nAn equivalent formulation of the property (8) of the connect ionCis as\nfollows:\nV,W\u2208 C, W/subsetnoteqlV=\u21d2adj(W)\u2229V/\\e}atio\\slash=\u2205. (16)\nDe\ufb01nition 8 A connected subset VofZdis called a local maximum set\noff\u2208 A(Zd)if\nmax\ny\u2208adj(V)f(y)<min\nx\u2208Vf(x).\nSimilarly Vis alocal minimum set if\nmin\ny\u2208adj(V)f(y)>max\nx\u2208Vf(x).\nThe next four theorems deal with di\ufb00erent aspects of the appli cation of\nLnandUnto functions in A(Zd). Their cumulative e\ufb00ect will be discussed\nat the end of the section. All theorems contain statements a) and b). Due to\nthe similarity we present only the proofs of a).\nTheorem 9 Letf\u2208 A(Zd)andx\u2208Zd. Then we have\na)Ln(f)(x)< f(x)if and only if there exists a local maximum set Vsuch\nthatx\u2208Vandcard(V)\u2264n;\nb)Un(f)(x)> f(x)if and only if there exists local minimum set Vsuch that\nx\u2208Vandcard(V)\u2264n.\nProof.a) Implication to the left. Suppose that there exists a local maximum\nsetV\u2208 Nk(x),k < n. Consider an arbitrary W\u2208 Nn(x) and let S=W\u2229V.\nThen, since the size of Wis larger than the size of Swe have W\\S/\\e}atio\\slash=\u2205.\nFurthermore, by (16) we have adj( S)\u2229W/\\e}atio\\slash=\u2205. Letz\u2208adj(S)\u2229W. Since\nadj(S)\u2229W\u2286W\\S=W\\V, we have that z /\u2208Vbutz\u2208adj(V).Then\nusing also that Vis a local maximum set we obtain\nmin\ny\u2208Wf(y)\u2264f(z)<min\nt\u2208Vf(t)\u2264f(x).\nSince the set W\u2208 Nn(p) is arbitrary, this inequality implies that Ln(f)(x)<\nf(x).\nImplication to the right. Suppose Ln(f)(x)< f(x). LetVbe the largest (in\nterms of \u2286) connected set containing xsuch that\nf(y)\u2265f(x), y\u2208V. (17)\nThe setVis obviously unique and can be constructed as V=\u03b3x(Y), where \u03b3x\nis the morphological point connected opening generated by x, see [19] or [20],\nandY={y\u2208Zd:f(y)\u2265f(x)}. We have f(z)< f(x),z\u2208adj(V), because\notherwise (17) is satis\ufb01ed on the larger connected set {z}\u2229V. Therefore\nmax\nz\u2208adj(V)f(z)< f(x) = min\ny\u2208Vf(y).\n7HenceVis a local maximum set.\nAssume that card( V)> n. It follows from (10) that there exists W\u2208\nNn(x) such that W\u2282V. Then\nLn(f)(x) = max\nV\u2208Nn(x)min\ny\u2208Vf(y)\u2265min\ny\u2208Wf(y)\u2265min\ny\u2208Vf(y) =f(x).\nThis contradicts the assumption Ln(f)(x)< f(x). Therefore, card( V)\u2264n.\nTheorem 10 Letf\u2208 A(Zd). Then\na) the size of any local maximum set of the function Ln(f)is larger than n;\nb) the size of any local minimum set of the function Un(f)is larger than n.\nProof.a) Assume the opposite, that is, there exists a local maximum setV\nofLn(f) such that card( U)\u2264n. By Theorem 9 we have that\nLn(Ln(f))(x)< Ln(f)(x), x\u2208V.\nSinceLnis idempotent, see Theorem 6, this implies the impossible in equality\nLn(f)(x)< Ln(f)(x), which completes the proof.\nTheorem 11 LetV\u2208 Cand letx\u2208adj(V).\na) Iff(x)\u2264min\ny\u2208Vf(y)thenLn(f)(x)\u2264min\ny\u2208VLn(f)(y);\nb) Iff(x)\u2265max\ny\u2208Vf(y)thenUn(f)(x)\u2265max\ny\u2208VUn(f)(y).\nProof. a) For any W\u2208 Nn(x) the set W\u222aVis connected and of size\nlarger than n+1. Therefore, by (10), for every y\u2208Vthere exists Sy\u2208 Nn(y)\nsuch that Sy\u2282W\u222aV. Then, using also the given inequality, for every y\u2208V\nandW\u2208 Nn(q) we have\nmin\nz\u2208Wf(z) = min\nz\u2208W\u222aVf(z)\u2264min\nz\u2208Syf(z)\u2264Ln(f)(y).\nHence\nLn(f)(x) = max\nW\u2208Nn(x)min\nz\u2208Wf(z)\u2264min\ny\u2208VLn(f)(y).\nTheorem 12 Letf\u2208 A(Zd)andV\u2208 C.\na) IfVis a local minimum set of Ln(f)then there exists a local minimum set\nWoffsuch that W\u2286V.\nb) IfVis a local maximum set of Un(f)then there exists a local maximum\nsetWoffsuch that W\u2286V.\nProof.a) LetVbe a local minimum set of Ln(f). Then\nmin\ny\u2208adj(V)f(y)\u2265min\ny\u2208adj(V)Ln(f)(y)> Ln(f)(x)\u2200x\u2208V.\nLetq\u2208adj(V) be such that f(q) = min\ny\u2208adj(V)f(y) and let\nY={y\u2208V:f(y)< f(q)}.\n8An easy application of Theorem 11 shows that Y/\\e}atio\\slash=\u2205. Lett\u2208Yand letWbe\nthe largest (with respect to inclusion) connected subset of Ywhich contains t.\nAs in the proof of Theorem 9, the set Wcan be obtained through W=\u03b3t(Y).\nFor every z\u2208adj(W) we have f(z)\u2265f(q)>maxy\u2208Wf(y). Therefore Wis a\nlocal minimum set of f.\nTheorems 9\u201312 provide the following characterization of th e e\ufb00ect of the\noperators LnandUnof a function f\u2208 A(Zd):\n\u2022The application of Ln(Un) removes local maximum (minimum) sets of\nsize smaller or equal to n.\n\u2022The operator Ln(Un) does not a\ufb00ect the local minimum (maximum)\nsets in the sense that such sets may be a\ufb00ected only as a result o f the\nremoval of local maximum (minimum) sets. However, no new loc al min-\nimum sets are created where there were none. This does not exc lude the\npossibility that the action of Ln(Un) may enlarge existing local max-\nimum (minimum) sets or join two or more local maximum (minimu m)\nsets offinto one local maximum (minimum) set of Ln(f) (Un(f)).\n\u2022Ln(f) =f(Un(f) =f) if and only if fdoes not have local maximum\n(minimum) sets of size nor less;\nFurthermore, as an immediate consequence of Theorem 10 and T heorem\n12 we obtain the following corollary.\nCorollary 13 For every f\u2208 A(Zd)the functions (Ln\u25e6Un)(f)and(Un\u25e6\nLn)(f)have neither local maximum sets nor local minimum sets of siz enor\nless. Furthermore,\n(Ln\u25e6Un)(f) = (Un\u25e6Ln)(f) =f\nif and only if fdoes not have local maximum sets or local minimum sets of\nsize less than or equal to n.\nWe should remark that in the one dimensional setting, the seq uences with-\nout local maximum sets or local minimum sets of size less than or equal to\nnare exactly the so-called n-monotone sequences. Hence Corollary 13 gener-\nalizes the respective results in the LULU theory of sequence s, [13, Theorem\n3.3].\n4 The LULU semi-group\nIn this section we consider the operators Ln,Unand their compositions. The\nmain result is that Ln,Un,Ln\u25e6UnandUn\u25e6Lnform a semi-group with respect\nto composition with a composition table as given in Table 1. F urthermore, the\nsemi-group is totaly ordered as in (4) with respect to the poi nt-wise de\ufb01ned\npartial order (3).\nTheorem 14 The operators Ln\u25e6UnandUn\u25e6Lnare idempotent, that is,\nLn\u25e6Un\u25e6Ln\u25e6Un=Ln\u25e6Un, (18)\nUn\u25e6Ln\u25e6Un\u25e6Ln=Un\u25e6Ln. (19)\n9Proof.Using the order properties in Theorem 5 and the idempotence o fLn\nandUn, see Theorem 6, we have\nLn\u25e6Un\u25e6Ln\u25e6Un\u2264Ln\u25e6Un\u25e6id\u25e6Un=Ln\u25e6Un\u25e6Un=Ln\u25e6Un\nLn\u25e6Un\u25e6Ln\u25e6Un\u2265Ln\u25e6id\u25e6Ln\u25e6Un=Ln\u25e6Ln\u25e6Un=Ln\u25e6Un\nwhich implies (18). The equality (19) is proved similarly.\nTheorem 15 For any n\u2208Nwe have\nLn\u25e6Un\u25e6Ln=Un\u25e6Ln. (20)\nProof.It follows from Theorem 5 that\nLn\u25e6Un\u25e6Ln\u2264id\u25e6Un\u25e6Ln=Un\u25e6Ln. (21)\nAssume that (20) is violated. In view of (21), this means that there exists\nf\u2208 A(Zd) andz\u2208Zdsuch that\nLn(Un(Ln(f)))(z)< Un(Ln(f))(z).\nIt follows from Theorem 9 that there exists k\u2264nandV\u2208 Nk(z) such that V\nis a local maximum set for Un(Ln(f))(z). Then, by Theorem 12, there exists\nW\u2286Vsuch that Wis a local maximum set of the function Ln(f). We have\ncard(W)\u2264k\u2264n. However, Ln(f) does not have any local maximum sets\nof size less than or equal to n, see Theorem 10. This contradiction completes\nthe proof.\nAs in the case of sequences, the key result for the set\n{Ln, Un, Ln\u25e6Un, Un\u25e6Ln} (22)\nto be closed under composition is the equality in Theorem 15. Now one can\neasily derive the rest of the formulas for the compositions o f the operators in\nthis set. The composition table is indeed as given in Table 1. Furthermore,\nTheorem 15 implies the total order on the set (22) as in (4). In deed, we have\nLn=id\u25e6Ln\u2264Un\u25e6Ln=Ln\u25e6Un\u25e6Ln\u2264Ln\u25e6Un\u25e6id=Ln\u25e6Un\u2264id\u25e6Un=Un\nTherefore, the operators LnandUnfor functions on Zdgenerate via composi-\ntion a semi-group with exactly the same algebraic and order s tructure as the\nsemi-group generated by the operators LnandUnfor sequences.\n5 Discrete pulse transform of images\nIn this section we apply the LULU operators de\ufb01ned and invest igated in the\npreceding sections to derive a discrete pulse decompositio n of images. A\ngrayscale imageisgiven throughafunction fonarectangular domain\u2126 \u2282Z2,\nthe value of fbeing the luminosity at the respective pixel. For the theore tical\nstudy it is more convenient to assume that the functions are d e\ufb01ned on the\nwhole space Z2. To this end one can, for example, de\ufb01ne fon the set Z2\\\u2126\nas a constant, e.g. 0. Hence we consider the set A(Z2).\n10ij\nij\nij\nFigure 1: Neighbors of ( i,j)\nAppropriate connections for images are de\ufb01ned through a rel ationronZ2\nre\ufb02ecting what we consider neighbors of a pixel in the given c ontext. Figure\n1 gives some examples of the the neighbors of the pixel ( i,j).\nWe call a set C\u2286R2connected if for any two pixels p,q\u2208Cthere exists\na set of pixels {p1,p2,...,pk} \u2286Csuch that each pixel is neighbor to the next\none,pis neighbor to p1andpkis neighbor to q. We assume that the neighbor\nrelationronZ2is such that\n\u2022ris re\ufb02exive, symmetric and shift invariant (23)\n\u2022((i,j),(i\u00b11,j))\u2208rand ((i,j),(i,j\u00b11))\u2208r,for alli,j\u2208Z.(24)\nTheconditions(23)\u2013(24)ensurethatthesetofconnectedse tCde\ufb01nedthrough\nthis relation is a connection in terms of De\ufb01nition 3 and sati s\ufb01es the condi-\ntions (6)\u2013(8). Hence we can apply the operators LnandUndiscussed in the\npreceding sections to functions on Z2. Similar to the case of sequences we\nobtain a decomposition of a function f\u2208 A(Z2) by applying iteratively the\noperators Ln,Unwithnincreasing from 1 to \u221e. This can be done in dif-\nferent ways depending on sequencing of the LnandUn. Since this section is\nintended as a demonstration rather than presenting a compre hensive discrete\npulse transform theory, we will take one particular case whe nUnfollowsLn.\nDe\ufb01ne the operators Fn,n\u2208N, byF1=U1\u25e6L1andFn=Un\u25e6Ln\u25e6Fn\u22121.\nThen for any f\u2208 A(Z2) andm\u22651 we have\nf= (id\u2212U1\u25e6L1)(f)+((id\u2212U2\u25e6L2)\u25e6F1)(f)+((id\u2212U3\u25e6L3)\u25e6F2)(f)\n+...+((id\u2212Um\u25e6Lm)\u25e6Fm\u22121)(f)+Fm(f) (25)\nDe\ufb01nition 16 A function \u03c6\u2208 A(Z2)is called a pulse if there exist a con-\nnected set Vand a real number \u03b1such that\n\u03c6(x) =/braceleftbigg\u03b1ifx\u2208V\n0 ifx\u2208Z2\\V.\nThe setVis called support of the pulse \u03c6and is denoted by supp(\u03c6).\nFigure 2 gives an example of a pulse. It should be remarked tha t the\nsupport of a pulse may generally have any shape, the only rest riction being\nthat it is connected.\nThe usefulness of the representation (25) of a function f\u2208 A(Z2) is in the\nfact that all terms are sums of pulses as stated in the next the orem.\n11       \nFigure 2: Pulse\nTheorem 17 Letf\u2208 A(Z2).\na) For every n\u2208Nthe function ((id\u2212Un\u25e6Ln)\u25e6Fn\u22121(f)is a sum of discrete\npulses with disjoint support, that is, there exist \u03b3(n)\u2208Nand discrete\npulses\u03c6ns,s= 1,...,\u03b3(n), such that\n((id\u2212Un\u25e6Ln)\u25e6Fn\u22121)(f) =\u03b3(n)/summationdisplay\ns=1\u03c6ns (26)\nand\nsupp(\u03c6ns1)\u2229supp(\u03c6ns2) =\u2205fors1/\\e}atio\\slash=s2. (27)\nb) Letn1,n2,s1,s2\u2208Nbe such that n1< n2,1\u2264s1\u2264\u03b3(n1)and1\u2264s2\u2264\n\u03b3(n2). Then\nsupp(\u03c6n1s1)\u2229supp(\u03c6n2s2)/\\e}atio\\slash=\u2205=\u21d2supp(\u03c6n1s1)\u2282supp(\u03c6n2s2) (28)\nProof.a) Denote g=Fn\u22121(f). We have\n((id\u2212Un\u25e6Ln)\u25e6Fn\u22121)(f) = (id\u2212Ln)(g)+(id\u2212Un)(Ln(g)),(29)\nwhere the \ufb01rst term in the sum on the right hand side is nonnega tive while\nthe second one is nonpositive. Let x\u2208Z2be such that (( id\u2212Ln)(g)>0. It\nfollows from Theorem 9 that there exists a local maximum set Vofgsuch\nthatx\u2208Vand card( V)\u2264n. Sinceg= (Un\u22121\u25e6Ln\u22121)(Fn\u22122(f)) does not have\nlocal maximum set of size smaller than n, see Corollary 13, this implies that\ncard(V) =nand that gis a constant on V. Furthermore, (( id\u2212Ln)(g)(y) = 0\nfory\u2208adj(V). Indeed, if (( id\u2212Ln)(g)(y)>0 for some y\u2208adj(V), then\nybelongs to a local maximum set Wofgand card( W)\u2264n. However, any\nmaximum set containing ymust contain Vas well which implies card( W)\u2265\nn+1, a contradiction. In this way we obtain that the support of (id\u2212Ln)(g)\nis a union of disjoint connected sets of size n, that is,\nsupp((id\u2212Ln)(g)) =V1\u222aV2\u222a...\u222aV\u03b31(n),\n12whereVs\u2208 C, card(Vs) =n,s= 1,...,\u03b31(n) andVs1\u2229Vs2=\u2205fors1/\\e}atio\\slash=s2.\nFurthermore, ( id\u2212Ln)(g) is a constant on each set Vs. If (id\u2212Ln)(g)(x) =\u03b1s\nforx\u2208Vs, then\n(id\u2212Ln)(g) =\u03b31(n)/summationdisplay\ns=1\u03c6ns, (30)\nwhere\n\u03c6ns(x) =/braceleftbigg\u03b1sifx\u2208Vs\n0 if x\u2208Z2\\Vs\nApplying the same approach to the second term in (29) we obtai n\nsupp((id\u2212Ln)(g)) =W1\u222aW2\u222a...\u222aW\u03b32(n),\nwhereWs\u2208 C, card(Ws) =n,s= 1,...,\u03b32(n),Ws1\u2229Ws2=\u2205fors1/\\e}atio\\slash=s2and\n(id\u2212Un)(Ln(g)) =\u03b3(n)/summationdisplay\ns=\u03b31(n)+1\u03c6ns, (31)\nwhere\u03b3(n) =\u03b31(n)+\u03b32(n) and supp( \u03c6ns) =Ws\u2212\u03b31(n),s=\u03b31(n)+1,...,\u03b3(n).\nNote that \u03c6ns,s= 1,...,\u03b31(n), are upward (positive) pulses while \u03c6ns,s=\n\u03b31(n) + 1,...,\u03b3(n) are downward (negative) pulses. We obtain (26) by sub-\nstituting (30) and (31) in (29). It only remains to show that Vs1\u2229Ws2=\u2205\nfor alls1= 1,...,\u03b31(n),s2= 1,...,\u03b32(n). Indeed, assume that Vs1\u2229Ws2/\\e}atio\\slash=\u2205.\nFrom the de\ufb01nition of the operator Ln, there exists y\u2208adj(Vs1) such that\nLn(g)(x) =g(y) forx\u2208Vs1\u2229{y}. Therefore, Vs1\u2229{y} \u2286Ws2, which implies\nthat card( Ws2)\u2265n+1. Since the size of each one of the sets W1,...W\u03b32(n)is\nn, none of them intersects Vs1.\nb) Let supp( \u03c6n1s1)\u2229supp(\u03c6n2s2)/\\e}atio\\slash=\u2205. It follows from the construction\nof (26) derived in a) that the functions Fn(f) andLn+1(Fn(f)),n\u2265n1, are\nconstants on the set supp( \u03c6n1s1). Furthermore, the set supp( \u03c6n2s2) is a local\nmaximum set of Fn2\u22121(f) or a local minimum set of Ln2(Fn2\u22121(f)). From\nthe de\ufb01nition of local maximum set and local minimum set it fo llows that\nsupp(\u03c6n1s1)\u2282supp(\u03c6n2s2).\nUsing Theorem 17, the equality (25) can be written in the form\nf=m/summationdisplay\nk=1\u03b3(k)/summationdisplay\ns=1\u03c6ks+Fm(f). (32)\nIf the function fhas \ufb01nite support, e.g. as in the case of images, then Fm(f)\nis a constant for a su\ufb03ciently large m. Then we have\nf=m/summationdisplay\nk=1\u03b3(k)/summationdisplay\ns=1\u03c6ks+c, (33)\nwherec=Fm(f)(x),x\u2208Z2. The equality (33) is a discrete pulse decom-\nposition of f, where the pulses have the properties (27)\u2013(28). It is gener ally\naccepted that an image is perceived through the contrast, th at is, the di\ufb00er-\nence in the luminosity of neighbor pixels. The discrete puls e transform (33)\nextracts all such di\ufb00erences as single pulses. Hence, (33) ca n be a useful tool\n13in the analysis of images. Since the information in an image i s in the con-\ntrast, the total variation of the luminosity function is an i mportant measureof\nthe quantity of this information. Image recovery and noise r emoval via total\nvariation minimization are discussed in [3] and [16]. It sho uld be noted that\nthere are several de\ufb01nition of total variation of functions of multi-dimensional\nargument (Arzel variation, Vitali variation, Pierpont var iation, Hardy varia-\ntion, etc.). In the applications cited above the total varia tion is the L1norm\nof a vector norm of the gradient of the function. Here we consi der a discrete\nanalogue of this concept.\nDe\ufb01nition 18 Letf\u2208 A(Z2). If\n\u221e/summationdisplay\ni=\u2212\u221e\u221e/summationdisplay\nj=\u2212\u221e(|f(i+1,j)\u2212f(i,j)|+|f(i,j+1)\u2212f(i,j)|)<\u221e(34)\nthenfis said to be of bounded variation. The sum on the left side on th e\ninequality (34) is called total variation of fand is denoted by TV(f).\nAs mentioned in the introduction, the LULU operators for seq uences are\ntotal variation preserving. We show here that their two-dim ensional counter-\nparts considered in this section have the same property with respect to the\ntotal variation as given in De\ufb01nition 18.\nLet us denote by BV(Z2) the set of all functions of bounded variation in\nA(Z2). Clearly, all functions of \ufb01nite support are in BV(Z2). In particular,\nthe luminosity functions of images are in BV(Z2). The total variation given\nin De\ufb01nition 18 is a semi-norm on BV(Z2). In particular, this implies that\nTV(f+g)\u2264TV(f)+TV(g). (35)\nThe total preservation property is de\ufb01ned for operators on BV(Z2) as in\nDe\ufb01nition 2, where Zis replaced by Z2.\nTheorem 19 The operators Ln,Un, n=1,2,..., and their compositions are\nall total variation preserving.\nProof.Letf\u2208BV(Z2) and (i,j)\u2208Z2. We will show that\n|f(i,j)\u2212f(i+1,j)|=|Ln(f)(i,j)\u2212Ln(f)(i+1,j)|\n+|(id\u2212Ln)(f)(i,j)\u2212(id\u2212Ln)(f)(i+1,j)|(36)\nCase 1.Ln(f)(i,j)< f(i,j). In follows from Theorem 9 that there exists\na local maximum set Vsuch that ( i,j)\u2208Vand card( V)\u2264n. Without\nloss of generality we may assume that Vis the largest set with the said\nproperties. Then Ln(f)(x) =f(z),x\u2208V, wherez\u2208adj(V) is such that\nf(z) = max\ny\u2208adj(V)(f)(y). Since ( i+1,j) is a neighbor to ( i,j), see (24), we have\neither (i+1,j)\u2208Vor (i+1,j)\u2208adj(V).\nCase 1.1 (i+1,j)\u2208V. ThenLn(f)(i,j)\u2212Ln(f)(i+1,j) =f(z)\u2212f(z) = 0\nand (36) trivially holds.\nCase 1.2 (i+ 1,j)\u2208adj(V). Then ( i+ 1,j) cannot be element of a local\nmaximum set of size smaller or equal to n. Therefore, Ln(f)(i+ 1,j) =\nf(i+1,j)\u2264f(z) =Ln(f)(i,j), which implies (36).\n14Case 2.Ln(f)(i,j) =f(i,j). IfLn(f)(i+1,j) =f(i+1,j) the equality (36)\ntrivially holds. If Ln(f)(i+1,j)< f(i+1,j), then weobtain (36) by repeating\nthe argument in Case 1.2 where the points ( i,j) and (i+1,j) change places.\nSimilarly to (36) we prove that\n|f(i,j)\u2212f(i,j+1)|=|Ln(f)(i,j)\u2212Ln(f)(i,j+1)|\n+|(id\u2212Ln)(f)(i,j)\u2212(id\u2212Ln)(f)(i,j+1)|\nThen by De\ufb01nition 18 we have\nTV(f) =TV(Ln(f))+TV((id\u2212Ln)(f)).\nThe total variation preserving property of Unis proved in a similar way.\nIn order to complete the proof we show that the composition A\u25e6Bof\nany two total variation preserving operators AandBonBV(Z2) is also total\nvariation preserving. Using the total variation preservin g property of Aand\nBand (35) we have\nTV(f) =TV(B(f))+TV((id\u2212B)(f))\n=TV(A(B(f)))+TV((id\u2212A)(B(f)))+TV((id\u2212B)(f))\n\u2265TV((A\u25e6B)(f))+TV(((id\u2212A)\u25e6B+id\u2212B)(f))\n=TV((A\u25e6B)(f))+TV((id\u2212A\u25e6B)(f)).\nFrom (35) we also obtain TV(f)\u2264TV((A\u25e6B)(f)) +TV((id\u2212A\u25e6B)(f)).\nTherefore TV(f) =TV((A\u25e6B)(f))+TV((id\u2212A\u25e6B)(f)).\nLet function f\u2208 A(Z2) have \ufb01nite support, e.g. as in the case of images.\nThenf\u2208BV(Z2). Using Theorem 19 the discrete pulse decomposition (33)\nis total variation preserving in the sense that\nTV(f) =m/summationdisplay\nk=1\u03b3(k)/summationdisplay\ns=1TV(\u03c6ks). (37)\nWe should remark that representing a function as a sum of puls es can be done\nin many di\ufb00erent ways. However, in general, such decompositi ons increase\nthe total variation, that is, we might have strict inequalit y in (37) instead of\nequality. The equality in (37) means that no additional tota l variation, or\nnoise, is created via the decomposition.\n6 Partial reconstructionsand noiseremoval\nPossibly the simplest application of the discrete pulse dec omposition (33)\nis via partial reconstructions of images. This can be used fo r example in\nremoving noise or extracting features of interest. Random n oise has very\ndistinctive discrete pulse decomposition characterized b y fast decrease of the\nnumber of pulses with the increase of the pulse size. The numb er of pulses\nin decomposition (33) versus their size for a 300 \u00d7400 image of random noise\n(the luminosity at each pixel is an independent uniformly di stributed random\nvariable) is plotted on Figure 3. It is apparent that random n oise seldom\ngenerates pulses of large size. In fact, 90% of the pulses rep resented on Figure\n150 5 10 15 20 25 3000.511.522.533.544.5x 104\nPulse SizeNumber of Pulses\n0.5 11.5 22.5 33.5\nx 10405101520253035\nPulse Size (100 \u2212 39000)Number of Pulses\nFigure 3: Pulse distribution of random noise\nFigure 4: A sea image\n3 are of size less than or equal to 20 and only about 2% have size greater than\n100. Hence by removing the pulse of small support we remove la rge portion of\nany impulsive noise. Figure 5 gives in the same format the pul se distribution\nof the image on Figure 4. A large portion of the pulses has smal l support\nbut, unlike Figure 3, we have also signi\ufb01cant number of pulse s with relatively\nlarger support. Partial reconstruction of the image by usin g pulses of selected\nsizes is given on Figure 6. We can consider (a) as removing of i mpulsive noise,\n(b) as extraction of small features and (c) as extraction of l arge features.\nAcknowledgements\nThe authors would like to thank Carl Rohwer for his suggestio ns and useful\ndiscussions."}
{"category": "abstract", "text": "The LULU operators for sequences are extended to multi-dime nsional ar-\nrays via the morphological concept of connection in a way whi ch preserves\ntheir essential properties, e.g. they are separators and fo rm a four element\nfully ordered semi-group. The power of the operators is demo nstrated by\nderiving a total variation preserving discrete pulse decom position of images.\nKeywords"}
